<html>
<head>
<title>ALGLIB++ User Reference Manual</title>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<style type="text/css">
h1		{ font-family: Arial, Helvetica, sans-serif; color: #000000; font-size: 36pt; margin-top: 0; margin-bottom: 0; }
h2		{ font-family: Arial, Helvetica, sans-serif; color: #003300; font-size: 20pt; margin-bottom: 0; }
h3		{ font-family: Arial, Helvetica, sans-serif; color: #006600; font-size: 15pt; margin-bottom: 0; }
h4		{ font-family: Arial, Helvetica, sans-serif; color: #666600; font-size: 14pt; margin-bottom: 0; }
h5		{ font-family: Arial, Helvetica, sans-serif; color: #660000; font-size: 14pt; margin-top: 10pt; margin-bottom: 0; }
h6		{ font-family: Arial, Helvetica, sans-serif; color: #000066; font-size: 13pt; margin-top: 10pt; margin-bottom: 0; }
sheader		{ }
.inlineheader	{ background-color: #e8e8e8; padding: 0.1em; font-weight: bold; }
.pagecontent	{ font-family: Verdana, Arial, sans-serif; font-size: 10pt; text-align: justify; }
.pageheader	{ width: 50em; }
.source		{ font-family: "Courier New"; font-size: 1.0em; margin-top: 0; margin-bottom: 0; }
.narration	{ font-family: "Courier New"; font-size: 1.0em; margin-top: 0; margin-bottom: 0; color: navy; }
.declaration	{ font-family: "Courier New"; font-size: 1.0em; margin-top: 0; margin-bottom: 0; color: blue; }
code		{ font-family: "Courier New"; font-size: 1.0em; }
.p_example	{ margin-left: 4em; font-family: "Courier New"; font-size: 1.0em; }
.p_note		{ margin-left: 10%; margin-right: 10%; font-size: 80%; }
.p_att		{ margin-left: 10%; margin-right: 10%; color: red; font-weight: bold; }
.p_code_head	{ margin-left: 5%; margin-top: 0; margin-bottom: 0; padding: 5px; width: 90%; font-family: "Courier New", monospace; background-color: #aaaaaa; font-weight: bold; color: white; }
.p_code_body	{ margin-left: 5%; margin-top: 0; margin-bottom: 0; padding: 5px; width: 90%; font-family: "Courier New", monospace; background-color: #f0f0f0; }
.p_code		{ margin-left: 5%; margin-top: 0; margin-bottom: 0; padding: 5px; width: 90%; font-family: "Courier New", monospace; background-color: #f0f0f0; }
.s_code		{ font-family: "Courier New"; background-color: #f0f0f0; }
.s_str		{ font-family: "Courier New"; color: blue; font-weight: bold; }
.s_comment	{ color: navy; font-style: italic; }
.s_preprocessor	{ color: green; }
a		{ color: #000077; text-decoration: underline; }
a:link		{ color: #0000ff; text-decoration: underline; }
a:visited	{ color: #000077; text-decoration: underline; }
a:active	{ color: #6666ff; text-decoration: underline; }
a:hover		{ color: #ff6666; text-decoration: underline; }
a.toc		{ color: #000077; text-decoration: none; }
a.toc:link	{ color: #0000ff; text-decoration: none; }
a.toc:visited	{ color: #000077; text-decoration: none; }
a.toc:active	{ color: #6666ff; text-decoration: none; }
a.toc:hover	{ color: #ff6666; text-decoration: none; }
a.nav		{ color: #000077; font-weight: bold; text-decoration: none; }
a.nav:link	{ color: #0000ff; font-weight: bold; text-decoration: none; }
a.nav:visited	{ color: #000077; font-weight: bold; text-decoration: none; }
a.nav:active	{ color: #6666ff; font-weight: bold; text-decoration: none; }
a.nav:hover	{ color: #ff6666; font-weight: bold; text-decoration: none; }
.cond		{ color: blue; }
.const		{ color: #222222; }
.func		{ color: #111111; }
</style>
</head>
<body>
<div class=pagecontent>
<h1>ALGLIB++ Reference Manual</h1>
Based on the <a href="https://www.alglib.net/translator/man/manual.cpp.html">ALGLIB Reference Manual (C++ version)</a>.<br/>
<font size=2>[
<a href=#int_main>Introduction</a> |
<a href=#gs_structure>ALGLIB++ Structure</a> |
<a href=#gs_compatibility>Compatibility</a> |
<a href=#gs_compiling>Compiling ALGLIB++</a> |
<a href=#gs_using>Using ALGLIB++</a> |
<a href=#gs_future>Future Enhancements in ALGLIB++</a> |
<a href=#gs_advanced>Advanced Topics</a> |
<a href=#alglib_main>ALGLIB++ Packages and Subpackages</a> |
<a href="Notes.htm">ALGLIB++ Supplementary Notes</a>
]</font>
</p>
<p>
<a name=int_main class=sheader></a><h2>1. Introduction</h2>
<font size=2>[
<a href=#int_whatisalglib>ALGLIB &rArr; ALGLIB++</a> |
<a href=#int_license>Versions and Licenses</a> |
<a href=#int_doc_license>Documentation License</a> |
<a href=#gs_guide>Reference Manual and User Guide</a> |
<a href=#int_ack>Acknowledgements</a> |
<a href=#int_refs>References &amp; Related Links</a>
]</font>
</p>
<p>
<a name=int_whatisalglib class=sheader></a><h3>1.1. ALGLIB &rArr; ALGLIB++</h3>
ALGLIB++ is a derivative of ALGLIB that is being used as an intermediate form in a longer-term reengineering/refactoring
process, in which it is being recoded into native C++ (similar also to where it was before Version 3), the extra layer and duplication being removed in preparation of providing more direct support for multi-threading, as well as additional tests and modules and eventually a scripting language front end.
The original source, ALGLIB, is a cross-platform numerical analysis and data mining library, that supports several programming languages as well as several operating systems (Windows, *nix family).
A demo module, as described in the supplement <a href="Notes.htm#supp_CppDemoModule">A1. C++ Demo Module</a>, based on the <a href=#unit_idw>IDW</a> and <a href=#unit_knn>KNN</a> subpackages, has been provided illustrating a possible direction forward.
</p>
<p>
The features include:
<ul>
<li>Data analysis (classification/regression, including neural networks)</li>
<li>Optimization (both linear and convex non-linear, and even non-convex) and nonlinear solvers</li>
<li>Interpolation and linear/nonlinear least-squares fitting</li>
<li>Linear algebra (direct algorithms, EVD/SVD), direct and iterative linear solvers</li>
<li>Fast, high-precision implementations of special functions using rational interpolation</li>
<li>Fast Fourier Transform and many other algorithms (numerical integration, ODEs, statistics)</li>
</ul>
</p>
<p>
The different language versions of ALGLIB were generated from a common core, with the C++ version providing limited (but unofficial) support for the C90 dialect of C.
This feature made it necessary to simulate, within C, features that would otherwise be native to C++, and then to provide a C++ wrapper on top of this.
Correspondingly, there were two separate namesspaces: <code>alglib_impl</code>, which contains the C version, and <code>alglib</code>, which contains the C++ wrappers.
ALGLIB++ has retained most of this structure for the time being and initially as much of the original coding as possible, but has reduced or eliminated much of the global infrastructure as a first step for its elimination and replacement by multi-threaded native C++ code.
As such, it represents an intermediate form bridging between ALGLIB, itself, and the future library that ALGLIB++ is being transformed into.
</p>
<p>
The recoded version will be expanded into a larger library that will include more features and applications dedicated to Machine Learning, advanced Digital Signal Processing, Graphics and Sound Processing and Natural Language Processing.
The larger library, in turn, is being embedded within a custom-designed operating system specifically geared for more advanced Artificial Intelligence applications in such areas as program synthesis, theory induction, theorem synthesis, higher order logic theorem proving, proof validation, music and story composition, personality design &amp; synthesis, &quot;artificial reality&quot; (or &quot;deep fakes&quot;), advanced computer vision, robotics, choreography, music &amp; DJ automation, &quot;second brain&quot; enteric physiology (the &quot;emotion chip&quot;), etc.
</p>
<p>
The reductions from ALGLIB carred out in ALGLIB++ included removing much of the global infrastructure (the "God-objects"), to clear the way for this later conversion.
Apart from these changes, however, the original version is left mostly intact, other than minor changes in layout to the comments, with most of the recoding only to eliminate the global objects, to simplify parts of the C++ wrapper interface,
and to resolve or correct some of the problems that affect ALGLIB that are partly masked by or compensated for by the extra infrastructure in ALGLIB.
In particular, the extra coding originally in ALGLIB needed to resolve (compiler-related) problems with floating point comparsions has been removed, and the limited multi-threading support provided by ALGLIB, by way of its global <code>ae_state</code> "god-object" has also been removed, in favor of future inclusion of explicit "thread local" declarations.
This is in keeping with the more recent revisions of ISO C/C++, as well as other languages, like CPython and C#, but may not be as widely used as, say, C90 (or C99).
Although no support for thread local declarations is included in ALGLIB++, a few hooks are provided for those with C/C++-11 compilers who may wish to experiment with adding in such support.
In addition, other elements of the global infrastructure have been wrapped up into macros, and some of the underlying operations have been simplified in keeping with the direction that ALGLIB, itself, appears to be headed in (particularly: simplifications in the C++ "object"-type constructors and destructors).
</p>
<p>
Further information on the revision history has been provided in the supplement <a href="Notes.htm#supp_History">A6. ALGLIB &rArr; ALGLIB++ Revision Sequence</a>
</p>
<p>
<b><font color=red>A warning in advance:</font></b> full support for multi-threading (particularly: thread-local variables) requires C++11 and it is increasingly likely that ALGLIB++ will be going in that direction.
There are many iterative routines in ALGLIB++ that are meant for use in continual operation over long periods of time.
The most natural way to handle them is as worker threads that send request messages for function calls and status messages for reports.
An older, less natural, way is to use function pointers; but a few iteration routines (such as <code>minlmiteration()</code>) may be set up in any of several mutually incompatible configurations.
Originally, in ALGLIB, a decision was made early on to avoid using function pointers and the library predates the inclusion of multi-threading in C, C++, C#, CPython,
so a manually-threaded interface was used for each iteration routine.
This design was inherited by ALGLIB++ and has not yet been modified, but will be.
Over the short term, the prototypes of the iteration routines (such as <code>minlmiteration</code>), &mdash; which are in both the <code>alglib_impl</code> and <code>alglib</code> namespaces but are not documented here (and were not documented by ALGLIB either), will undergo changes that will render them incompatible with ALGLIB.
</p>
<p>
<a name=int_license class=sheader></a><h3>1.2. Versions and Licenses</h3>
The original form of ALGLIB, provided by ALGLIB Project (the company behind ALGLIB), came in the following forms with the following features:
<ul>
<li>
<b>ALGLIB Free Edition</b><br/>
With full functionality but limited performance and license:<br/>
mostly targeted for serial applications with only little (but unofficial) multi-threading support and no low-level optimizations (generic C or C# code).
It is distributed under a more restrictive non-commercial license that is not well-suited for commercial applications:
<ul>
<li>
<b>ALGLIB for C++</b> and <b>ALGLIB for C#</b><br/>
are distributed under the <a href='http://www.fsf.org/licensing/licenses'>GPL 2+, GPL license version 2</a>
or at your option any later version.
</li>
<li>
<b>ALGLIB for Delphi</b> and <b>ALGLIB for CPython</b><br/>
are distributed under <a href='http://www.alglib.net/download.php'>ALGLIB Personal and Academic Use License Agreement</a>.
</li>
</ul>
</li>
<li>
<b>ALGLIB Commercial Edition</b><br/>
The high-performance version of ALGLIB with business-friendly license:<br/>
with extensive low-level optimizations, with both multi-threading and multi-core support.
It is distributed under commercial-friendly license that can be found here: <a href='http://www.alglib.net/commercial.php'>http://www.alglib.net/commercial.php</a>.
</li>
</ul>
</p>
<p>
ALGLIB++ is being provided as a benefit to the original developers and to those who may wish to do further experimentation on the coding.
It is derived from the <b>ALGLIB for C++</b> version of <b>ALGLIB Free Edition</b> and inherits its more restrictive license.
An exception to this, however, is made to allow for changes made here to be included into future versions of ALGLIB.
<p>
Therefore, ALGLIB++ is also distributed under the <a href='http://www.fsf.org/licensing/licenses'>GPL 2+, GPL license version 2</a> or at your option any later version,
with special exception made to ALGLIB Project, who may integrate this into their future versions of the Commercial Edition of ALGLIB under the same license listed for it above.
</p>
<p>

<a name=int_doc_license class=sheader></a><h3>1.3. Documentation License</h3>
This reference manual is derived from the <a href="https://www.alglib.net/translator/man/manual.cpp.html">ALGLIB Reference Manual (C++ version)</a>. It inherits the same license, which is reproduced below with updates to include ALGLIB++:
<div style='width: 80%;'>
<u>This reference manual is licensed under BSD-like documentation license</u>:
</p>
<p>
Copyright 1994-2017 by Sergey Bochkanov, ALGLIB Project. All rights reserved.
Copyright 2019-2020 Lydia Marie Williamson, Mark Hopkins Consulting. ALGLIB++. Project Lydia. All rights reserved.
</p>
<p>
Redistribution and use of this document (ALGLIB++ Reference Manual) with or without modification,
are permitted provided that such redistributions will retain the above copyright notice,
this condition and the following disclaimer as the first (or last) lines of this file.
</p>
<p>
THIS DOCUMENTATION IS PROVIDED BY THE ALGLIB AND ALGLIB++ PROJECTS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,
INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE ALGLIB AND ALGLIB++ PROJECTS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS DOCUMENTATION, EVEN IF ADVISED
OF THE POSSIBILITY OF SUCH DAMAGE.
</div>
</p>
<p>
<a name=gs_guide class=sheader></a><h3>1.4. Reference Manual and User Guide</h3>
Here, in the ALGLIB++ Reference Manual, a full description of all publicly accessible ALGLIB++ units is provided, with examples.
The Reference Manual focuses on documenting:
<ul>
<li><b>Packages</b>: A collated set of related code modules covering a broad application domain</li>
<li><b>Subpackages</b>: A component code module that is specialized to an application within that domain</li>
<li><b>Classes</b>: The C++ classes and C structures defined in the subpackages</li>
<li><b>Functions</b>: The functions/subroutines used in the subpackages</li>
</ul>
Free software needs free documentation - that's why ALGLIB++ Reference Manual is licensed under BSD-like documentation license.
This documentation is inherited from the ALGLIB Reference Manual.
</p>
ALGLIB Project also provides the <a href="http://www.alglib.net/#book">ALGLIB User Guide</a>,
which is distributed under less-permissive license for personal use only.
Further details can be found in <a href="https://www.alglib.net/translator/man/manual.cpp.html#gs_guide">the corresponding section here</a> of the ALGLIB Reference Manual. Because the recoding has been kept minimal, so far, most the User Guide should apply to ALGLIB++, as well.
</p>
</p>
<p>
<a name=int_ack class=sheader></a><h3>1.5. Acknowledgements</h3>
In addition to the <a href="https://www.alglib.net/translator/man/manual.cpp.html#int_ack">Acknowledgements</a> originally listed under the ALGLIB Reference Manual,
we add an acknowledgement to the primary developer of ALGLIB, Sergey Bochkanov,
for carrying out the research behind the development of the routines, their implementation, the development of the all-important regression tests
and for maintaining an interactive forum for further discussions.
</p>
<p>
Two of the acknowledgements that were originally made are in relation to the distributors of libraries that preparations are being made to integrate with ALGLIB++ into the library that is being built from it:
a C++ translation of <a href="http://www.netlib.org/lapack/">LAPACK</a>, and a recoded version of <a href="http://gmplib.org/">GNU MP</a>
that will be more finely-honed to the needs of ALGLIB++, and which may also integrate some of the high-powered routines defined in ALGLIB++.
</p>
<p>
The revised layout used in this reference manual is patterned, in part, on that used in <a href="https://pubs.opengroup.org/onlinepubs/9699919799/">"The Open Group Base Specifications Issue 7"</a> version of the POSIX standard.
</p>
<p>
Finally: a personal acknowledgment to Mark, for helping me gain fluency in programming, giving me a few tricks of the trade, and providing some tools for the automation of the task, itself.
</p>
<p>
<a name=int_refs class=sheader></a><h3>1.6. References &amp; Related Links</h3>
<ul><li>
<b>ALGLIB Wikipedia Page:</b> (<a href="https://en.wikipedia.org/wiki/ALGLIB">https://en.wikipedia.org/wiki/ALGLIB</a>)<br/>
</li><li>
<b>ALGLIB Alternatives:</b> (<a href="https://dotnet.libhunt.com/alglib-alternatives">https://dotnet.libhunt.com/alglib-alternatives</a>)<br/>
A listing of and comparison with similar packages - some of whose functionalities and features/advantages may be incorporated into ALGLIB++.
</li><li>
<b>DotNumerics, AlgLib, dnAnalytics, Math.net, F# for Numerics, Mtxvec?</b> (<a href="https://visbud.blogspot.com/2019/05/dotnumerics-alglib-dnanalytics-mathnet.html">https://visbud.blogspot.com/2019/05/dotnumerics-alglib-dnanalytics-mathnet.html</a>)<br/>
Discussion of ALGLIB and other packages similar to it.
</li><li>
<b>MQL5</b>: (<a href="https://www.mql5.com/en/docs">https://www.mql5.com/en/docs</a>)<br/>
"MetaQuotes Language 5 (MQL5) is a high-level language designed for developing technical indicators, trading robots and utility applications, which automate financial trading. MQL5 has been developed by MetaQuotes Software Corp. for their trading platform. The language syntax is very close to C++ enabling programmers to develop applications in the object-oriented programming (OOP) style."
</li><li>
<b>Intel MKL</b>: (<a href="https://software.intel.com/en-us/mkl">https://software.intel.com/en-us/mkl</a>)<br/>
Linear Algebra, Fast Fourier Transforms (FFT), Vector Statistics &amp; Data Fitting, Vector Math &amp; Miscellaneous Solvers
"The Fastest and Most-Used Math Library for IntelÂ®-Based Systems"
</li></ul>
<p>
<a name=gs_structure class=sheader></a><h2>2. ALGLIB++ Structure</h2>
<font size=2>[
<a href=#gs_packages>Packages</a> |
<a href=#gs_subpackages>Subpackages</a> |
<a href=#gs_osscomm>The Commercial Version of ALGLIB</a>
]</font>
</p>
<a name=gs_packages class=sheader></a><h3>2.1. Packages</h3>
ALGLIB++ is a C++ interface to a computational core that was originally written mostly in C.
Both C library and C++ wrapper are derived from code in ALGLIB that was automatically generated by code generation tools developed within the ALGLIB project.
Before version 3.0, the library was provided as a large number of units that have since been merged into the following <i>package</i> units:
<ul>
<li><b>AlgLibMisc.cpp</b> &mdash; contains different algorithms which are hard to classify</li>
<li><b>DataAnalysis.cpp</b> &mdash; contains data mining algorithms</li>
<li><b>DiffEquations.cpp</b> &mdash; contains differential equation solvers</li>
<li><b>FastTransforms.cpp</b> &mdash; contains FFT and other related algorithms</li>
<li><b>Integration.cpp</b> &mdash; contains numerical integration algorithms</li>
<li><b>Interpolation.cpp</b> &mdash; contains interpolation algorithms</li>
<li><b>LinAlg.cpp</b> &mdash; contains linear algebra algorithms</li>
<li><b>Optimization.cpp</b> &mdash; contains optimization algorithms</li>
<li><b>Solvers.cpp</b> &mdash; contains linear and nonlinear solvers</li>
<li><b>SpecialFunctions.cpp</b> &mdash; contains special functions</li>
<li><b>Statistics.cpp</b> &mdash; statistics</li>
</ul>
and the following support units:
<ul>
<li><b>AlgLibInternal.cpp</b> &mdash; contains internal functions which are used by other packages, but not exposed to the external world as part of the API</li>
<li><b>Ap.cpp</b> &mdash; contains publicly accessible vector/matrix classes, most important and general functions and other "basic" functionality; also outside the API, with implementation details that are subject to change.</li>
</ul>
In addition, the following testing modules are provided, which also serve as demonstration of the code in use:
<ul>
<li><b>TestC.cpp</b> &mdash; &quot;Test Core&quot; test made directly on the ALGLIB++ C core,</li>
<li><b>TestI.cpp</b> &mdash; &quot;Test Interface&quot; tests for the ALGLIB++ C++ API,</li>
<li><b>TestX.cpp</b> &mdash; sanity checks and speed tests for the ALGLIB++ C++ API,</li>
<li><b>TestY.cpp</b>, <b>TestZ.cpp</b> &mdash; other tests that are supplementary to TestX.</li>
</ul>
</p>
<p>
The packages have been organized to reduce the number of cross-package dependencies.
Currently, these are the essential dependencies:
<ul>
<li>{DataAnalysis,Interpolation} &rarr; Optimization &rarr; Solvers &rarr; LinAlg &rarr; AlgLibMisc &rarr; AlgLibInternal &rarr; Ap</li>
<li>{DiffEquations,FastTransforms} &rarr; AlgLibInternal</li>
<li>SpecialFunctions &rarr; AlgLibMisc</li>
<li>Interpolation &rarr; Integration &rarr; {LinAlg,SpecialFunctions}</li>
<li>DataAnalysis &rarr; Statistics &rarr; {LinAlg,SpecialFunctions}</li>
</ul>
In addition are the following dependencies, which are subsumed within them:
<ul>
<li>(All packages) &rarr; Ap</li>
<li>{DataAnalysis,Interpolation,Statistics,Optimization,Solvers,LinAlg,SpecialFunctions} &rarr; AlgLibInternal</li>
<li>{DataAnalysis,Interpolation,Statistics} &rarr; AlgLibMisc</li>
<li>{DataAnalysis,Interpolation,Optimization} &rarr; LinAlg</li>
<li>{DataAnalysis,Interpolation} &rarr; Solvers</li>
<li>DataAnalysis &rarr; SpecialFunctions</li>
</ul>
Finally, for the test modules are the dependencies
<ul>
<li>{TestC,TestI,TestX,TestZ} &rarr; {DataAnalysis,Interpolation,DiffEquations,FastTransforms} (and all other modules)</li>
<li>TestY &rarr; LinAlg (and Ap)</li>
</ul>
</p>
<p>
<a name=gs_subpackages class=sheader></a><h3>2.2. Subpackages</h3>
Within each package are contained 1, 2 or more <i>subpackages</i>: the original pre-3.0 version units of ALGLIB.
For example, <code>LinAlg.cpp</code> was compiled in 3.0 from 14 *.cpp files (for the C++ interface) and 14 *.c files (for the computational core).
These files provide different functionality: one of them calculates triangular factorizations, another generates random matrices, and so on.
</p>
<p>
The Reference Manual has been organized to reflect this structure.
Under <a href=#alglib_main>ALGLIB++ Packages and Subpackages</a>, is a list of packages, and under each package is a list of the subpackages it contains.
For example, <code>LinAlg.cpp</code> includes <code>trfac</code>, <code>svd</code>, <code>evd</code> and other subpackages.
The subpackages are no longer separated into their own files, namespaces or other entities.
Thus, for instance, to use the <code>svd</code> package would compiling with <code>LinAlg.cpp</code> and including <code>LinAlg.h</code> where svd is used;
instead of including such files as <code>svd.c</code>, <code>svd.cpp</code> or <code>svd.h</code>
However, each subpackage has been collated in one place within the package it is contained in: the code segment in the *.cpp file and the declaration segment in the matching *.h file
</p>
<p>
A detailed list of subpackage dependencies has been provided in the supplement <a href="Notes.htm#supp_Links">A4. ALGLIB++ Package Dependencies</a>.
</p>
<p>
<a name=gs_osscomm class=sheader></a><h3>2.3. The Commercial Version of ALGLIB</h3>
Some routines in ALGLIB++ have alternate versions in the commercial edition of ALGLIB that are specfically geared toward multi-threaded form and (for Intel systems) multi-core or SIMD-optimized form.
More details may be found under the <a href="https://www.alglib.net/translator/man/manual.cpp.html#gs_osscomm">corresponding section of the ALGLIB Reference Manual</a>.
Also with the commercial version was extra commentary in the ALGLIB Reference Manual of the form
<pre class=narration>
  ! COMMERCIAL EDITION OF ALGLIB:
  ! ...
  ! (Further commentary describing the enhancements provided for this function)
  ! ...
  ! We recommend you to read 'Working with the Commercial Version' section  of
  ! ALGLIB Reference Manual in order to find out how to  use  performance-
  ! related features provided by commercial edition of ALGLIB.
</pre>
that may be found there, but which has been excluded from the ALGLIB++ Reference Manual.
Detailed description of commercial version can be found <a href="https://www.alglib.net/translator/man/manual.cpp.html#gs_comm">here</a> in the ALGLIB Reference Manual.
A list of routines with commercial support in ALGLIB, broken down by subpackage, has been provided in the supplement
<a href="Notes.htm#supp_Commercial">A5. ALGLIB Commercial Version: Notes</a>.
</p>
<p>
<a name=gs_compatibility class=sheader></a><h2>3. Compatibility</h2>
<font size=2>[
<a href=#gs_compatibility_cpu>CPU</a> |
<a href=#gs_compatibility_os>OS</a> |
<a href=#gs_compatibility_compiler>Compiler</a> |
<a href=#gs_compatibility_opt>Optimization Settings</a>
]</font>
</p>
<p>
<a name=gs_compatibility_cpu class=sheader></a><h3>3.1. CPU</h3>
ALGLIB++ has been tested on an x86 CPU, but should inherit the compatibility possessed by ALGLIB with x86_64, ARM and SPARC and other CPU's which:
<ul>
<li>support double precision arithmetic</li>
<li>compiles with IEEE 754 floating point standard (especially in its handling of IEEE special values)</li>
<li>either big-endian or little-endian (but not mixed-endian) byte order</li>
</ul>
This includes most contemporary mainstream CPU's.
This will require testing and verification.
</p>
<p class=p_note>
On Intel architectures, ALGLIB was designed to work with both FPU-based and SIMD-based implementations of floating point math.
The Intel FPU uses an 80-bit internal representation which is not strictly compatible with the IEEE-standard representations used in C and C++.
Not all compilers handle this consistently, and the discrepancy can lead to domain errors for floating-point comparisons.
This was compensated for in ALGLIB by replacing all of the operations {&lt;,&gt;,&le;,&ge;,==,!=} by functions.
</p>
<p class=p_note>
However, the FPU already provides the means to convert the 80-bit internal representation to IEEE-standard form,
which some compilers such as GCC fail to make proper use of.
To compensate for this, the CPU-identification function of ALGLIB has been remade into a CPU-identifying startup function in ALGLIB++.
A similar solution may need to be adopted for other compilers that target Intel, such as VCC; but none has been implemented or tested here.
</p>
<p>
<a name=gs_compatibility_os class=sheader></a><h3>3.2. OS</h3>
ALGLIB++, like the open source version of ALGLIB, is always OS-agnostic, even in the presence of OS-specific definitions; i.e. no OS-specific preprocessor definitions,
It will work under any POSIX-compatible OS and under Windows and is compatible with any OS which supports the C++98 standard library.
In the case of Windows, however, note the above proviso on floating-point comparison operations.
</p>
<p>
<a name=gs_compatibility_compiler class=sheader></a><h3>3.3. Compiler</h3>
ALGLIB++ is compatible with any C++ compiler which:
<ul>
<li>supports 32-bit and 64-bit signed integer datatypes</li>
<li>emits code which handles <i>comparisons</i> with IEEE special values without raising exceptions.
The operation <code>x/0</code> need not return <code>INF</code>,
but the compiler should allow one to <i>compare</i> double precision values with infinity or NAN without raising exceptions.</li>
</ul>
</p>
<p>
All contemporary compilers that are in widespread use satisfy these requirements.
However, some <i>very</i> old compilers (older versions of Borland C++ Builder, for example) may emit code which does not correctly work with IEEE special values.
If you use one of these old compilers, we recommend that you run the test suite to ensure that ALGLIB++ library works.
</p>
<p>
<a name=gs_compatibility_opt class=sheader></a><h3>3.4. Optimization Settings</h3>
ALGLIB++ should inherit from ALGLIB compatibility with any kind of optimizing compiler in which:
<ul>
<li>the <b>volatile</b> modifier is correctly handled (i.e. compiler does <b>not</b> optimize volatile reads/writes)</li>
<li>optimized code correctly handles IEEE special values</li>
</ul>
</p>
<p>
Generally, all kinds of optimization that were marked by compiler vendor as "safe" are possible.
For example, ALGLIB can be compiled:
<ul>
<li>under MSVC: with /O1, /O2, /Og, /Os, /Ox, /Ot, /Oy, /fp:precise, /fp:except, /fp:strict</li>
<li>under GCC: with -O1, -O2, -O3, -Os</li>
</ul>
This has been only verified for ALGLIB++ with GCC.
</p>
<p>
On the other hand, the following "unsafe" optimizations will break ALGLIB:
<ul>
<li>under MSVC: /fp:fast</li>
<li>under GCC: -Ofast, -ffast-math</li>
</ul>
For MSVC, it is not known if any of these will break ALGLIB++.
ALGLIB++ has been successfully compiled and verified with GCC using -ffast-math, but only in a limited setting where its use appears to have no significant effect.
In particular, its use in combination with the -O{1,2,3,s} optimizer options is not supported by GCC.
For example, -ffast-math -O3 will produce inconsistent code.
</p>
<p>
<a name=gs_compiling class=sheader></a><h2>4. Compiling ALGLIB++</h2>
<font size=2>[
<a href=#gs_attaching>Adding to Your Project</a> |
<a href=#gs_configuring>Configuring for Your Compiler</a> |
<a href=#gs_configuring2>Improving Performance (CPU/OS-Specific Optimizations)</a> |
<a href=#gs_compile_examples>Examples</a>
]</font>
</p>
<p>
<a name=gs_attaching class=sheader></a><h3>4.1. Adding to Your Project</h3>
ALGLIB++, like ALGLIB, is set up so that it can be directly included into your projects.
There are no configuation files or Makefile for producing a library.
Instead, its files are organized into a small number of "packages", each dedicated to a separate domain, and each having its own source file and header.
</p>
<p>
All you need to do is add to your project whatever packages you need.
Under the most commonly-used compilers (GCC, MSVC) this will work with little or no additional settings.
In other cases you will need to define several preprocessor definitions, which will be discussed further below.
</p>
<p>
"Adding to your project" means:
<ul>
<li><i>compile</i> the .cpp files of whatever packages you need with the rest of your project, and</li>
<li><i>include</i> the corresponding .h file(s) in whatever source files you use the respective package(s) in.</li>
</ul>
For Intel/Linux/GCC, a Makefile has been provided that will compile the ALGLIB++ testing routines (described under <a href=#gs_testing>Testing ALGLIB++</a>):
<ul>
<li>as an example illustrating how ALGLIB++ may be included in applications, with the application here being that of testing ALGLIB++,</li>
<li>as a template to experiment with customizing for your configuration of processor, operating system and compiler,</li>
<li>as a way to test and verify your configuration; the makefile is, in effect, an implementation of "make test" or "make check".</li>
</ul>
</p>
<p>
ALGLIB, itself, was originally designed with no project files or makefiles, because it was determined by its developers that:
<ul>
<li>users generally prefer to integrate the source code directly into their projects rather than using a separate static library, which normally would only take a few minutes,</li>
<li>script-based build systems were tried before, but were not widely-used by ALGLIB users,</li>
<li>different programming environments (such a Visual Studio or GCC) use significantly different methods for building, and mandating any one or several of them with explicit project files or makefiles would introduce a dependency on programming environment and limits the portability of the system.</li>
</ul>
Anyone who has ever tried to download a Linux library or run an automated installer on Windows will be familiar with the problems that can occur in delegating the build process to an opaque or black box.
The inclusion of a Makefile for testing ALGLIB++ was a compromise to strike a more even balance between these approaches.
The portability issue is best resolved by adhering to the POSIX standard.
</p>
<p>
<a name=gs_configuring class=sheader></a><h3>4.2. Configuring for Your Compiler</h3>
Several compiler-dependent types have been defined in Ap.h that may require configuring if you are not using any of the compilers that ALGLIB++ recognizes.
For most current versions MSVC or GCC, nothing needs to be done, but older versons of these compilers or other compilers may require fine-tuning the definitions of these types. They are as follows:
<ul>
<li><b>alglib_impl::ae_int32_t</b> &mdash; 32-bit signed integer</li>
<li><b>alglib_impl::ae_int64_t</b> &mdash; 64-bit signed integer</li>
<li><b>alglib_impl::ae_uint64_t</b> &mdash; 64-bit unsigned integer</li>
<li><b>alglib_impl::ae_int_t</b> &mdash; pointer-width signed integer</li>
</ul>
For GCC and Sun Studio they are defined respectively as <code>int</code> (which current compilers set to 32 bits), <code>signed long long</code>, <code>unsigned long long</code> and <code>ptrdiff_t</code>.
For MSVC the same definitions are used, except that ae_int64_t and ae_uint64_t are defined respectively as <code>_int64</code> and <code>unsigned _int64</code>.
<p>
In case of incompatibilities you may:
<ul>
<li>define the <code>AE_HAVE_STDINT</code> conditional symbol, if your compiler provides <code>stdint.h</code>,</li>
<li>or define <code>AE_INT32_T</code>, <code>AE_INT64_T</code>, <code>AE_UINT64_T</code> and/or <code>AE_INT_T</code>
to suitable datatype names, for whichever of the 4 respective types you need to fine-tune.</li>
</ul>
</p>
<p>
<a name=gs_configuring2 class=sheader></a><h3>4.3. Improving Performance (CPU/OS-Specific Optimizations)</h3>
ALGLIB++ can be optimized by:
<ul>
<li>compiling with advanced optimization turned on, or</li>
<li>by defining <code>AE_CPU=AE_INTEL</code>, if you are compiling ALGLIB++ to an Intel processor.</li>
</ul>
When the <code>AE_CPU</code> macro is defined to <code>AE_INTEL</code>,
ALGLIB++ will use the <code>cpuid</code> instruction of the Intel CPU to determine whether your CPU supports SSE2-capable code.
If so, then it will use SSE2 intrinsics which are portable across different compilers and efficient enough for most practical purposes.
</p>
<p>
<a name=gs_compile_examples class=sheader></a><h3>4.4. Examples</h3>
<font size=2>[
<a href=#gs_compile_examples_set>Introduction</a> |
<a href=#gs_compile_examples_win>Compiling under Windows</a> |
<a href=#gs_compile_examples_linux>Compiling under Linux</a>
]</font>
</p>
<p>
<a name=gs_compile_examples_set class=sheader></a><h4>4.4.1. Introduction</h4>
We will illustrate several compilation scenarios here for ALGLIB++.
</p>
<p>
These examples assume that your current directory is a source file <code>Demo.cpp</code> for your project,
and that the source and header files for whichever ALGLIB++ packages you with to use are also in that directory.
The ALGLIB++ packages required are LinAlg, AlgLibMisc, AlgLibInternal and Ap and the corresponding .cpp and .h files should be present.
The source file Demo.cpp carries out matrix multiplication using ALGLIB++
and then tests the performance of the <i>GEMM</i> function, displaying the results to the console.
A similar demo is provided as one of the test files: TestY.cpp.
Further information may be found in the supplement <a href="Notes.htm#supp_DemoFile">A2. ALGLIB++ Demo</a>.
</p>
<p>
<a name=gs_compile_examples_win class=sheader></a><h4>4.4.2. Compiling under Windows</h4>
<b><font color=red>Note:</font></b> the description here for Windows implementation has been adapted from <a href="https://www.alglib.net/translator/man/manual.cpp.html#gs_compile_examples_win">the description given here for ALGLIB</a>, but has not been verified for ALGLIB++.
If you are using Windows, you will need to experiment with this.
</p>

<pre class=p_code_head>
Demo.cpp (WINDOWS EXAMPLE)
</pre>
<pre class=p_code_body>
#include &lt;windows.h&gt;
#include "LinAlg.h"

using namespace alglib;

<b>double</b> counter() {
   return 0.001*GetTickCount();
}

<b>int</b> main() {
   real_2d_array a, b, c;
   <b>int</b> n = 2000;
   <b>int</b> i, j;
   <b>double</b> timeneeded, flops;
// Initialize the arrays.
   a.setlength(n, n);
   b.setlength(n, n);
   c.setlength(n, n);
   <b>for</b> (i = 0; i &lt; n; i++) <b>for</b> (j = 0; j &lt; n; j++) {
      a[i][j] = randomreal() - 0.5;
      b[i][j] = randomreal() - 0.5;
      c[i][j] = 0.0;
   }
// Set global threading settings (applied to all ALGLIB++ functions);
// the default is to perform serial computations, unless parallel execution is activated.
// Parallel execution tries to utilize all cores; this behavior can be changed with setnworkers().
   setglobalthreading(parallel);
// Perform matrix-matrix product.
   flops = 2*pow(n, 3);
   timeneeded = counter();
   rmatrixgemm(n, n, n, 1.0, a, 0, 0, 0, b, 0, 0, 1, 0.0, c, 0, 0);
   timeneeded = counter() - timeneeded;
// Evaluate the performance.
   printf("Performance: %.1f GFLOPS\n", 1.0E-9*flops/timeneeded);
   return 0;
}
</pre>

<p>
The examples below cover Windows compilation from command line with MSVC.
It is very straightforward to adapt them to compilation from MSVC IDE - or to other compilers.
We assume that you already called <code>%VCINSTALLDIR%\bin\amd64\vcvars64.bat</code> batch file
which loads 64-bit build environment (or its 32-bit counterpart).
We used 3.2 GHz 4-core CPU for this test.
</p>
<p>
The first example covers platform-agnostic compilation without optimization settings - the simplest way to compile ALGLIB++.
We start by compiling the ALGLIB++ *.cpp files with <code>Demo.cpp</code>.
In this and the following examples we will omit compiler output for the sake of simplicity.
</p>
<p><div align=center>
<b><font size=-1><font color=red>NOTE:</font></b>
The following figures are those cited for ALGLIB <a href="https://www.alglib.net/translator/man/manual.cpp.html#gs_compile_examples_win">in the ALGLIB reference</a>.
The performance of ALGLIB++, when compiled under Windows:MSVC:x64/x86, has not yet been determined.
</font>
</div></p>
<pre class=p_code_head>
OS-agnostic mode, with no compiler optimizations
</pre>
<pre class=p_code_body style='white-space: pre-wrap;'>
<b>&gt;</b> cl /I. /EHsc /FeDemo.exe *.cpp
<b>&gt;</b> Demo.exe
Performance: 0.7 GFLOPS
</pre>
<pre class=p_code_head>
OS-agnostic mode, with <b><font color=red>/Ox</font></b> optimization
</pre>
<pre class=p_code_body style='white-space: pre-wrap;'>
<b>&gt;</b> cl /I. /EHsc /FeDemo.exe <span style='font-weight: bold; color: red;'>/Ox</span> *.cpp
<b>&gt;</b> Demo.exe
Performance: 0.9 GFLOPS
</pre>
<pre class=p_code_head>
OS-agnostic mode, with <b><font color=red>SSE2</font></b>-detection enabled: ALGLIB++ is told it is on x86/x64
</pre>
<pre class=p_code_body>
<b>&gt;</b> cl /I. /EHsc /FeDemo.exe /Ox <span style='font-weight: bold; color: red;'>/DAE_CPU=AE_INTEL</span> *.cpp
<b>&gt;</b> Demo.exe
Performance: 4.5 GFLOPS
</pre>

<p>
Well, in the first run 0.7 GFLOPS is not very impressive for a 3.2GHz CPU.
The second run is better, but still not impressed.
For the third test, we turn on optimizations for x86 architecture: define <code>AE_CPU=AE_INTEL</code>.
With this option any SSE2 supported provided by the CPU, will be exploited to make for further speedup.
</p>
<p>
In the Commercial Edition of ALGLIB, multi-core support is available for this platform, as well as the ability to link in Intel MKL.
This will provide further speedup (e.g. for a 4-core processor the original developers cited a speedup to 16.0 GFLOPS with multi-core support and a further speedup to 33.1 GFLOPS with MKL extensions added in)
</p>
<p>
<a name=gs_compile_examples_linux class=sheader></a><h4>4.4.3. Compiling under Linux</h4>
In these examples, the same source file <code>Demo.cpp</code> has been adapted by changing the OS-dependent code in counter() to work with Linux.
We'll show how performance of this program continually increases as we add more and more sophisticated compiler options.
</p>

<pre class=p_code_head>
Demo.cpp (LINUX EXAMPLE)
</pre>
<pre class=p_code_body>
#include &lt;sys/time.h&gt;
#include "LinAlg.h"

using namespace alglib;

<b>double</b> counter() {
   struct timeval now;
   alglib_impl::ae_int64_t r, v;
   gettimeofday(&amp;now, NULL);
   v = now.tv_sec;
   r = v*1000;
   v = now.tv_usec/1000;
   r += v;
   return 0.001*r;
}

<b>int</b> main() {
   real_2d_array a, b, c;
   <b>int</b> n = 2000;
   <b>int</b> i, j;
   <b>double</b> timeneeded, flops;
// Initialize the arrays.
   a.setlength(n, n);
   b.setlength(n, n);
   c.setlength(n, n);
   <b>for</b> (i = 0; i &lt; n; i++) <b>for</b> (j = 0; j &lt; n; j++) {
      a[i][j] = randomreal() - 0.5;
      b[i][j] = randomreal() - 0.5;
      c[i][j] = 0.0;
   }
// Set global threading settings (applied to all ALGLIB++ functions);
// the default is to perform serial computations, unless parallel execution is activated.
// Parallel execution tries to utilize all cores; this behavior can be changed with setnworkers().
   setglobalthreading(parallel);
// Perform matrix-matrix product.
   flops = 2*pow(n, 3);
   timeneeded = counter();
   rmatrixgemm(n, n, n, 1.0, a, 0, 0, 0, b, 0, 0, 1, 0.0, c, 0, 0);
   timeneeded = counter() - timeneeded;
// Evaluate the performance.
   printf("Performance: %.1f GFLOPS\n", 1.0E-9*flops/timeneeded);
   return 0;
}
</pre>

<p>
The examples below cover Linux compilation on Linux from the command line with GCC.
The results cited by the original developers were on a 2-core x64 Intel CPU with 2x Hyperthreading enabled.
The results cited below are on an x86 Intel Core 2 Duo processor
</p>
<p>
The first example covers platform-agnostic compilation without optimization settings - the simplest way to compile ALGLIB++.
We start by compiling the ALGLIB++ *.cpp files with <code>Demo.cpp</code>.
In this and the following examples we will omit compiler output for the sake of simplicity.
Similar speedups may be seen in the test file <code>TestX.cpp</code>; roughly 3x with -O3 optimization included and 6x with SSE2 support also included.
</p>

<pre class=p_code_head>
OS-agnostic mode, with no compiler optimizations
</pre>
<pre class=p_code_body style='white-space: pre-wrap;'>
<b>&gt;</b> g++ -I. -o Demo.out *.cpp &amp;&amp; ./Demo.out
Performance: 0.6 GFLOPS
</pre>
<pre class=p_code_head>
OS-agnostic mode, with -O3 optimization
</pre>
<pre class=p_code_body style='white-space: pre-wrap;'>
<b>&gt;</b> g++ -I. -o Demo.out <span style='font-weight: bold; color: red;'>-O3</span> *.cpp &amp;&amp; ./Demo.out
Performance: 2.0 GFLOPS
</pre>
<pre class=p_code_head>
OS-agnostic mode, with <b><font color=red>SSE2</font></b>-detection enabled: ALGLIB++ is told it is on x86/x64
</pre>
<pre class=p_code_body style='white-space: pre-wrap;'>
<b>&gt;</b> g++ -I. -o Demo.out -O3 <span style='font-weight: bold; color: red;'>-lpthread -DAE_OS=AE_LINUX -DAE_CPU=AE_INTEL -msse2</span> *.cpp &amp;&amp; ./Demo.out
Performance: 3.4 GFLOPS
</pre>
<p>
<font size=2>The <code>-lpthread</code>: there is further discussion about the use of the pthreads, whose library was included in the third test, in the <a href=#gs_using_threadsafety class=sheader>Thread-Safety</a> section below.</font>
</p>
<p>
For the first run, 0.6 GFLOPS is not very fast.
The second run, after <code>-O3</code> is added to the compiler parameters, is better, but still not impressive.
For the third test, we optimize further for x86 architecture by defining <code>AE_CPU=AE_INTEL</code>.
With this option any SSE2 supported provided by the CPU, will be exploited to make for further speedup.
</p>
<p>
Further improvements are supported in the Commercial Edition of ALGLIB similar to those of the Windows example.
The original developers cited further 1.8x and 6.76x increases beyond the last example, respectively, with multi-core support added in and pthreads used and then with MKL extensions also added in.
</p>
<p>
<a name=gs_using class=sheader></a><h2>5. Using ALGLIB++</h2>
<font size=2>[
<a href=#gs_using_threadsafety>Thread-Safety</a> |
<a href=#gs_global> Global Definitions</a> |
<a href=#gs_datatypes>Datatypes</a> |
<a href=#gs_constants>Constants</a> |
<a href=#gs_stdfunctions>Functions</a> |
<a href=#gs_vecmat>Working with Vectors and Matrices</a> |
<a href=#gs_functions>Using Functions: 'Expert' and 'Friendly' Interfaces</a> |
<a href=#gs_errors>Handling Errors</a> |
<a href=#gs_blas>Working with Level 1 BLAS Functions</a> |
<a href=#gs_csv>Reading Data from CSV Files</a>
]</font>
</p>
<p>
<a name=gs_using_threadsafety class=sheader></a><h3>5.1. Thread-Safety</h3>
By encapsulating the use of global shared variables in global objects,
thread-safety was ensured in both open source and commercial versions of ALGLIB, but only with the limitation that
<b>different user threads work with different instances of objects/arrays</b>,
For ALGLIB++, the use of global objects (sometimes called "God-objects")
was deemed to be too high of a price to pay for this feature,
especially now that languages such as CPython, C#, C and C++ now have native support for threads and thread-local declarations.
If your compiler supports this and if you wish to experiment with this, you may start by redefining the macro AutoS in Ap.h from "static" to "static Thread_local".
Other changes will need to be made; for instance, eliminating the use of the external library pthreads, including &lt; threads.h&gt; and replacing the pthreads routines by routines declared in &lt; threads.h &gt; .
None of this has been tested or implemented yet, but may be added in a later revision of ALGLIB++.
</p>
<p>
It is still possible to run ALGLIB++ in multiple threads in limited settings; for instance, running the native C code alongside the C++ wrapper code, as is done in TestX.cpp. But, the execution of two or more instances of iteration routines (where AutoS variables are defined), or C++ wrapper routines in conjunction with them will not be thread-safe. Further limitations, as described below, are inherited from ALGLIB and will also apply here to any thread-local modification done to ALGLIB++.
</p>
<p>
<b>Any kind</b> of sharing of ALGLIB++ objects/arrays between different threads is potentially hazardous.
Even when this object is <b>seemingly</b> used in read-only mode!
</p>
<p class=p_note>
Say, you use ALGLIB++ neural network <i>NET</i> to process two input vectors <i>X0</i> and <i>X1</i>, and get two output vectors <i>Y0</i> and <i>Y1</i>.
You may decide that neural network is used in read-only mode which does not change state of <i>NET</i>, because output is written to distinct arrays <i>Y</i>.
Thus, you may want to process these vectors from parallel threads.
<br/><br/>
But it is <b>not</b> read-only operation, even if it looks like that!
Neural network object <i>NET</i> allocates internal temporary buffers, which are modified by neural processing functions.
Thus, sharing one instance of neural network between two threads is thread-unsafe!
</p>
<p>
It may be possible to by-pass these limitations &mdash; at a potential cost of efficiency and secondary storage &mdash; by running different instances of ALGLIB++ as separate processes and using the ALGLIB++ serialization and deserialization routines to communicate shared data between the processes.
</p>
<p>
<a name=gs_global class=sheader></a><h3>5.2. Global Definitions</h3>
ALGLIB++ defines several conditional symbols (all start with "AE_" which means "<b>A</b>LGLIB <b>e</b>nvironment") and two namespaces:
<ul>
<li><code>alglib_impl</code>:
contains computational core; written mostly in C, but meant for internal use only.

None of the routines in this namespace are part of the API documented in <a href=#alglib_main>ALGLIB++ Packages and Subpackages</a>.
If you attempt to include only the alglib_impl namepace in a C program, you are pretty much on your own.
At minimum, you will need a compiler that supports C99;
some routines now have default values (e.g. ae_complex_from_{i,d} in Ap.{cpp,h}) and you may also need to move some of the <code>const</code> or startup routines (e.g. the CPU-identifying function ae_cpuid()) to the head of the <code>main()</code> routine.
No protection against memory leaks or system crashes is guaranteed if you make direct use of <code>alglib_impl</code>.
Future compatibility with C should not be assumed: it is being recoded into C++.
It is undergoing structural changes and simplifications, as part of the process of nativizing it to C++ that have already distanced it from compatibility with C (or C99) too far for any simple fixes to undo, except with the aid of a translator.
</li>
<li><code>alglib</code>:
contains API interface; written in C++, this is meant for inclusion in your applications.
Most of the routines are C++ wrappers for the C code or C structure types implemented in <code>alglib_impl</code>, which provides user-friendly C++ interface with automatic memory management, exception handling and all other nice features.
</li>
</ul>
There are a fairly large number of features unique to C++ (such as shared memory pools, private versus public declarations) that are simulated in the <code>alglib_impl</code> namespace in C, but which would be more effectively done in C++.
Therefore, ALGLIB++ is, itself, just an intermediate stage of a larger conversion and expansion of ALGLIB to a new library in which the C-code will be eliminated, directly written in native C++ and multi-threading more directly supported, and the namespace dichotomy removed.
You are encouraged to experiment with this code, as well. To this end, a re-coding and re-implementation of one of the packages in native C++ has been separately provided.
</p>
<p>
<a name=gs_datatypes class=sheader></a><h3>5.3. Datatypes</h3>
ALGLIB++ (<code>Ap.h</code> header) defines several "basic" datatypes (types which are used by all packages) and many package-specific datatypes. The "basic" datatypes are:
<ul>
<li><code>ae_int_t</code> &mdash; signed integer type used by library</li>
<li><code>complex</code> &mdash; double precision complex datatype, safer replacement for <code>std::complex</code></li>
<li><code>ap_error</code> &mdash; exception which is thrown by library</li>
<li><code>boolean_1d_array</code> &mdash; 1-dimensional boolean array</li>
<li><code>integer_1d_array</code> &mdash; 1-dimensional integer array</li>
<li><code>real_1d_array</code> &mdash; 1-dimensional real (double precision) array</li>
<li><code>complex_1d_array</code> &mdash; 1-dimensional complex array</li>
<li><code>boolean_2d_array</code> &mdash; 2-dimensional boolean array</li>
<li><code>integer_2d_array</code> &mdash; 2-dimensional integer array</li>
<li><code>real_2d_array</code> &mdash; 2-dimensional real (double precision) array</li>
<li><code>complex_2d_array</code> &mdash; 2-dimensional complex array</li>
</ul>
The 1D and 2D array types may be phased out and replaced by a vector and matrix template type in later versions of ALGLIB++.
</p>
<p>
Package-specific datatypes are classes which can be divided into two distinct groups:
<ul>
<li>"struct-like" classes with public fields which you can access directly.</li>
<li>"object-like" classes which have no public fields. You should use ALGLIB++ functions to work with them.</li>
</ul>
The object-like classes are declared using the <code>DecClass</code> and defined using the <code>DefClass</code> in the <code>alglib</code> namespace. For technical reasons, each class is implemented in two tiers; but both the declarations and implementation have been simplified somewhat, with respect to the design used in ALGLIB, with the further aim of further reducing this to a single tier.
</p>
<p>
Notes on the implementation of the object classes, which differs from that used in ALGLIB, has been provided in the supplement
<a href="Notes.htm#supp_Classes">A3. ALGLIB++ Object Classes</a>.
</p>
<p>
<a name=gs_constants class=sheader></a><h3>5.4. Constants</h3>
The most important constants (defined in the <code>Ap.h</code> header) from ALGLIB++ namespace are:
<ul>
<li><code>machineepsilon</code> &mdash; small number which is <i>slightly larger</i> than the double precision &epsilon;</li>
<li><code>maxrealnumber</code> &mdash; very large number which is <i>slightly smaller</i> than the maximum real number</li>
<li><code>minrealnumber</code> &mdash; very small number which is <i>slightly larger</i> than the minimum positive real number</li>
</ul>
In addition, the following are inherited from the native C++ library:
<ul>
<li><code>std::NAN</code> &mdash; NAN (non-signalling under most platforms except for PA-RISC, where it is signalling;
but when PA-RISC CPU is in its default state, it is silently converted to the quiet NAN)</li>
<li><code>+std::INFINITY</code> &mdash; positive infinity</li>
<li><code>-std::INFINITY</code> &mdash; negative infinity</li>
</ul>
</p>
<p>
<a name=gs_stdfunctions class=sheader></a><h3>5.5. Functions</h3>
The most important "basic" functions from ALGLIB++ namespace (<code>Ap.h</code> header) are:
<ul>
<li><code>randomreal()</code> &mdash; returns random real number from [0,1)</li>
<li><code>randominteger(mx)</code> &mdash; returns random integer number from [0,nx); <i>mx</i> must be less than RAND_MAX</li>
<li><code>std::isnan(x)</code> &mdash; checks whether the number x is NAN</li>
<li><code>isposinf(x)</code> &mdash; checks whether the number x is +INF</li>
<li><code>isneginf(x)</code> &mdash; checks whether the number x is -INF</li>
</ul>
In addition, the following are inherited C++, itself, or from the native C++ library:
<ul>
<li><code>A {==,!=,&lt;,&le;,&gt;,&ge;} B</code> &mdash; IEEE-compliant comparisons of two double precision numbers.
Discussed in further detail below.</li>
<li><code>std::isinf(x)</code> &mdash; checks whether the number x is +INF or -INF</li>
<li><code>std::isfinite(x)</code> &mdash; checks whether the number x is finite value (possibly subnormalized)</li>
</ul>
</p>
<p>
<b><font color=red>Note:</font></b>
Floating-point comparisons require special handling; particularly in the types of numeric and scientific applications that ALGLIB++ is geared for.
Not all CPU's have have or use native floating point operations that conform strictly to IEEE 754.
In particular, Intel x86/x64 use an 80-bit FPU, which may produce internal results that lie outside the (64-bit) range of numbers conforming to the IEEE standard.
Depending on the CPU and compiler being used, floating point operations may not be rounded correctly to the precision required by the programming languaage being compiled.
For C and C++, this becomes an issue if floating point operations need to produce results of types "float" or "double".
</p>
<p>
In the original ALGLIB distribution, the comparison operations ==,!=,&lt;,&le;,&gt;,&ge; were encapsulated respectively in the functions alglib_impl::ae_fp_{eq,neq,less,less_eq,greater,greater_eq} and alglib::fp_{eq,neq,less,less_eq,greater,greater_eq}.
Calling them passes arguments that are rounded to the required precision before-hand, before a comparison is made;
whereas a direct comparison may work directly with the higher precision numerals -- but applying IEEE 754 standards to the comparison operations.
If the numbers being compared lie outside the range recognized by IEEE 754 (particularly, if they are too close to 0)
they would be converted to NAN and comparisons would produce the wrong results.
</p>
<p>
For this reason, an OS/compiler-dependent startup routine is embedded in the function ae_cpuid(), which is called internally at startup to initialize the global parameter CurCPU.
Originally in ALGLIB, ae_cpuid() was made a part of the API; now it is encapsulated in Ap.cpp and replaced by the global constant CurCPU.
If you are extending the GPL version to work with different CPU's off the same code image, this will require running ALGLIB++ on each as separate threads or processes.
</p>
<p>
<a name=gs_vecmat class=sheader></a><h3>5.6. Working with Vectors and Matrices</h3>
ALGLIB++ (<code>Ap.h</code> header) supports variable-sized arrays of 1 dimension (vectors) and 2 dimensions (matrices), with zero-based indexing.
</p>
<p>
Arrays may be created by the array constructor functions with or without initialization; uninitialized arrays are made initially empty. Attempts to address an empty array may cause the program failure.
</p>
<p>
Arrays can also be copied or assigned to another. In both cases, a new and separately allocated copy is created. Array objects are not shared between each other, although they may be shared with C/C++ arrays.
</p>
<p>
You can also create array from formatted string like
   <span class=s_str>"[]"</span>,
   <span class=s_str>"[true,FALSE,tRUe]"</span>,
   <span class=s_str>"[[]]]"</span> or
   <span class=s_str>"[[1,2],[3.2,4],[5.2]]"</span> (note: <span class=s_str>'.'</span> is used as decimal point independently from locale settings).
</p>

<pre class=p_example>
boolean_1d_array b1;
b1 = <span class=s_str>"[true]"</span>;

real_2d_array r2(<span class=s_str>"[[2,3],[3,4]]"</span>);
real_2d_array r2_1(<span class=s_str>"[[]]"</span>);
real_2d_array r2_2(r2);
r2_1 = r2;

complex_1d_array c2;
c2 = <span class=s_str>"[]"</span>;
c2 = <span class=s_str>"[0]"</span>;
c2 = <span class=s_str>"[1,2i]"</span>;
c2 = <span class=s_str>"[+1-2i,-1+5i]"</span>;
c2 = <span class=s_str>"[ 4i-2,  8i+2]"</span>;
c2 = <span class=s_str>"[+4i-2, +8i+2]"</span>;
c2 = <span class=s_str>"[-4i-2, -8i+2]"</span>;
</pre>

<p>
An empty array can be sized, and prevously-allocated array resized using <code>setlength()</code>.
The original contents, if any, are erased; and the newly (re)allocated array is created with initially undefined elements.
</p>

<pre class=p_example>
boolean_1d_array b1;
b1.setlength(2);

integer_2d_array r2;
r2.setlength(4,3);
</pre>

<p>
Arrays can also be (re)sized with initialized values using <code>setcontent()</code>.
The dimensions of the array to be allocated are indicate,
and a pointer is provided to the data the newly allocated array will be initialized with.
Vectors are stored in contiguous order, matrices are stored row by row.
</p>

<pre class=p_example>
real_1d_array r1;
double _r1[] = {2, 3};
r1.setcontent(2,_r1);

real_2d_array r2;
double _r2[] = {11, 12, 13, 21, 22, 23};
r2.setcontent(2,3,_r2);
</pre>

<p>
C/C++ arrays can also be shared with an array object by using <code>attach()</code>.
The vector/matrix object that is attached to becomes a read/write proxy for the C/C++ array.
This feature is only supported for real vector/matrix objects with already-allocated double precision arrays,
Attaching to boolean/integer/complex arrays is not supported.
</p>

<pre class=p_example>
real_1d_array r1;
double a1[] = {2, 3};
r1.attach_to_ptr(2,a1);

real_2d_array r2;
double a2[] = {11, 12, 13, 21, 22, 23};
r2.attach_to_ptr(2,3,_r2);
</pre>

<p>
Array elements, may be accessed by either <code>operator()</code> or <code>operator[]</code>.
For instance, the <code>[i,j]</code> element of array <code>a</code> may be indexed as <code>a(i,j)</code> or <code>a[i][j]</code>.
</p>

<pre class=p_example>
integer_1d_array a(<span class=s_str>"[1,2,3]"</span>);
integer_1d_array b(<span class=s_str>"[3,9,27]"</span>);
a[0] = b(0);

integer_2d_array c(<span class=s_str>"[[1,2,3],[9,9,9]]"</span>);
integer_2d_array d(<span class=s_str>"[[3,9,27],[8,8,8]]"</span>);
d[1][1] = c(0,0);
</pre>

<p>
The elements of 1-dimensional array are stored contiguously and may be accessed directly with <code>getcontent()</code>, which returns pointer to the array's memory.
This capability is not present for 2-dimensional arrays, since the elements need not be contiguous from row to row.
However, since 2-dimensional arrays are stored in row-major order with aligned rows (i.e. but with a distance between rows is generally not equal to number of columns), you can still create references to individual array elements.
You can get the stride (distance between consecutive elements in different rows) using <code>getstride()</code>.
</p>

<pre class=p_example>
   integer_1d_array a(<span class=s_str>"[1,2]"</span>);
   real_2d_array b(<span class=s_str>"[[0,1],[10,11]]"</span>);

   ae_int_t *a_row = a.getcontent();

<span class=s_comment>// all three pointers point to the same location</span>
   double *b_row0 = &amp;b[0][0];
   double *b_row0_2 = &amp;b(0,0);
   double *b_row0_3 = b[0];

<span class=s_comment>// advancing to the next row of 2-dimensional array</span>
   double *b_row1 = b_row0 + b.getstride();
</pre>

<p>
Finally, you can get the size of a vector with <code>length()</code> and the sizes of a matrix with <code>rows()</code> or <code>cols()</code>:
</p>

<pre class=p_example>
integer_1d_array a(<span class=s_str>"[1,2]"</span>);
real_2d_array b(<span class=s_str>"[[0,1],[10,11]]"</span>);

printf(<span class=s_str>"%ld\n"</span>, (long)a.length());
printf(<span class=s_str>"%ld\n"</span>, (long)b.rows());
printf(<span class=s_str>"%ld\n"</span>, (long)b.cols());
</pre>

<p>
<a name=gs_functions class=sheader></a><h3>5.7. Using Functions: 'Expert' and 'Friendly' Interfaces</h3>
Most ALGLIB++ functions provide two interfaces:
<ul>
<li><b>'expert'</b>:
<ul>
<li>tries to automatically determine size of input arguments</li>
<li>throws an exception when arguments have inconsistent size (for example, square matrix is expected, but non-square is passed;
another example - two parameters must have same size, but have different size)</li>
<li>if semantics of input parameter assumes that it is symmetric/Hermitian matrix, checks that lower triangle is equal to upper triangle (conjugate of upper triangle) and throws an exception otherwise</li>
<li>if semantics of output parameter assumes that it is symmetric/Hermitian matrix, returns full matrix (both upper and lower triangles)</li>
</ul>
</li>
<li><b>'friendly'</b>:
<ul>
<li>
requires the caller to explicitly specify size of input arguments.
If vector/matrix is larger than size being specified (say, N), only N leading elements are used
</li>
<li>
if the semantics of the input parameter assumes that it is symmetric/Hermitian matrix, uses <i>only the upper or lower triangle</i> of the input matrix and requires the caller <i>to specify what triangle to use</i>
</li>
<li>
if the semantics of the output parameter assumes that it is symmetric/Hermitian matrix, returns only the upper or lower triangle
(when you look at a specific function, it is clear what triangle is returned)
</li>
</ul>
</li>
</ul>
</p>
<p>
Here are several examples of 'friendly' and 'expert' interfaces:
</p>

<pre class=p_example>
<span class=s_preprocessor>#include "Interpolation.h"</span>

<span style='color: gray;'>...</span>

real_1d_array x(<span class=s_str>"[0,1,2,3]"</span>);
real_1d_array y(<span class=s_str>"[1,5,3,9]"</span>);
real_1d_array y2(<span class=s_str>"[1,5,3,9,0]"</span>);
spline1dinterpolant s;
spline1dbuildlinear(x, y, 4, s);  <span class=s_comment>// 'Expert' interface is used.</span>
spline1dbuildlinear(x, y, s);     <span class=s_comment>// 'Friendly' interface: the input size is automatically determined.</span>
spline1dbuildlinear(x, y2, 4, s); <span class=s_comment>// y2.length() is 5, but it will work.</span>
spline1dbuildlinear(x, y2, s);    <span class=s_comment>// It won't work because sizes of x and y2 are inconsistent.</span>
</pre>

<p>
'Friendly' interface - matrix semantics:
</p>

<pre class=p_example>
<span class=s_preprocessor>#include "LinAlg.h"</span>

<span style='color: gray;'>...</span>

   real_2d_array a;
   matinvreport rep;
   ae_int_t info;
<span class=s_comment>// 'Friendly' interface: spdmatrixinverse() accepts and returns a symmetric matrix.</span>
<span class=s_comment>// A symmetric positive definite matrix.</span>
   a = <span class=s_str>"[[2,1],[1,2]]"</span>;
<span class=s_comment>// After this line, a will contain [[0.66,-0.33],[-0.33,0.66]] which is symmetric too.</span>
   spdmatrixinverse(a, info, rep);
<span class=s_comment>// You may try to pass a non-symmetric matrix,</span>
   a = <span class=s_str>"[[2,1],[0,2]]"</span>;
<span class=s_comment>// but an exception will be thrown in such a case.</span>
   spdmatrixinverse(a, info, rep);
</pre>

<p>
The same function but with the 'expert' interface:
</p>

<pre class=p_example>
<span class=s_preprocessor>#include "LinAlg.h"</span>
<span style='color: gray;'>...</span>

   real_2d_array a;
   matinvreport rep;
   ae_int_t info;
<span class=s_comment>// 'Expert' interface, spdmatrixinverse().</span>
<span class=s_comment>// Only the upper triangle is used; a[1][0] is initialized by NAN, but it can be an arbitrary number.</span>
<span class=s_comment>// After this line, a will contain [[0.66,-0.33],[NAN,0.66]].</span>
<span class=s_comment>// Only the upper triangle is modified.</span>
<span class=s_comment>// n: 2, isupper: true - the upper triangle is used.</span>
   spdmatrixinverse(a, 2, true, info, rep);
</pre>
<p>
<a name=gs_errors class=sheader></a><h3>5.8. Handling Errors</h3>
For error-handling, ALGLIB++ will:
<ul>
<li>return an error code; for functions that return an error code <b>and</b> have a corresponding value for this kind of error, or</li>
<li>throw an exception; for functions that either do not return an error code or return one with no code for the error being reported). An exception object has <code>msg</code> parameter which contains short description of error.</li>
</li>
</ul>
</p>
<p>
To make things clear we consider several examples of error handling.
</p>
<p>
<u>Example 1</u>. <a href="#sub_mincgcreate">mincgreate()</a> creates a nonlinear CG optimizer. It accepts problem size <code>N</code> and initial point <code>X</code>.
Several things can go wrong - you may pass array which is too short, filled by NAN's, or otherwise pass incorrect data.
However, this function returns no error code - so it throws an exception in case something goes wrong.
There is no other way to tell the caller that something went wrong.
</p>
<p>
<u>Example 2</u>. <a href="#sub_rmatrixinverse">rmatrixinverse</a>() calculates the inverse matrix.
It returns an error code, which is set to <code>+1</code> when the problem is solved and is set to <code>-3</code> if singular matrix was passed to the function.
However, there is no error code for a matrix which is non-square or contains infinities (although it is possible to modify ALGLIB++ to include one).
So if you pass a singular matrix to <code>rmatrixinverse</code>, you will get completion code <code>-3</code>.
But if you pass a matrix which contains INF in any of its elements, then <code>ap_error</code> will be thrown.
</p>
<p>
Error codes are used to report "frequent" errors, which can occur during normal execution of user program.
Exceptions are used to report "rare" errors, which are result of serious flaws in your program (or ALGLIB++) -
infinities/NAN's in the inputs, inconsistent inputs, etc.
</p>
<p>
<a name=gs_blas class=sheader></a><h3>5.9. Working with Level 1 BLAS Functions</h3>
ALGLIB++ (<code>Ap.h</code> header) includes following Level 1 BLAS functions:
<ul>
<li><code>vdotproduct()</code> family, which allows to calculate dot product of two real or complex vectors</li>
<li><code>vmove()</code> family, which allows to copy real/complex vector to another location with optimal multiplication by real/complex value</li>
<li><code>vmoveneg()</code> family, which allows to copy real/complex vector to another location with multiplication by -1</li>
<li><code>vadd()</code> and <code>vsub()</code> families, which allows to add or subtract two real/complex vectors with optimal multiplication by real/complex value</li>
<li><code>vmul()</code> family, which implements in-place multiplication of real/complex vector by real/complex value</li>
</ul>
Both ALGLIB++ and the ALGLIB code it inherited contain several routines adapted from earlier versions of LAPACK.
A more complete implementation of the latest version of LAPACK is being prepared for inclusion with ALGLIB++ &mdash; in fact, a C++ translation of the entire package: LAPACK++.
</p>
<p>
Each Level 1 BLAS function accepts an input vector <code>A</code> and output vector <code>B</code>, which are assumed to be non-overlapping; and an input stride <code>dA</code> and output stride <code>dB</code>, which are expected to be positive.
For functions operating with complex vectors, the parameters <code>CjA</code> and <code>CjB</code>, wherever they appear, specify respectively whether <code>A</code> and <code>B</code> is to be conjugated or not.
Strings which start in 'N' or 'n' (e.g. "No conj") specify no conjugation.
All other strings specify conjugation, though "Conj" is recommended for use in such cases.
</p>
<p>
For each real/complex function there exists "simple" companion which accepts no stride or conjugation modifier.
"Simple" function assumes that input/output stride is +1, and no input conjugation is required.
</p>

<pre class=p_example>
real_1d_array rvec(<span class=s_str>"[0,1,2,3]"</span>);
real_2d_array rmat(<span class=s_str>"[[1,2],[3,4]]"</span>);
complex_1d_array cvec(<span class=s_str>"[0+1i,1+2i,2-1i,3-2i]"</span>);
complex_2d_array cmat(<span class=s_str>"[[3i,1],[9,2i]]"</span>);
vmove(rvec, 1, rmat[0], rmat.getstride(), 2); // Now rvec is [1,3,2,3].
vmove(cvec, 1, cmat[0], rmat.getstride(), "No conj", 2); // Now cvec is [3i, 9, 2-1i, 3-2i].
vmove(cvec + 2, 1, cmat[0], 1, "Conj", 2); // Now cvec is [3i, 9, -3i,  1].
</pre>

<p>
Here is full list of Level 1 BLAS functions implemented in ALGLIB++:
</p>
<pre class=source>
<b>double</b> vdotproduct(<b>const</b> <b>double</b> *A, ae_int_t dA, <b>const</b> <b>double</b> *B, ae_int_t dB, ae_int_t N);
<b>double</b> vdotproduct(<b>const</b> <b>double</b> *A, <b>const</b> <b>double</b> *B, ae_int_t N);
complex vdotproduct(<b>const</b> complex *A, ae_int_t dA, <b>const</b> char *CjA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N);
complex vdotproduct(<b>const</b> complex *A, <b>const</b> complex *B, ae_int_t N);

<b>void</b> vmove(<b>double</b> *A, ae_int_t dA, <b>const</b> <b>double</b> *B, ae_int_t dB, ae_int_t N);
<b>void</b> vmove(<b>double</b> *A, <b>const</b> <b>double</b> *B, ae_int_t N);
<b>void</b> vmove(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N);
<b>void</b> vmove(complex *A, <b>const</b> complex *B, ae_int_t N);

<b>void</b> vmoveneg(<b>double</b> *A, ae_int_t dA, <b>const</b> <b>double</b> *B, ae_int_t dB, ae_int_t N);
<b>void</b> vmoveneg(<b>double</b> *A, <b>const</b> <b>double</b> *B, ae_int_t N);
<b>void</b> vmoveneg(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N);
<b>void</b> vmoveneg(complex *A, <b>const</b> complex *B, ae_int_t N);

<b>void</b> vmove(<b>double</b> *A, ae_int_t dA, <b>const</b> <b>double</b> *B, ae_int_t dB, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vmove(<b>double</b> *A, <b>const</b> <b>double</b> *B, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vmove(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vmove(complex *A, <b>const</b> complex *B, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vmove(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N, complex Alpha);
<b>void</b> vmove(complex *A, <b>const</b> complex *B, ae_int_t N, complex Alpha);

<b>void</b> vadd(<b>double</b> *A, ae_int_t dA, <b>const</b> <b>double</b> *B, ae_int_t dB, ae_int_t N);
<b>void</b> vadd(<b>double</b> *A, <b>const</b> <b>double</b> *B, ae_int_t N);
<b>void</b> vadd(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N);
<b>void</b> vadd(complex *A, <b>const</b> complex *B, ae_int_t N);
<b>void</b> vadd(<b>double</b> *A, ae_int_t dA, <b>const</b> <b>double</b> *B, ae_int_t dB, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vadd(<b>double</b> *A, <b>const</b> <b>double</b> *B, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vadd(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vadd(complex *A, <b>const</b> complex *B, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vadd(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N, complex Alpha);
<b>void</b> vadd(complex *A, <b>const</b> complex *B, ae_int_t N, complex Alpha);

<b>void</b> vsub(<b>double</b> *A, ae_int_t dA, <b>const</b> <b>double</b> *B, ae_int_t dB, ae_int_t N);
<b>void</b> vsub(<b>double</b> *A, <b>const</b> <b>double</b> *B, ae_int_t N);
<b>void</b> vsub(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N);
<b>void</b> vsub(complex *A, <b>const</b> complex *B, ae_int_t N);
<b>void</b> vsub(<b>double</b> *A, ae_int_t dA, <b>const</b> <b>double</b> *B, ae_int_t dB, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vsub(<b>double</b> *A, <b>const</b> <b>double</b> *B, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vsub(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vsub(complex *A, <b>const</b> complex *B, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vsub(complex *A, ae_int_t dA, <b>const</b> complex *B, ae_int_t dB, <b>const</b> char *CjB, ae_int_t N, complex Alpha);
<b>void</b> vsub(complex *A, <b>const</b> complex *B, ae_int_t N, complex Alpha);

<b>void</b> vmul(<b>double</b> *A, ae_int_t dA, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vmul(<b>double</b> *A, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vmul(complex *A, ae_int_t dA, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vmul(complex *A, ae_int_t N, <b>double</b> Alpha);
<b>void</b> vmul(complex *A, ae_int_t dA, ae_int_t N, complex Alpha);
<b>void</b> vmul(complex *A, ae_int_t N, complex Alpha);
</pre>
<p>
<a name=gs_csv class=sheader></a><h3>5.10. Reading Data from CSV Files</h3>
ALGLIB++ (<code>Ap.h</code> header) read data from a CSV file using <code>read_csv()</code>.
The entire file is loaded into memory as a double precision matrix (a <code>real_2d_array</code> object).
This function provides:
<ul>
<li>support for ASCII encoding and UTF-8 without BOM (in header names)</li>
<li>the ability to use any character (comma/tab/space) as a field separator (as long as it is distinct from one used for decimal point)</li>
<li>the ability to use both comma and full stop as a decimal point (as long is decimal point is distinct from field separator).
<li>support for Unix and Windows text files (CR versus CRLF)</li>
</ul>
</p>
<p>
See the comments under <code>read_csv()</code> for more information about its functionality.
</p>
<p>
<a name=gs_future class=sheader></a><h2>6. Future Enhancements in ALGLIB++</h2>
None of the following features, available only in the Commercial version of ALGLIB for C++, are present in ALGLIB++, although some of them are being planned for later inclusion:
<ul>
<li><b>License</b>. A commercial-friendly license.</li>
<li>
<b>Low-level optimizations</b>; specifically geared for Intel/AMD platforms for SIMD-optimized versions of more computationally-intensive functions
</li>
<li>
<b>Multithreading</b>, as well as <i>Simultaneous multithreading (SMT)</i>; a.k.a <i>Hyper-threading</i> on Intel or and <i>Cluster-based Multithreading</i> on AMD; which is a CPU design where several (usually two) <i>logical</i> cores share resources of one <i>physical</i> core.
There is, however, limited multi-threading support in ALGLIB++, with future inclusion of more complete native C/C++ support planned.
</li>
<li>
<b>Integrated Intel MKL</b>.
There may be future support provided for the Intel MKL library, which has a large variety of routines not just for linear algebra, which is what the Commercial version of ALGLIB uses, but also for neural net algorithms.
</li>
</ul>
Further information on the features available in the Commercial version may be found here: <a href="https://www.alglib.net/translator/man/manual.cpp.html#gs_comm">Working with the Commercial Version of ALGLIB</a>.
</p>
<p>
There were a fair number of stub routines in ALGLIB Free Edition that were vestiges of the Commercial version; most, but not all of them have been eliminated from ALGLIB++.
Some still remain.
For instance, <code>setnworkers()</code> is a stub and AE_NWORKERS is vestigial.
</p>
<p>
Part of the rationale for stepping away from implementations geared for optimizations with "modern" CPU's (i.e. CPU's current at the time of writing or implementation) is that both they and their capabilities are changing fast &mdash; and in ways that quickly obsolete the need for many of those optimizations. But, as this is happening, if the code is over-designed for specific CPU's or CPU architectures or designed for speed gains that are outstripped by later CPU's, then it is tied down by a significant loss of maintainability that comes from the presence of the legacy "optimized" code.
</p>
<p>
Future enhancements or additions include a larger set of DSP (and multi-precision arithmetic) routines in FastTransforms, the inclusion of a C++ version of LAPACK, the expansion of the serialization routines to handle file I/O, as well as real-time I/O (sound to/from spectrum, voice I/O, graphics I/O, etc.), routines for formal language/automata theory (regular expressions using an algebraic implementation, context-free expressions, etc.), application modules for computer vision, machine learning implementations, natural language processing, routines for boolean and concept lattice optimization, etc.
<p>
<a name=gs_advanced class=sheader></a><h2>7. Advanced Topics</h2>
<font size=2>[
<a href=#gs_exceptionfree>Exception-Free Mode</a> |
<a href=#gs_partial_compiling>Partial Compilation</a> |
<a href=#gs_testing>Testing ALGLIB++</a>
]</font>
</p>
<p>
<a name=gs_exceptionfree class=sheader></a><h3>7.1. Exception-Free Mode</h3>
ALGLIB++ can be compiled in exception-free mode, with exceptions
(<code>throw</code>/<code>try</code>/<code>catch</code> constructs) being disabled at compiler level.
Such a feature is sometimes used by developers of embedded software.
</p>
<p>
ALGLIB++ uses a two-level model of errors:
<ul>
<li>
"expected" errors (like degeneracy of linear system or inconsistency of linear constraints)
are reported with dedicated completion codes, and
</li>
<li>
"critical" errors (like <code>malloc</code> failures, unexpected NANs/INFs in the input data and so on)
are reported with exceptions.
The idea is that it is hard to put (and handle) completion codes in <i>every</i> ALGLIB function,
so we use exceptions to signal errors which should never happen under normal circumstances.
</li>
</ul>
</p>
<p>
Errors handlers are implemented as C++ wrappers written around a C core in <code>alglib_impl</code> that uses completion codes and
<code>setjmp</code>/<code>longjmp</code> functions; with memory management coordinated
so as to ensure that there be no memory leakage when we make <code>longjmp</code> to the error handler.
So, the only point where C++ exceptions are actually used is on a boundary between the C core and C++ interface.
</p>
<p>
By default, ALGLIB will throw an exception with short textual description of the situation.
You may, instead, choose to work without exceptions, in which case ALGLIB will set a global error flag
and silently return from the current function/constructor/... instead of throwing an exception.
The error flag is not thread-local; so exception-free error-handling will only function properly for single-threaded programs.
It can still be used with multithreaded programs, but there is no way to determine which thread caused an "exception without exceptions".
In addition, exception-free mode is incompatible with OS-aware compiling:
you can not have <code>AE_OS=???</code> defined together with <code>AE_NO_EXCEPTIONS</code>.
</p>
<p>
To enable the exception-free method of reporting critical errors, #defining the following two preprocessor symbols at the global level:
<ul>
<li><code>AE_NO_EXCEPTIONS</code> - to switch from exception-based to exception-free code</li>
<li><code>AE_THREADING=AE_SERIAL_UNSAFE</code> - to confirm that you are aware of limitations associated
with exception-free mode (it does not support multithreading)</li>
</ul>
</p>
<p>
After you #define all the necessary preprocessor symbols, two functions will appear in <code>alglib</code> namespace:
<ul>
<li>
<code><b>bool</b> get_error_flag(<b>const char</b> **p_msg = NULL)</code>, which returns current error status (<code><b>true</b></code> is returned on error),
with optional <code><b>char</b>**</code> parameter used to get human-readable error message.
</li>
<li>
<code><b>void</b> clear_error_flag()</code>, which clears error flag (ALGLIB functions set flag on failure, but do not clear it on successful calls)
</li>
</ul>
</p>
<p>
You <u>must</u> check error flag after EVERY operation with ALGLIB objects and functions.
In addition to calling computational ALGLIB functions, following kinds of operations may result in "exception":
<ul>
<li>
calling the default constructor for an ALGLIB object (simply instantiating an object may result in an "exception").
Due to their large size, ALGLIB objects are allocated dynamically (they look like value types, but internally everything is stored in the heap memory).
Thus every constructor, even the default one, makes at least one <code>malloc()</code> call which may fail.
This feature, inherited from ALGLIB, is still present in ALGLIB++, but with a significant reduction in the the number of places where memory allocation is required or used.
</li>
<li>
calling copy/assignment constructors for ALGLIB objects (same reason - <code>malloc</code>)
</li>
<li>
resizing arrays with <code>setlength()</code>/<code>setcontent()</code> and attaching to external memory with <code>attach_to_ptr()</code>
</li>
</ul>
</p>
<p>
<a name=gs_partial_compiling class=sheader></a><h3>7.2. Partial Compilation</h3>
As of version 3.13.0, ALGLIB included the ability to selectively compile subpackages.
It has been eliminated from ALGLIB++ with a possible future splitting into subpackage files in mind, as ALGLIB had before version 3.
For those who wish to carry out partial compilation, each of the package files has been cleanly separated into their component subpackages, each occupying a contiguous section within the package file.
Subpackage dependencies are listed both at the head of each section within the package file, as well as in the <a href="Notes.htm#supp_Links">A4. ALGLIB++ Package Dependencies</a> supplementary section.
</p>
<p>
A lesser degree of partial compilation can be achieved by selectively using only the package files required. The Makefile provided contains a dependency list for package files and should suffice, if you wish to compile ALGLIB into your project in this way, instead.
</p>
<p>
If, on the other hand, you wish to compile selectively by subpackage, then what you need to do is:
<ul>
<li>copy Ap.h and Ap.cpp</li>
<li>copy whichever other package files contain both the subpackages you wish to use (both *.cpp and *.h) and all the subpackages they depend on, directly or indirectly</li>
<li>within each copy, eliminate all the subpackages, except those that you will need.</li>
</ul>
For instance, if you need to use nearestneighbor, then the following subpackages (listed with their dependencies) will be needed:
<ul>
<li>(AlgLibMisc) nearestneighbor &rarr; (AlgLibInternal) scodes, tsort</li>
<li>(AlgLibInternal) scodes &rarr; apserv</li>
<li>(AlgLibInternal) tsort</li>
<li>(AlgLibInternal) apserv</li>
</ul>
In that case, you would copy Ap.cpp and Ap.h; then copy AlgLibMisc.cpp, AlgLibMisc.h, eliminating all the submodules within each copy, except nearestneighbor; and finally copy AlgLibInternal.cpp, AlgLibInternal.h, keeping only the scodes, apserv and tsort subpackages within each copy.
</p>
<p>
Partial compilation may be useful for two purposes:
<ul>
<li>
to eliminate dead code, if the linker is unable to do so
(it happens with some compilers when ALGLIB is compiled as part of the shared library)
</li>
<li>
to reduce build time.
</li>
</ul>
Neither of these are as much an issue as they were with older machines and compilers, and it is just easier to work directly with the package files instead.
<p>
<a name=gs_testing class=sheader></a><h3>7.3. Testing ALGLIB++</h3>
<b>Note:</b> In the original distribution of the GPL version of ALGLIB, the test suite is included separately under a different directory (tests/), the source being placed in its own directory (src/). Here, they are included together as part of one package, with the test suite being provided as an example implementation and use of the library itself. Although they provide, in many cases, stress tests of the various parts of the library that push it beyond conditions it was designed for, this can be illustrative in itself (to show you where the limits lie) and the methods used in the routines in many cases still exemplify the intended use of the library.
</p>
<p>
There are three test suites in ALGLIB:
<ul><li>
<b>Computational tests</b> are located in <code>TestC.cpp</code>.
They are focused on numerical properties of algorithms, stress testing and "deep" tests (large automatically generated problems).
They require only a few minutes to finish even on older machines (example: the test times listed under the supplementary section <a href="Notes.htm#supp_History3">A6.3 Test Results</a>).
</li>
<li>
<b>Interface tests</b> are located in <code>TestI.cpp</code>.
These tests are focused on the abilities to correctly pass data between computational core and caller, to detect simple problems in inputs,
and to at least compile ALGLIB with your compiler.
They are very fast (about a minute to finish including compilation time).
</li>
<li>
<b>Extended tests</b> are located in <code>TestX.cpp</code> (with further tests provided in more recent versions of ALGLIB in <code>TestY.cpp</code> and <code>TestZ.cpp</code>; originally named <code>test_xpart0.cpp</code> and <code>test_xne.cpp</code> respectively.)
These tests are focused on testing some special properties
(say, testing that cloning object indeed results in 100% independent copy being created)
and the performance of several chosen algorithms.
</li>
</ul>
</p>
<p>
Running test suite is easy - just
<ol>
<li>compile one of these files (<code>TestC.cpp</code>, <code>TestI.cpp</code> or <code>TestX.cpp</code>)
along with the rest of the library</li>
<li>launch the resulting executable. It may take from several seconds (interface tests) to several minutes (computational tests) to get the final results</li>
</ol>
</p>
<p>
If you want to be sure that ALGLIB will work with some sophisticated optimization settings, set the corresponding flags during compile time.
If your compiler/system are not in the list of supported ones, we recommend you to run both test suites. But if you are running out of time, run at least <code>TestI.cpp</code>.
</p>
<p>
The Makefile provided will compile all of the tests and then run them in sequence. Running any of the tests individually can also be done through the Makefile; for instance, "make TestI &amp;&amp; ./TestI" in a POSIX command-line terminal window will compile TestI and (if it successfully compiles) run it. The Makefile currently configured for the GCC compiler on Linux, to compile with full optiomization (-O3). You will need to make and experment with making the necessary adjustments to get it to work on your platform, if you are using a different compiler or operating system and/or with different optimization settings or compiler options.
</p>
<p>
<a name=alglib_main class=sheader></a><h2>8. ALGLIB++ Packages and Subpackages</h2>
<font size=2>[
<a href=#pck_AlgLibInternal>AlgLibInternal</a> |
<a href=#pck_AlgLibMisc>AlgLibMisc</a> |
<a href=#pck_DataAnalysis>DataAnalysis</a> |
<a href=#pck_DiffEquations>DiffEquations</a> |
<a href=#pck_FastTransforms>FastTransforms</a> |
<a href=#pck_Integration>Integration</a> |
<a href=#pck_Interpolation>Interpolation</a> |
<a href=#pck_LinAlg>LinAlg</a> |
<a href=#pck_Optimization>Optimization</a> |
<a href=#pck_Solvers>Solvers</a> |
<a href=#pck_SpecialFunctions>SpecialFunctions</a> |
<a href=#pck_Statistics>Statistics</a>
]</font>
</p>
<p>
The following packages are included in the ALGLIB++ distribution:
<table align=center border=1><tbody>
<tr align=left valign=top bgColor=#aaaaaa><td>Ap</td><td>(Internal) Core routines for vectors and matrices, memory management and other functions to emulate native C++ functionality.</td></tr>
<tr align=left valign=top bgColor=#aaaaaa><td><a href=#pck_AlgLibInternal>AlgLibInternal</a></td><td>(Internal) Auxiliary and support routines used by the other packages.</td></tr>
<tr align=left valign=top><td><a href=#pck_AlgLibMisc class=toc>AlgLibMisc</a></td><td>Miscellaneous routines (random number generator, KD-trees and debugger interface).</td></tr>
<tr align=left valign=top><td><a href=#pck_DataAnalysis class=toc>DataAnalysis</a></td><td>Classification, regression, other neural net routines, suited for Big Data and Machine Learning.</td></tr>
<tr align=left valign=top><td><a href=#pck_DiffEquations class=toc>DiffEquations</a></td><td>Numerical methods for differential equations.</td></tr>
<tr align=left valign=top><td><a href=#pck_FastTransforms class=toc>FastTransforms</a></td><td>Core DSP routines for FFT with applications to FHT, convolution and correlation.</td></tr>
<tr align=left valign=top><td><a href=#pck_Integration class=toc>Integration</a></td><td>Numerical methods for integration.</td></tr>
<tr align=left valign=top><td><a href=#pck_Interpolation class=toc>Interpolation</a></td><td>Interpolation and fitting, both regular and irregular; including specialized implementations for 1, 2 and 3 dimensions.</td></tr>
<tr align=left valign=top><td><a href=#pck_LinAlg class=toc>LinAlg</a></td><td>Linear algebra (matrix and vector operations, eigendata decomposition).</td></tr>
<tr align=left valign=top><td><a href=#pck_Optimization class=toc>Optimization</a></td><td>Optimization, linear/convex/non-linear; univariate and multivariate.</td></tr>
<tr align=left valign=top><td><a href=#pck_Solvers class=toc>Solvers</a></td><td>Solution methods for linear and nonlinear systems of equations.</td></tr>
<tr align=left valign=top><td><a href=#pck_SpecialFunctions class=toc>SpecialFunctions</a></td><td>Special Functions, with fast implementations primarily using rational function interpolation.</td></tr>
<tr align=left valign=top><td><a href=#pck_Statistics class=toc>Statistics</a></td><td>Summary statistics, distributions and hypothesis testing.</td></tr>
</tbody></table>
The examples listed in the following sections match those of <code>TestI.cpp</code>, and include most of the tests described therein.
For performance reasons, functions may be implemented with slight differences from how they are documented.
In particular, while output parameters are documented and declared and implemented as <code>Type &amp;Par</code>,
input parameters are documented as <code>Type Par</code>, but declared and implemented
as <code>const Type Par</code> for the scalar types <code>bool</code>, <code>ae_int_t</code>, <code>double</code> and <code>complex</code>,
and as <code>const Type &amp;Par</code> for all other types.
This is discussed in greater depth by Stroustrup et al. <a href="https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Rf-in">in-parameter passing with constant references</a>.
For user-supplied functions in the iteration routines and their high-level versions, however, the prototypes actually required are made explicit.
For both these routines as well as for the serializer/unserializer routines, the documented and declared/implemented versions match.
</p>
<p>
<a name=pck_AlgLibInternal class=sheader></a><h3>8.0. AlgLibInternal Package</h3>
The subpackages included with AlgLibInternal, primarily meant for internal use, are not documented in any detail here.
They are, however, included in the dependencies listed under the other subpackages.
Therefore, a short summary is provided here for each of the subpackages.
<table align=center border=1><tbody>
<tr align=left valign=top><td><a name=unit_scodes>scodes</a></td><td>Serialization codes.</td></tr>
<tr align=left valign=top><td><a name=unit_apserv>apserv</a></td><td>Auxiliary routines, mostly stubs/legacies from the Commercial Edition of ALGLIB.</td></tr>
<tr align=left valign=top><td><a name=unit_tsort>tsort</a></td><td>Basic tag sort routines.<br/>Depends on: <a href=#unit_apserv>apserv</a>.</td></tr>
<tr align=left valign=top><td><a name=unit_ablasmkl>ablasmkl</a></td><td>ABLAS Hooks for MKL (or other external library) ABLAS routines (used only in <a href=#unit_sparse>sparse</a> and <a href=#unit_ablas>ablas</a>).</td></tr>
<tr align=left valign=top><td><a name=unit_ablasf>ablasf</a></td><td>ABLAS Hooks for fast ABLAS routines (used only in <a href=#unit_ablas>ablas</a>).</td></tr>
<tr align=left valign=top><td><a name=unit_creflections>creflections</a></td><td>Complex reflection transformation (used only in <a href=#unit_matgen>matgen</a> and <a href=#unit_ortfac>ortfac</a>).</td></tr>
<tr align=left valign=top><td><a name=unit_rotations>rotations</a></td><td>Matrix rotation routines.</td></tr>
<tr align=left valign=top><td><a name=unit_trlinsolve>trlinsolve</a></td><td>"Safe" triangular matrix solver.</td></tr>
<tr align=left valign=top><td><a name=unit_safesolve>safesolve</a></td><td>"Safe" matrix solver (used only in <a href=#unit_rcond>rcond</a>).</td></tr>
<tr align=left valign=top><td><a name=unit_hblas>hblas</a></td><td>Basic BLAS routines for Hermitian matrices (used only in <a href=#unit_ortfac>ortfac</a>).</td></tr>
<tr align=left valign=top><td><a name=unit_sblas>sblas</a></td><td>Basic BLAS routines for Symmetric matrices (used only in <a href=#unit_ortfac>ortfac</a>).<br/>Depends on: <a href=#unit_apserv>apserv</a>.</td></tr>
<tr align=left valign=top><td><a name=unit_blas>blas</a></td><td>Basic BLAS routines for general matrices.</td></tr>
<tr align=left valign=top><td><a name=unit_linmin>linmin</a></td><td>Line search minimizers (mcsrch and Armijo search).</td></tr>
<tr align=left valign=top><td><a name=unit_xblas>xblas</a></td><td>Basic BLAS routines for extra precision (used only in <a href=#unit_directdensesolvers>directdensesolvers</a>).</td></tr>
<tr align=left valign=top><td><a name=unit_basicstatops>basicstatops</a></td><td>Ranking procedures for statistics (used only in <a href=#unit_evd>evd</a> and <a href=#unit_basestat>basestat</a>).<br/>Depends on: <a href=#unit_tsort>tsort</a>.</td></tr>
<tr align=left valign=top><td><a name=unit_hpccores>hpccores</a></td><td>Basic routines for HPC computations (used only in <a href=#unit_mlpbase>mlpbase</a>).</td></tr>
<tr align=left valign=top><td><a name=unit_ntheory>ntheory</a></td><td>Basic number theoretic support routines for FFT (used only in <a href=#unit_ftbase>ftbase</a>).</td></tr>
<tr align=left valign=top><td><a name=unit_ftbase>ftbase</a></td><td>An FFT engine, similar to (but not as extensive as) FFTW (used only in <a href=#unit_fft>fft</a>).<br/>Depends on: <a href=#unit_apserv>apserv</a>, <a href=#unit_ntheory>ntheory</a>.</td></tr>
<tr align=left valign=top><td><a name=unit_nearunityunit>nearunityunit</a></td><td>Basic "near unity" support routines log, exp and cos for the binomial distribution (used only in <a href=#unit_binomialdistr>binomialdistr</a>).</td></tr>
</tbody></table>
</p>
<p>
<a name=pck_AlgLibMisc class=sheader></a><h3>8.1. AlgLibMisc Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_hqrnd class=toc>hqrnd</a></td><td>High quality random numbers generator</td></tr>
<tr align=left valign=top><td><a href=#unit_nearestneighbor class=toc>nearestneighbor</a></td><td>Nearest neighbor search: approximate and exact</td></tr>
<tr align=left valign=top><td><a href=#unit_xdebug class=toc>xdebug</a></td><td>Debug functions to test the ALGLIB interface generator: not meant for use in production code.</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_hqrnd></a><h4 class=pageheader>8.1.1. hqrnd Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_hqrndstate class=toc>hqrndstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_hqrndcontinuous class=toc>hqrndcontinuous</a> |
<a href=#sub_hqrnddiscrete class=toc>hqrnddiscrete</a> |
<a href=#sub_hqrndexponential class=toc>hqrndexponential</a> |
<a href=#sub_hqrndmiduniformr class=toc>hqrndmiduniformr</a> |
<a href=#sub_hqrndnormal class=toc>hqrndnormal</a> |
<a href=#sub_hqrndnormal2 class=toc>hqrndnormal2</a> |
<a href=#sub_hqrndnormalm class=toc>hqrndnormalm</a> |
<a href=#sub_hqrndnormalv class=toc>hqrndnormalv</a> |
<a href=#sub_hqrndrandomize class=toc>hqrndrandomize</a> |
<a href=#sub_hqrndseed class=toc>hqrndseed</a> |
<a href=#sub_hqrnduniformi class=toc>hqrnduniformi</a> |
<a href=#sub_hqrnduniformr class=toc>hqrnduniformr</a> |
<a href=#sub_hqrndunit2 class=toc>hqrndunit2</a>
]</font>
</div>
<a name=struct_hqrndstate></a><h6 class=pageheader>hqrndstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
Portable high quality random number generator state.
Initialized with HQRNDRandomize() or HQRNDSeed().

Fields:
    S1, S2      -   seed values
    V           -   precomputed value
    MagicV      -   'magic' value used to determine whether State structure
                    was correctly initialized.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> hqrndstate {
};
</pre>
<a name=sub_hqrndcontinuous></a><h6 class=pageheader>hqrndcontinuous Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function generates random number from continuous  distribution  given
by finite sample X.

Inputs:
    State   -   high quality random number generator, must be
                initialized with HQRNDRandomize() or HQRNDSeed().
        X   -   finite sample, array[N] (can be larger, in this  case only
                leading N elements are used). THIS ARRAY MUST BE SORTED BY
                ASCENDING.
        N   -   number of elements to use, N &ge; 1

Result:
    this function returns random number from continuous distribution which
    tries to approximate X as mush as possible. min(X) &le; Result &le; max(X).
ALGLIB: Copyright 08.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hqrndcontinuous(hqrndstate state, real_1d_array x, ae_int_t n);
</pre>
<a name=sub_hqrnddiscrete></a><h6 class=pageheader>hqrnddiscrete Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function generates  random number from discrete distribution given by
finite sample X.

Inputs:
    State   -   high quality random number generator, must be
                initialized with HQRNDRandomize() or HQRNDSeed().
        X   -   finite sample
        N   -   number of elements to use, N &ge; 1

Result:
    this function returns one of the X[i] for random i=0..N-1
ALGLIB: Copyright 08.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hqrnddiscrete(hqrndstate state, real_1d_array x, ae_int_t n);
</pre>
<a name=sub_hqrndexponential></a><h6 class=pageheader>hqrndexponential Function</h6>
<hr width=600 align=left>
<pre class=narration>
Random number generator: exponential distribution

State structure must be initialized with HQRNDRandomize() or HQRNDSeed().
ALGLIB: Copyright 11.08.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hqrndexponential(hqrndstate state, <b>double</b> lambdav);
</pre>
<a name=sub_hqrndmiduniformr></a><h6 class=pageheader>hqrndmiduniformr Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function generates random real number in (-1,+1),
not including interval boundaries

State structure must be initialized with HQRNDRandomize() or HQRNDSeed().
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hqrndmiduniformr(hqrndstate state);
</pre>
<a name=sub_hqrndnormal></a><h6 class=pageheader>hqrndnormal Function</h6>
<hr width=600 align=left>
<pre class=narration>
Random number generator: normal numbers

This function generates one random number from normal distribution.
Its performance is equal to that of HQRNDNormal2()

State structure must be initialized with HQRNDRandomize() or HQRNDSeed().
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hqrndnormal(hqrndstate state);
</pre>
<a name=sub_hqrndnormal2></a><h6 class=pageheader>hqrndnormal2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Random number generator: normal numbers

This function generates two independent random numbers from normal
distribution. Its performance is equal to that of HQRNDNormal()

State structure must be initialized with HQRNDRandomize() or HQRNDSeed().
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hqrndnormal2(hqrndstate state, <b>double</b> &amp;x1, <b>double</b> &amp;x2);
</pre>
<a name=sub_hqrndnormalm></a><h6 class=pageheader>hqrndnormalm Function</h6>
<hr width=600 align=left>
<pre class=narration>
Random number generator: matrix with random entries (normal distribution)

This function generates MxN random matrix.

State structure must be initialized with HQRNDRandomize() or HQRNDSeed().
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hqrndnormalm(hqrndstate state, ae_int_t m, ae_int_t n, real_2d_array &amp;x);
</pre>
<a name=sub_hqrndnormalv></a><h6 class=pageheader>hqrndnormalv Function</h6>
<hr width=600 align=left>
<pre class=narration>
Random number generator: vector with random entries (normal distribution)

This function generates N random numbers from normal distribution.

State structure must be initialized with HQRNDRandomize() or HQRNDSeed().
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hqrndnormalv(hqrndstate state, ae_int_t n, real_1d_array &amp;x);
</pre>
<a name=sub_hqrndrandomize></a><h6 class=pageheader>hqrndrandomize Function</h6>
<hr width=600 align=left>
<pre class=narration>
HQRNDState  initialization  with  random  values  which come from standard
RNG.
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hqrndrandomize(hqrndstate &amp;state);
</pre>
<a name=sub_hqrndseed></a><h6 class=pageheader>hqrndseed Function</h6>
<hr width=600 align=left>
<pre class=narration>
HQRNDState initialization with seed values
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hqrndseed(ae_int_t s1, ae_int_t s2, hqrndstate &amp;state);
</pre>
<a name=sub_hqrnduniformi></a><h6 class=pageheader>hqrnduniformi Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function generates random integer number in [0, N)

1. State structure must be initialized with HQRNDRandomize() or HQRNDSeed()
2. N can be any positive number except for very large numbers:
   * close to 2^31 on 32-bit systems
   * close to 2^62 on 64-bit systems
   An exception will be generated if N is too large.
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t hqrnduniformi(hqrndstate state, ae_int_t n);
</pre>
<a name=sub_hqrnduniformr></a><h6 class=pageheader>hqrnduniformr Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function generates random real number in (0,1),
not including interval boundaries

State structure must be initialized with HQRNDRandomize() or HQRNDSeed().
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hqrnduniformr(hqrndstate state);
</pre>
<a name=sub_hqrndunit2></a><h6 class=pageheader>hqrndunit2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Random number generator: random X and Y such that X^2+Y^2=1

State structure must be initialized with HQRNDRandomize() or HQRNDSeed().
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hqrndunit2(hqrndstate state, <b>double</b> &amp;x, <b>double</b> &amp;y);
</pre>
<a name=unit_nearestneighbor></a><h4 class=pageheader>8.1.2. nearestneighbor Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_kdtree class=toc>kdtree</a> |
<a href=#struct_kdtreerequestbuffer class=toc>kdtreerequestbuffer</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_kdtreebuild class=toc>kdtreebuild</a> |
<a href=#sub_kdtreebuildtagged class=toc>kdtreebuildtagged</a> |
<a href=#sub_kdtreecreaterequestbuffer class=toc>kdtreecreaterequestbuffer</a> |
<a href=#sub_kdtreequeryaknn class=toc>kdtreequeryaknn</a> |
<a href=#sub_kdtreequerybox class=toc>kdtreequerybox</a> |
<a href=#sub_kdtreequeryknn class=toc>kdtreequeryknn</a> |
<a href=#sub_kdtreequeryresultsdistances class=toc>kdtreequeryresultsdistances</a> |
<a href=#sub_kdtreequeryresultsdistancesi class=toc>kdtreequeryresultsdistancesi</a> |
<a href=#sub_kdtreequeryresultstags class=toc>kdtreequeryresultstags</a> |
<a href=#sub_kdtreequeryresultstagsi class=toc>kdtreequeryresultstagsi</a> |
<a href=#sub_kdtreequeryresultsx class=toc>kdtreequeryresultsx</a> |
<a href=#sub_kdtreequeryresultsxi class=toc>kdtreequeryresultsxi</a> |
<a href=#sub_kdtreequeryresultsxy class=toc>kdtreequeryresultsxy</a> |
<a href=#sub_kdtreequeryresultsxyi class=toc>kdtreequeryresultsxyi</a> |
<a href=#sub_kdtreequeryrnn class=toc>kdtreequeryrnn</a> |
<a href=#sub_kdtreequeryrnnu class=toc>kdtreequeryrnnu</a> |
<a href=#sub_kdtreeserialize class=toc>kdtreeserialize</a> |
<a href=#sub_kdtreetsqueryaknn class=toc>kdtreetsqueryaknn</a> |
<a href=#sub_kdtreetsquerybox class=toc>kdtreetsquerybox</a> |
<a href=#sub_kdtreetsqueryknn class=toc>kdtreetsqueryknn</a> |
<a href=#sub_kdtreetsqueryresultsdistances class=toc>kdtreetsqueryresultsdistances</a> |
<a href=#sub_kdtreetsqueryresultstags class=toc>kdtreetsqueryresultstags</a> |
<a href=#sub_kdtreetsqueryresultsx class=toc>kdtreetsqueryresultsx</a> |
<a href=#sub_kdtreetsqueryresultsxy class=toc>kdtreetsqueryresultsxy</a> |
<a href=#sub_kdtreetsqueryrnn class=toc>kdtreetsqueryrnn</a> |
<a href=#sub_kdtreetsqueryrnnu class=toc>kdtreetsqueryrnnu</a> |
<a href=#sub_kdtreeunserialize class=toc>kdtreeunserialize</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_nneighbor_d_1 class=toc>nneighbor_d_1</a></td><td width=15>&nbsp;</td><td>Nearest neighbor search, KNN queries</td></tr>
<tr align=left valign=top><td><a href=#example_nneighbor_d_2 class=toc>nneighbor_d_2</a></td><td width=15>&nbsp;</td><td>Serialization of KD-trees</td></tr>
</table>
</div>
<a name=struct_kdtree></a><h6 class=pageheader>kdtree Class</h6>
<hr width=600 align=left>
<pre class=narration>
KD-tree object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> kdtree {
};
</pre>
<a name=struct_kdtreerequestbuffer></a><h6 class=pageheader>kdtreerequestbuffer Class</h6>
<hr width=600 align=left>
<pre class=narration>
Buffer object which is used to perform nearest neighbor  requests  in  the
multithreaded mode (multiple threads working with same KD-tree object).

This object should be created with KDTreeCreateRequestBuffer().
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> kdtreerequestbuffer {
};
</pre>
<a name=sub_kdtreebuild></a><h6 class=pageheader>kdtreebuild Function</h6>
<hr width=600 align=left>
<pre class=narration>
KD-tree creation

This subroutine creates KD-tree from set of X-values and optional Y-values

Inputs:
    XY      -   dataset, array[0..N-1,0..NX+NY-1].
                one row corresponds to one point.
                first NX columns contain X-values, next NY (NY may be zero)
                columns may contain associated Y-values
    N       -   number of points, N &ge; 0.
    NX      -   space dimension, NX &ge; 1.
    NY      -   number of optional Y-values, NY &ge; 0.
    NormType-   norm type:
                * 0 denotes infinity-norm
                * 1 denotes 1-norm
                * 2 denotes 2-norm (Euclidean norm)

Outputs:
    KDT     -   KD-tree

NOTES

1. KD-tree  creation  have O(N*logN) complexity and O(N*(2*NX+NY))  memory
   requirements.
2. Although KD-trees may be used with any combination of N  and  NX,  they
   are more efficient than brute-force search only when N &gt;&gt; 4^NX. So they
   are most useful in low-dimensional tasks (NX=2, NX=3). NX=1  is another
   inefficient case, because  simple  binary  search  (without  additional
   structures) is much more efficient in such tasks than KD-trees.
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreebuild(real_2d_array xy, ae_int_t n, ae_int_t nx, ae_int_t ny, ae_int_t normtype, kdtree &amp;kdt);
<b>void</b> kdtreebuild(real_2d_array xy, ae_int_t nx, ae_int_t ny, ae_int_t normtype, kdtree &amp;kdt);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nneighbor_d_1 class=nav>nneighbor_d_1</a> | <a href=#example_nneighbor_d_2 class=nav>nneighbor_d_2</a> ]</p>
<a name=sub_kdtreebuildtagged></a><h6 class=pageheader>kdtreebuildtagged Function</h6>
<hr width=600 align=left>
<pre class=narration>
KD-tree creation

This  subroutine  creates  KD-tree  from set of X-values, integer tags and
optional Y-values

Inputs:
    XY      -   dataset, array[0..N-1,0..NX+NY-1].
                one row corresponds to one point.
                first NX columns contain X-values, next NY (NY may be zero)
                columns may contain associated Y-values
    Tags    -   tags, array[0..N-1], contains integer tags associated
                with points.
    N       -   number of points, N &ge; 0
    NX      -   space dimension, NX &ge; 1.
    NY      -   number of optional Y-values, NY &ge; 0.
    NormType-   norm type:
                * 0 denotes infinity-norm
                * 1 denotes 1-norm
                * 2 denotes 2-norm (Euclidean norm)

Outputs:
    KDT     -   KD-tree

NOTES

1. KD-tree  creation  have O(N*logN) complexity and O(N*(2*NX+NY))  memory
   requirements.
2. Although KD-trees may be used with any combination of N  and  NX,  they
   are more efficient than brute-force search only when N &gt;&gt; 4^NX. So they
   are most useful in low-dimensional tasks (NX=2, NX=3). NX=1  is another
   inefficient case, because  simple  binary  search  (without  additional
   structures) is much more efficient in such tasks than KD-trees.
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreebuildtagged(real_2d_array xy, integer_1d_array tags, ae_int_t n, ae_int_t nx, ae_int_t ny, ae_int_t normtype, kdtree &amp;kdt);
<b>void</b> kdtreebuildtagged(real_2d_array xy, integer_1d_array tags, ae_int_t nx, ae_int_t ny, ae_int_t normtype, kdtree &amp;kdt);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nneighbor_d_1 class=nav>nneighbor_d_1</a> ]</p>
<a name=sub_kdtreecreaterequestbuffer></a><h6 class=pageheader>kdtreecreaterequestbuffer Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates buffer  structure  which  can  be  used  to  perform
parallel KD-tree requests.

KD-tree subpackage provides two sets of request functions - ones which use
internal buffer of KD-tree object  (these  functions  are  single-threaded
because they use same buffer, which can not shared between  threads),  and
ones which use external buffer.

This function is used to initialize external buffer.

Inputs:
    KDT         -   KD-tree which is associated with newly created buffer

Outputs:
    Buf         -   external buffer.

IMPORTANT: KD-tree buffer should be used only with  KD-tree  object  which
           was used to initialize buffer. Any attempt to use buffer   with
           different object is dangerous - you  may  get  integrity  check
           failure (exception) because sizes of internal arrays do not fit
           to dimensions of KD-tree structure.
ALGLIB: Copyright 18.03.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreecreaterequestbuffer(kdtree kdt, kdtreerequestbuffer &amp;buf);
</pre>
<a name=sub_kdtreequeryaknn></a><h6 class=pageheader>kdtreequeryaknn Function</h6>
<hr width=600 align=left>
<pre class=narration>
K-NN query: approximate K nearest neighbors

IMPORTANT: this function can not be used in multithreaded code because  it
           uses internal temporary buffer of kd-tree object, which can not
           be shared between multiple threads.  If  you  want  to  perform
           parallel requests, use function  which  uses  external  request
           buffer: KDTreeTsQueryAKNN() (&quot;Ts&quot; stands for &quot;thread-safe&quot;).

Inputs:
    KDT         -   KD-tree
    X           -   point, array[0..NX-1].
    K           -   number of neighbors to return, K &ge; 1
    SelfMatch   -   whether self-matches are allowed:
                    * if True, nearest neighbor may be the point itself
                      (if it exists in original dataset)
                    * if False, then only points with non-zero distance
                      are returned
                    * if not given, considered True
    Eps         -   approximation factor, Eps &ge; 0. eps-approximate  nearest
                    neighbor  is  a  neighbor  whose distance from X is at
                    most (1+eps) times distance of true nearest neighbor.

Result:
    number of actual neighbors found (either K or N, if K &gt; N).

NOTES
    significant performance gain may be achieved only when Eps  is  is  on
    the order of magnitude of 1 or larger.

This  subroutine  performs  query  and  stores  its result in the internal
structures of the KD-tree. You can use  following  subroutines  to  obtain
these results:
* KDTreeQueryResultsX() to get X-values
* KDTreeQueryResultsXY() to get X- and Y-values
* KDTreeQueryResultsTags() to get tag values
* KDTreeQueryResultsDistances() to get distances
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreequeryaknn(kdtree kdt, real_1d_array x, ae_int_t k, <b>bool</b> selfmatch, <b>double</b> eps);
ae_int_t kdtreequeryaknn(kdtree kdt, real_1d_array x, ae_int_t k, <b>double</b> eps);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nneighbor_d_1 class=nav>nneighbor_d_1</a> ]</p>
<a name=sub_kdtreequerybox></a><h6 class=pageheader>kdtreequerybox Function</h6>
<hr width=600 align=left>
<pre class=narration>
Box query: all points within user-specified box.

IMPORTANT: this function can not be used in multithreaded code because  it
           uses internal temporary buffer of kd-tree object, which can not
           be shared between multiple threads.  If  you  want  to  perform
           parallel requests, use function  which  uses  external  request
           buffer: KDTreeTsQueryBox() (&quot;Ts&quot; stands for &quot;thread-safe&quot;).

Inputs:
    KDT         -   KD-tree
    BoxMin      -   lower bounds, array[0..NX-1].
    BoxMax      -   upper bounds, array[0..NX-1].

Result:
    number of actual neighbors found (in [0,N]).

This  subroutine  performs  query  and  stores  its result in the internal
structures of the KD-tree. You can use  following  subroutines  to  obtain
these results:
* KDTreeQueryResultsX() to get X-values
* KDTreeQueryResultsXY() to get X- and Y-values
* KDTreeQueryResultsTags() to get tag values
* KDTreeQueryResultsDistances() returns zeros for this request

NOTE: this particular query returns unordered results, because there is no
      meaningful way of  ordering  points.  Furthermore,  no 'distance' is
      associated with points - it is either INSIDE  or OUTSIDE (so request
      for distances will return zeros).
ALGLIB: Copyright 14.05.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreequerybox(kdtree kdt, real_1d_array boxmin, real_1d_array boxmax);
</pre>
<a name=sub_kdtreequeryknn></a><h6 class=pageheader>kdtreequeryknn Function</h6>
<hr width=600 align=left>
<pre class=narration>
K-NN query: K nearest neighbors

IMPORTANT: this function can not be used in multithreaded code because  it
           uses internal temporary buffer of kd-tree object, which can not
           be shared between multiple threads.  If  you  want  to  perform
           parallel requests, use function  which  uses  external  request
           buffer: KDTreeTsQueryKNN() (&quot;Ts&quot; stands for &quot;thread-safe&quot;).

Inputs:
    KDT         -   KD-tree
    X           -   point, array[0..NX-1].
    K           -   number of neighbors to return, K &ge; 1
    SelfMatch   -   whether self-matches are allowed:
                    * if True, nearest neighbor may be the point itself
                      (if it exists in original dataset)
                    * if False, then only points with non-zero distance
                      are returned
                    * if not given, considered True

Result:
    number of actual neighbors found (either K or N, if K &gt; N).

This  subroutine  performs  query  and  stores  its result in the internal
structures of the KD-tree. You can use  following  subroutines  to  obtain
these results:
* KDTreeQueryResultsX() to get X-values
* KDTreeQueryResultsXY() to get X- and Y-values
* KDTreeQueryResultsTags() to get tag values
* KDTreeQueryResultsDistances() to get distances
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreequeryknn(kdtree kdt, real_1d_array x, ae_int_t k, <b>bool</b> selfmatch);
ae_int_t kdtreequeryknn(kdtree kdt, real_1d_array x, ae_int_t k);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nneighbor_d_1 class=nav>nneighbor_d_1</a> ]</p>
<a name=sub_kdtreequeryresultsdistances></a><h6 class=pageheader>kdtreequeryresultsdistances Function</h6>
<hr width=600 align=left>
<pre class=narration>
Distances from last query

This function retuns results stored in  the  internal  buffer  of  kd-tree
object. If you performed buffered requests (ones which  use  instances  of
kdtreerequestbuffer class), you  should  call  buffered  version  of  this
function - kdtreetsqueryresultsdistances().

Inputs:
    KDT     -   KD-tree
    R       -   possibly pre-allocated buffer. If X is too small to store
                result, it is resized. If size(X) is enough to store
                result, it is left unchanged.

Outputs:
    R       -   filled with distances (in corresponding norm)

NOTES
1. points are ordered by distance from the query point (first = closest)
2. if  XY is larger than required to store result, only leading part  will
   be overwritten; trailing part will be left unchanged. So  if  on  input
   XY = [[A,B],[C,D]], and result is [1,2],  then  on  exit  we  will  get
   XY = [[1,2],[C,D]]. This is done purposely to increase performance;  if
   you want function  to  resize  array  according  to  result  size,  use
   function with same name and suffix 'I'.

SEE ALSO
* KDTreeQueryResultsX()             X-values
* KDTreeQueryResultsXY()            X- and Y-values
* KDTreeQueryResultsTags()          tag values
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreequeryresultsdistances(kdtree kdt, real_1d_array &amp;r);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nneighbor_d_1 class=nav>nneighbor_d_1</a> ]</p>
<a name=sub_kdtreequeryresultsdistancesi></a><h6 class=pageheader>kdtreequeryresultsdistancesi Function</h6>
<hr width=600 align=left>
<pre class=narration>
Distances from last query; 'interactive' variant for languages like Python
which  support  constructs   like  &quot;R = KDTreeQueryResultsDistancesI(KDT)&quot;
and interactive mode of interpreter.

This function allocates new array on each call,  so  it  is  significantly
slower than its 'non-interactive' counterpart, but it is  more  convenient
when you call it from command line.
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreequeryresultsdistancesi(kdtree kdt, real_1d_array &amp;r);
</pre>
<a name=sub_kdtreequeryresultstags></a><h6 class=pageheader>kdtreequeryresultstags Function</h6>
<hr width=600 align=left>
<pre class=narration>
Tags from last query

This function retuns results stored in  the  internal  buffer  of  kd-tree
object. If you performed buffered requests (ones which  use  instances  of
kdtreerequestbuffer class), you  should  call  buffered  version  of  this
function - kdtreetsqueryresultstags().

Inputs:
    KDT     -   KD-tree
    Tags    -   possibly pre-allocated buffer. If X is too small to store
                result, it is resized. If size(X) is enough to store
                result, it is left unchanged.

Outputs:
    Tags    -   filled with tags associated with points,
                or, when no tags were supplied, with zeros

NOTES
1. points are ordered by distance from the query point (first = closest)
2. if  XY is larger than required to store result, only leading part  will
   be overwritten; trailing part will be left unchanged. So  if  on  input
   XY = [[A,B],[C,D]], and result is [1,2],  then  on  exit  we  will  get
   XY = [[1,2],[C,D]]. This is done purposely to increase performance;  if
   you want function  to  resize  array  according  to  result  size,  use
   function with same name and suffix 'I'.

SEE ALSO
* KDTreeQueryResultsX()             X-values
* KDTreeQueryResultsXY()            X- and Y-values
* KDTreeQueryResultsDistances()     distances
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreequeryresultstags(kdtree kdt, integer_1d_array &amp;tags);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nneighbor_d_1 class=nav>nneighbor_d_1</a> ]</p>
<a name=sub_kdtreequeryresultstagsi></a><h6 class=pageheader>kdtreequeryresultstagsi Function</h6>
<hr width=600 align=left>
<pre class=narration>
Tags  from  last  query;  'interactive' variant for languages like  Python
which  support  constructs  like &quot;Tags = KDTreeQueryResultsTagsI(KDT)&quot; and
interactive mode of interpreter.

This function allocates new array on each call,  so  it  is  significantly
slower than its 'non-interactive' counterpart, but it is  more  convenient
when you call it from command line.
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreequeryresultstagsi(kdtree kdt, integer_1d_array &amp;tags);
</pre>
<a name=sub_kdtreequeryresultsx></a><h6 class=pageheader>kdtreequeryresultsx Function</h6>
<hr width=600 align=left>
<pre class=narration>
X-values from last query.

This function retuns results stored in  the  internal  buffer  of  kd-tree
object. If you performed buffered requests (ones which  use  instances  of
kdtreerequestbuffer class), you  should  call  buffered  version  of  this
function - kdtreetsqueryresultsx().

Inputs:
    KDT     -   KD-tree
    X       -   possibly pre-allocated buffer. If X is too small to store
                result, it is resized. If size(X) is enough to store
                result, it is left unchanged.

Outputs:
    X       -   rows are filled with X-values

NOTES
1. points are ordered by distance from the query point (first = closest)
2. if  XY is larger than required to store result, only leading part  will
   be overwritten; trailing part will be left unchanged. So  if  on  input
   XY = [[A,B],[C,D]], and result is [1,2],  then  on  exit  we  will  get
   XY = [[1,2],[C,D]]. This is done purposely to increase performance;  if
   you want function  to  resize  array  according  to  result  size,  use
   function with same name and suffix 'I'.

SEE ALSO
* KDTreeQueryResultsXY()            X- and Y-values
* KDTreeQueryResultsTags()          tag values
* KDTreeQueryResultsDistances()     distances
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreequeryresultsx(kdtree kdt, real_2d_array &amp;x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nneighbor_d_1 class=nav>nneighbor_d_1</a> ]</p>
<a name=sub_kdtreequeryresultsxi></a><h6 class=pageheader>kdtreequeryresultsxi Function</h6>
<hr width=600 align=left>
<pre class=narration>
X-values from last query; 'interactive' variant for languages like  Python
which   support    constructs   like  &quot;X = KDTreeQueryResultsXI(KDT)&quot;  and
interactive mode of interpreter.

This function allocates new array on each call,  so  it  is  significantly
slower than its 'non-interactive' counterpart, but it is  more  convenient
when you call it from command line.
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreequeryresultsxi(kdtree kdt, real_2d_array &amp;x);
</pre>
<a name=sub_kdtreequeryresultsxy></a><h6 class=pageheader>kdtreequeryresultsxy Function</h6>
<hr width=600 align=left>
<pre class=narration>
X- and Y-values from last query

This function retuns results stored in  the  internal  buffer  of  kd-tree
object. If you performed buffered requests (ones which  use  instances  of
kdtreerequestbuffer class), you  should  call  buffered  version  of  this
function - kdtreetsqueryresultsxy().

Inputs:
    KDT     -   KD-tree
    XY      -   possibly pre-allocated buffer. If XY is too small to store
                result, it is resized. If size(XY) is enough to store
                result, it is left unchanged.

Outputs:
    XY      -   rows are filled with points: first NX columns with
                X-values, next NY columns - with Y-values.

NOTES
1. points are ordered by distance from the query point (first = closest)
2. if  XY is larger than required to store result, only leading part  will
   be overwritten; trailing part will be left unchanged. So  if  on  input
   XY = [[A,B],[C,D]], and result is [1,2],  then  on  exit  we  will  get
   XY = [[1,2],[C,D]]. This is done purposely to increase performance;  if
   you want function  to  resize  array  according  to  result  size,  use
   function with same name and suffix 'I'.

SEE ALSO
* KDTreeQueryResultsX()             X-values
* KDTreeQueryResultsTags()          tag values
* KDTreeQueryResultsDistances()     distances
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreequeryresultsxy(kdtree kdt, real_2d_array &amp;xy);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nneighbor_d_1 class=nav>nneighbor_d_1</a> ]</p>
<a name=sub_kdtreequeryresultsxyi></a><h6 class=pageheader>kdtreequeryresultsxyi Function</h6>
<hr width=600 align=left>
<pre class=narration>
XY-values from last query; 'interactive' variant for languages like Python
which   support    constructs   like &quot;XY = KDTreeQueryResultsXYI(KDT)&quot; and
interactive mode of interpreter.

This function allocates new array on each call,  so  it  is  significantly
slower than its 'non-interactive' counterpart, but it is  more  convenient
when you call it from command line.
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreequeryresultsxyi(kdtree kdt, real_2d_array &amp;xy);
</pre>
<a name=sub_kdtreequeryrnn></a><h6 class=pageheader>kdtreequeryrnn Function</h6>
<hr width=600 align=left>
<pre class=narration>
R-NN query: all points within R-sphere centered at X, ordered by  distance
between point and X (by ascending).

NOTE: it is also possible to perform undordered queries performed by means
      of kdtreequeryrnnu() and kdtreetsqueryrnnu() functions. Such queries
      are faster because we do not have to use heap structure for sorting.

IMPORTANT: this function can not be used in multithreaded code because  it
           uses internal temporary buffer of kd-tree object, which can not
           be shared between multiple threads.  If  you  want  to  perform
           parallel requests, use function  which  uses  external  request
           buffer: kdtreetsqueryrnn() (&quot;Ts&quot; stands for &quot;thread-safe&quot;).

Inputs:
    KDT         -   KD-tree
    X           -   point, array[0..NX-1].
    R           -   radius of sphere (in corresponding norm), R &gt; 0
    SelfMatch   -   whether self-matches are allowed:
                    * if True, nearest neighbor may be the point itself
                      (if it exists in original dataset)
                    * if False, then only points with non-zero distance
                      are returned
                    * if not given, considered True

Result:
    number of neighbors found, &ge; 0

This  subroutine  performs  query  and  stores  its result in the internal
structures of the KD-tree. You can use  following  subroutines  to  obtain
actual results:
* KDTreeQueryResultsX() to get X-values
* KDTreeQueryResultsXY() to get X- and Y-values
* KDTreeQueryResultsTags() to get tag values
* KDTreeQueryResultsDistances() to get distances
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreequeryrnn(kdtree kdt, real_1d_array x, <b>double</b> r, <b>bool</b> selfmatch);
ae_int_t kdtreequeryrnn(kdtree kdt, real_1d_array x, <b>double</b> r);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nneighbor_d_1 class=nav>nneighbor_d_1</a> ]</p>
<a name=sub_kdtreequeryrnnu></a><h6 class=pageheader>kdtreequeryrnnu Function</h6>
<hr width=600 align=left>
<pre class=narration>
R-NN query: all points within R-sphere  centered  at  X,  no  ordering  by
distance as undicated by &quot;U&quot; suffix (faster that ordered query, for  large
queries - significantly faster).

IMPORTANT: this function can not be used in multithreaded code because  it
           uses internal temporary buffer of kd-tree object, which can not
           be shared between multiple threads.  If  you  want  to  perform
           parallel requests, use function  which  uses  external  request
           buffer: kdtreetsqueryrnn() (&quot;Ts&quot; stands for &quot;thread-safe&quot;).

Inputs:
    KDT         -   KD-tree
    X           -   point, array[0..NX-1].
    R           -   radius of sphere (in corresponding norm), R &gt; 0
    SelfMatch   -   whether self-matches are allowed:
                    * if True, nearest neighbor may be the point itself
                      (if it exists in original dataset)
                    * if False, then only points with non-zero distance
                      are returned
                    * if not given, considered True

Result:
    number of neighbors found, &ge; 0

This  subroutine  performs  query  and  stores  its result in the internal
structures of the KD-tree. You can use  following  subroutines  to  obtain
actual results:
* KDTreeQueryResultsX() to get X-values
* KDTreeQueryResultsXY() to get X- and Y-values
* KDTreeQueryResultsTags() to get tag values
* KDTreeQueryResultsDistances() to get distances

As indicated by &quot;U&quot; suffix, this function returns unordered results.
ALGLIB: Copyright 01.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreequeryrnnu(kdtree kdt, real_1d_array x, <b>double</b> r, <b>bool</b> selfmatch);
ae_int_t kdtreequeryrnnu(kdtree kdt, real_1d_array x, <b>double</b> r);
</pre>
<a name=sub_kdtreeserialize></a><h6 class=pageheader>kdtreeserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: serialization
These functions serialize a data structure to a C++ string or stream.
* serialization can be freely moved across 32-bit and 64-bit systems,
  and different byte orders. For example, you can serialize a string
  on a SPARC and unserialize it on an x86.
* ALGLIB++ serialization is compatible with serialization in ALGLIB,
  in both directions.
Important properties of s_out:
* it contains alphanumeric characters, dots, underscores, minus signs
* these symbols are grouped into words, which are separated by spaces
  and Windows-style (CR+LF) newlines
ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreeserialize(kdtree &amp;obj, std::string &amp;s_out);
<b>void</b> kdtreeserialize(kdtree &amp;obj, std::ostream &amp;s_out);
</pre>
<a name=sub_kdtreetsqueryaknn></a><h6 class=pageheader>kdtreetsqueryaknn Function</h6>
<hr width=600 align=left>
<pre class=narration>
K-NN query: approximate K nearest neighbors, using thread-local buffer.

You can call this function from multiple threads for same kd-tree instance,
assuming that different instances of buffer object are passed to different
threads.

Inputs:
    KDT         -   KD-tree
    Buf         -   request buffer  object  created  for  this  particular
                    instance of kd-tree structure with kdtreecreaterequestbuffer()
                    function.
    X           -   point, array[0..NX-1].
    K           -   number of neighbors to return, K &ge; 1
    SelfMatch   -   whether self-matches are allowed:
                    * if True, nearest neighbor may be the point itself
                      (if it exists in original dataset)
                    * if False, then only points with non-zero distance
                      are returned
                    * if not given, considered True
    Eps         -   approximation factor, Eps &ge; 0. eps-approximate  nearest
                    neighbor  is  a  neighbor  whose distance from X is at
                    most (1+eps) times distance of true nearest neighbor.

Result:
    number of actual neighbors found (either K or N, if K &gt; N).

NOTES
    significant performance gain may be achieved only when Eps  is  is  on
    the order of magnitude of 1 or larger.

This  subroutine  performs  query  and  stores  its result in the internal
structures  of  the  buffer object. You can use following  subroutines  to
obtain these results (pay attention to &quot;buf&quot; in their names):
* KDTreeTsQueryResultsX() to get X-values
* KDTreeTsQueryResultsXY() to get X- and Y-values
* KDTreeTsQueryResultsTags() to get tag values
* KDTreeTsQueryResultsDistances() to get distances

IMPORTANT: kd-tree buffer should be used only with  KD-tree  object  which
           was used to initialize buffer. Any attempt to use biffer   with
           different object is dangerous - you  may  get  integrity  check
           failure (exception) because sizes of internal arrays do not fit
           to dimensions of KD-tree structure.
ALGLIB: Copyright 18.03.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreetsqueryaknn(kdtree kdt, kdtreerequestbuffer buf, real_1d_array x, ae_int_t k, <b>bool</b> selfmatch, <b>double</b> eps);
ae_int_t kdtreetsqueryaknn(kdtree kdt, kdtreerequestbuffer buf, real_1d_array x, ae_int_t k, <b>double</b> eps);
</pre>
<a name=sub_kdtreetsquerybox></a><h6 class=pageheader>kdtreetsquerybox Function</h6>
<hr width=600 align=left>
<pre class=narration>
Box query: all points within user-specified box, using thread-local buffer.

You can call this function from multiple threads for same kd-tree instance,
assuming that different instances of buffer object are passed to different
threads.

Inputs:
    KDT         -   KD-tree
    Buf         -   request buffer  object  created  for  this  particular
                    instance of kd-tree structure with kdtreecreaterequestbuffer()
                    function.
    BoxMin      -   lower bounds, array[0..NX-1].
    BoxMax      -   upper bounds, array[0..NX-1].

Result:
    number of actual neighbors found (in [0,N]).

This  subroutine  performs  query  and  stores  its result in the internal
structures  of  the  buffer object. You can use following  subroutines  to
obtain these results (pay attention to &quot;ts&quot; in their names):
* KDTreeTsQueryResultsX() to get X-values
* KDTreeTsQueryResultsXY() to get X- and Y-values
* KDTreeTsQueryResultsTags() to get tag values
* KDTreeTsQueryResultsDistances() returns zeros for this query

NOTE: this particular query returns unordered results, because there is no
      meaningful way of  ordering  points.  Furthermore,  no 'distance' is
      associated with points - it is either INSIDE  or OUTSIDE (so request
      for distances will return zeros).

IMPORTANT: kd-tree buffer should be used only with  KD-tree  object  which
           was used to initialize buffer. Any attempt to use biffer   with
           different object is dangerous - you  may  get  integrity  check
           failure (exception) because sizes of internal arrays do not fit
           to dimensions of KD-tree structure.
ALGLIB: Copyright 14.05.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreetsquerybox(kdtree kdt, kdtreerequestbuffer buf, real_1d_array boxmin, real_1d_array boxmax);
</pre>
<a name=sub_kdtreetsqueryknn></a><h6 class=pageheader>kdtreetsqueryknn Function</h6>
<hr width=600 align=left>
<pre class=narration>
K-NN query: K nearest neighbors, using external thread-local buffer.

You can call this function from multiple threads for same kd-tree instance,
assuming that different instances of buffer object are passed to different
threads.

Inputs:
    KDT         -   kd-tree
    Buf         -   request buffer  object  created  for  this  particular
                    instance of kd-tree structure with kdtreecreaterequestbuffer()
                    function.
    X           -   point, array[0..NX-1].
    K           -   number of neighbors to return, K &ge; 1
    SelfMatch   -   whether self-matches are allowed:
                    * if True, nearest neighbor may be the point itself
                      (if it exists in original dataset)
                    * if False, then only points with non-zero distance
                      are returned
                    * if not given, considered True

Result:
    number of actual neighbors found (either K or N, if K &gt; N).

This  subroutine  performs  query  and  stores  its result in the internal
structures  of  the  buffer object. You can use following  subroutines  to
obtain these results (pay attention to &quot;buf&quot; in their names):
* KDTreeTsQueryResultsX() to get X-values
* KDTreeTsQueryResultsXY() to get X- and Y-values
* KDTreeTsQueryResultsTags() to get tag values
* KDTreeTsQueryResultsDistances() to get distances

IMPORTANT: kd-tree buffer should be used only with  KD-tree  object  which
           was used to initialize buffer. Any attempt to use biffer   with
           different object is dangerous - you  may  get  integrity  check
           failure (exception) because sizes of internal arrays do not fit
           to dimensions of KD-tree structure.
ALGLIB: Copyright 18.03.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreetsqueryknn(kdtree kdt, kdtreerequestbuffer buf, real_1d_array x, ae_int_t k, <b>bool</b> selfmatch);
ae_int_t kdtreetsqueryknn(kdtree kdt, kdtreerequestbuffer buf, real_1d_array x, ae_int_t k);
</pre>
<a name=sub_kdtreetsqueryresultsdistances></a><h6 class=pageheader>kdtreetsqueryresultsdistances Function</h6>
<hr width=600 align=left>
<pre class=narration>
Distances from last query associated with kdtreerequestbuffer object.

This function retuns results stored in  the  internal  buffer  of  kd-tree
object. If you performed buffered requests (ones which  use  instances  of
kdtreerequestbuffer class), you  should  call  buffered  version  of  this
function - KDTreeTsqueryresultsdistances().

Inputs:
    KDT     -   KD-tree
    Buf     -   request  buffer  object  created   for   this   particular
                instance of kd-tree structure.
    R       -   possibly pre-allocated buffer. If X is too small to store
                result, it is resized. If size(X) is enough to store
                result, it is left unchanged.

Outputs:
    R       -   filled with distances (in corresponding norm)

NOTES
1. points are ordered by distance from the query point (first = closest)
2. if  XY is larger than required to store result, only leading part  will
   be overwritten; trailing part will be left unchanged. So  if  on  input
   XY = [[A,B],[C,D]], and result is [1,2],  then  on  exit  we  will  get
   XY = [[1,2],[C,D]]. This is done purposely to increase performance;  if
   you want function  to  resize  array  according  to  result  size,  use
   function with same name and suffix 'I'.

SEE ALSO
* KDTreeQueryResultsX()             X-values
* KDTreeQueryResultsXY()            X- and Y-values
* KDTreeQueryResultsTags()          tag values
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreetsqueryresultsdistances(kdtree kdt, kdtreerequestbuffer buf, real_1d_array &amp;r);
</pre>
<a name=sub_kdtreetsqueryresultstags></a><h6 class=pageheader>kdtreetsqueryresultstags Function</h6>
<hr width=600 align=left>
<pre class=narration>
Tags from last query associated with kdtreerequestbuffer object.

This function retuns results stored in  the  internal  buffer  of  kd-tree
object. If you performed buffered requests (ones which  use  instances  of
kdtreerequestbuffer class), you  should  call  buffered  version  of  this
function - KDTreeTsqueryresultstags().

Inputs:
    KDT     -   KD-tree
    Buf     -   request  buffer  object  created   for   this   particular
                instance of kd-tree structure.
    Tags    -   possibly pre-allocated buffer. If X is too small to store
                result, it is resized. If size(X) is enough to store
                result, it is left unchanged.

Outputs:
    Tags    -   filled with tags associated with points,
                or, when no tags were supplied, with zeros

NOTES
1. points are ordered by distance from the query point (first = closest)
2. if  XY is larger than required to store result, only leading part  will
   be overwritten; trailing part will be left unchanged. So  if  on  input
   XY = [[A,B],[C,D]], and result is [1,2],  then  on  exit  we  will  get
   XY = [[1,2],[C,D]]. This is done purposely to increase performance;  if
   you want function  to  resize  array  according  to  result  size,  use
   function with same name and suffix 'I'.

SEE ALSO
* KDTreeQueryResultsX()             X-values
* KDTreeQueryResultsXY()            X- and Y-values
* KDTreeQueryResultsDistances()     distances
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreetsqueryresultstags(kdtree kdt, kdtreerequestbuffer buf, integer_1d_array &amp;tags);
</pre>
<a name=sub_kdtreetsqueryresultsx></a><h6 class=pageheader>kdtreetsqueryresultsx Function</h6>
<hr width=600 align=left>
<pre class=narration>
X-values from last query associated with kdtreerequestbuffer object.

Inputs:
    KDT     -   KD-tree
    Buf     -   request  buffer  object  created   for   this   particular
                instance of kd-tree structure.
    X       -   possibly pre-allocated buffer. If X is too small to store
                result, it is resized. If size(X) is enough to store
                result, it is left unchanged.

Outputs:
    X       -   rows are filled with X-values

NOTES
1. points are ordered by distance from the query point (first = closest)
2. if  XY is larger than required to store result, only leading part  will
   be overwritten; trailing part will be left unchanged. So  if  on  input
   XY = [[A,B],[C,D]], and result is [1,2],  then  on  exit  we  will  get
   XY = [[1,2],[C,D]]. This is done purposely to increase performance;  if
   you want function  to  resize  array  according  to  result  size,  use
   function with same name and suffix 'I'.

SEE ALSO
* KDTreeQueryResultsXY()            X- and Y-values
* KDTreeQueryResultsTags()          tag values
* KDTreeQueryResultsDistances()     distances
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreetsqueryresultsx(kdtree kdt, kdtreerequestbuffer buf, real_2d_array &amp;x);
</pre>
<a name=sub_kdtreetsqueryresultsxy></a><h6 class=pageheader>kdtreetsqueryresultsxy Function</h6>
<hr width=600 align=left>
<pre class=narration>
X- and Y-values from last query associated with kdtreerequestbuffer object.

Inputs:
    KDT     -   KD-tree
    Buf     -   request  buffer  object  created   for   this   particular
                instance of kd-tree structure.
    XY      -   possibly pre-allocated buffer. If XY is too small to store
                result, it is resized. If size(XY) is enough to store
                result, it is left unchanged.

Outputs:
    XY      -   rows are filled with points: first NX columns with
                X-values, next NY columns - with Y-values.

NOTES
1. points are ordered by distance from the query point (first = closest)
2. if  XY is larger than required to store result, only leading part  will
   be overwritten; trailing part will be left unchanged. So  if  on  input
   XY = [[A,B],[C,D]], and result is [1,2],  then  on  exit  we  will  get
   XY = [[1,2],[C,D]]. This is done purposely to increase performance;  if
   you want function  to  resize  array  according  to  result  size,  use
   function with same name and suffix 'I'.

SEE ALSO
* KDTreeQueryResultsX()             X-values
* KDTreeQueryResultsTags()          tag values
* KDTreeQueryResultsDistances()     distances
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreetsqueryresultsxy(kdtree kdt, kdtreerequestbuffer buf, real_2d_array &amp;xy);
</pre>
<a name=sub_kdtreetsqueryrnn></a><h6 class=pageheader>kdtreetsqueryrnn Function</h6>
<hr width=600 align=left>
<pre class=narration>
R-NN query: all points within  R-sphere  centered  at  X,  using  external
thread-local buffer, sorted by distance between point and X (by ascending)

You can call this function from multiple threads for same kd-tree instance,
assuming that different instances of buffer object are passed to different
threads.

NOTE: it is also possible to perform undordered queries performed by means
      of kdtreequeryrnnu() and kdtreetsqueryrnnu() functions. Such queries
      are faster because we do not have to use heap structure for sorting.

Inputs:
    KDT         -   KD-tree
    Buf         -   request buffer  object  created  for  this  particular
                    instance of kd-tree structure with kdtreecreaterequestbuffer()
                    function.
    X           -   point, array[0..NX-1].
    R           -   radius of sphere (in corresponding norm), R &gt; 0
    SelfMatch   -   whether self-matches are allowed:
                    * if True, nearest neighbor may be the point itself
                      (if it exists in original dataset)
                    * if False, then only points with non-zero distance
                      are returned
                    * if not given, considered True

Result:
    number of neighbors found, &ge; 0

This  subroutine  performs  query  and  stores  its result in the internal
structures  of  the  buffer object. You can use following  subroutines  to
obtain these results (pay attention to &quot;buf&quot; in their names):
* KDTreeTsQueryResultsX() to get X-values
* KDTreeTsQueryResultsXY() to get X- and Y-values
* KDTreeTsQueryResultsTags() to get tag values
* KDTreeTsQueryResultsDistances() to get distances

IMPORTANT: kd-tree buffer should be used only with  KD-tree  object  which
           was used to initialize buffer. Any attempt to use biffer   with
           different object is dangerous - you  may  get  integrity  check
           failure (exception) because sizes of internal arrays do not fit
           to dimensions of KD-tree structure.
ALGLIB: Copyright 18.03.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreetsqueryrnn(kdtree kdt, kdtreerequestbuffer buf, real_1d_array x, <b>double</b> r, <b>bool</b> selfmatch);
ae_int_t kdtreetsqueryrnn(kdtree kdt, kdtreerequestbuffer buf, real_1d_array x, <b>double</b> r);
</pre>
<a name=sub_kdtreetsqueryrnnu></a><h6 class=pageheader>kdtreetsqueryrnnu Function</h6>
<hr width=600 align=left>
<pre class=narration>
R-NN query: all points within  R-sphere  centered  at  X,  using  external
thread-local buffer, no ordering by distance as undicated  by  &quot;U&quot;  suffix
(faster that ordered query, for large queries - significantly faster).

You can call this function from multiple threads for same kd-tree instance,
assuming that different instances of buffer object are passed to different
threads.

Inputs:
    KDT         -   KD-tree
    Buf         -   request buffer  object  created  for  this  particular
                    instance of kd-tree structure with kdtreecreaterequestbuffer()
                    function.
    X           -   point, array[0..NX-1].
    R           -   radius of sphere (in corresponding norm), R &gt; 0
    SelfMatch   -   whether self-matches are allowed:
                    * if True, nearest neighbor may be the point itself
                      (if it exists in original dataset)
                    * if False, then only points with non-zero distance
                      are returned
                    * if not given, considered True

Result:
    number of neighbors found, &ge; 0

This  subroutine  performs  query  and  stores  its result in the internal
structures  of  the  buffer object. You can use following  subroutines  to
obtain these results (pay attention to &quot;buf&quot; in their names):
* KDTreeTsQueryResultsX() to get X-values
* KDTreeTsQueryResultsXY() to get X- and Y-values
* KDTreeTsQueryResultsTags() to get tag values
* KDTreeTsQueryResultsDistances() to get distances

As indicated by &quot;U&quot; suffix, this function returns unordered results.

IMPORTANT: kd-tree buffer should be used only with  KD-tree  object  which
           was used to initialize buffer. Any attempt to use biffer   with
           different object is dangerous - you  may  get  integrity  check
           failure (exception) because sizes of internal arrays do not fit
           to dimensions of KD-tree structure.
ALGLIB: Copyright 18.03.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t kdtreetsqueryrnnu(kdtree kdt, kdtreerequestbuffer buf, real_1d_array x, <b>double</b> r, <b>bool</b> selfmatch);
ae_int_t kdtreetsqueryrnnu(kdtree kdt, kdtreerequestbuffer buf, real_1d_array x, <b>double</b> r);
</pre>
<a name=sub_kdtreeunserialize></a><h6 class=pageheader>kdtreeunserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: unserialization
These functions unserialize a data structure from a C++ string or stream.
Important properties of s_in:
* any combination of spaces, tabs, Windows or Unix stype newlines can
  be used as separators, so as to allow flexible reformatting of the
  stream or string from text or XML files.
* But you should not insert separators into the middle of the "words"
  nor you should change case of letters.
ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kdtreeunserialize(<b>const</b> std::string &amp;s_in, kdtree &amp;obj);
<b>void</b> kdtreeunserialize(<b>const</b> std::istream &amp;s_in, kdtree &amp;obj);
</pre>
<a name=example_nneighbor_d_1></a><h6 class=pageheader>nneighbor_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;AlgLibMisc.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_2d_array a = <font color=blue><b>&quot;[[0,0],[0,1],[1,0],[1,1]]&quot;</b></font>;
   ae_int_t nx = 2;
   ae_int_t ny = 0;
   ae_int_t normtype = 2;
   kdtree kdt;
   real_1d_array x;
   real_2d_array r = <font color=blue><b>&quot;[[]]&quot;</b></font>;
   ae_int_t k;
   kdtreebuild(a, nx, ny, normtype, kdt);
   x = <font color=blue><b>&quot;[-1,0]&quot;</b></font>;
   k = kdtreequeryknn(kdt, x, 1);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(k)); <font color=navy>// EXPECTED: 1</font>
   kdtreequeryresultsx(kdt, r);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, r.tostring(1).c_str()); <font color=navy>// EXPECTED: [[0,0]]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_nneighbor_d_2></a><h6 class=pageheader>nneighbor_d_2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;AlgLibMisc.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_2d_array a = <font color=blue><b>&quot;[[0,0],[0,1],[1,0],[1,1]]&quot;</b></font>;
   ae_int_t nx = 2;
   ae_int_t ny = 0;
   ae_int_t normtype = 2;
   kdtree kdt0;
   kdtree kdt1;
   std::string s;
   real_1d_array x;
   real_2d_array r0 = <font color=blue><b>&quot;[[]]&quot;</b></font>;
   real_2d_array r1 = <font color=blue><b>&quot;[[]]&quot;</b></font>;
<font color=navy>// Build tree and serialize it</font>
   kdtreebuild(a, nx, ny, normtype, kdt0);
   kdtreeserialize(kdt0, s);
   kdtreeunserialize(s, kdt1);
<font color=navy>// Compare results from KNN queries</font>
   x = <font color=blue><b>&quot;[-1,0]&quot;</b></font>;
   kdtreequeryknn(kdt0, x, 1);
   kdtreequeryresultsx(kdt0, r0);
   kdtreequeryknn(kdt1, x, 1);
   kdtreequeryresultsx(kdt1, r1);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, r0.tostring(1).c_str()); <font color=navy>// EXPECTED: [[0,0]]</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, r1.tostring(1).c_str()); <font color=navy>// EXPECTED: [[0,0]]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_xdebug></a><h4 class=pageheader>8.1.3. xdebug Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_xdebugrecord1 class=toc>xdebugrecord1</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_xdebugb1appendcopy class=toc>xdebugb1appendcopy</a> |
<a href=#sub_xdebugb1count class=toc>xdebugb1count</a> |
<a href=#sub_xdebugb1not class=toc>xdebugb1not</a> |
<a href=#sub_xdebugb1outeven class=toc>xdebugb1outeven</a> |
<a href=#sub_xdebugb2count class=toc>xdebugb2count</a> |
<a href=#sub_xdebugb2not class=toc>xdebugb2not</a> |
<a href=#sub_xdebugb2outsin class=toc>xdebugb2outsin</a> |
<a href=#sub_xdebugb2transpose class=toc>xdebugb2transpose</a> |
<a href=#sub_xdebugc1appendcopy class=toc>xdebugc1appendcopy</a> |
<a href=#sub_xdebugc1neg class=toc>xdebugc1neg</a> |
<a href=#sub_xdebugc1outeven class=toc>xdebugc1outeven</a> |
<a href=#sub_xdebugc1sum class=toc>xdebugc1sum</a> |
<a href=#sub_xdebugc2neg class=toc>xdebugc2neg</a> |
<a href=#sub_xdebugc2outsincos class=toc>xdebugc2outsincos</a> |
<a href=#sub_xdebugc2sum class=toc>xdebugc2sum</a> |
<a href=#sub_xdebugc2transpose class=toc>xdebugc2transpose</a> |
<a href=#sub_xdebugi1appendcopy class=toc>xdebugi1appendcopy</a> |
<a href=#sub_xdebugi1neg class=toc>xdebugi1neg</a> |
<a href=#sub_xdebugi1outeven class=toc>xdebugi1outeven</a> |
<a href=#sub_xdebugi1sum class=toc>xdebugi1sum</a> |
<a href=#sub_xdebugi2neg class=toc>xdebugi2neg</a> |
<a href=#sub_xdebugi2outsin class=toc>xdebugi2outsin</a> |
<a href=#sub_xdebugi2sum class=toc>xdebugi2sum</a> |
<a href=#sub_xdebugi2transpose class=toc>xdebugi2transpose</a> |
<a href=#sub_xdebuginitrecord1 class=toc>xdebuginitrecord1</a> |
<a href=#sub_xdebugmaskedbiasedproductsum class=toc>xdebugmaskedbiasedproductsum</a> |
<a href=#sub_xdebugr1appendcopy class=toc>xdebugr1appendcopy</a> |
<a href=#sub_xdebugr1neg class=toc>xdebugr1neg</a> |
<a href=#sub_xdebugr1outeven class=toc>xdebugr1outeven</a> |
<a href=#sub_xdebugr1sum class=toc>xdebugr1sum</a> |
<a href=#sub_xdebugr2neg class=toc>xdebugr2neg</a> |
<a href=#sub_xdebugr2outsin class=toc>xdebugr2outsin</a> |
<a href=#sub_xdebugr2sum class=toc>xdebugr2sum</a> |
<a href=#sub_xdebugr2transpose class=toc>xdebugr2transpose</a>
]</font>
</div>
<a name=struct_xdebugrecord1></a><h6 class=pageheader>xdebugrecord1 Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> xdebugrecord1 {
   ae_int_t i;
   complex c;
   real_1d_array a;
};
</pre>
<a name=sub_xdebugb1appendcopy></a><h6 class=pageheader>xdebugb1appendcopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Appends copy of array to itself.
Array is passed using &quot;var&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugb1appendcopy(boolean_1d_array &amp;a);
</pre>
<a name=sub_xdebugb1count></a><h6 class=pageheader>xdebugb1count Function</h6>
<hr width=600 align=left>
<pre class=narration>
Counts number of True values in the boolean 1D array.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t xdebugb1count(boolean_1d_array a);
</pre>
<a name=sub_xdebugb1not></a><h6 class=pageheader>xdebugb1not Function</h6>
<hr width=600 align=left>
<pre class=narration>
Replace all values in array by NOT(a[i]).
Array is passed using &quot;shared&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugb1not(boolean_1d_array a);
</pre>
<a name=sub_xdebugb1outeven></a><h6 class=pageheader>xdebugb1outeven Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generate N-element array with even-numbered elements set to True.
Array is passed using &quot;out&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugb1outeven(ae_int_t n, boolean_1d_array &amp;a);
</pre>
<a name=sub_xdebugb2count></a><h6 class=pageheader>xdebugb2count Function</h6>
<hr width=600 align=left>
<pre class=narration>
Counts number of True values in the boolean 2D array.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t xdebugb2count(boolean_2d_array a);
</pre>
<a name=sub_xdebugb2not></a><h6 class=pageheader>xdebugb2not Function</h6>
<hr width=600 align=left>
<pre class=narration>
Replace all values in array by NOT(a[i]).
Array is passed using &quot;shared&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugb2not(boolean_2d_array a);
</pre>
<a name=sub_xdebugb2outsin></a><h6 class=pageheader>xdebugb2outsin Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generate MxN matrix with elements set to &quot;sin(3*I+5*J) &gt; 0&quot;
Array is passed using &quot;out&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugb2outsin(ae_int_t m, ae_int_t n, boolean_2d_array &amp;a);
</pre>
<a name=sub_xdebugb2transpose></a><h6 class=pageheader>xdebugb2transpose Function</h6>
<hr width=600 align=left>
<pre class=narration>
Transposes array.
Array is passed using &quot;var&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugb2transpose(boolean_2d_array &amp;a);
</pre>
<a name=sub_xdebugc1appendcopy></a><h6 class=pageheader>xdebugc1appendcopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Appends copy of array to itself.
Array is passed using &quot;var&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugc1appendcopy(complex_1d_array &amp;a);
</pre>
<a name=sub_xdebugc1neg></a><h6 class=pageheader>xdebugc1neg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Replace all values in array by -A[I]
Array is passed using &quot;shared&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugc1neg(complex_1d_array a);
</pre>
<a name=sub_xdebugc1outeven></a><h6 class=pageheader>xdebugc1outeven Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generate N-element array with even-numbered A[K] set to (x,y) = (K*0.25, K*0.125)
and odd-numbered ones are set to 0.

Array is passed using &quot;out&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugc1outeven(ae_int_t n, complex_1d_array &amp;a);
</pre>
<a name=sub_xdebugc1sum></a><h6 class=pageheader>xdebugc1sum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns sum of elements in the array.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
complex xdebugc1sum(complex_1d_array a);
</pre>
<a name=sub_xdebugc2neg></a><h6 class=pageheader>xdebugc2neg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Replace all values in array by -a[i,j]
Array is passed using &quot;shared&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugc2neg(complex_2d_array a);
</pre>
<a name=sub_xdebugc2outsincos></a><h6 class=pageheader>xdebugc2outsincos Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generate MxN matrix with elements set to &quot;sin(3*I+5*J),cos(3*I+5*J)&quot;
Array is passed using &quot;out&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugc2outsincos(ae_int_t m, ae_int_t n, complex_2d_array &amp;a);
</pre>
<a name=sub_xdebugc2sum></a><h6 class=pageheader>xdebugc2sum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns sum of elements in the array.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
complex xdebugc2sum(complex_2d_array a);
</pre>
<a name=sub_xdebugc2transpose></a><h6 class=pageheader>xdebugc2transpose Function</h6>
<hr width=600 align=left>
<pre class=narration>
Transposes array.
Array is passed using &quot;var&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugc2transpose(complex_2d_array &amp;a);
</pre>
<a name=sub_xdebugi1appendcopy></a><h6 class=pageheader>xdebugi1appendcopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Appends copy of array to itself.
Array is passed using &quot;var&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugi1appendcopy(integer_1d_array &amp;a);
</pre>
<a name=sub_xdebugi1neg></a><h6 class=pageheader>xdebugi1neg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Replace all values in array by -A[I]
Array is passed using &quot;shared&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugi1neg(integer_1d_array a);
</pre>
<a name=sub_xdebugi1outeven></a><h6 class=pageheader>xdebugi1outeven Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generate N-element array with even-numbered A[I] set to I, and odd-numbered
ones set to 0.

Array is passed using &quot;out&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugi1outeven(ae_int_t n, integer_1d_array &amp;a);
</pre>
<a name=sub_xdebugi1sum></a><h6 class=pageheader>xdebugi1sum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns sum of elements in the array.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t xdebugi1sum(integer_1d_array a);
</pre>
<a name=sub_xdebugi2neg></a><h6 class=pageheader>xdebugi2neg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Replace all values in array by -a[i,j]
Array is passed using &quot;shared&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugi2neg(integer_2d_array a);
</pre>
<a name=sub_xdebugi2outsin></a><h6 class=pageheader>xdebugi2outsin Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generate MxN matrix with elements set to &quot;Sign(sin(3*I+5*J))&quot;
Array is passed using &quot;out&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugi2outsin(ae_int_t m, ae_int_t n, integer_2d_array &amp;a);
</pre>
<a name=sub_xdebugi2sum></a><h6 class=pageheader>xdebugi2sum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns sum of elements in the array.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t xdebugi2sum(integer_2d_array a);
</pre>
<a name=sub_xdebugi2transpose></a><h6 class=pageheader>xdebugi2transpose Function</h6>
<hr width=600 align=left>
<pre class=narration>
Transposes array.
Array is passed using &quot;var&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugi2transpose(integer_2d_array &amp;a);
</pre>
<a name=sub_xdebuginitrecord1></a><h6 class=pageheader>xdebuginitrecord1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Creates and returns XDebugRecord1 structure:
* integer and complex fields of Rec1 are set to 1 and 1+i correspondingly
* array field of Rec1 is set to [2,3]
ALGLIB: Copyright 27.05.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebuginitrecord1(xdebugrecord1 &amp;rec1);
</pre>
<a name=sub_xdebugmaskedbiasedproductsum></a><h6 class=pageheader>xdebugmaskedbiasedproductsum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns sum of a[i,j]*(1+b[i,j]) such that c[i,j] is True
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> xdebugmaskedbiasedproductsum(ae_int_t m, ae_int_t n, real_2d_array a, real_2d_array b, boolean_2d_array c);
</pre>
<a name=sub_xdebugr1appendcopy></a><h6 class=pageheader>xdebugr1appendcopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Appends copy of array to itself.
Array is passed using &quot;var&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugr1appendcopy(real_1d_array &amp;a);
</pre>
<a name=sub_xdebugr1neg></a><h6 class=pageheader>xdebugr1neg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Replace all values in array by -A[I]
Array is passed using &quot;shared&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugr1neg(real_1d_array a);
</pre>
<a name=sub_xdebugr1outeven></a><h6 class=pageheader>xdebugr1outeven Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generate N-element array with even-numbered A[I] set to I*0.25,
and odd-numbered ones are set to 0.

Array is passed using &quot;out&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugr1outeven(ae_int_t n, real_1d_array &amp;a);
</pre>
<a name=sub_xdebugr1sum></a><h6 class=pageheader>xdebugr1sum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns sum of elements in the array.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> xdebugr1sum(real_1d_array a);
</pre>
<a name=sub_xdebugr2neg></a><h6 class=pageheader>xdebugr2neg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Replace all values in array by -a[i,j]
Array is passed using &quot;shared&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugr2neg(real_2d_array a);
</pre>
<a name=sub_xdebugr2outsin></a><h6 class=pageheader>xdebugr2outsin Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generate MxN matrix with elements set to &quot;sin(3*I+5*J)&quot;
Array is passed using &quot;out&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugr2outsin(ae_int_t m, ae_int_t n, real_2d_array &amp;a);
</pre>
<a name=sub_xdebugr2sum></a><h6 class=pageheader>xdebugr2sum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns sum of elements in the array.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> xdebugr2sum(real_2d_array a);
</pre>
<a name=sub_xdebugr2transpose></a><h6 class=pageheader>xdebugr2transpose Function</h6>
<hr width=600 align=left>
<pre class=narration>
Transposes array.
Array is passed using &quot;var&quot; convention.
ALGLIB: Copyright 11.10.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> xdebugr2transpose(real_2d_array &amp;a);
</pre>
</p>
<p>
<a name=pck_DataAnalysis class=sheader></a><h3>8.2. DataAnalysis Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_bdss class=toc>bdss</a></td><td>Basic dataset functions</td></tr>
<tr align=left valign=top><td><a href=#unit_clustering class=toc>clustering</a></td><td>Clustering functions (hierarchical, k-means, k-means++)</td></tr>
<tr align=left valign=top><td><a href=#unit_datacomp class=toc>datacomp</a></td><td>Backward compatibility functions</td></tr>
<tr align=left valign=top><td><a href=#unit_dforest class=toc>dforest</a></td><td>Decision forest classifier (regression model)</td></tr>
<tr align=left valign=top><td><a href=#unit_filters class=toc>filters</a></td><td>Different filters used in data analysis</td></tr>
<tr align=left valign=top><td><a href=#unit_knn class=toc>knn</a></td><td>K Nearest Neighbors classification/regression</td></tr>
<tr align=left valign=top><td><a href=#unit_lda class=toc>lda</a></td><td>Linear discriminant analysis</td></tr>
<tr align=left valign=top><td><a href=#unit_linreg class=toc>linreg</a></td><td>Linear models</td></tr>
<tr align=left valign=top><td><a href=#unit_logit class=toc>logit</a></td><td>Logit models</td></tr>
<tr align=left valign=top><td><a href=#unit_mcpd class=toc>mcpd</a></td><td>Markov Chains for Population/Proportional Data</td></tr>
<tr align=left valign=top><td><a href=#unit_mlpbase class=toc>mlpbase</a></td><td>Basic functions for neural networks</td></tr>
<tr align=left valign=top><td><a href=#unit_mlpe class=toc>mlpe</a></td><td>Basic functions for neural ensemble models</td></tr>
<tr align=left valign=top><td><a href=#unit_mlptrain class=toc>mlptrain</a></td><td>Neural network training</td></tr>
<tr align=left valign=top><td><a href=#unit_pca class=toc>pca</a></td><td>Principal component analysis</td></tr>
<tr align=left valign=top><td><a href=#unit_ssa class=toc>ssa</a></td><td>Singular Spectrum Analysis</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_bdss></a><h4 class=pageheader>8.2.1. bdss Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_dsoptimalsplit2 class=toc>dsoptimalsplit2</a> |
<a href=#sub_dsoptimalsplit2fast class=toc>dsoptimalsplit2fast</a>
]</font>
</div>
<a name=sub_dsoptimalsplit2></a><h6 class=pageheader>dsoptimalsplit2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Optimal binary classification

Algorithms finds optimal (=with minimal cross-entropy) binary partition.
Internal subroutine.

Inputs:
    A       -   array[0..N-1], variable
    C       -   array[0..N-1], class numbers (0 or 1).
    N       -   array size

Outputs:
    Info    -   completetion code:
                * -3, all values of A[] are same (partition is impossible)
                * -2, one of C[] is incorrect (&lt; 0, &gt; 1)
                * -1, incorrect pararemets were passed (N &le; 0).
                *  1, OK
    Threshold-  partiton boundary. Left part contains values which are
                strictly less than Threshold. Right part contains values
                which are greater than or equal to Threshold.
    PAL, PBL-   probabilities P(0|v &lt; Threshold) and P(1|v &lt; Threshold)
    PAR, PBR-   probabilities P(0|v &ge; Threshold) and P(1|v &ge; Threshold)
    CVE     -   cross-validation estimate of cross-entropy
ALGLIB: Copyright 22.05.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dsoptimalsplit2(real_1d_array a, integer_1d_array c, ae_int_t n, ae_int_t &amp;info, <b>double</b> &amp;threshold, <b>double</b> &amp;pal, <b>double</b> &amp;pbl, <b>double</b> &amp;par, <b>double</b> &amp;pbr, <b>double</b> &amp;cve);
</pre>
<a name=sub_dsoptimalsplit2fast></a><h6 class=pageheader>dsoptimalsplit2fast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Optimal partition, internal subroutine. Fast version.

Accepts:
    A       array[0..N-1]       array of attributes     array[0..N-1]
    C       array[0..N-1]       array of class labels
    TiesBuf array[0..N]         temporaries (ties)
    CntBuf  array[0..2*NC-1]    temporaries (counts)
    Alpha                       centering factor (0 &le; alpha &le; 1, recommended value - 0.05)
    BufR    array[0..N-1]       temporaries
    BufI    array[0..N-1]       temporaries

Outputs:
    Info    error code (&quot;&gt; 0&quot;=OK, &quot;&lt; 0&quot;=bad)
    RMS     training set RMS error
    CVRMS   leave-one-out RMS error

Note:
    content of all arrays is changed by subroutine;
    it doesn't allocate temporaries.
ALGLIB: Copyright 11.12.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dsoptimalsplit2fast(real_1d_array &amp;a, integer_1d_array &amp;c, integer_1d_array &amp;tiesbuf, integer_1d_array &amp;cntbuf, real_1d_array &amp;bufr, integer_1d_array &amp;bufi, ae_int_t n, ae_int_t nc, <b>double</b> alpha, ae_int_t &amp;info, <b>double</b> &amp;threshold, <b>double</b> &amp;rms, <b>double</b> &amp;cvrms);
</pre>
<a name=unit_clustering></a><h4 class=pageheader>8.2.2. clustering Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_ahcreport class=toc>ahcreport</a> |
<a href=#struct_clusterizerstate class=toc>clusterizerstate</a> |
<a href=#struct_kmeansreport class=toc>kmeansreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_clusterizercreate class=toc>clusterizercreate</a> |
<a href=#sub_clusterizergetdistances class=toc>clusterizergetdistances</a> |
<a href=#sub_clusterizergetkclusters class=toc>clusterizergetkclusters</a> |
<a href=#sub_clusterizerrunahc class=toc>clusterizerrunahc</a> |
<a href=#sub_clusterizerrunkmeans class=toc>clusterizerrunkmeans</a> |
<a href=#sub_clusterizerseparatedbycorr class=toc>clusterizerseparatedbycorr</a> |
<a href=#sub_clusterizerseparatedbydist class=toc>clusterizerseparatedbydist</a> |
<a href=#sub_clusterizersetahcalgo class=toc>clusterizersetahcalgo</a> |
<a href=#sub_clusterizersetdistances class=toc>clusterizersetdistances</a> |
<a href=#sub_clusterizersetkmeansinit class=toc>clusterizersetkmeansinit</a> |
<a href=#sub_clusterizersetkmeanslimits class=toc>clusterizersetkmeanslimits</a> |
<a href=#sub_clusterizersetpoints class=toc>clusterizersetpoints</a> |
<a href=#sub_clusterizersetseed class=toc>clusterizersetseed</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_clst_ahc class=toc>clst_ahc</a></td><td width=15>&nbsp;</td><td>Simple hierarchical clusterization with Euclidean distance function</td></tr>
<tr align=left valign=top><td><a href=#example_clst_distance class=toc>clst_distance</a></td><td width=15>&nbsp;</td><td>Clusterization with different metric types</td></tr>
<tr align=left valign=top><td><a href=#example_clst_kclusters class=toc>clst_kclusters</a></td><td width=15>&nbsp;</td><td>Obtaining K top clusters from clusterization tree</td></tr>
<tr align=left valign=top><td><a href=#example_clst_kmeans class=toc>clst_kmeans</a></td><td width=15>&nbsp;</td><td>Simple k-means clusterization</td></tr>
<tr align=left valign=top><td><a href=#example_clst_linkage class=toc>clst_linkage</a></td><td width=15>&nbsp;</td><td>Clusterization with different linkage types</td></tr>
</table>
</div>
<a name=struct_ahcreport></a><h6 class=pageheader>ahcreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure  is used to store results of the agglomerative hierarchical
clustering (AHC).

Following information is returned:

* TerminationType - completion code:
  * 1   for successful completion of algorithm
  * -5  inappropriate combination of  clustering  algorithm  and  distance
        function was used. As for now, it  is  possible  only when  Ward's
        method is called for dataset with non-Euclidean distance function.
  In case negative completion code is returned,  other  fields  of  report
  structure are invalid and should not be used.

* NPoints contains number of points in the original dataset

* Z contains information about merges performed  (see below).  Z  contains
  indexes from the original (unsorted) dataset and it can be used when you
  need to know what points were merged. However, it is not convenient when
  you want to build a dendrograd (see below).

* if  you  want  to  build  dendrogram, you  can use Z, but it is not good
  option, because Z contains  indexes from  unsorted  dataset.  Dendrogram
  built from such dataset is likely to have intersections. So, you have to
  reorder you points before building dendrogram.
  Permutation which reorders point is returned in P. Another representation
  of  merges,  which  is  more  convenient for dendorgram construction, is
  returned in PM.

* more information on format of Z, P and PM can be found below and in the
  examples from ALGLIB Reference Manual.

FORMAL DESCRIPTION OF FIELDS:
    NPoints         number of points
    Z               array[NPoints-1,2],  contains   indexes   of  clusters
                    linked in pairs to  form  clustering  tree.  I-th  row
                    corresponds to I-th merge:
                    * Z[I,0] - index of the first cluster to merge
                    * Z[I,1] - index of the second cluster to merge
                    * Z[I,0] &lt; Z[I,1]
                    * clusters are  numbered  from 0 to 2*NPoints-2,  with
                      indexes from 0 to NPoints-1 corresponding to  points
                      of the original dataset, and indexes from NPoints to
                      2*NPoints-2  correspond  to  clusters  generated  by
                      subsequent  merges  (I-th  row  of Z creates cluster
                      with index NPoints+I).

                    IMPORTANT: indexes in Z[] are indexes in the ORIGINAL,
                    unsorted dataset. In addition to  Z algorithm  outputs
                    permutation which rearranges points in such  way  that
                    subsequent merges are  performed  on  adjacent  points
                    (such order is needed if you want to build dendrogram).
                    However,  indexes  in  Z  are  related  to   original,
                    unrearranged sequence of points.

    P               array[NPoints], permutation which reorders points  for
                    dendrogram  construction.  P[i] contains  index of the
                    position  where  we  should  move  I-th  point  of the
                    original dataset in order to apply merges PZ/PM.

    PZ              same as Z, but for permutation of points given  by  P.
                    The  only  thing  which  changed  are  indexes  of the
                    original points; indexes of clusters remained same.

    MergeDist       array[NPoints-1], contains distances between  clusters
                    being merged (MergeDist[i] correspond to merge  stored
                    in Z[i,...]):
                    * CLINK, SLINK and  average  linkage algorithms report
                      &quot;raw&quot;, unmodified distance metric.
                    * Ward's   method   reports   weighted   intra-cluster
                      variance, which is equal to ||Ca-Cb||^2 * Sa*Sb/(Sa+Sb).
                      Here  A  and  B  are  clusters being merged, Ca is a
                      center of A, Cb is a center of B, Sa is a size of A,
                      Sb is a size of B.

    PM              array[NPoints-1,6], another representation of  merges,
                    which is suited for dendrogram construction. It  deals
                    with rearranged points (permutation P is applied)  and
                    represents merges in a form which different  from  one
                    used by Z.
                    For each I from 0 to NPoints-2, I-th row of PM represents
                    merge performed on two clusters C0 and C1. Here:
                    * C0 contains points with indexes PM[I,0]...PM[I,1]
                    * C1 contains points with indexes PM[I,2]...PM[I,3]
                    * indexes stored in PM are given for dataset sorted
                      according to permutation P
                    * PM[I,1]=PM[I,2]-1 (only adjacent clusters are merged)
                    * PM[I,0] &le; PM[I,1], PM[I,2] &le; PM[I,3], i.e. both
                      clusters contain at least one point
                    * heights of &quot;subdendrograms&quot; corresponding  to  C0/C1
                      are stored in PM[I,4]  and  PM[I,5].  Subdendrograms
                      corresponding   to   single-point   clusters    have
                      height=0. Dendrogram of the merge result has  height
                      H=max(H0,H1)+1.

NOTE: there is one-to-one correspondence between merges described by Z and
      PM. I-th row of Z describes same merge of clusters as I-th row of PM,
      with &quot;left&quot; cluster from Z corresponding to the &quot;left&quot; one from PM.
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> ahcreport {
   ae_int_t terminationtype;
   ae_int_t npoints;
   integer_1d_array p;
   integer_2d_array z;
   integer_2d_array pz;
   integer_2d_array pm;
   real_1d_array mergedist;
};
</pre>
<a name=struct_clusterizerstate></a><h6 class=pageheader>clusterizerstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure is a clusterization engine.
You should not try to access its fields directly.
Use ALGLIB functions in order to work with this object.
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> clusterizerstate {
};
</pre>
<a name=struct_kmeansreport></a><h6 class=pageheader>kmeansreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This  structure   is  used  to  store  results of the  k-means  clustering
algorithm.

Following information is always returned:
* NPoints contains number of points in the original dataset
* TerminationType contains completion code, negative on failure, positive
  on success
* K contains number of clusters

For positive TerminationType we return:
* NFeatures contains number of variables in the original dataset
* C, which contains centers found by algorithm
* CIdx, which maps points of the original dataset to clusters

FORMAL DESCRIPTION OF FIELDS:
    NPoints         number of points, &ge; 0
    NFeatures       number of variables, &ge; 1
    TerminationType completion code:
                    * -5 if  distance  type  is  anything  different  from
                         Euclidean metric
                    * -3 for degenerate dataset: a) less  than  K  distinct
                         points, b) K=0 for non-empty dataset.
                    * +1 for successful completion
    K               number of clusters
    C               array[K,NFeatures], rows of the array store centers
    CIdx            array[NPoints], which contains cluster indexes
    IterationsCount actual number of iterations performed by clusterizer.
                    If algorithm performed more than one random restart,
                    total number of iterations is returned.
    Energy          merit function, &quot;energy&quot;, sum  of  squared  deviations
                    from cluster centers
ALGLIB: Copyright 27.11.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> kmeansreport {
   ae_int_t npoints;
   ae_int_t nfeatures;
   ae_int_t terminationtype;
   ae_int_t iterationscount;
   <b>double</b> energy;
   ae_int_t k;
   real_2d_array c;
   integer_1d_array cidx;
};
</pre>
<a name=sub_clusterizercreate></a><h6 class=pageheader>clusterizercreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function initializes clusterizer object. Newly initialized object  is
empty, i.e. it does not contain dataset. You should use it as follows:
1. creation
2. dataset is added with ClusterizerSetPoints()
3. additional parameters are set
3. clusterization is performed with one of the clustering functions
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizercreate(clusterizerstate &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_clst_ahc class=nav>clst_ahc</a> | <a href=#example_clst_kmeans class=nav>clst_kmeans</a> | <a href=#example_clst_linkage class=nav>clst_linkage</a> | <a href=#example_clst_distance class=nav>clst_distance</a> | <a href=#example_clst_kclusters class=nav>clst_kclusters</a> ]</p>
<a name=sub_clusterizergetdistances></a><h6 class=pageheader>clusterizergetdistances Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns distance matrix for dataset

Inputs:
    XY      -   array[NPoints,NFeatures], dataset
    NPoints -   number of points, &ge; 0
    NFeatures-  number of features, &ge; 1
    DistType-   distance function:
                *  0    Chebyshev distance  (L-inf norm)
                *  1    city block distance (L1 norm)
                *  2    Euclidean distance  (L2 norm, non-squared)
                * 10    Pearson correlation:
                        dist(a,b) = 1-corr(a,b)
                * 11    Absolute Pearson correlation:
                        dist(a,b) = 1-|corr(a,b)|
                * 12    Uncentered Pearson correlation (cosine of the angle):
                        dist(a,b) = a'*b/(|a|*|b|)
                * 13    Absolute uncentered Pearson correlation
                        dist(a,b) = |a'*b|/(|a|*|b|)
                * 20    Spearman rank correlation:
                        dist(a,b) = 1-rankcorr(a,b)
                * 21    Absolute Spearman rank correlation
                        dist(a,b) = 1-|rankcorr(a,b)|

Outputs:
    D       -   array[NPoints,NPoints], distance matrix
                (full matrix is returned, with lower and upper triangles)

NOTE:  different distance functions have different performance penalty:
       * Euclidean or Pearson correlation distances are the fastest ones
       * Spearman correlation distance function is a bit slower
       * city block and Chebyshev distances are order of magnitude slower

       The reason behing difference in performance is that correlation-based
       distance functions are computed using optimized linear algebra kernels,
       while Chebyshev and city block distance functions are computed using
       simple nested loops with two branches at each iteration.
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizergetdistances(real_2d_array xy, ae_int_t npoints, ae_int_t nfeatures, ae_int_t disttype, real_2d_array &amp;d);
</pre>
<a name=sub_clusterizergetkclusters></a><h6 class=pageheader>clusterizergetkclusters Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function takes as input clusterization report Rep,  desired  clusters
count K, and builds top K clusters from hierarchical clusterization  tree.
It returns assignment of points to clusters (array of cluster indexes).

Inputs:
    Rep     -   report from ClusterizerRunAHC() performed on XY
    K       -   desired number of clusters, 1 &le; K &le; NPoints.
                K can be zero only when NPoints=0.

Outputs:
    CIdx    -   array[NPoints], I-th element contains cluster index  (from
                0 to K-1) for I-th point of the dataset.
    CZ      -   array[K]. This array allows  to  convert  cluster  indexes
                returned by this function to indexes used by  Rep.Z.  J-th
                cluster returned by this function corresponds to  CZ[J]-th
                cluster stored in Rep.Z/PZ/PM.
                It is guaranteed that CZ[I] &lt; CZ[I+1].

NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
      Although  they  were  obtained  by  manipulation with top K nodes of
      dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
      function does not return information about hierarchy.  Each  of  the
      clusters stand on its own.

NOTE: Cluster indexes returned by this function  does  not  correspond  to
      indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
      representation of the dataset (dendrogram), or you work with  &quot;flat&quot;
      representation returned by this function.  Each  of  representations
      has its own clusters indexing system (former uses [0, 2*NPoints-2]),
      while latter uses [0..K-1]), although  it  is  possible  to  perform
      conversion from one system to another by means of CZ array, returned
      by this function, which allows you to convert indexes stored in CIdx
      to the numeration system used by Rep.Z.

NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
      it will perform many times faster than  for  K=100.  Its  worst-case
      performance is O(N*K), although in average case  it  perform  better
      (up to O(N*log(K))).
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizergetkclusters(ahcreport rep, ae_int_t k, integer_1d_array &amp;cidx, integer_1d_array &amp;cz);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_clst_linkage class=nav>clst_linkage</a> | <a href=#example_clst_kclusters class=nav>clst_kclusters</a> ]</p>
<a name=sub_clusterizerrunahc></a><h6 class=pageheader>clusterizerrunahc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs agglomerative hierarchical clustering

NOTE: Agglomerative  hierarchical  clustering  algorithm  has two  phases:
      distance matrix calculation and clustering  itself. Only first phase
      (distance matrix  calculation)  is  accelerated  by  Intel  MKL  and
      multithreading. Thus, acceleration is significant only for medium or
      high-dimensional problems.

      Although activating multithreading gives some speedup  over  single-
      threaded execution, you  should  not  expect  nearly-linear  scaling
      with respect to cores count.

Inputs:
    S       -   clusterizer state, initialized by ClusterizerCreate()

Outputs:
    Rep     -   clustering results; see description of AHCReport
                structure for more information.

NOTE 1: hierarchical clustering algorithms require large amounts of memory.
        In particular, this implementation needs  sizeof(double)*NPoints^2
        bytes, which are used to store distance matrix. In  case  we  work
        with user-supplied matrix, this amount is multiplied by 2 (we have
        to store original matrix and to work with its copy).

        For example, problem with 10000 points  would require 800M of RAM,
        even when working in a 1-dimensional space.
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizerrunahc(clusterizerstate s, ahcreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_clst_ahc class=nav>clst_ahc</a> | <a href=#example_clst_kmeans class=nav>clst_kmeans</a> | <a href=#example_clst_linkage class=nav>clst_linkage</a> | <a href=#example_clst_distance class=nav>clst_distance</a> | <a href=#example_clst_kclusters class=nav>clst_kclusters</a> ]</p>
<a name=sub_clusterizerrunkmeans></a><h6 class=pageheader>clusterizerrunkmeans Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs clustering by k-means++ algorithm.

You may change algorithm properties by calling:
* ClusterizerSetKMeansLimits() to change number of restarts or iterations
* ClusterizerSetKMeansInit() to change initialization algorithm

By  default,  one  restart  and  unlimited number of iterations are  used.
Initialization algorithm is chosen automatically.

NOTE: k-means clustering  algorithm has two  phases:  selection of initial
      centers and clustering  itself.  ALGLIB  parallelizes  both  phases.
      Parallel version is optimized for the following  scenario: medium or
      high-dimensional problem (8 or more dimensions) with large number of
      points and clusters. However, some speed-up  can  be  obtained  even
      when assumptions above are violated.

Inputs:
    S       -   clusterizer state, initialized by ClusterizerCreate()
    K       -   number of clusters, K &ge; 0.
                K  can  be  zero only when algorithm is called  for  empty
                dataset,  in   this   case   completion  code  is  set  to
                success (+1).
                If  K=0  and  dataset  size  is  non-zero,  we   can   not
                meaningfully assign points to some center  (there  are  no
                centers because K=0) and  return  -3  as  completion  code
                (failure).

Outputs:
    Rep     -   clustering results; see description of KMeansReport
                structure for more information.

NOTE 1: k-means  clustering  can  be  performed  only  for  datasets  with
        Euclidean  distance  function.  Algorithm  will  return   negative
        completion code in Rep.TerminationType in case dataset  was  added
        to clusterizer with DistType other than Euclidean (or dataset  was
        specified by distance matrix instead of explicitly given points).

NOTE 2: by default, k-means uses non-deterministic seed to initialize  RNG
        which is used to select initial centers. As  result,  each  run of
        algorithm may return different values. If you  need  deterministic
        behavior, use ClusterizerSetSeed() function.
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizerrunkmeans(clusterizerstate s, ae_int_t k, kmeansreport &amp;rep);
</pre>
<a name=sub_clusterizerseparatedbycorr></a><h6 class=pageheader>clusterizerseparatedbycorr Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  accepts  AHC  report  Rep,  desired  maximum  intercluster
correlation and returns top clusters from hierarchical clusterization tree
which are separated by correlation R or LOWER.

It returns assignment of points to clusters (array of cluster indexes).

There is one more function with similar name - ClusterizerSeparatedByDist,
which returns clusters with intercluster distance equal  to  R  or  HIGHER
(note: higher for distance, lower for correlation).

Inputs:
    Rep     -   report from ClusterizerRunAHC() performed on XY
    R       -   desired maximum intercluster correlation, -1 &le; R &le; +1

Outputs:
    K       -   number of clusters, 1 &le; K &le; NPoints
    CIdx    -   array[NPoints], I-th element contains cluster index  (from
                0 to K-1) for I-th point of the dataset.
    CZ      -   array[K]. This array allows  to  convert  cluster  indexes
                returned by this function to indexes used by  Rep.Z.  J-th
                cluster returned by this function corresponds to  CZ[J]-th
                cluster stored in Rep.Z/PZ/PM.
                It is guaranteed that CZ[I] &lt; CZ[I+1].

NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
      Although  they  were  obtained  by  manipulation with top K nodes of
      dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
      function does not return information about hierarchy.  Each  of  the
      clusters stand on its own.

NOTE: Cluster indexes returned by this function  does  not  correspond  to
      indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
      representation of the dataset (dendrogram), or you work with  &quot;flat&quot;
      representation returned by this function.  Each  of  representations
      has its own clusters indexing system (former uses [0, 2*NPoints-2]),
      while latter uses [0..K-1]), although  it  is  possible  to  perform
      conversion from one system to another by means of CZ array, returned
      by this function, which allows you to convert indexes stored in CIdx
      to the numeration system used by Rep.Z.

NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
      it will perform many times faster than  for  K=100.  Its  worst-case
      performance is O(N*K), although in average case  it  perform  better
      (up to O(N*log(K))).
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizerseparatedbycorr(ahcreport rep, <b>double</b> r, ae_int_t &amp;k, integer_1d_array &amp;cidx, integer_1d_array &amp;cz);
</pre>
<a name=sub_clusterizerseparatedbydist></a><h6 class=pageheader>clusterizerseparatedbydist Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  accepts  AHC  report  Rep,  desired  minimum  intercluster
distance and returns top clusters from  hierarchical  clusterization  tree
which are separated by distance R or HIGHER.

It returns assignment of points to clusters (array of cluster indexes).

There is one more function with similar name - ClusterizerSeparatedByCorr,
which returns clusters with intercluster correlation equal to R  or  LOWER
(note: higher for distance, lower for correlation).

Inputs:
    Rep     -   report from ClusterizerRunAHC() performed on XY
    R       -   desired minimum intercluster distance, R &ge; 0

Outputs:
    K       -   number of clusters, 1 &le; K &le; NPoints
    CIdx    -   array[NPoints], I-th element contains cluster index  (from
                0 to K-1) for I-th point of the dataset.
    CZ      -   array[K]. This array allows  to  convert  cluster  indexes
                returned by this function to indexes used by  Rep.Z.  J-th
                cluster returned by this function corresponds to  CZ[J]-th
                cluster stored in Rep.Z/PZ/PM.
                It is guaranteed that CZ[I] &lt; CZ[I+1].

NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
      Although  they  were  obtained  by  manipulation with top K nodes of
      dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
      function does not return information about hierarchy.  Each  of  the
      clusters stand on its own.

NOTE: Cluster indexes returned by this function  does  not  correspond  to
      indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
      representation of the dataset (dendrogram), or you work with  &quot;flat&quot;
      representation returned by this function.  Each  of  representations
      has its own clusters indexing system (former uses [0, 2*NPoints-2]),
      while latter uses [0..K-1]), although  it  is  possible  to  perform
      conversion from one system to another by means of CZ array, returned
      by this function, which allows you to convert indexes stored in CIdx
      to the numeration system used by Rep.Z.

NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
      it will perform many times faster than  for  K=100.  Its  worst-case
      performance is O(N*K), although in average case  it  perform  better
      (up to O(N*log(K))).
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizerseparatedbydist(ahcreport rep, <b>double</b> r, ae_int_t &amp;k, integer_1d_array &amp;cidx, integer_1d_array &amp;cz);
</pre>
<a name=sub_clusterizersetahcalgo></a><h6 class=pageheader>clusterizersetahcalgo Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets agglomerative hierarchical clustering algorithm

Inputs:
    S       -   clusterizer state, initialized by ClusterizerCreate()
    Algo    -   algorithm type:
                * 0     complete linkage (default algorithm)
                * 1     single linkage
                * 2     unweighted average linkage
                * 3     weighted average linkage
                * 4     Ward's method

NOTE: Ward's method works correctly only with Euclidean  distance,  that's
      why algorithm will return negative termination  code  (failure)  for
      any other distance type.

      It is possible, however,  to  use  this  method  with  user-supplied
      distance matrix. It  is  your  responsibility  to pass one which was
      calculated with Euclidean distance function.
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizersetahcalgo(clusterizerstate s, ae_int_t algo);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_clst_ahc class=nav>clst_ahc</a> | <a href=#example_clst_kmeans class=nav>clst_kmeans</a> | <a href=#example_clst_linkage class=nav>clst_linkage</a> | <a href=#example_clst_distance class=nav>clst_distance</a> | <a href=#example_clst_kclusters class=nav>clst_kclusters</a> ]</p>
<a name=sub_clusterizersetdistances></a><h6 class=pageheader>clusterizersetdistances Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function adds dataset given by distance  matrix  to  the  clusterizer
structure. It is important that dataset is not  given  explicitly  -  only
distance matrix is given.

This function overrides all previous calls  of  ClusterizerSetPoints()  or
ClusterizerSetDistances().

Inputs:
    S       -   clusterizer state, initialized by ClusterizerCreate()
    D       -   array[NPoints,NPoints], distance matrix given by its upper
                or lower triangle (main diagonal is  ignored  because  its
                entries are expected to be zero).
    NPoints -   number of points
    IsUpper -   whether upper or lower triangle of D is given.

NOTE 1: different clustering algorithms have different limitations:
        * agglomerative hierarchical clustering algorithms may be used with
          any kind of distance metric, including one  which  is  given  by
          distance matrix
        * k-means++ clustering algorithm may be used only  with  Euclidean
          distance function and explicitly given points - it  can  not  be
          used with dataset given by distance matrix
        Thus, if you call this function, you will be unable to use k-means
        clustering algorithm to process your problem.
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizersetdistances(clusterizerstate s, real_2d_array d, ae_int_t npoints, <b>bool</b> isupper);
<b>void</b> clusterizersetdistances(clusterizerstate s, real_2d_array d, <b>bool</b> isupper);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_clst_distance class=nav>clst_distance</a> ]</p>
<a name=sub_clusterizersetkmeansinit></a><h6 class=pageheader>clusterizersetkmeansinit Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets k-means  initialization  algorithm.  Several  different
algorithms can be chosen, including k-means++.

Inputs:
    S       -   clusterizer state, initialized by ClusterizerCreate()
    InitAlgo-   initialization algorithm:
                * 0  automatic selection ( different  versions  of  ALGLIB
                     may select different algorithms)
                * 1  random initialization
                * 2  k-means++ initialization  (best  quality  of  initial
                     centers, but long  non-parallelizable  initialization
                     phase with bad cache locality)
                * 3  &quot;fast-greedy&quot;  algorithm  with  efficient,  easy   to
                     parallelize initialization. Quality of initial centers
                     is  somewhat  worse  than  that  of  k-means++.  This
                     algorithm is a default one in the current version  of
                     ALGLIB.
                *-1  &quot;debug&quot; algorithm which always selects first  K  rows
                     of dataset; this algorithm is used for debug purposes
                     only. Do not use it in the industrial code!
ALGLIB: Copyright 21.01.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizersetkmeansinit(clusterizerstate s, ae_int_t initalgo);
</pre>
<a name=sub_clusterizersetkmeanslimits></a><h6 class=pageheader>clusterizersetkmeanslimits Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets k-means properties:  number  of  restarts and maximum
number of iterations per one run.

Inputs:
    S       -   clusterizer state, initialized by ClusterizerCreate()
    Restarts-   restarts count, &ge; 1.
                k-means++ algorithm performs several restarts and  chooses
                best set of centers (one with minimum squared distance).
    MaxIts  -   maximum number of k-means iterations performed during  one
                run. &ge; 0, zero value means that algorithm performs unlimited
                number of iterations.
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizersetkmeanslimits(clusterizerstate s, ae_int_t restarts, ae_int_t maxits);
</pre>
<a name=sub_clusterizersetpoints></a><h6 class=pageheader>clusterizersetpoints Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function adds dataset to the clusterizer structure.

This function overrides all previous calls  of  ClusterizerSetPoints()  or
ClusterizerSetDistances().

Inputs:
    S       -   clusterizer state, initialized by ClusterizerCreate()
    XY      -   array[NPoints,NFeatures], dataset
    NPoints -   number of points, &ge; 0
    NFeatures-  number of features, &ge; 1
    DistType-   distance function:
                *  0    Chebyshev distance  (L-inf norm)
                *  1    city block distance (L1 norm)
                *  2    Euclidean distance  (L2 norm), non-squared
                * 10    Pearson correlation:
                        dist(a,b) = 1-corr(a,b)
                * 11    Absolute Pearson correlation:
                        dist(a,b) = 1-|corr(a,b)|
                * 12    Uncentered Pearson correlation (cosine of the angle):
                        dist(a,b) = a'*b/(|a|*|b|)
                * 13    Absolute uncentered Pearson correlation
                        dist(a,b) = |a'*b|/(|a|*|b|)
                * 20    Spearman rank correlation:
                        dist(a,b) = 1-rankcorr(a,b)
                * 21    Absolute Spearman rank correlation
                        dist(a,b) = 1-|rankcorr(a,b)|

NOTE 1: different distance functions have different performance penalty:
        * Euclidean or Pearson correlation distances are the fastest ones
        * Spearman correlation distance function is a bit slower
        * city block and Chebyshev distances are order of magnitude slower

        The reason behing difference in performance is that correlation-based
        distance functions are computed using optimized linear algebra kernels,
        while Chebyshev and city block distance functions are computed using
        simple nested loops with two branches at each iteration.

NOTE 2: different clustering algorithms have different limitations:
        * agglomerative hierarchical clustering algorithms may be used with
          any kind of distance metric
        * k-means++ clustering algorithm may be used only  with  Euclidean
          distance function
        Thus, list of specific clustering algorithms you may  use  depends
        on distance function you specify when you set your dataset.
ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizersetpoints(clusterizerstate s, real_2d_array xy, ae_int_t npoints, ae_int_t nfeatures, ae_int_t disttype);
<b>void</b> clusterizersetpoints(clusterizerstate s, real_2d_array xy, ae_int_t disttype);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_clst_ahc class=nav>clst_ahc</a> | <a href=#example_clst_kmeans class=nav>clst_kmeans</a> | <a href=#example_clst_linkage class=nav>clst_linkage</a> | <a href=#example_clst_distance class=nav>clst_distance</a> | <a href=#example_clst_kclusters class=nav>clst_kclusters</a> ]</p>
<a name=sub_clusterizersetseed></a><h6 class=pageheader>clusterizersetseed Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  seed  which  is  used to initialize internal RNG. By
default, deterministic seed is used - same for each run of clusterizer. If
you specify non-deterministic  seed  value,  then  some  algorithms  which
depend on random initialization (in current version: k-means)  may  return
slightly different results after each run.

Inputs:
    S       -   clusterizer state, initialized by ClusterizerCreate()
    Seed    -   seed:
                * positive values = use deterministic seed for each run of
                  algorithms which depend on random initialization
                * zero or negative values = use non-deterministic seed
ALGLIB: Copyright 08.06.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> clusterizersetseed(clusterizerstate s, ae_int_t seed);
</pre>
<a name=example_clst_ahc></a><h6 class=pageheader>clst_ahc Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// The very simple clusterization example</font>
<font color=navy>//</font>
<font color=navy>// We have a set of points in 2D space:</font>
<font color=navy>//     (P0,P1,P2,P3,P4) = ((1,1),(1,2),(4,1),(2,3),(4,1.5))</font>
<font color=navy>//</font>
<font color=navy>//  |</font>
<font color=navy>//  |     P3</font>
<font color=navy>//  |</font>
<font color=navy>//  | P1          </font>
<font color=navy>//  |             P4</font>
<font color=navy>//  | P0          P2</font>
<font color=navy>//  |-------------------------</font>
<font color=navy>//</font>
<font color=navy>// We want to perform Agglomerative Hierarchic Clusterization (AHC),</font>
<font color=navy>// using complete linkage (default algorithm) and Euclidean distance</font>
<font color=navy>// (default metric).</font>
<font color=navy>//</font>
<font color=navy>// In order to <b>do</b> that, we:</font>
<font color=navy>// * create clusterizer with clusterizercreate()</font>
<font color=navy>// * set points XY and metric (2=Euclidean) with clusterizersetpoints()</font>
<font color=navy>// * run AHC algorithm with clusterizerrunahc</font>
<font color=navy>//</font>
<font color=navy>// You may see that clusterization itself is a minor part of the example,</font>
<font color=navy>// most of which is dominated by comments :)</font>
   clusterizerstate s;
   ahcreport rep;
   real_2d_array xy = <font color=blue><b>&quot;[[1,1],[1,2],[4,1],[2,3],[4,1.5]]&quot;</b></font>;

   clusterizercreate(s);
   clusterizersetpoints(s, xy, 2);
   clusterizerrunahc(s, rep);
<font color=navy>// Now we've built our clusterization tree. Rep.z contains information which</font>
<font color=navy>// is required to build dendrogram. I-th row of rep.z represents one merge</font>
<font color=navy>// operation, with first cluster to merge having index rep.z[I,0] and second</font>
<font color=navy>// one having index rep.z[I,1]. Merge result has index NPoints+I.</font>
<font color=navy>//</font>
<font color=navy>// Clusters with indexes less than NPoints are single-point initial clusters,</font>
<font color=navy>// <b>while</b> ones with indexes from NPoints to 2*NPoints-2 are multi-point</font>
<font color=navy>// clusters created during merges.</font>
<font color=navy>//</font>
<font color=navy>// In our example, Z=[[2,4], [0,1], [3,6], [5,7]]</font>
<font color=navy>//</font>
<font color=navy>// It means that:</font>
<font color=navy>// * first, we merge C2=(P2) and C4=(P4),    and create C5=(P2,P4)</font>
<font color=navy>// * then, we merge  C2=(P0) and C1=(P1),    and create C6=(P0,P1)</font>
<font color=navy>// * then, we merge  C3=(P3) and C6=(P0,P1), and create C7=(P0,P1,P3)</font>
<font color=navy>// * finally, we merge C5 and C7 and create C8=(P0,P1,P2,P3,P4)</font>
<font color=navy>//</font>
<font color=navy>// Thus, we have following dendrogram:</font>
<font color=navy>//  </font>
<font color=navy>//      ------8-----</font>
<font color=navy>//      |          |</font>
<font color=navy>//      |      ----7----</font>
<font color=navy>//      |      |       |</font>
<font color=navy>//   ---5---   |    ---6---</font>
<font color=navy>//   |     |   |    |     |</font>
<font color=navy>//   P2   P4   P3   P0   P1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, rep.z.tostring().c_str()); <font color=navy>// EXPECTED: [[2,4],[0,1],[3,6],[5,7]]</font>
<font color=navy>//</font>
<font color=navy>// We've built dendrogram above by reordering our dataset.</font>
<font color=navy>//</font>
<font color=navy>// Without such reordering it would be impossible to build dendrogram without</font>
<font color=navy>// intersections. Luckily, ahcreport structure contains two additional fields</font>
<font color=navy>// which help to build dendrogram from your data:</font>
<font color=navy>// * rep.p, which contains permutation applied to dataset</font>
<font color=navy>// * rep.pm, which contains another representation of merges </font>
<font color=navy>//</font>
<font color=navy>// In our example we have:</font>
<font color=navy>// * P=[3,4,0,2,1]</font>
<font color=navy>// * PZ=[[0,0,1,1,0,0],[3,3,4,4,0,0],[2,2,3,4,0,1],[0,1,2,4,1,2]]</font>
<font color=navy>//</font>
<font color=navy>// Permutation array P tells us that P0 should be moved to position 3,</font>
<font color=navy>// P1 moved to position 4, P2 moved to position 0 and so on:</font>
<font color=navy>//</font>
<font color=navy>//   (P0 P1 P2 P3 P4) &rArr; (P2 P4 P3 P0 P1)</font>
<font color=navy>//</font>
<font color=navy>// Merges array PZ tells us how to perform merges on the sorted dataset.</font>
<font color=navy>// One row of PZ corresponds to one merge operations, with first pair of</font>
<font color=navy>// elements denoting first of the clusters to merge (start index, end</font>
<font color=navy>// index) and next pair of elements denoting second of the clusters to</font>
<font color=navy>// merge. Clusters being merged are always adjacent, with first one on</font>
<font color=navy>// the left and second one on the right.</font>
<font color=navy>//</font>
<font color=navy>// For example, first row of PZ tells us that clusters [0,0] and [1,1] are</font>
<font color=navy>// merged (single-point clusters, with first one containing P2 and second</font>
<font color=navy>// one containing P4). Third row of PZ tells us that we merge one single-</font>
<font color=navy>// point cluster [2,2] with one two-point cluster [3,4].</font>
<font color=navy>//</font>
<font color=navy>// There are two more elements in each row of PZ. These are the helper</font>
<font color=navy>// elements, which denote HEIGHT (not size) of left and right subdendrograms.</font>
<font color=navy>// For example, according to PZ, first two merges are performed on clusterization</font>
<font color=navy>// trees of height 0, <b>while</b> next two merges are performed on 0-1 and 1-2</font>
<font color=navy>// pairs of trees correspondingly.</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, rep.p.tostring().c_str()); <font color=navy>// EXPECTED: [3,4,0,2,1]</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, rep.pm.tostring().c_str()); <font color=navy>// EXPECTED: [[0,0,1,1,0,0],[3,3,4,4,0,0],[2,2,3,4,0,1],[0,1,2,4,1,2]]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_clst_distance></a><h6 class=pageheader>clst_distance Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We have three points in 4D space:</font>
<font color=navy>//     (P0,P1,P2) = ((1, 2, 1, 2), (6, 7, 6, 7), (7, 6, 7, 6))</font>
<font color=navy>//</font>
<font color=navy>// We want to try clustering them with different distance functions.</font>
<font color=navy>// Distance function is chosen when we add dataset to the clusterizer.</font>
<font color=navy>// We can choose several distance types - Euclidean, city block, Chebyshev,</font>
<font color=navy>// several correlation measures or user-supplied distance matrix.</font>
<font color=navy>//</font>
<font color=navy>// Here we'll try three distances: Euclidean, Pearson correlation,</font>
<font color=navy>// user-supplied distance matrix. Different distance functions lead</font>
<font color=navy>// to different choices being made by algorithm during clustering.</font>
   clusterizerstate s;
   ahcreport rep;
   ae_int_t disttype;
   real_2d_array xy = <font color=blue><b>&quot;[[1, 2, 1, 2], [6, 7, 6, 7], [7, 6, 7, 6]]&quot;</b></font>;
   clusterizercreate(s);

<font color=navy>// With Euclidean distance function (disttype=2) two closest points</font>
<font color=navy>// are P1 and P2, thus:</font>
<font color=navy>// * first, we merge P1 and P2 to form C3=[P1,P2]</font>
<font color=navy>// * second, we merge P0 and C3 to form C4=[P0,P1,P2]</font>
   disttype = 2;
   clusterizersetpoints(s, xy, disttype);
   clusterizerrunahc(s, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, rep.z.tostring().c_str()); <font color=navy>// EXPECTED: [[1,2],[0,3]]</font>

<font color=navy>// With Pearson correlation distance function (disttype=10) situation</font>
<font color=navy>// is different - distance between P0 and P1 is zero, thus:</font>
<font color=navy>// * first, we merge P0 and P1 to form C3=[P0,P1]</font>
<font color=navy>// * second, we merge P2 and C3 to form C4=[P0,P1,P2]</font>
   disttype = 10;
   clusterizersetpoints(s, xy, disttype);
   clusterizerrunahc(s, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, rep.z.tostring().c_str()); <font color=navy>// EXPECTED: [[0,1],[2,3]]</font>

<font color=navy>// Finally, we try clustering with user-supplied distance matrix:</font>
<font color=navy>//     [ 0 3 1 ]</font>
<font color=navy>// P = [ 3 0 3 ], where P[i,j] = dist(Pi,Pj)</font>
<font color=navy>//     [ 1 3 0 ]</font>
<font color=navy>//</font>
<font color=navy>// * first, we merge P0 and P2 to form C3=[P0,P2]</font>
<font color=navy>// * second, we merge P1 and C3 to form C4=[P0,P1,P2]</font>
   real_2d_array d = <font color=blue><b>&quot;[[0,3,1],[3,0,3],[1,3,0]]&quot;</b></font>;
   clusterizersetdistances(s, d, true);
   clusterizerrunahc(s, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, rep.z.tostring().c_str()); <font color=navy>// EXPECTED: [[0,2],[1,3]]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_clst_kclusters></a><h6 class=pageheader>clst_kclusters Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We have a set of points in 2D space:</font>
<font color=navy>//     (P0,P1,P2,P3,P4) = ((1,1),(1,2),(4,1),(2,3),(4,1.5))</font>
<font color=navy>//</font>
<font color=navy>//  |</font>
<font color=navy>//  |     P3</font>
<font color=navy>//  |</font>
<font color=navy>//  | P1          </font>
<font color=navy>//  |             P4</font>
<font color=navy>//  | P0          P2</font>
<font color=navy>//  |-------------------------</font>
<font color=navy>//</font>
<font color=navy>// We perform Agglomerative Hierarchic Clusterization (AHC) and we want</font>
<font color=navy>// to get top K clusters from clusterization tree <b>for</b> different K.</font>
   clusterizerstate s;
   ahcreport rep;
   real_2d_array xy = <font color=blue><b>&quot;[[1,1],[1,2],[4,1],[2,3],[4,1.5]]&quot;</b></font>;
   integer_1d_array cidx;
   integer_1d_array cz;

   clusterizercreate(s);
   clusterizersetpoints(s, xy, 2);
   clusterizerrunahc(s, rep);

<font color=navy>// with K=5, every points is assigned to its own cluster:</font>
<font color=navy>// C0=P0, C1=P1 and so on...</font>
   clusterizergetkclusters(rep, 5, cidx, cz);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, cidx.tostring().c_str()); <font color=navy>// EXPECTED: [0,1,2,3,4]</font>

<font color=navy>// with K=1 we have one large cluster C0=[P0,P1,P2,P3,P4,P5]</font>
   clusterizergetkclusters(rep, 1, cidx, cz);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, cidx.tostring().c_str()); <font color=navy>// EXPECTED: [0,0,0,0,0]</font>

<font color=navy>// with K=3 we have three clusters C0=[P3], C1=[P2,P4], C2=[P0,P1]</font>
   clusterizergetkclusters(rep, 3, cidx, cz);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, cidx.tostring().c_str()); <font color=navy>// EXPECTED: [2,2,1,0,1]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_clst_kmeans></a><h6 class=pageheader>clst_kmeans Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// The very simple clusterization example</font>
<font color=navy>//</font>
<font color=navy>// We have a set of points in 2D space:</font>
<font color=navy>//     (P0,P1,P2,P3,P4) = ((1,1),(1,2),(4,1),(2,3),(4,1.5))</font>
<font color=navy>//</font>
<font color=navy>//  |</font>
<font color=navy>//  |     P3</font>
<font color=navy>//  |</font>
<font color=navy>//  | P1          </font>
<font color=navy>//  |             P4</font>
<font color=navy>//  | P0          P2</font>
<font color=navy>//  |-------------------------</font>
<font color=navy>//</font>
<font color=navy>// We want to perform k-means++ clustering with K=2.</font>
<font color=navy>//</font>
<font color=navy>// In order to <b>do</b> that, we:</font>
<font color=navy>// * create clusterizer with clusterizercreate()</font>
<font color=navy>// * set points XY and metric (must be Euclidean, distype=2) with clusterizersetpoints()</font>
<font color=navy>// * (optional) set number of restarts from random positions to 5</font>
<font color=navy>// * run k-means algorithm with clusterizerrunkmeans()</font>
<font color=navy>//</font>
<font color=navy>// You may see that clusterization itself is a minor part of the example,</font>
<font color=navy>// most of which is dominated by comments :)</font>
   clusterizerstate s;
   kmeansreport rep;
   real_2d_array xy = <font color=blue><b>&quot;[[1,1],[1,2],[4,1],[2,3],[4,1.5]]&quot;</b></font>;

   clusterizercreate(s);
   clusterizersetpoints(s, xy, 2);
   clusterizersetkmeanslimits(s, 5, 0);
   clusterizerrunkmeans(s, 2, rep);
<font color=navy>// We've performed clusterization, and it succeeded (completion code is +1).</font>
<font color=navy>//</font>
<font color=navy>// Now first center is stored in the first row of rep.c, second one is stored</font>
<font color=navy>// in the second row. rep.cidx can be used to determine which center is</font>
<font color=navy>// closest to some specific point of the dataset.</font>
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 1</font>

<font color=navy>// We called clusterizersetpoints() with disttype=2 because k-means++</font>
<font color=navy>// algorithm does NOT support metrics other than Euclidean. But what <b>if</b> we</font>
<font color=navy>// try to use some other metric?</font>
<font color=navy>//</font>
<font color=navy>// We change metric type by calling clusterizersetpoints() one more time,</font>
<font color=navy>// and try to run k-means algo again. It fails.</font>
   clusterizersetpoints(s, xy, 0);
   clusterizerrunkmeans(s, 2, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: -5</font>
   <b>return</b> 0;
}
</pre>
<a name=example_clst_linkage></a><h6 class=pageheader>clst_linkage Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We have a set of points in 1D space:</font>
<font color=navy>//     (P0,P1,P2,P3,P4) = (1, 3, 10, 16, 20)</font>
<font color=navy>//</font>
<font color=navy>// We want to perform Agglomerative Hierarchic Clusterization (AHC),</font>
<font color=navy>// using either complete or single linkage and Euclidean distance</font>
<font color=navy>// (default metric).</font>
<font color=navy>//</font>
<font color=navy>// First two steps merge P0/P1 and P3/P4 independently of the linkage type.</font>
<font color=navy>// However, third step depends on linkage type being used:</font>
<font color=navy>// * in case of complete linkage P2=10 is merged with [P0,P1]</font>
<font color=navy>// * in case of single linkage P2=10 is merged with [P3,P4]</font>
   clusterizerstate s;
   ahcreport rep;
   real_2d_array xy = <font color=blue><b>&quot;[[1],[3],[10],[16],[20]]&quot;</b></font>;
   integer_1d_array cidx;
   integer_1d_array cz;

   clusterizercreate(s);
   clusterizersetpoints(s, xy, 2);

<font color=navy>// use complete linkage, reduce set down to 2 clusters.</font>
<font color=navy>// print clusterization with clusterizergetkclusters(2).</font>
<font color=navy>// P2 must belong to [P0,P1]</font>
   clusterizersetahcalgo(s, 0);
   clusterizerrunahc(s, rep);
   clusterizergetkclusters(rep, 2, cidx, cz);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, cidx.tostring().c_str()); <font color=navy>// EXPECTED: [1,1,1,0,0]</font>

<font color=navy>// use single linkage, reduce set down to 2 clusters.</font>
<font color=navy>// print clusterization with clusterizergetkclusters(2).</font>
<font color=navy>// P2 must belong to [P2,P3]</font>
   clusterizersetahcalgo(s, 1);
   clusterizerrunahc(s, rep);
   clusterizergetkclusters(rep, 2, cidx, cz);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, cidx.tostring().c_str()); <font color=navy>// EXPECTED: [0,0,1,1,1]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_datacomp></a><h4 class=pageheader>8.2.3. datacomp Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_kmeansgenerate class=toc>kmeansgenerate</a>
]</font>
</div>
<a name=sub_kmeansgenerate></a><h6 class=pageheader>kmeansgenerate Function</h6>
<hr width=600 align=left>
<pre class=narration>
k-means++ clusterization.
Backward compatibility function, we recommend to use CLUSTERING subpackage
as better replacement.
ALGLIB: Copyright 21.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> kmeansgenerate(real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t k, ae_int_t restarts, ae_int_t &amp;info, real_2d_array &amp;c, integer_1d_array &amp;xyc);
</pre>
<a name=unit_dforest></a><h4 class=pageheader>8.2.4. dforest Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_decisionforest class=toc>decisionforest</a> |
<a href=#struct_decisionforestbuffer class=toc>decisionforestbuffer</a> |
<a href=#struct_decisionforestbuilder class=toc>decisionforestbuilder</a> |
<a href=#struct_dfreport class=toc>dfreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_dfavgce class=toc>dfavgce</a> |
<a href=#sub_dfavgerror class=toc>dfavgerror</a> |
<a href=#sub_dfavgrelerror class=toc>dfavgrelerror</a> |
<a href=#sub_dfbinarycompression class=toc>dfbinarycompression</a> |
<a href=#sub_dfbuilderbuildrandomforest class=toc>dfbuilderbuildrandomforest</a> |
<a href=#sub_dfbuildercreate class=toc>dfbuildercreate</a> |
<a href=#sub_dfbuildergetprogress class=toc>dfbuildergetprogress</a> |
<a href=#sub_dfbuilderpeekprogress class=toc>dfbuilderpeekprogress</a> |
<a href=#sub_dfbuildersetdataset class=toc>dfbuildersetdataset</a> |
<a href=#sub_dfbuildersetimportancenone class=toc>dfbuildersetimportancenone</a> |
<a href=#sub_dfbuildersetimportanceoobgini class=toc>dfbuildersetimportanceoobgini</a> |
<a href=#sub_dfbuildersetimportancepermutation class=toc>dfbuildersetimportancepermutation</a> |
<a href=#sub_dfbuildersetimportancetrngini class=toc>dfbuildersetimportancetrngini</a> |
<a href=#sub_dfbuildersetrdfalgo class=toc>dfbuildersetrdfalgo</a> |
<a href=#sub_dfbuildersetrdfsplitstrength class=toc>dfbuildersetrdfsplitstrength</a> |
<a href=#sub_dfbuildersetrndvars class=toc>dfbuildersetrndvars</a> |
<a href=#sub_dfbuildersetrndvarsauto class=toc>dfbuildersetrndvarsauto</a> |
<a href=#sub_dfbuildersetrndvarsratio class=toc>dfbuildersetrndvarsratio</a> |
<a href=#sub_dfbuildersetseed class=toc>dfbuildersetseed</a> |
<a href=#sub_dfbuildersetsubsampleratio class=toc>dfbuildersetsubsampleratio</a> |
<a href=#sub_dfbuildrandomdecisionforest class=toc>dfbuildrandomdecisionforest</a> |
<a href=#sub_dfbuildrandomdecisionforestx1 class=toc>dfbuildrandomdecisionforestx1</a> |
<a href=#sub_dfclassify class=toc>dfclassify</a> |
<a href=#sub_dfcreatebuffer class=toc>dfcreatebuffer</a> |
<a href=#sub_dfprocess class=toc>dfprocess</a> |
<a href=#sub_dfprocess0 class=toc>dfprocess0</a> |
<a href=#sub_dfprocessi class=toc>dfprocessi</a> |
<a href=#sub_dfrelclserror class=toc>dfrelclserror</a> |
<a href=#sub_dfrmserror class=toc>dfrmserror</a> |
<a href=#sub_dfserialize class=toc>dfserialize</a> |
<a href=#sub_dftsprocess class=toc>dftsprocess</a> |
<a href=#sub_dfunserialize class=toc>dfunserialize</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_randomforest_cls class=toc>randomforest_cls</a></td><td width=15>&nbsp;</td><td>Simple classification with random forests</td></tr>
<tr align=left valign=top><td><a href=#example_randomforest_reg class=toc>randomforest_reg</a></td><td width=15>&nbsp;</td><td>Simple regression with decision forest</td></tr>
</table>
</div>
<a name=struct_decisionforest></a><h6 class=pageheader>decisionforest Class</h6>
<hr width=600 align=left>
<pre class=narration>
Decision forest (random forest) model.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> decisionforest {
};
</pre>
<a name=struct_decisionforestbuffer></a><h6 class=pageheader>decisionforestbuffer Class</h6>
<hr width=600 align=left>
<pre class=narration>
Buffer object which is used to perform  various  requests  (usually  model
inference) in the multithreaded mode (multiple threads working  with  same
DF object).

This object should be created with DFCreateBuffer().
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> decisionforestbuffer {
};
</pre>
<a name=struct_decisionforestbuilder></a><h6 class=pageheader>decisionforestbuilder Class</h6>
<hr width=600 align=left>
<pre class=narration>
A random forest (decision forest) builder object.

Used to store dataset and specify decision forest training algorithm settings.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> decisionforestbuilder {
};
</pre>
<a name=struct_dfreport></a><h6 class=pageheader>dfreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Decision forest training report.

==== training/oob errors ====

Following fields store training set errors:
* relclserror           -   fraction of misclassified cases, [0,1]
* avgce                 -   average cross-entropy in bits per symbol
* rmserror              -   root-mean-square error
* avgerror              -   average error
* avgrelerror           -   average relative error

Out-of-bag estimates are stored in fields with same names, but &quot;oob&quot; prefix.

For classification problems:
* RMS, AVG and AVGREL errors are calculated for posterior probabilities

For regression problems:
* RELCLS and AVGCE errors are zero

==== variable importance ====

Following fields are used to store variable importance information:

* topvars               -   variables ordered from the most  important  to
                            less  important  ones  (according  to  current
                            choice of importance raiting).
                            For example, topvars[0] contains index of  the
                            most important variable, and topvars[0:2]  are
                            indexes of 3 most important ones and so on.

* varimportances        -   array[nvars], ratings (the  larger,  the  more
                            important the variable  is,  always  in  [0,1]
                            range).
                            By default, filled  by  zeros  (no  importance
                            ratings are  provided  unless  you  explicitly
                            request them).
                            Zero rating means that variable is not important,
                            however you will rarely encounter such a thing,
                            in many cases  unimportant  variables  produce
                            nearly-zero (but nonzero) ratings.

Variable importance report must be EXPLICITLY requested by calling:
* dfbuildersetimportancegini() function, if you need out-of-bag Gini-based
  importance rating also known as MDI  (fast to  calculate,  resistant  to
  overfitting  issues,   but   has   some   bias  towards  continuous  and
  high-cardinality categorical variables)
* dfbuildersetimportancetrngini() function, if you need training set Gini-
  -based importance rating (what other packages typically report).
* dfbuildersetimportancepermutation() function, if you  need  permutation-
  based importance rating also known as MDA (slower to calculate, but less
  biased)
* dfbuildersetimportancenone() function,  if  you  do  not  need  importance
  ratings - ratings will be zero, topvars[] will be [0,1,2,...]

Different importance ratings (Gini or permutation) produce  non-comparable
values. Although in all cases rating values lie in [0,1] range, there  are
exist differences:
* informally speaking, Gini importance rating tends to divide &quot;unit amount
  of importance&quot;  between  several  important  variables, i.e. it produces
  estimates which roughly sum to 1.0 (or less than 1.0, if your  task  can
  not be solved exactly). If all variables  are  equally  important,  they
  will have same rating,  roughly  1/NVars,  even  if  every  variable  is
  critically important.
* from the other side, permutation importance tells us what percentage  of
  the model predictive power will be ruined  by  permuting  this  specific
  variable. It does not produce estimates which  sum  to  one.  Critically
  important variable will have rating close  to  1.0,  and  you  may  have
  multiple variables with such a rating.

More information on variable importance ratings can be found  in  comments
on the dfbuildersetimportancegini() and dfbuildersetimportancepermutation()
functions.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> dfreport {
   <b>double</b> relclserror;
   <b>double</b> avgce;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
   <b>double</b> oobrelclserror;
   <b>double</b> oobavgce;
   <b>double</b> oobrmserror;
   <b>double</b> oobavgerror;
   <b>double</b> oobavgrelerror;
   integer_1d_array topvars;
   real_1d_array varimportances;
};
</pre>
<a name=sub_dfavgce></a><h6 class=pageheader>dfavgce Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average cross-entropy (in bits per element) on the test set

Inputs:
    DF      -   decision forest model
    XY      -   test set
    NPoints -   test set size

Result:
    CrossEntropy/(NPoints*LN(2)).
    Zero if model solves regression task.
ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dfavgce(decisionforest df, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_dfavgerror></a><h6 class=pageheader>dfavgerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average error on the test set

Inputs:
    DF      -   decision forest model
    XY      -   test set
    NPoints -   test set size

Result:
    Its meaning for regression task is obvious. As for
    classification task, it means average error when estimating posterior
    probabilities.
ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dfavgerror(decisionforest df, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_dfavgrelerror></a><h6 class=pageheader>dfavgrelerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average relative error on the test set

Inputs:
    DF      -   decision forest model
    XY      -   test set
    NPoints -   test set size

Result:
    Its meaning for regression task is obvious. As for
    classification task, it means average relative error when estimating
    posterior probability of belonging to the correct class.
ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dfavgrelerror(decisionforest df, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_dfbinarycompression></a><h6 class=pageheader>dfbinarycompression Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs binary compression of the decision forest.

Original decision forest produced by the  forest  builder  is stored using
64-bit representation for all numbers - offsets, variable  indexes,  split
points.

It is possible to significantly reduce model size by means of:
* using compressed  dynamic encoding for integers  (offsets  and  variable
  indexes), which uses just 1 byte to store small ints  (less  than  128),
  just 2 bytes for larger values (less than 128^2) and so on
* storing floating point numbers using 8-bit exponent and 16-bit mantissa

As  result,  model  needs  significantly  less  memory (compression factor
depends on  variable and class counts). In particular:
* NVars &lt; 128   and NClasses &lt; 128 result in 4.4x-5.7x model size reduction
* NVars &lt; 16384 and NClasses &lt; 128 result in 3.7x-4.5x model size reduction

Such storage format performs lossless compression  of  all  integers,  but
compression of floating point values (split values) is lossy, with roughly
0.01% relative error introduced during rounding. Thus, we recommend you to
re-evaluate model accuracy after compression.

Another downside  of  compression  is  ~1.5x reduction  in  the  inference
speed due to necessity of dynamic decompression of the compressed model.

Inputs:
    DF      -   decision forest built by forest builder

Outputs:
    DF      -   replaced by compressed forest

Result:
    compression factor (in-RAM size of the compressed model vs than of the
    uncompressed one), positive number larger than 1.0
ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dfbinarycompression(decisionforest df);
</pre>
<a name=sub_dfbuilderbuildrandomforest></a><h6 class=pageheader>dfbuilderbuildrandomforest Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds decision forest according to current settings using
dataset internally stored in the builder object. Dense algorithm is used.

NOTE: this   function   uses   dense  algorithm  for  forest  construction
      independently from the dataset format (dense or sparse).

NOTE: forest built with this function is  stored  in-memory  using  64-bit
      data structures for offsets/indexes/split values. It is possible  to
      convert  forest  into  more  memory-efficient   compressed    binary
      representation.  Depending  on  the  problem  properties,  3.7x-5.7x
      compression factors are possible.

      The downsides of compression are (a) slight reduction in  the  model
      accuracy and (b) ~1.5x reduction in  the  inference  speed  (due  to
      increased complexity of the storage format).

      See comments on dfbinarycompression() for more info.

Default settings are used by the algorithm; you can tweak  them  with  the
help of the following functions:
* dfbuildersetrfactor() - to control a fraction of the  dataset  used  for
  subsampling
* dfbuildersetrandomvars() - to control number of variables randomly chosen
  for decision rule creation

Inputs:
    S           -   decision forest builder object
    NTrees      -   NTrees &ge; 1, number of trees to train

Outputs:
    DF          -   decision forest. You can compress this forest to  more
                    compact 16-bit representation with dfbinarycompression()
    Rep         -   report, see below for information on its fields.

==== report information produced by forest construction function ====

Decision forest training report includes following information:
* training set errors
* out-of-bag estimates of errors
* variable importance ratings

Following fields are used to store information:
* training set errors are stored in rep.relclserror, rep.avgce, rep.rmserror,
  rep.avgerror and rep.avgrelerror
* out-of-bag estimates of errors are stored in rep.oobrelclserror, rep.oobavgce,
  rep.oobrmserror, rep.oobavgerror and rep.oobavgrelerror

Variable importance reports, if requested by dfbuildersetimportancegini(),
dfbuildersetimportancetrngini() or dfbuildersetimportancepermutation()
call, are stored in:
* rep.varimportances field stores importance ratings
* rep.topvars stores variable indexes ordered from the most important to
  less important ones

You can find more information about report fields in:
* comments on dfreport structure
* comments on dfbuildersetimportancegini function
* comments on dfbuildersetimportancetrngini function
* comments on dfbuildersetimportancepermutation function
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuilderbuildrandomforest(decisionforestbuilder s, ae_int_t ntrees, decisionforest &amp;df, dfreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuildercreate></a><h6 class=pageheader>dfbuildercreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine creates DecisionForestBuilder  object  which  is  used  to
train decision forests.

By default, new builder stores empty dataset and some  reasonable  default
settings. At the very least, you should specify dataset prior to  building
decision forest. You can also tweak settings of  the  forest  construction
algorithm (recommended, although default setting should work well).

Following actions are mandatory:
* calling dfbuildersetdataset() to specify dataset
* calling dfbuilderbuildrandomforest()  to  build  decision  forest  using
  current dataset and default settings

Additionally, you may call:
* dfbuildersetrndvars() or dfbuildersetrndvarsratio() to specify number of
  variables randomly chosen for each split
* dfbuildersetsubsampleratio() to specify fraction of the dataset randomly
  subsampled to build each tree
* dfbuildersetseed() to control random seed chosen for tree construction

Inputs:
    none

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildercreate(decisionforestbuilder &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuildergetprogress></a><h6 class=pageheader>dfbuildergetprogress Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is an alias for dfbuilderpeekprogress(), left in ALGLIB  for
backward compatibility reasons.
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dfbuildergetprogress(decisionforestbuilder s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuilderpeekprogress></a><h6 class=pageheader>dfbuilderpeekprogress Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to peek into  decision  forest  construction process
from some other thread and get current progress indicator.

It returns value in [0,1].

Inputs:
    S           -   decision forest builder object used  to  build  forest
                    in some other thread

Result:
    progress value, in [0,1]
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dfbuilderpeekprogress(decisionforestbuilder s);
</pre>
<a name=sub_dfbuildersetdataset></a><h6 class=pageheader>dfbuildersetdataset Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine adds dense dataset to the internal storage of the  builder
object. Specifying your dataset in the dense format means that  the  dense
version of the forest construction algorithm will be invoked.

Inputs:
    S           -   decision forest builder object
    XY          -   array[NPoints,NVars+1] (minimum size; actual size  can
                    be larger, only leading part is used anyway), dataset:
                    * first NVars elements of each row store values of the
                      independent variables
                    * last  column  store class number (in 0...NClasses-1)
                      or real value of the dependent variable
    NPoints     -   number of rows in the dataset, NPoints &ge; 1
    NVars       -   number of independent variables, NVars &ge; 1
    NClasses    -   indicates type of the problem being solved:
                    * NClasses &ge; 2 means  that  classification  problem  is
                      solved  (last  column  of  the  dataset stores class
                      number)
                    * NClasses=1 means that regression problem  is  solved
                      (last column of the dataset stores variable value)

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetdataset(decisionforestbuilder s, real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuildersetimportancenone></a><h6 class=pageheader>dfbuildersetimportancenone Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  tells  decision  forest  construction  algorithm  to  skip
variable importance estimation.

Inputs:
    S           -   decision forest builder object

Outputs:
    S           -   decision forest builder object. Next call to the forest
                    construction function will result in forest being built
                    without variable importance estimation.
ALGLIB: Copyright 29.07.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetimportancenone(decisionforestbuilder s);
</pre>
<a name=sub_dfbuildersetimportanceoobgini></a><h6 class=pageheader>dfbuildersetimportanceoobgini Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  tells  decision  forest  construction  algorithm  to   use
out-of-bag version of Gini variable importance estimation (also  known  as
OOB-MDI).

This version of importance estimation algorithm analyzes mean decrease  in
impurity (MDI) on out-of-bag sample during splits. The result  is  divided
by impurity at the root node in order to produce estimate in [0,1] range.

Such estimates are fast to calculate and resistant to  overfitting  issues
(thanks to the  out-of-bag  estimates  used). However, OOB Gini rating has
following downsides:
* there exist some bias towards continuous and high-cardinality categorical
  variables
* Gini rating allows us to order variables by importance, but it  is  hard
  to define importance of the variable by itself.

NOTE: informally speaking, MDA (permutation importance) rating answers the
      question  &quot;what  part  of  the  model  predictive power is ruined by
      permuting k-th variable?&quot; while MDI tells us &quot;what part of the model
      predictive power was achieved due to usage of k-th variable&quot;.

      Thus, MDA rates each variable independently at &quot;0 to 1&quot;  scale while
      MDI (and OOB-MDI too) tends to divide &quot;unit  amount  of  importance&quot;
      between several important variables.

      If  all  variables  are  equally  important,  they  will  have  same
      MDI/OOB-MDI rating, equal (for OOB-MDI: roughly equal)  to  1/NVars.
      However, roughly  same  picture  will  be  produced   for  the  &quot;all
      variables provide information no one is critical&quot; situation  and for
      the &quot;all variables are critical, drop any one, everything is ruined&quot;
      situation.

      Contrary to that, MDA will rate critical variable as ~1.0 important,
      and important but non-critical variable will  have  less  than  unit
      rating.

NOTE: quite an often MDA and MDI return same results. It generally happens
      on problems with low test set error (a few  percents  at  most)  and
      large enough training set to avoid overfitting.

      The difference between MDA, MDI and OOB-MDI becomes  important  only
      on &quot;hard&quot; tasks with high test set error and/or small training set.

Inputs:
    S           -   decision forest builder object

Outputs:
    S           -   decision forest builder object. Next call to the forest
                    construction function will produce:
                    * importance estimates in rep.varimportances field
                    * variable ranks in rep.topvars field
ALGLIB: Copyright 29.07.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetimportanceoobgini(decisionforestbuilder s);
</pre>
<a name=sub_dfbuildersetimportancepermutation></a><h6 class=pageheader>dfbuildersetimportancepermutation Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  tells  decision  forest  construction  algorithm  to   use
permutation variable importance estimator (also known as MDA).

This version of importance estimation algorithm analyzes mean increase  in
out-of-bag sum of squared  residuals  after  random  permutation  of  J-th
variable. The result is divided by error computed with all variables being
perturbed in order to produce R-squared-like estimate in [0,1] range.

Such estimate  is  slower to calculate than Gini-based rating  because  it
needs multiple inference runs for each of variables being studied.

ALGLIB uses parallelized and highly  optimized  algorithm  which  analyzes
path through the decision tree and allows  to  handle  most  perturbations
in O(1) time; nevertheless, requesting MDA importances may increase forest
construction time from 10% to 200% (or more,  if  you  have  thousands  of
variables).

However, MDA rating has following benefits over Gini-based ones:
* no bias towards specific variable types
* ability to directly evaluate &quot;absolute&quot; importance of some  variable  at
  &quot;0 to 1&quot; scale (contrary to Gini-based rating, which returns comparative
  importances).

NOTE: informally speaking, MDA (permutation importance) rating answers the
      question  &quot;what  part  of  the  model  predictive power is ruined by
      permuting k-th variable?&quot; while MDI tells us &quot;what part of the model
      predictive power was achieved due to usage of k-th variable&quot;.

      Thus, MDA rates each variable independently at &quot;0 to 1&quot;  scale while
      MDI (and OOB-MDI too) tends to divide &quot;unit  amount  of  importance&quot;
      between several important variables.

      If  all  variables  are  equally  important,  they  will  have  same
      MDI/OOB-MDI rating, equal (for OOB-MDI: roughly equal)  to  1/NVars.
      However, roughly  same  picture  will  be  produced   for  the  &quot;all
      variables provide information no one is critical&quot; situation  and for
      the &quot;all variables are critical, drop any one, everything is ruined&quot;
      situation.

      Contrary to that, MDA will rate critical variable as ~1.0 important,
      and important but non-critical variable will  have  less  than  unit
      rating.

NOTE: quite an often MDA and MDI return same results. It generally happens
      on problems with low test set error (a few  percents  at  most)  and
      large enough training set to avoid overfitting.

      The difference between MDA, MDI and OOB-MDI becomes  important  only
      on &quot;hard&quot; tasks with high test set error and/or small training set.

Inputs:
    S           -   decision forest builder object

Outputs:
    S           -   decision forest builder object. Next call to the forest
                    construction function will produce:
                    * importance estimates in rep.varimportances field
                    * variable ranks in rep.topvars field
ALGLIB: Copyright 29.07.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetimportancepermutation(decisionforestbuilder s);
</pre>
<a name=sub_dfbuildersetimportancetrngini></a><h6 class=pageheader>dfbuildersetimportancetrngini Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  tells  decision  forest  construction  algorithm  to   use
Gini impurity based variable importance estimation (also known as MDI).

This version of importance estimation algorithm analyzes mean decrease  in
impurity (MDI) on training sample during  splits.  The result  is  divided
by impurity at the root node in order to produce estimate in [0,1] range.

Such estimates are fast to calculate and beautifully  normalized  (sum  to
one) but have following downsides:
* They ALWAYS sum to 1.0, even if output is completely unpredictable. I.e.
  MDI allows to order variables by importance, but does not  tell us about
  &quot;absolute&quot; importances of variables
* there exist some bias towards continuous and high-cardinality categorical
  variables

NOTE: informally speaking, MDA (permutation importance) rating answers the
      question  &quot;what  part  of  the  model  predictive power is ruined by
      permuting k-th variable?&quot; while MDI tells us &quot;what part of the model
      predictive power was achieved due to usage of k-th variable&quot;.

      Thus, MDA rates each variable independently at &quot;0 to 1&quot;  scale while
      MDI (and OOB-MDI too) tends to divide &quot;unit  amount  of  importance&quot;
      between several important variables.

      If  all  variables  are  equally  important,  they  will  have  same
      MDI/OOB-MDI rating, equal (for OOB-MDI: roughly equal)  to  1/NVars.
      However, roughly  same  picture  will  be  produced   for  the  &quot;all
      variables provide information no one is critical&quot; situation  and for
      the &quot;all variables are critical, drop any one, everything is ruined&quot;
      situation.

      Contrary to that, MDA will rate critical variable as ~1.0 important,
      and important but non-critical variable will  have  less  than  unit
      rating.

NOTE: quite an often MDA and MDI return same results. It generally happens
      on problems with low test set error (a few  percents  at  most)  and
      large enough training set to avoid overfitting.

      The difference between MDA, MDI and OOB-MDI becomes  important  only
      on &quot;hard&quot; tasks with high test set error and/or small training set.

Inputs:
    S           -   decision forest builder object

Outputs:
    S           -   decision forest builder object. Next call to the forest
                    construction function will produce:
                    * importance estimates in rep.varimportances field
                    * variable ranks in rep.topvars field
ALGLIB: Copyright 29.07.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetimportancetrngini(decisionforestbuilder s);
</pre>
<a name=sub_dfbuildersetrdfalgo></a><h6 class=pageheader>dfbuildersetrdfalgo Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets random decision forest construction algorithm.

As for now, only one decision forest construction algorithm is supported -
a dense &quot;baseline&quot; RDF algorithm.

Inputs:
    S           -   decision forest builder object
    AlgoType    -   algorithm type:
                    * 0 = baseline dense RDF

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetrdfalgo(decisionforestbuilder s, ae_int_t algotype);
</pre>
<a name=sub_dfbuildersetrdfsplitstrength></a><h6 class=pageheader>dfbuildersetrdfsplitstrength Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  split  selection  algorithm used by decision  forest
classifier. You may choose several algorithms, with  different  speed  and
quality of the results.

Inputs:
    S           -   decision forest builder object
    SplitStrength-  split type:
                    * 0 = split at the random position, fastest one
                    * 1 = split at the middle of the range
                    * 2 = strong split at the best point of the range (default)

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetrdfsplitstrength(decisionforestbuilder s, ae_int_t splitstrength);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuildersetrndvars></a><h6 class=pageheader>dfbuildersetrndvars Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets number  of  variables  (in  [1,NVars]  range)  used  by
decision forest construction algorithm.

The default option is to use roughly sqrt(NVars) variables.

Inputs:
    S           -   decision forest builder object
    RndVars     -   number of randomly selected variables; values  outside
                    of [1,NVars] range are silently clipped.

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetrndvars(decisionforestbuilder s, ae_int_t rndvars);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuildersetrndvarsauto></a><h6 class=pageheader>dfbuildersetrndvarsauto Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function tells decision forest builder to automatically choose number
of  variables  used  by  decision forest construction  algorithm.  Roughly
sqrt(NVars) variables will be used.

Inputs:
    S           -   decision forest builder object

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetrndvarsauto(decisionforestbuilder s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuildersetrndvarsratio></a><h6 class=pageheader>dfbuildersetrndvarsratio Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets number of variables used by decision forest construction
algorithm as a fraction of total variable count (0,1) range.

The default option is to use roughly sqrt(NVars) variables.

Inputs:
    S           -   decision forest builder object
    F           -   round(NVars*F) variables are selected

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetrndvarsratio(decisionforestbuilder s, <b>double</b> f);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuildersetseed></a><h6 class=pageheader>dfbuildersetseed Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets seed used by internal RNG for  random  subsampling  and
random selection of variable subsets.

By default random seed is used, i.e. every time you build decision forest,
we seed generator with new value  obtained  from  system-wide  RNG.  Thus,
decision forest builder returns non-deterministic results. You can  change
such behavior by specyfing fixed positive seed value.

Inputs:
    S           -   decision forest builder object
    SeedVal     -   seed value:
                    * positive values are used for seeding RNG with fixed
                      seed, i.e. subsequent runs on same data will return
                      same decision forests
                    * non-positive seed means that random seed is used
                      for every run of builder, i.e. subsequent  runs  on
                      same  datasets  will  return   slightly   different
                      decision forests

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetseed(decisionforestbuilder s, ae_int_t seedval);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuildersetsubsampleratio></a><h6 class=pageheader>dfbuildersetsubsampleratio Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets size of dataset subsample generated the decision forest
construction algorithm. Size is specified as a fraction of  total  dataset
size.

The default option is to use 50% of the dataset for training, 50% for  the
OOB estimates. You can decrease fraction F down to 10%, 1% or  even  below
in order to reduce overfitting.

Inputs:
    S           -   decision forest builder object
    F           -   fraction of the dataset to use, in (0,1] range. Values
                    outside of this range will  be  silently  clipped.  At
                    least one element is always selected for the  training
                    set.

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildersetsubsampleratio(decisionforestbuilder s, <b>double</b> f);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfbuildrandomdecisionforest></a><h6 class=pageheader>dfbuildrandomdecisionforest Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds random decision forest.

--------- DEPRECATED VERSION! USE DECISION FOREST BUILDER OBJECT ---------
ALGLIB: Copyright 19.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildrandomdecisionforest(real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t ntrees, <b>double</b> r, ae_int_t &amp;info, decisionforest &amp;df, dfreport &amp;rep);
</pre>
<a name=sub_dfbuildrandomdecisionforestx1></a><h6 class=pageheader>dfbuildrandomdecisionforestx1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds random decision forest.

--------- DEPRECATED VERSION! USE DECISION FOREST BUILDER OBJECT ---------
ALGLIB: Copyright 19.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfbuildrandomdecisionforestx1(real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t ntrees, ae_int_t nrndvars, <b>double</b> r, ae_int_t &amp;info, decisionforest &amp;df, dfreport &amp;rep);
</pre>
<a name=sub_dfclassify></a><h6 class=pageheader>dfclassify Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns most probable class number for an  input  X.  It  is
same as calling  dfprocess(model,x,y), then determining i=argmax(y[i]) and
returning i.

A class number in [0,NOut) range in returned for classification  problems,
-1 is returned when this function is called for regression problems.

IMPORTANT: this function is thread-unsafe and modifies internal structures
           of the model! You can not use same model  object  for  parallel
           evaluation from several threads.

           Use dftsprocess()  with independent  thread-local  buffers,  if
           you need thread-safe evaluation.

Inputs:
    Model   -   decision forest model
    X       -   input vector,  array[0..NVars-1].

Result:
    class number, -1 for regression tasks
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t dfclassify(decisionforest model, real_1d_array x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfcreatebuffer></a><h6 class=pageheader>dfcreatebuffer Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates buffer  structure  which  can  be  used  to  perform
parallel inference requests.

DF subpackage  provides two sets of computing functions - ones  which  use
internal buffer of DF model  (these  functions are single-threaded because
they use same buffer, which can not  shared  between  threads),  and  ones
which use external buffer.

This function is used to initialize external buffer.

Inputs:
    Model       -   DF model which is associated with newly created buffer

Outputs:
    Buf         -   external buffer.

IMPORTANT: buffer object should be used only with model which was used  to
           initialize buffer. Any attempt to  use  buffer  with  different
           object is dangerous - you  may   get  integrity  check  failure
           (exception) because sizes of internal  arrays  do  not  fit  to
           dimensions of the model structure.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfcreatebuffer(decisionforest model, decisionforestbuffer &amp;buf);
</pre>
<a name=sub_dfprocess></a><h6 class=pageheader>dfprocess Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inference using decision forest

IMPORTANT: this  function  is  thread-unsafe  and  may   modify   internal
           structures of the model! You can not use same model  object for
           parallel evaluation from several threads.

           Use dftsprocess()  with  independent  thread-local  buffers  if
           you need thread-safe evaluation.

Inputs:
    DF      -   decision forest model
    X       -   input vector,  array[NVars]
    Y       -   possibly preallocated buffer, reallocated if too small

Outputs:
    Y       -   result. Regression estimate when solving regression  task,
                vector of posterior probabilities for classification task.

See also DFProcessI.
ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfprocess(decisionforest df, real_1d_array x, real_1d_array &amp;y);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfprocess0></a><h6 class=pageheader>dfprocess0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns first component of the  inferred  vector  (i.e.  one
with index #0).

It is a convenience wrapper for dfprocess() intended for either:
* 1-dimensional regression problems
* 2-class classification problems

In the former case this function returns inference result as scalar, which
is definitely more convenient that wrapping it as vector.  In  the  latter
case it returns probability of object belonging to class #0.

If you call it for anything different from two cases above, it  will  work
as defined, i.e. return y[0], although it is of less use in such cases.

IMPORTANT: this function is thread-unsafe and modifies internal structures
           of the model! You can not use same model  object  for  parallel
           evaluation from several threads.

           Use dftsprocess() with  independent  thread-local  buffers,  if
           you need thread-safe evaluation.

Inputs:
    Model   -   DF model
    X       -   input vector,  array[0..NVars-1].

Result:
    Y[0]
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dfprocess0(decisionforest model, real_1d_array x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_randomforest_cls class=nav>randomforest_cls</a> | <a href=#example_randomforest_reg class=nav>randomforest_reg</a> ]</p>
<a name=sub_dfprocessi></a><h6 class=pageheader>dfprocessi Function</h6>
<hr width=600 align=left>
<pre class=narration>
'interactive' variant of DFProcess for languages like Python which support
constructs like &quot;Y = DFProcessI(DF,X)&quot; and interactive mode of interpreter

This function allocates new array on each call,  so  it  is  significantly
slower than its 'non-interactive' counterpart, but it is  more  convenient
when you call it from command line.

IMPORTANT: this  function  is  thread-unsafe  and  may   modify   internal
           structures of the model! You can not use same model  object for
           parallel evaluation from several threads.

           Use dftsprocess()  with  independent  thread-local  buffers  if
           you need thread-safe evaluation.
ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfprocessi(decisionforest df, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_dfrelclserror></a><h6 class=pageheader>dfrelclserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Relative classification error on the test set

Inputs:
    DF      -   decision forest model
    XY      -   test set
    NPoints -   test set size

Result:
    percent of incorrectly classified cases.
    Zero if model solves regression task.
ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dfrelclserror(decisionforest df, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_dfrmserror></a><h6 class=pageheader>dfrmserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
RMS error on the test set

Inputs:
    DF      -   decision forest model
    XY      -   test set
    NPoints -   test set size

Result:
    root mean square error.
    Its meaning for regression task is obvious. As for
    classification task, RMS error means error when estimating posterior
    probabilities.
ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dfrmserror(decisionforest df, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_dfserialize></a><h6 class=pageheader>dfserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: serialization
These functions serialize a data structure to a C++ string or stream.
* serialization can be freely moved across 32-bit and 64-bit systems,
  and different byte orders. For example, you can serialize a string
  on a SPARC and unserialize it on an x86.
* ALGLIB++ serialization is compatible with serialization in ALGLIB,
  in both directions.
Important properties of s_out:
* it contains alphanumeric characters, dots, underscores, minus signs
* these symbols are grouped into words, which are separated by spaces
  and Windows-style (CR+LF) newlines
ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfserialize(decisionforest &amp;obj, std::string &amp;s_out);
<b>void</b> dfserialize(decisionforest &amp;obj, std::ostream &amp;s_out);
</pre>
<a name=sub_dftsprocess></a><h6 class=pageheader>dftsprocess Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inference using decision forest

Thread-safe procesing using external buffer for temporaries.

This function is thread-safe (i.e.  you  can  use  same  DF   model  from
multiple threads) as long as you use different buffer objects for different
threads.

Inputs:
    DF      -   decision forest model
    Buf     -   buffer object, must be  allocated  specifically  for  this
                model with dfcreatebuffer().
    X       -   input vector,  array[NVars]
    Y       -   possibly preallocated buffer, reallocated if too small

Outputs:
    Y       -   result. Regression estimate when solving regression  task,
                vector of posterior probabilities for classification task.

See also DFProcessI.
ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dftsprocess(decisionforest df, decisionforestbuffer buf, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_dfunserialize></a><h6 class=pageheader>dfunserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: unserialization
These functions unserialize a data structure from a C++ string or stream.
Important properties of s_in:
* any combination of spaces, tabs, Windows or Unix stype newlines can
  be used as separators, so as to allow flexible reformatting of the
  stream or string from text or XML files.
* But you should not insert separators into the middle of the "words"
  nor you should change case of letters.
ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> dfunserialize(<b>const</b> std::string &amp;s_in, decisionforest &amp;obj);
<b>void</b> dfunserialize(<b>const</b> std::istream &amp;s_in, decisionforest &amp;obj);
</pre>
<a name=example_randomforest_cls></a><h6 class=pageheader>randomforest_cls Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// The very simple classification example: classify points (x,y) in 2D space</font>
<font color=navy>// as ones with x &ge; 0 and ones with x &lt; 0 (y is ignored, but our classifier</font>
<font color=navy>// has to find out it).</font>
<font color=navy>//</font>
<font color=navy>// First, we have to create decision forest builder object, load dataset and</font>
<font color=navy>// specify training settings. Our dataset is specified as matrix, which has</font>
<font color=navy>// following format:</font>
<font color=navy>//</font>
<font color=navy>//     x0 y0 class0</font>
<font color=navy>//     x1 y1 class1</font>
<font color=navy>//     x2 y2 class2</font>
<font color=navy>//     ....</font>
<font color=navy>//</font>
<font color=navy>// Here xi and yi can be any values (and in fact you can have any number of</font>
<font color=navy>// independent variables), and classi MUST be integer number in [0,NClasses)</font>
<font color=navy>// range. In our example we denote points with x &ge; 0 as <b>class</b> #0, and</font>
<font color=navy>// ones with negative xi as <b>class</b> #1.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: <b>if</b> you want to solve regression problem, specify NClasses=1. In</font>
<font color=navy>//       this case last column of xy can be any numeric value.</font>
<font color=navy>//</font>
<font color=navy>// For the sake of simplicity, our example includes only 4-point dataset.</font>
<font color=navy>// However, random forests are able to cope with extremely large datasets</font>
<font color=navy>// having millions of examples.</font>
   decisionforestbuilder builder;
   ae_int_t nvars = 2;
   ae_int_t nclasses = 2;
   ae_int_t npoints = 4;
   real_2d_array xy = <font color=blue><b>&quot;[[1,1,0],[1,-1,0],[-1,1,1],[-1,-1,1]]&quot;</b></font>;

   dfbuildercreate(builder);
   dfbuildersetdataset(builder, xy, npoints, nvars, nclasses);

<font color=navy>// in our example we train decision forest using full sample - it allows us</font>
<font color=navy>// to get zero classification error. However, in practical applications smaller</font>
<font color=navy>// values are used: 50%, 25%, 5% or even less.</font>
   dfbuildersetsubsampleratio(builder, 1.0);

<font color=navy>// we train random forest with just one tree; again, in real life situations</font>
<font color=navy>// you typically need from 50 to 500 trees.</font>
   ae_int_t ntrees = 1;
   decisionforest forest;
   dfreport rep;
   dfbuilderbuildrandomforest(builder, ntrees, forest, rep);

<font color=navy>// with such settings (100% of the training set is used) you can expect</font>
<font color=navy>// zero classification error. Beautiful results, but remember - in real life</font>
<font color=navy>// you <b>do</b> not need zero TRAINING SET error, you need good generalization.</font>

   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(rep.relclserror)); <font color=navy>// EXPECTED: 0.0000</font>

<font color=navy>// now, let's perform some simple processing with dfprocess()</font>
   real_1d_array x = <font color=blue><b>&quot;[+1,0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[]&quot;</b></font>;
   dfprocess(forest, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(3).c_str()); <font color=navy>// EXPECTED: [+1,0]</font>

<font color=navy>// another option is to use dfprocess0() which returns just first component</font>
<font color=navy>// of the output vector y. ideal <b>for</b> regression problems and binary classifiers.</font>
   <b>double</b> y0;
   y0 = dfprocess0(forest, x);
   printf(<font color=blue><b>&quot;%.3f\n&quot;</b></font>, <b>double</b>(y0)); <font color=navy>// EXPECTED: 1.000</font>

<font color=navy>// finally, you can use dfclassify() which returns most probable <b>class</b> index (i.e. argmax y[i]).</font>
   ae_int_t i;
   i = dfclassify(forest, x);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(i)); <font color=navy>// EXPECTED: 0</font>
   <b>return</b> 0;
}
</pre>
<a name=example_randomforest_reg></a><h6 class=pageheader>randomforest_reg Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// The very simple regression example: model f(x,y)=x+y</font>
<font color=navy>//</font>
<font color=navy>// First, we have to create DF builder object, load dataset and specify</font>
<font color=navy>// training settings. Our dataset is specified as matrix, which has following</font>
<font color=navy>// format:</font>
<font color=navy>//</font>
<font color=navy>//     x0 y0 f0</font>
<font color=navy>//     x1 y1 f1</font>
<font color=navy>//     x2 y2 f2</font>
<font color=navy>//     ....</font>
<font color=navy>//</font>
<font color=navy>// Here xi and yi can be any values, and fi is a dependent function value.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: you can also solve classification problems with DF models, see</font>
<font color=navy>//       another example <b>for</b> this unit.</font>
   decisionforestbuilder builder;
   ae_int_t nvars = 2;
   ae_int_t nclasses = 1;
   ae_int_t npoints = 4;
   real_2d_array xy = <font color=blue><b>&quot;[[1,1,+2],[1,-1,0],[-1,1,0],[-1,-1,-2]]&quot;</b></font>;

   dfbuildercreate(builder);
   dfbuildersetdataset(builder, xy, npoints, nvars, nclasses);

<font color=navy>// in our example we train decision forest using full sample - it allows us</font>
<font color=navy>// to get zero classification error. However, in practical applications smaller</font>
<font color=navy>// values are used: 50%, 25%, 5% or even less.</font>
   dfbuildersetsubsampleratio(builder, 1.0);

<font color=navy>// we train random forest with just one tree; again, in real life situations</font>
<font color=navy>// you typically need from 50 to 500 trees.</font>
   ae_int_t ntrees = 1;
   decisionforest model;
   dfreport rep;
   dfbuilderbuildrandomforest(builder, ntrees, model, rep);

<font color=navy>// with such settings (full sample is used) you can expect zero RMS error on the</font>
<font color=navy>// training set. Beautiful results, but remember - in real life you <b>do</b> not</font>
<font color=navy>// need zero TRAINING SET error, you need good generalization.</font>

   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(rep.rmserror)); <font color=navy>// EXPECTED: 0.0000</font>

<font color=navy>// now, let's perform some simple processing with dfprocess()</font>
   real_1d_array x = <font color=blue><b>&quot;[+1,+1]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[]&quot;</b></font>;
   dfprocess(model, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(3).c_str()); <font color=navy>// EXPECTED: [+2]</font>

<font color=navy>// another option is to use dfprocess0() which returns just first component</font>
<font color=navy>// of the output vector y. ideal <b>for</b> regression problems and binary classifiers.</font>
   <b>double</b> y0;
   y0 = dfprocess0(model, x);
   printf(<font color=blue><b>&quot;%.3f\n&quot;</b></font>, <b>double</b>(y0)); <font color=navy>// EXPECTED: 2.000</font>

<font color=navy>// there also exist another convenience function, dfclassify(),</font>
<font color=navy>// but it does not work <b>for</b> regression problems - it always returns -1.</font>
   ae_int_t i;
   i = dfclassify(model, x);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(i)); <font color=navy>// EXPECTED: -1</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_filters></a><h4 class=pageheader>8.2.5. filters Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_filterema class=toc>filterema</a> |
<a href=#sub_filterlrma class=toc>filterlrma</a> |
<a href=#sub_filtersma class=toc>filtersma</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_filters_d_ema class=toc>filters_d_ema</a></td><td width=15>&nbsp;</td><td>EMA(alpha) filter</td></tr>
<tr align=left valign=top><td><a href=#example_filters_d_lrma class=toc>filters_d_lrma</a></td><td width=15>&nbsp;</td><td>LRMA(k) filter</td></tr>
<tr align=left valign=top><td><a href=#example_filters_d_sma class=toc>filters_d_sma</a></td><td width=15>&nbsp;</td><td>SMA(k) filter</td></tr>
</table>
</div>
<a name=sub_filterema></a><h6 class=pageheader>filterema Function</h6>
<hr width=600 align=left>
<pre class=narration>
Filters: exponential moving averages.

This filter replaces array by results of EMA(alpha) filter. EMA(alpha) is
defined as filter which replaces X[] by S[]:
    S[0] = X[0]
    S[t] = alpha*X[t] + (1-alpha)*S[t-1]

Inputs:
    X           -   array[N], array to process. It can be larger than N,
                    in this case only first N points are processed.
    N           -   points count, N &ge; 0
    alpha       -   0 &lt; alpha &le; 1, smoothing parameter.

Outputs:
    X           -   array, whose first N elements were processed
                    with EMA(alpha)

NOTE 1: this function uses efficient in-place  algorithm  which  does not
        allocate temporary arrays.

NOTE 2: this algorithm uses BOTH previous points and  current  one,  i.e.
        new value of X[i] depends on BOTH previous point and X[i] itself.

NOTE 3: technical analytis users quite often work  with  EMA  coefficient
        expressed in DAYS instead of fractions. If you want to  calculate
        EMA(N), where N is a number of days, you can use alpha=2/(N+1).
ALGLIB: Copyright 25.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> filterema(real_1d_array &amp;x, ae_int_t n, <b>double</b> alpha);
<b>void</b> filterema(real_1d_array &amp;x, <b>double</b> alpha);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_filters_d_ema class=nav>filters_d_ema</a> ]</p>
<a name=sub_filterlrma></a><h6 class=pageheader>filterlrma Function</h6>
<hr width=600 align=left>
<pre class=narration>
Filters: linear regression moving averages.

This filter replaces array by results of LRMA(K) filter.

LRMA(K) is defined as filter which, for each data  point,  builds  linear
regression  model  using  K  prevous  points (point itself is included in
these K points) and calculates value of this linear model at the point in
question.

Inputs:
    X           -   array[N], array to process. It can be larger than N,
                    in this case only first N points are processed.
    N           -   points count, N &ge; 0
    K           -   K &ge; 1 (K can be larger than N ,  such  cases  will  be
                    correctly handled). Window width. K=1 corresponds  to
                    identity transformation (nothing changes).

Outputs:
    X           -   array, whose first N elements were processed with SMA(K)

NOTE 1: this function uses efficient in-place  algorithm  which  does not
        allocate temporary arrays.

NOTE 2: this algorithm makes only one pass through array and uses running
        sum  to speed-up calculation of the averages. Additional measures
        are taken to ensure that running sum on a long sequence  of  zero
        elements will be correctly reset to zero even in the presence  of
        round-off error.

NOTE 3: this  is  unsymmetric version of the algorithm,  which  does  NOT
        averages points after the current one. Only X[i], X[i-1], ... are
        used when calculating new value of X[i]. We should also note that
        this algorithm uses BOTH previous points and  current  one,  i.e.
        new value of X[i] depends on BOTH previous point and X[i] itself.
ALGLIB: Copyright 25.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> filterlrma(real_1d_array &amp;x, ae_int_t n, ae_int_t k);
<b>void</b> filterlrma(real_1d_array &amp;x, ae_int_t k);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_filters_d_lrma class=nav>filters_d_lrma</a> ]</p>
<a name=sub_filtersma></a><h6 class=pageheader>filtersma Function</h6>
<hr width=600 align=left>
<pre class=narration>
Filters: simple moving averages (unsymmetric).

This filter replaces array by results of SMA(K) filter. SMA(K) is defined
as filter which averages at most K previous points (previous - not points
AROUND central point) - or less, in case of the first K-1 points.

Inputs:
    X           -   array[N], array to process. It can be larger than N,
                    in this case only first N points are processed.
    N           -   points count, N &ge; 0
    K           -   K &ge; 1 (K can be larger than N ,  such  cases  will  be
                    correctly handled). Window width. K=1 corresponds  to
                    identity transformation (nothing changes).

Outputs:
    X           -   array, whose first N elements were processed with SMA(K)

NOTE 1: this function uses efficient in-place  algorithm  which  does not
        allocate temporary arrays.

NOTE 2: this algorithm makes only one pass through array and uses running
        sum  to speed-up calculation of the averages. Additional measures
        are taken to ensure that running sum on a long sequence  of  zero
        elements will be correctly reset to zero even in the presence  of
        round-off error.

NOTE 3: this  is  unsymmetric version of the algorithm,  which  does  NOT
        averages points after the current one. Only X[i], X[i-1], ... are
        used when calculating new value of X[i]. We should also note that
        this algorithm uses BOTH previous points and  current  one,  i.e.
        new value of X[i] depends on BOTH previous point and X[i] itself.
ALGLIB: Copyright 25.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> filtersma(real_1d_array &amp;x, ae_int_t n, ae_int_t k);
<b>void</b> filtersma(real_1d_array &amp;x, ae_int_t k);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_filters_d_sma class=nav>filters_d_sma</a> ]</p>
<a name=example_filters_d_ema></a><h6 class=pageheader>filters_d_ema Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Here we demonstrate EMA(0.5) filtering <b>for</b> time series.</font>
   real_1d_array x = <font color=blue><b>&quot;[5,6,7,8]&quot;</b></font>;
<font color=navy>// Apply filter.</font>
<font color=navy>// We should get [5, 5.5, 6.25, 7.125] as result</font>
   filterema(x, 0.5);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(4).c_str()); <font color=navy>// EXPECTED: [5,5.5,6.25,7.125]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_filters_d_lrma></a><h6 class=pageheader>filters_d_lrma Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Here we demonstrate LRMA(3) filtering <b>for</b> time series.</font>
   real_1d_array x = <font color=blue><b>&quot;[7,8,8,9,12,12]&quot;</b></font>;
<font color=navy>// Apply filter.</font>
<font color=navy>// We should get [7.0000, 8.0000, 8.1667, 8.8333, 11.6667, 12.5000] as result</font>
<font color=navy>//    </font>
   filterlrma(x, 3);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(4).c_str()); <font color=navy>// EXPECTED: [7.0000,8.0000,8.1667,8.8333,11.6667,12.5000]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_filters_d_sma></a><h6 class=pageheader>filters_d_sma Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Here we demonstrate SMA(k) filtering <b>for</b> time series.</font>
   real_1d_array x = <font color=blue><b>&quot;[5,6,7,8]&quot;</b></font>;
<font color=navy>// Apply filter.</font>
<font color=navy>// We should get [5, 5.5, 6.5, 7.5] as result</font>
   filtersma(x, 2);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(4).c_str()); <font color=navy>// EXPECTED: [5,5.5,6.5,7.5]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_knn></a><h4 class=pageheader>8.2.6. knn Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_knnbuffer class=toc>knnbuffer</a> |
<a href=#struct_knnbuilder class=toc>knnbuilder</a> |
<a href=#struct_knnmodel class=toc>knnmodel</a> |
<a href=#struct_knnreport class=toc>knnreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_knnallerrors class=toc>knnallerrors</a> |
<a href=#sub_knnavgce class=toc>knnavgce</a> |
<a href=#sub_knnavgerror class=toc>knnavgerror</a> |
<a href=#sub_knnavgrelerror class=toc>knnavgrelerror</a> |
<a href=#sub_knnbuilderbuildknnmodel class=toc>knnbuilderbuildknnmodel</a> |
<a href=#sub_knnbuildercreate class=toc>knnbuildercreate</a> |
<a href=#sub_knnbuildersetdatasetcls class=toc>knnbuildersetdatasetcls</a> |
<a href=#sub_knnbuildersetdatasetreg class=toc>knnbuildersetdatasetreg</a> |
<a href=#sub_knnbuildersetnorm class=toc>knnbuildersetnorm</a> |
<a href=#sub_knnclassify class=toc>knnclassify</a> |
<a href=#sub_knncreatebuffer class=toc>knncreatebuffer</a> |
<a href=#sub_knnprocess class=toc>knnprocess</a> |
<a href=#sub_knnprocess0 class=toc>knnprocess0</a> |
<a href=#sub_knnprocessi class=toc>knnprocessi</a> |
<a href=#sub_knnrelclserror class=toc>knnrelclserror</a> |
<a href=#sub_knnrewritekeps class=toc>knnrewritekeps</a> |
<a href=#sub_knnrmserror class=toc>knnrmserror</a> |
<a href=#sub_knnserialize class=toc>knnserialize</a> |
<a href=#sub_knntsprocess class=toc>knntsprocess</a> |
<a href=#sub_knnunserialize class=toc>knnunserialize</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_knn_cls class=toc>knn_cls</a></td><td width=15>&nbsp;</td><td>Simple classification with KNN model</td></tr>
<tr align=left valign=top><td><a href=#example_knn_reg class=toc>knn_reg</a></td><td width=15>&nbsp;</td><td>Simple classification with KNN model</td></tr>
</table>
</div>
<a name=struct_knnbuffer></a><h6 class=pageheader>knnbuffer Class</h6>
<hr width=600 align=left>
<pre class=narration>
Buffer object which is used to perform  various  requests  (usually  model
inference) in the multithreaded mode (multiple threads working  with  same
KNN object).

This object should be created with KNNCreateBuffer().
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> knnbuffer {
};
</pre>
<a name=struct_knnbuilder></a><h6 class=pageheader>knnbuilder Class</h6>
<hr width=600 align=left>
<pre class=narration>
A KNN builder object; this object encapsulates  dataset  and  all  related
settings, it is used to create an actual instance of KNN model.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> knnbuilder {
};
</pre>
<a name=struct_knnmodel></a><h6 class=pageheader>knnmodel Class</h6>
<hr width=600 align=left>
<pre class=narration>
KNN model, can be used for classification or regression
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> knnmodel {
};
</pre>
<a name=struct_knnreport></a><h6 class=pageheader>knnreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
KNN training report.

Following fields store training set errors:
* relclserror       -   fraction of misclassified cases, [0,1]
* avgce             -   average cross-entropy in bits per symbol
* rmserror          -   root-mean-square error
* avgerror          -   average error
* avgrelerror       -   average relative error

For classification problems:
* RMS, AVG and AVGREL errors are calculated for posterior probabilities

For regression problems:
* RELCLS and AVGCE errors are zero
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> knnreport {
   <b>double</b> relclserror;
   <b>double</b> avgce;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
};
</pre>
<a name=sub_knnallerrors></a><h6 class=pageheader>knnallerrors Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculates all kinds of errors for the model in one call.

Inputs:
    Model   -   KNN model
    XY      -   test set:
                * one row per point
                * first NVars columns store independent variables
                * depending on problem type:
                  * next column stores class number in [0,NClasses) -  for
                    classification problems
                  * next NOut columns  store  dependent  variables  -  for
                    regression problems
    NPoints -   test set size, NPoints &ge; 0

Outputs:
    Rep     -   following fields are loaded with errors for both regression
                and classification models:
                * rep.rmserror - RMS error for the output
                * rep.avgerror - average error
                * rep.avgrelerror - average relative error
                following fields are set only  for classification  models,
                zero for regression ones:
                * relclserror   - relative classification error, in [0,1]
                * avgce - average cross-entropy in bits per dataset entry

NOTE: the cross-entropy metric is too unstable when used to  evaluate  KNN
      models (such models can report exactly  zero probabilities),  so  we
      do not recommend using it.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnallerrors(knnmodel model, real_2d_array xy, ae_int_t npoints, knnreport &amp;rep);
</pre>
<a name=sub_knnavgce></a><h6 class=pageheader>knnavgce Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average cross-entropy (in bits per element) on the test set

Inputs:
    Model   -   KNN model
    XY      -   test set
    NPoints -   test set size

Result:
    CrossEntropy/NPoints.
    Zero if model solves regression task.

NOTE: the cross-entropy metric is too unstable when used to  evaluate  KNN
      models (such models can report exactly  zero probabilities),  so  we
      do not recommend using it.

NOTE: if  you  need several different kinds of error metrics, it is better
      to use knnallerrors() which computes all error metric  with just one
      pass over dataset.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> knnavgce(knnmodel model, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_knnavgerror></a><h6 class=pageheader>knnavgerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average error on the test set

Its meaning for regression task is obvious. As for classification problems,
average error means error when estimating posterior probabilities.

Inputs:
    Model   -   KNN model
    XY      -   test set
    NPoints -   test set size

Result:
    average error

NOTE: if  you  need several different kinds of error metrics, it is better
      to use knnallerrors() which computes all error metric  with just one
      pass over dataset.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> knnavgerror(knnmodel model, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_knnavgrelerror></a><h6 class=pageheader>knnavgrelerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average relative error on the test set

Its meaning for regression task is obvious. As for classification problems,
average relative error means error when estimating posterior probabilities.

Inputs:
    Model   -   KNN model
    XY      -   test set
    NPoints -   test set size

Result:
    average relative error

NOTE: if  you  need several different kinds of error metrics, it is better
      to use knnallerrors() which computes all error metric  with just one
      pass over dataset.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> knnavgrelerror(knnmodel model, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_knnbuilderbuildknnmodel></a><h6 class=pageheader>knnbuilderbuildknnmodel Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds KNN model  according  to  current  settings,  using
dataset internally stored in the builder object.

The model being built performs inference using Eps-approximate  K  nearest
neighbors search algorithm, with:
* K=1,  Eps=0 corresponding to the &quot;nearest neighbor algorithm&quot;
* K &gt; 1,  Eps=0 corresponding to the &quot;K nearest neighbors algorithm&quot;
* K &ge; 1, Eps &gt; 0 corresponding to &quot;approximate nearest neighbors algorithm&quot;

An approximate KNN is a good option for high-dimensional  datasets  (exact
KNN works slowly when dimensions count grows).

An ALGLIB implementation of kd-trees is used to perform k-nn searches.

Inputs:
    S       -   KNN builder object
    K       -   number of neighbors to search for, K &ge; 1
    Eps     -   approximation factor:
                * Eps=0 means that exact kNN search is performed
                * Eps &gt; 0 means that (1+Eps)-approximate search is performed

Outputs:
    Model       -   KNN model
    Rep         -   report
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnbuilderbuildknnmodel(knnbuilder s, ae_int_t k, <b>double</b> eps, knnmodel &amp;model, knnreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_knn_cls class=nav>knn_cls</a> | <a href=#example_knn_reg class=nav>knn_reg</a> ]</p>
<a name=sub_knnbuildercreate></a><h6 class=pageheader>knnbuildercreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine creates KNNBuilder object which is used to train KNN models.

By default, new builder stores empty dataset and some  reasonable  default
settings. At the very least, you should specify dataset prior to  building
KNN model. You can also tweak settings of the model construction algorithm
(recommended, although default settings should work well).

Following actions are mandatory:
* calling knnbuildersetdataset() to specify dataset
* calling knnbuilderbuildknnmodel() to build KNN model using current
  dataset and default settings

Additionally, you may call:
* knnbuildersetnorm() to change norm being used

Inputs:
    none

Outputs:
    S           -   KNN builder
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnbuildercreate(knnbuilder &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_knn_cls class=nav>knn_cls</a> | <a href=#example_knn_reg class=nav>knn_reg</a> ]</p>
<a name=sub_knnbuildersetdatasetcls></a><h6 class=pageheader>knnbuildersetdatasetcls Function</h6>
<hr width=600 align=left>
<pre class=narration>
Specifies classification problem (two  or  more  classes  are  predicted).
There also exists &quot;regression&quot; version of this function.

This subroutine adds dense dataset to the internal storage of the  builder
object. Specifying your dataset in the dense format means that  the  dense
version of the KNN construction algorithm will be invoked.

Inputs:
    S           -   KNN builder object
    XY          -   array[NPoints,NVars+1] (note:   actual   size  can  be
                    larger, only leading part is used anyway), dataset:
                    * first NVars elements of each row store values of the
                      independent variables
                    * next element stores class index, in [0,NClasses)
    NPoints     -   number of rows in the dataset, NPoints &ge; 1
    NVars       -   number of independent variables, NVars &ge; 1
    NClasses    -   number of classes, NClasses &ge; 2

Outputs:
    S           -   KNN builder
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnbuildersetdatasetcls(knnbuilder s, real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_knn_cls class=nav>knn_cls</a> ]</p>
<a name=sub_knnbuildersetdatasetreg></a><h6 class=pageheader>knnbuildersetdatasetreg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Specifies regression problem (one or more continuous  output variables are
predicted). There also exists &quot;classification&quot; version of this function.

This subroutine adds dense dataset to the internal storage of the  builder
object. Specifying your dataset in the dense format means that  the  dense
version of the KNN construction algorithm will be invoked.

Inputs:
    S           -   KNN builder object
    XY          -   array[NPoints,NVars+NOut] (note: actual  size  can  be
                    larger, only leading part is used anyway), dataset:
                    * first NVars elements of each row store values of the
                      independent variables
                    * next NOut elements store  values  of  the  dependent
                      variables
    NPoints     -   number of rows in the dataset, NPoints &ge; 1
    NVars       -   number of independent variables, NVars &ge; 1
    NOut        -   number of dependent variables, NOut &ge; 1

Outputs:
    S           -   KNN builder
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnbuildersetdatasetreg(knnbuilder s, real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nout);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_knn_reg class=nav>knn_reg</a> ]</p>
<a name=sub_knnbuildersetnorm></a><h6 class=pageheader>knnbuildersetnorm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets norm type used for neighbor search.

Inputs:
    S           -   decision forest builder object
    NormType    -   norm type:
                    * 0      inf-norm
                    * 1      1-norm
                    * 2      Euclidean norm (default)

Outputs:
    S           -   decision forest builder
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnbuildersetnorm(knnbuilder s, ae_int_t nrmtype);
</pre>
<a name=sub_knnclassify></a><h6 class=pageheader>knnclassify Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns most probable class number for an  input  X.  It  is
same as calling knnprocess(model,x,y), then determining i=argmax(y[i]) and
returning i.

A class number in [0,NOut) range in returned for classification  problems,
-1 is returned when this function is called for regression problems.

IMPORTANT: this function is thread-unsafe and modifies internal structures
           of the model! You can not use same model  object  for  parallel
           evaluation from several threads.

           Use knntsprocess() with independent  thread-local  buffers,  if
           you need thread-safe evaluation.

Inputs:
    Model   -   KNN model
    X       -   input vector,  array[0..NVars-1].

Result:
    class number, -1 for regression tasks
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t knnclassify(knnmodel model, real_1d_array x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_knn_cls class=nav>knn_cls</a> | <a href=#example_knn_reg class=nav>knn_reg</a> ]</p>
<a name=sub_knncreatebuffer></a><h6 class=pageheader>knncreatebuffer Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates buffer  structure  which  can  be  used  to  perform
parallel KNN requests.

KNN subpackage provides two sets of computing functions - ones  which  use
internal buffer of KNN model (these  functions are single-threaded because
they use same buffer, which can not  shared  between  threads),  and  ones
which use external buffer.

This function is used to initialize external buffer.

Inputs:
    Model       -   KNN model which is associated with newly created buffer

Outputs:
    Buf         -   external buffer.

IMPORTANT: buffer object should be used only with model which was used  to
           initialize buffer. Any attempt to  use  buffer  with  different
           object is dangerous - you  may   get  integrity  check  failure
           (exception) because sizes of internal  arrays  do  not  fit  to
           dimensions of the model structure.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knncreatebuffer(knnmodel model, knnbuffer &amp;buf);
</pre>
<a name=sub_knnprocess></a><h6 class=pageheader>knnprocess Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inference using KNN model.

See also knnprocess0(), knnprocessi() and knnclassify() for options with a
bit more convenient interface.

IMPORTANT: this function is thread-unsafe and modifies internal structures
           of the model! You can not use same model  object  for  parallel
           evaluation from several threads.

           Use knntsprocess() with independent  thread-local  buffers,  if
           you need thread-safe evaluation.

Inputs:
    Model   -   KNN model
    X       -   input vector,  array[0..NVars-1].
    Y       -   possible preallocated buffer. Reused if long enough.

Outputs:
    Y       -   result. Regression estimate when solving regression  task,
                vector of posterior probabilities for classification task.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnprocess(knnmodel model, real_1d_array x, real_1d_array &amp;y);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_knn_cls class=nav>knn_cls</a> | <a href=#example_knn_reg class=nav>knn_reg</a> ]</p>
<a name=sub_knnprocess0></a><h6 class=pageheader>knnprocess0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns first component of the  inferred  vector  (i.e.  one
with index #0).

It is a convenience wrapper for knnprocess() intended for either:
* 1-dimensional regression problems
* 2-class classification problems

In the former case this function returns inference result as scalar, which
is definitely more convenient that wrapping it as vector.  In  the  latter
case it returns probability of object belonging to class #0.

If you call it for anything different from two cases above, it  will  work
as defined, i.e. return y[0], although it is of less use in such cases.

IMPORTANT: this function is thread-unsafe and modifies internal structures
           of the model! You can not use same model  object  for  parallel
           evaluation from several threads.

           Use knntsprocess() with independent  thread-local  buffers,  if
           you need thread-safe evaluation.

Inputs:
    Model   -   KNN model
    X       -   input vector,  array[0..NVars-1].

Result:
    Y[0]
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> knnprocess0(knnmodel model, real_1d_array x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_knn_cls class=nav>knn_cls</a> | <a href=#example_knn_reg class=nav>knn_reg</a> ]</p>
<a name=sub_knnprocessi></a><h6 class=pageheader>knnprocessi Function</h6>
<hr width=600 align=left>
<pre class=narration>
'interactive' variant of knnprocess()  for  languages  like  Python  which
support constructs like &quot;y = knnprocessi(model,x)&quot; and interactive mode of
the interpreter.

This function allocates new array on each call,  so  it  is  significantly
slower than its 'non-interactive' counterpart, but it is  more  convenient
when you call it from command line.

IMPORTANT: this  function  is  thread-unsafe  and  may   modify   internal
           structures of the model! You can not use same model  object for
           parallel evaluation from several threads.

           Use knntsprocess()  with  independent  thread-local  buffers if
           you need thread-safe evaluation.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnprocessi(knnmodel model, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_knnrelclserror></a><h6 class=pageheader>knnrelclserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Relative classification error on the test set

Inputs:
    Model   -   KNN model
    XY      -   test set
    NPoints -   test set size

Result:
    percent of incorrectly classified cases.
    Zero if model solves regression task.

NOTE: if  you  need several different kinds of error metrics, it is better
      to use knnallerrors() which computes all error metric  with just one
      pass over dataset.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> knnrelclserror(knnmodel model, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_knnrewritekeps></a><h6 class=pageheader>knnrewritekeps Function</h6>
<hr width=600 align=left>
<pre class=narration>
Changing search settings of KNN model.

K and EPS parameters of KNN  (AKNN)  search  are  specified  during  model
construction. However, plain KNN algorithm with Euclidean distance  allows
you to change them at any moment.

NOTE: future versions of KNN model may support advanced versions  of  KNN,
      such as NCA or LMNN. It is possible that such algorithms won't allow
      you to change search settings on the fly. If you call this  function
      for an algorithm which does not support on-the-fly changes, it  will
      throw an exception.

Inputs:
    Model   -   KNN model
    K       -   K &ge; 1, neighbors count
    EPS     -   accuracy of the EPS-approximate NN search. Set to 0.0,  if
                you want to perform &quot;classic&quot; KNN search.  Specify  larger
                values  if  you  need  to  speed-up  high-dimensional  KNN
                queries.

Outputs:
    nothing on success, exception on failure
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnrewritekeps(knnmodel model, ae_int_t k, <b>double</b> eps);
</pre>
<a name=sub_knnrmserror></a><h6 class=pageheader>knnrmserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
RMS error on the test set.

Its meaning for regression task is obvious. As for classification problems,
RMS error means error when estimating posterior probabilities.

Inputs:
    Model   -   KNN model
    XY      -   test set
    NPoints -   test set size

Result:
    root mean square error.

NOTE: if  you  need several different kinds of error metrics, it is better
      to use knnallerrors() which computes all error metric  with just one
      pass over dataset.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> knnrmserror(knnmodel model, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_knnserialize></a><h6 class=pageheader>knnserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: serialization
These functions serialize a data structure to a C++ string or stream.
* serialization can be freely moved across 32-bit and 64-bit systems,
  and different byte orders. For example, you can serialize a string
  on a SPARC and unserialize it on an x86.
* ALGLIB++ serialization is compatible with serialization in ALGLIB,
  in both directions.
Important properties of s_out:
* it contains alphanumeric characters, dots, underscores, minus signs
* these symbols are grouped into words, which are separated by spaces
  and Windows-style (CR+LF) newlines
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnserialize(knnmodel &amp;obj, std::string &amp;s_out);
<b>void</b> knnserialize(knnmodel &amp;obj, std::ostream &amp;s_out);
</pre>
<a name=sub_knntsprocess></a><h6 class=pageheader>knntsprocess Function</h6>
<hr width=600 align=left>
<pre class=narration>
Thread-safe procesing using external buffer for temporaries.

This function is thread-safe (i.e.  you  can  use  same  KNN  model  from
multiple threads) as long as you use different buffer objects for different
threads.

Inputs:
    Model   -   KNN model
    Buf     -   buffer object, must be  allocated  specifically  for  this
                model with knncreatebuffer().
    X       -   input vector,  array[NVars]

Outputs:
    Y       -   result, array[NOut].   Regression  estimate  when  solving
                regression task,  vector  of  posterior  probabilities for
                a classification task.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knntsprocess(knnmodel model, knnbuffer buf, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_knnunserialize></a><h6 class=pageheader>knnunserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: unserialization
These functions unserialize a data structure from a C++ string or stream.
Important properties of s_in:
* any combination of spaces, tabs, Windows or Unix stype newlines can
  be used as separators, so as to allow flexible reformatting of the
  stream or string from text or XML files.
* But you should not insert separators into the middle of the "words"
  nor you should change case of letters.
ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> knnunserialize(<b>const</b> std::string &amp;s_in, knnmodel &amp;obj);
<b>void</b> knnunserialize(<b>const</b> std::istream &amp;s_in, knnmodel &amp;obj);
</pre>
<a name=example_knn_cls></a><h6 class=pageheader>knn_cls Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// The very simple classification example: classify points (x,y) in 2D space</font>
<font color=navy>// as ones with x &ge; 0 and ones with x &lt; 0 (y is ignored, but our classifier</font>
<font color=navy>// has to find out it).</font>
<font color=navy>//</font>
<font color=navy>// First, we have to create KNN builder object, load dataset and specify</font>
<font color=navy>// training settings. Our dataset is specified as matrix, which has following</font>
<font color=navy>// format:</font>
<font color=navy>//</font>
<font color=navy>//     x0 y0 class0</font>
<font color=navy>//     x1 y1 class1</font>
<font color=navy>//     x2 y2 class2</font>
<font color=navy>//     ....</font>
<font color=navy>//</font>
<font color=navy>// Here xi and yi can be any values (and in fact you can have any number of</font>
<font color=navy>// independent variables), and classi MUST be integer number in [0,NClasses)</font>
<font color=navy>// range. In our example we denote points with x &ge; 0 as <b>class</b> #0, and</font>
<font color=navy>// ones with negative xi as <b>class</b> #1.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: <b>if</b> you want to solve regression problem, specify dataset in similar</font>
<font color=navy>//       format, but with dependent variable(s) instead of <b>class</b> labels. You</font>
<font color=navy>//       can have dataset with multiple dependent variables, by the way!</font>
<font color=navy>//</font>
<font color=navy>// For the sake of simplicity, our example includes only 4-point dataset and</font>
<font color=navy>// really simple K=1 nearest neighbor search. Industrial problems typically</font>
<font color=navy>// need larger values of K.</font>
   knnbuilder builder;
   ae_int_t nvars = 2;
   ae_int_t nclasses = 2;
   ae_int_t npoints = 4;
   real_2d_array xy = <font color=blue><b>&quot;[[1,1,0],[1,-1,0],[-1,1,1],[-1,-1,1]]&quot;</b></font>;

   knnbuildercreate(builder);
   knnbuildersetdatasetcls(builder, xy, npoints, nvars, nclasses);

<font color=navy>// we build KNN model with k=1 and eps=0 (exact k-nn search is performed)</font>
   ae_int_t k = 1;
   <b>double</b> eps = 0;
   knnmodel model;
   knnreport rep;
   knnbuilderbuildknnmodel(builder, k, eps, model, rep);

<font color=navy>// with such settings (k=1 is used) you can expect zero classification</font>
<font color=navy>// error on training set. Beautiful results, but remember - in real life</font>
<font color=navy>// you <b>do</b> not need zero TRAINING SET error, you need good generalization.</font>

   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(rep.relclserror)); <font color=navy>// EXPECTED: 0.0000</font>

<font color=navy>// now, let's perform some simple processing with knnprocess()</font>
   real_1d_array x = <font color=blue><b>&quot;[+1,0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[]&quot;</b></font>;
   knnprocess(model, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(3).c_str()); <font color=navy>// EXPECTED: [+1,0]</font>

<font color=navy>// another option is to use knnprocess0() which returns just first component</font>
<font color=navy>// of the output vector y. ideal <b>for</b> regression problems and binary classifiers.</font>
   <b>double</b> y0;
   y0 = knnprocess0(model, x);
   printf(<font color=blue><b>&quot;%.3f\n&quot;</b></font>, <b>double</b>(y0)); <font color=navy>// EXPECTED: 1.000</font>

<font color=navy>// finally, you can use knnclassify() which returns most probable <b>class</b> index (i.e. argmax y[i]).</font>
   ae_int_t i;
   i = knnclassify(model, x);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(i)); <font color=navy>// EXPECTED: 0</font>
   <b>return</b> 0;
}
</pre>
<a name=example_knn_reg></a><h6 class=pageheader>knn_reg Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// The very simple regression example: model f(x,y)=x+y</font>
<font color=navy>//</font>
<font color=navy>// First, we have to create KNN builder object, load dataset and specify</font>
<font color=navy>// training settings. Our dataset is specified as matrix, which has following</font>
<font color=navy>// format:</font>
<font color=navy>//</font>
<font color=navy>//     x0 y0 f0</font>
<font color=navy>//     x1 y1 f1</font>
<font color=navy>//     x2 y2 f2</font>
<font color=navy>//     ....</font>
<font color=navy>//</font>
<font color=navy>// Here xi and yi can be any values, and fi is a dependent function value.</font>
<font color=navy>// By the way, with KNN algorithm you can even model functions with multiple</font>
<font color=navy>// dependent variables!</font>
<font color=navy>//</font>
<font color=navy>// NOTE: you can also solve classification problems with KNN models, see</font>
<font color=navy>//       another example <b>for</b> this unit.</font>
<font color=navy>//</font>
<font color=navy>// For the sake of simplicity, our example includes only 4-point dataset and</font>
<font color=navy>// really simple K=1 nearest neighbor search. Industrial problems typically</font>
<font color=navy>// need larger values of K.</font>
   knnbuilder builder;
   ae_int_t nvars = 2;
   ae_int_t nout = 1;
   ae_int_t npoints = 4;
   real_2d_array xy = <font color=blue><b>&quot;[[1,1,+2],[1,-1,0],[-1,1,0],[-1,-1,-2]]&quot;</b></font>;

   knnbuildercreate(builder);
   knnbuildersetdatasetreg(builder, xy, npoints, nvars, nout);

<font color=navy>// we build KNN model with k=1 and eps=0 (exact k-nn search is performed)</font>
   ae_int_t k = 1;
   <b>double</b> eps = 0;
   knnmodel model;
   knnreport rep;
   knnbuilderbuildknnmodel(builder, k, eps, model, rep);

<font color=navy>// with such settings (k=1 is used) you can expect zero RMS error on the</font>
<font color=navy>// training set. Beautiful results, but remember - in real life you <b>do</b> not</font>
<font color=navy>// need zero TRAINING SET error, you need good generalization.</font>

   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(rep.rmserror)); <font color=navy>// EXPECTED: 0.0000</font>

<font color=navy>// now, let's perform some simple processing with knnprocess()</font>
   real_1d_array x = <font color=blue><b>&quot;[+1,+1]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[]&quot;</b></font>;
   knnprocess(model, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(3).c_str()); <font color=navy>// EXPECTED: [+2]</font>

<font color=navy>// another option is to use knnprocess0() which returns just first component</font>
<font color=navy>// of the output vector y. ideal <b>for</b> regression problems and binary classifiers.</font>
   <b>double</b> y0;
   y0 = knnprocess0(model, x);
   printf(<font color=blue><b>&quot;%.3f\n&quot;</b></font>, <b>double</b>(y0)); <font color=navy>// EXPECTED: 2.000</font>

<font color=navy>// there also exist another convenience function, knnclassify(),</font>
<font color=navy>// but it does not work <b>for</b> regression problems - it always returns -1.</font>
   ae_int_t i;
   i = knnclassify(model, x);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(i)); <font color=navy>// EXPECTED: -1</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_lda></a><h4 class=pageheader>8.2.7. lda Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_fisherlda class=toc>fisherlda</a> |
<a href=#sub_fisherldan class=toc>fisherldan</a>
]</font>
</div>
<a name=sub_fisherlda></a><h6 class=pageheader>fisherlda Function</h6>
<hr width=600 align=left>
<pre class=narration>
Multiclass Fisher LDA

Subroutine finds coefficients of linear combination which optimally separates
training set on classes.

Inputs:
    XY          -   training set, array[0..NPoints-1,0..NVars].
                    First NVars columns store values of independent
                    variables, next column stores number of class (from 0
                    to NClasses-1) which dataset element belongs to. Fractional
                    values are rounded to nearest integer.
    NPoints     -   training set size, NPoints &ge; 0
    NVars       -   number of independent variables, NVars &ge; 1
    NClasses    -   number of classes, NClasses &ge; 2

Outputs:
    Info        -   return code:
                    * -4, if internal EVD subroutine hasn't converged
                    * -2, if there is a point with class number
                          outside of [0..NClasses-1].
                    * -1, if incorrect parameters was passed (NPoints &lt; 0,
                          NVars &lt; 1, NClasses &lt; 2)
                    *  1, if task has been solved
                    *  2, if there was a multicollinearity in training set,
                          but task has been solved.
    W           -   linear combination coefficients, array[0..NVars-1]
ALGLIB: Copyright 31.05.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fisherlda(real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t &amp;info, real_1d_array &amp;w);
</pre>
<a name=sub_fisherldan></a><h6 class=pageheader>fisherldan Function</h6>
<hr width=600 align=left>
<pre class=narration>
N-dimensional multiclass Fisher LDA

Subroutine finds coefficients of linear combinations which optimally separates
training set on classes. It returns N-dimensional basis whose vector are sorted
by quality of training set separation (in descending order).

Inputs:
    XY          -   training set, array[0..NPoints-1,0..NVars].
                    First NVars columns store values of independent
                    variables, next column stores number of class (from 0
                    to NClasses-1) which dataset element belongs to. Fractional
                    values are rounded to nearest integer.
    NPoints     -   training set size, NPoints &ge; 0
    NVars       -   number of independent variables, NVars &ge; 1
    NClasses    -   number of classes, NClasses &ge; 2

Outputs:
    Info        -   return code:
                    * -4, if internal EVD subroutine hasn't converged
                    * -2, if there is a point with class number
                          outside of [0..NClasses-1].
                    * -1, if incorrect parameters was passed (NPoints &lt; 0,
                          NVars &lt; 1, NClasses &lt; 2)
                    *  1, if task has been solved
                    *  2, if there was a multicollinearity in training set,
                          but task has been solved.
    W           -   basis, array[0..NVars-1,0..NVars-1]
                    columns of matrix stores basis vectors, sorted by
                    quality of training set separation (in descending order)
ALGLIB: Copyright 31.05.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fisherldan(real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t &amp;info, real_2d_array &amp;w);
</pre>
<a name=unit_linreg></a><h4 class=pageheader>8.2.8. linreg Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_linearmodel class=toc>linearmodel</a> |
<a href=#struct_lrreport class=toc>lrreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_lravgerror class=toc>lravgerror</a> |
<a href=#sub_lravgrelerror class=toc>lravgrelerror</a> |
<a href=#sub_lrbuild class=toc>lrbuild</a> |
<a href=#sub_lrbuilds class=toc>lrbuilds</a> |
<a href=#sub_lrbuildz class=toc>lrbuildz</a> |
<a href=#sub_lrbuildzs class=toc>lrbuildzs</a> |
<a href=#sub_lrpack class=toc>lrpack</a> |
<a href=#sub_lrprocess class=toc>lrprocess</a> |
<a href=#sub_lrrmserror class=toc>lrrmserror</a> |
<a href=#sub_lrunpack class=toc>lrunpack</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_linreg_d_basic class=toc>linreg_d_basic</a></td><td width=15>&nbsp;</td><td>Linear regression used to build the very basic model and unpack coefficients</td></tr>
</table>
</div>
<a name=struct_linearmodel></a><h6 class=pageheader>linearmodel Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> linearmodel {
};
</pre>
<a name=struct_lrreport></a><h6 class=pageheader>lrreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
LRReport structure contains additional information about linear model:
* C             -   covariation matrix,  array[0..NVars,0..NVars].
                    C[i,j] = Cov(A[i],A[j])
* RMSError      -   root mean square error on a training set
* AvgError      -   average error on a training set
* AvgRelError   -   average relative error on a training set (excluding
                    observations with zero function value).
* CVRMSError    -   leave-one-out cross-validation estimate of
                    generalization error. Calculated using fast algorithm
                    with O(NVars*NPoints) complexity.
* CVAvgError    -   cross-validation estimate of average error
* CVAvgRelError -   cross-validation estimate of average relative error

All other fields of the structure are intended for internal use and should
not be used outside ALGLIB.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> lrreport {
   real_2d_array c;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
   <b>double</b> cvrmserror;
   <b>double</b> cvavgerror;
   <b>double</b> cvavgrelerror;
   ae_int_t ncvdefects;
   integer_1d_array cvdefects;
};
</pre>
<a name=sub_lravgerror></a><h6 class=pageheader>lravgerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average error on the test set

Inputs:
    LM      -   linear model
    XY      -   test set
    NPoints -   test set size

Result:
    average error.
ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> lravgerror(linearmodel lm, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_lravgrelerror></a><h6 class=pageheader>lravgrelerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
RMS error on the test set

Inputs:
    LM      -   linear model
    XY      -   test set
    NPoints -   test set size

Result:
    average relative error.
ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> lravgrelerror(linearmodel lm, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_lrbuild></a><h6 class=pageheader>lrbuild Function</h6>
<hr width=600 align=left>
<pre class=narration>
Linear regression

Subroutine builds model:

    Y = A(0)*X[0] + ... + A(N-1)*X[N-1] + A(N)

and model found in ALGLIB format, covariation matrix, training set  errors
(rms,  average,  average  relative)   and  leave-one-out  cross-validation
estimate of the generalization error. CV  estimate calculated  using  fast
algorithm with O(NPoints*NVars) complexity.

When  covariation  matrix  is  calculated  standard deviations of function
values are assumed to be equal to RMS error on the training set.

Inputs:
    XY          -   training set, array [0..NPoints-1,0..NVars]:
                    * NVars columns - independent variables
                    * last column - dependent variable
    NPoints     -   training set size, NPoints &gt; NVars+1
    NVars       -   number of independent variables

Outputs:
    Info        -   return code:
                    * -255, in case of unknown internal error
                    * -4, if internal SVD subroutine haven't converged
                    * -1, if incorrect parameters was passed (NPoints &lt; NVars+2, NVars &lt; 1).
                    *  1, if subroutine successfully finished
    LM          -   linear model in the ALGLIB format. Use subroutines of
                    this unit to work with the model.
    AR          -   additional results
ALGLIB: Copyright 02.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lrbuild(real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t &amp;info, linearmodel &amp;lm, lrreport &amp;ar);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_linreg_d_basic class=nav>linreg_d_basic</a> ]</p>
<a name=sub_lrbuilds></a><h6 class=pageheader>lrbuilds Function</h6>
<hr width=600 align=left>
<pre class=narration>
Linear regression

Variant of LRBuild which uses vector of standatd deviations (errors in
function values).

Inputs:
    XY          -   training set, array [0..NPoints-1,0..NVars]:
                    * NVars columns - independent variables
                    * last column - dependent variable
    S           -   standard deviations (errors in function values)
                    array[0..NPoints-1], S[i] &gt; 0.
    NPoints     -   training set size, NPoints &gt; NVars+1
    NVars       -   number of independent variables

Outputs:
    Info        -   return code:
                    * -255, in case of unknown internal error
                    * -4, if internal SVD subroutine haven't converged
                    * -1, if incorrect parameters was passed (NPoints &lt; NVars+2, NVars &lt; 1).
                    * -2, if S[I] &le; 0
                    *  1, if subroutine successfully finished
    LM          -   linear model in the ALGLIB format. Use subroutines of
                    this unit to work with the model.
    AR          -   additional results
ALGLIB: Copyright 02.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lrbuilds(real_2d_array xy, real_1d_array s, ae_int_t npoints, ae_int_t nvars, ae_int_t &amp;info, linearmodel &amp;lm, lrreport &amp;ar);
</pre>
<a name=sub_lrbuildz></a><h6 class=pageheader>lrbuildz Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like LRBuild but builds model

    Y = A(0)*X[0] + ... + A(N-1)*X[N-1]

i.e. with zero constant term.
ALGLIB: Copyright 30.10.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lrbuildz(real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t &amp;info, linearmodel &amp;lm, lrreport &amp;ar);
</pre>
<a name=sub_lrbuildzs></a><h6 class=pageheader>lrbuildzs Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like LRBuildS, but builds model

    Y = A(0)*X[0] + ... + A(N-1)*X[N-1]

i.e. with zero constant term.
ALGLIB: Copyright 30.10.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lrbuildzs(real_2d_array xy, real_1d_array s, ae_int_t npoints, ae_int_t nvars, ae_int_t &amp;info, linearmodel &amp;lm, lrreport &amp;ar);
</pre>
<a name=sub_lrpack></a><h6 class=pageheader>lrpack Function</h6>
<hr width=600 align=left>
<pre class=narration>
&quot;Packs&quot; coefficients and creates linear model in ALGLIB format (LRUnpack
reversed).

Inputs:
    V           -   coefficients, array[0..NVars]
    NVars       -   number of independent variables

Outputs:
    LM          -   linear model.
ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lrpack(real_1d_array v, ae_int_t nvars, linearmodel &amp;lm);
</pre>
<a name=sub_lrprocess></a><h6 class=pageheader>lrprocess Function</h6>
<hr width=600 align=left>
<pre class=narration>
Procesing

Inputs:
    LM      -   linear model
    X       -   input vector,  array[0..NVars-1].

Result:
    value of linear model regression estimate
ALGLIB: Copyright 03.09.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> lrprocess(linearmodel lm, real_1d_array x);
</pre>
<a name=sub_lrrmserror></a><h6 class=pageheader>lrrmserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
RMS error on the test set

Inputs:
    LM      -   linear model
    XY      -   test set
    NPoints -   test set size

Result:
    root mean square error.
ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> lrrmserror(linearmodel lm, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_lrunpack></a><h6 class=pageheader>lrunpack Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacks coefficients of linear model.

Inputs:
    LM          -   linear model in ALGLIB format

Outputs:
    V           -   coefficients, array[0..NVars]
                    constant term (intercept) is stored in the V[NVars].
    NVars       -   number of independent variables (one less than number
                    of coefficients)
ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lrunpack(linearmodel lm, real_1d_array &amp;v, ae_int_t &amp;nvars);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_linreg_d_basic class=nav>linreg_d_basic</a> ]</p>
<a name=example_linreg_d_basic></a><h6 class=pageheader>linreg_d_basic Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// In this example we demonstrate linear fitting by f(x|a) = a*exp(0.5*x).</font>
<font color=navy>//</font>
<font color=navy>// We have:</font>
<font color=navy>// * xy - matrix of basic function values (exp(0.5*x)) and expected values</font>
   real_2d_array xy = <font color=blue><b>&quot;[[0.606531,1.133719],[0.670320,1.306522],[0.740818,1.504604],[0.818731,1.554663],[0.904837,1.884638],[1.000000,2.072436],[1.105171,2.257285],[1.221403,2.534068],[1.349859,2.622017],[1.491825,2.897713],[1.648721,3.219371]]&quot;</b></font>;
   ae_int_t info;
   ae_int_t nvars;
   linearmodel model;
   lrreport rep;
   real_1d_array c;

   lrbuildz(xy, 11, 1, info, model, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   lrunpack(model, c, nvars);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(4).c_str()); <font color=navy>// EXPECTED: [1.98650,0.00000]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_logit></a><h4 class=pageheader>8.2.9. logit Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_logitmodel class=toc>logitmodel</a> |
<a href=#struct_mnlreport class=toc>mnlreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_mnlavgce class=toc>mnlavgce</a> |
<a href=#sub_mnlavgerror class=toc>mnlavgerror</a> |
<a href=#sub_mnlavgrelerror class=toc>mnlavgrelerror</a> |
<a href=#sub_mnlclserror class=toc>mnlclserror</a> |
<a href=#sub_mnlpack class=toc>mnlpack</a> |
<a href=#sub_mnlprocess class=toc>mnlprocess</a> |
<a href=#sub_mnlprocessi class=toc>mnlprocessi</a> |
<a href=#sub_mnlrelclserror class=toc>mnlrelclserror</a> |
<a href=#sub_mnlrmserror class=toc>mnlrmserror</a> |
<a href=#sub_mnltrainh class=toc>mnltrainh</a> |
<a href=#sub_mnlunpack class=toc>mnlunpack</a>
]</font>
</div>
<a name=struct_logitmodel></a><h6 class=pageheader>logitmodel Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> logitmodel {
};
</pre>
<a name=struct_mnlreport></a><h6 class=pageheader>mnlreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
MNLReport structure contains information about training process:
* NGrad     -   number of gradient calculations
* NHess     -   number of Hessian calculations
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> mnlreport {
   ae_int_t ngrad;
   ae_int_t nhess;
};
</pre>
<a name=sub_mnlavgce></a><h6 class=pageheader>mnlavgce Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average cross-entropy (in bits per element) on the test set

Inputs:
    LM      -   logit model
    XY      -   test set
    NPoints -   test set size

Result:
    CrossEntropy/(NPoints*ln(2)).
ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mnlavgce(logitmodel lm, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mnlavgerror></a><h6 class=pageheader>mnlavgerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average error on the test set

Inputs:
    LM      -   logit model
    XY      -   test set
    NPoints -   test set size

Result:
    average error (error when estimating posterior probabilities).
ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mnlavgerror(logitmodel lm, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mnlavgrelerror></a><h6 class=pageheader>mnlavgrelerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average relative error on the test set

Inputs:
    LM      -   logit model
    XY      -   test set
    NPoints -   test set size

Result:
    average relative error (error when estimating posterior probabilities).
ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mnlavgrelerror(logitmodel lm, real_2d_array xy, ae_int_t ssize);
</pre>
<a name=sub_mnlclserror></a><h6 class=pageheader>mnlclserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Classification error on test set = MNLRelClsError*NPoints
ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t mnlclserror(logitmodel lm, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mnlpack></a><h6 class=pageheader>mnlpack Function</h6>
<hr width=600 align=left>
<pre class=narration>
&quot;Packs&quot; coefficients and creates logit model in ALGLIB format (MNLUnpack
reversed).

Inputs:
    A           -   model (see MNLUnpack)
    NVars       -   number of independent variables
    NClasses    -   number of classes

Outputs:
    LM          -   logit model.
ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mnlpack(real_2d_array a, ae_int_t nvars, ae_int_t nclasses, logitmodel &amp;lm);
</pre>
<a name=sub_mnlprocess></a><h6 class=pageheader>mnlprocess Function</h6>
<hr width=600 align=left>
<pre class=narration>
Procesing

Inputs:
    LM      -   logit model, passed by non-constant reference
                (some fields of structure are used as temporaries
                when calculating model output).
    X       -   input vector,  array[0..NVars-1].
    Y       -   (possibly) preallocated buffer; if size of Y is less than
                NClasses, it will be reallocated.If it is large enough, it
                is NOT reallocated, so we can save some time on reallocation.

Outputs:
    Y       -   result, array[0..NClasses-1]
                Vector of posterior probabilities for classification task.
ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mnlprocess(logitmodel lm, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_mnlprocessi></a><h6 class=pageheader>mnlprocessi Function</h6>
<hr width=600 align=left>
<pre class=narration>
'interactive'  variant  of  MNLProcess  for  languages  like  Python which
support constructs like &quot;Y = MNLProcess(LM,X)&quot; and interactive mode of the
interpreter

This function allocates new array on each call,  so  it  is  significantly
slower than its 'non-interactive' counterpart, but it is  more  convenient
when you call it from command line.
ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mnlprocessi(logitmodel lm, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_mnlrelclserror></a><h6 class=pageheader>mnlrelclserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Relative classification error on the test set

Inputs:
    LM      -   logit model
    XY      -   test set
    NPoints -   test set size

Result:
    percent of incorrectly classified cases.
ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mnlrelclserror(logitmodel lm, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mnlrmserror></a><h6 class=pageheader>mnlrmserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
RMS error on the test set

Inputs:
    LM      -   logit model
    XY      -   test set
    NPoints -   test set size

Result:
    root mean square error (error when estimating posterior probabilities).
ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mnlrmserror(logitmodel lm, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mnltrainh></a><h6 class=pageheader>mnltrainh Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine trains logit model.

Inputs:
    XY          -   training set, array[0..NPoints-1,0..NVars]
                    First NVars columns store values of independent
                    variables, next column stores number of class (from 0
                    to NClasses-1) which dataset element belongs to. Fractional
                    values are rounded to nearest integer.
    NPoints     -   training set size, NPoints &ge; 1
    NVars       -   number of independent variables, NVars &ge; 1
    NClasses    -   number of classes, NClasses &ge; 2

Outputs:
    Info        -   return code:
                    * -2, if there is a point with class number
                          outside of [0..NClasses-1].
                    * -1, if incorrect parameters was passed
                          (NPoints &lt; NVars+2, NVars &lt; 1, NClasses &lt; 2).
                    *  1, if task has been solved
    LM          -   model built
    Rep         -   training report
ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mnltrainh(real_2d_array xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t &amp;info, logitmodel &amp;lm, mnlreport &amp;rep);
</pre>
<a name=sub_mnlunpack></a><h6 class=pageheader>mnlunpack Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacks coefficients of logit model. Logit model have form:

    P(class=i) = S(i) / (S(0) + S(1) + ... +S(M-1))
          S(i) = exp(A[i,0]*X[0] + ... + A[i,N-1]*X[N-1] + A[i,N]), when i &lt; M-1
        S(M-1) = 1

Inputs:
    LM          -   logit model in ALGLIB format

Outputs:
    V           -   coefficients, array[0..NClasses-2,0..NVars]
    NVars       -   number of independent variables
    NClasses    -   number of classes
ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mnlunpack(logitmodel lm, real_2d_array &amp;a, ae_int_t &amp;nvars, ae_int_t &amp;nclasses);
</pre>
<a name=unit_mcpd></a><h4 class=pageheader>8.2.10. mcpd Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_mcpdreport class=toc>mcpdreport</a> |
<a href=#struct_mcpdstate class=toc>mcpdstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_mcpdaddbc class=toc>mcpdaddbc</a> |
<a href=#sub_mcpdaddec class=toc>mcpdaddec</a> |
<a href=#sub_mcpdaddtrack class=toc>mcpdaddtrack</a> |
<a href=#sub_mcpdcreate class=toc>mcpdcreate</a> |
<a href=#sub_mcpdcreateentry class=toc>mcpdcreateentry</a> |
<a href=#sub_mcpdcreateentryexit class=toc>mcpdcreateentryexit</a> |
<a href=#sub_mcpdcreateexit class=toc>mcpdcreateexit</a> |
<a href=#sub_mcpdresults class=toc>mcpdresults</a> |
<a href=#sub_mcpdsetbc class=toc>mcpdsetbc</a> |
<a href=#sub_mcpdsetec class=toc>mcpdsetec</a> |
<a href=#sub_mcpdsetlc class=toc>mcpdsetlc</a> |
<a href=#sub_mcpdsetpredictionweights class=toc>mcpdsetpredictionweights</a> |
<a href=#sub_mcpdsetprior class=toc>mcpdsetprior</a> |
<a href=#sub_mcpdsettikhonovregularizer class=toc>mcpdsettikhonovregularizer</a> |
<a href=#sub_mcpdsolve class=toc>mcpdsolve</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_mcpd_simple1 class=toc>mcpd_simple1</a></td><td width=15>&nbsp;</td><td>Simple unconstrained MCPD model (no entry/exit states)</td></tr>
<tr align=left valign=top><td><a href=#example_mcpd_simple2 class=toc>mcpd_simple2</a></td><td width=15>&nbsp;</td><td>Simple MCPD model (no entry/exit states) with equality constraints</td></tr>
</table>
</div>
<a name=struct_mcpdreport></a><h6 class=pageheader>mcpdreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure is a MCPD training report:
    InnerIterationsCount    -   number of inner iterations of the
                                underlying optimization algorithm
    OuterIterationsCount    -   number of outer iterations of the
                                underlying optimization algorithm
    NFEV                    -   number of merit function evaluations
    TerminationType         -   termination type
                                (same as for MinBLEIC optimizer, positive
                                values denote success, negative ones -
                                failure)
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> mcpdreport {
   ae_int_t inneriterationscount;
   ae_int_t outeriterationscount;
   ae_int_t nfev;
   ae_int_t terminationtype;
};
</pre>
<a name=struct_mcpdstate></a><h6 class=pageheader>mcpdstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure is a MCPD (Markov Chains for Population Data) solver.
You should use ALGLIB functions in order to work with this object.
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> mcpdstate {
};
</pre>
<a name=sub_mcpdaddbc></a><h6 class=pageheader>mcpdaddbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to add bound constraints  on  the  elements  of  the
transition matrix P.

MCPD solver has four types of constraints which can be placed on P:
* user-specified equality constraints (optional)
* user-specified bound constraints (optional)
* user-specified general linear constraints (optional)
* basic constraints (always present):
  * non-negativity: P[i,j] &ge; 0
  * consistency: every column of P sums to 1.0

Final  constraints  which  are  passed  to  the  underlying  optimizer are
calculated  as  intersection  of all present constraints. For example, you
may specify boundary constraint on P[0,0] and equality one:
    0.1 &le; P[0,0] &le; 0.9
    P[0,0]=0.5
Such  combination  of  constraints  will  be  silently  reduced  to  their
intersection, which is P[0,0]=0.5.

This  function  can  be  used to ADD bound constraint for one element of P
without changing constraints for other elements.

You  can  also  use  MCPDSetBC()  function  which  allows to  place  bound
constraints  on arbitrary subset of elements of P.   Set of constraints is
specified  by  BndL/BndU matrices, which may contain arbitrary combination
of finite numbers or infinities (like -INF &lt; x &le; 0.5 or 0.1 &le; x &lt; +INF).

These functions (MCPDSetBC and MCPDAddBC) interact as follows:
* there is internal matrix of bound constraints which is stored in the
  MCPD solver
* MCPDSetBC() replaces this matrix by another one (SET)
* MCPDAddBC() modifies one element of this matrix and  leaves  other  ones
  unchanged (ADD)
* thus  MCPDAddBC()  call  preserves  all  modifications  done by previous
  calls,  while  MCPDSetBC()  completely discards all changes  done to the
  equality constraints.

Inputs:
    S       -   solver
    I       -   row index of element being constrained
    J       -   column index of element being constrained
    BndL    -   lower bound
    BndU    -   upper bound
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdaddbc(mcpdstate s, ae_int_t i, ae_int_t j, <b>double</b> bndl, <b>double</b> bndu);
</pre>
<a name=sub_mcpdaddec></a><h6 class=pageheader>mcpdaddec Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to add equality constraints on the elements  of  the
transition matrix P.

MCPD solver has four types of constraints which can be placed on P:
* user-specified equality constraints (optional)
* user-specified bound constraints (optional)
* user-specified general linear constraints (optional)
* basic constraints (always present):
  * non-negativity: P[i,j] &ge; 0
  * consistency: every column of P sums to 1.0

Final  constraints  which  are  passed  to  the  underlying  optimizer are
calculated  as  intersection  of all present constraints. For example, you
may specify boundary constraint on P[0,0] and equality one:
    0.1 &le; P[0,0] &le; 0.9
    P[0,0]=0.5
Such  combination  of  constraints  will  be  silently  reduced  to  their
intersection, which is P[0,0]=0.5.

This function can be used to ADD equality constraint for one element of  P
without changing constraints for other elements.

You  can  also  use  MCPDSetEC()  function  which  allows  you  to specify
arbitrary set of equality constraints in one call.

These functions (MCPDSetEC and MCPDAddEC) interact as follows:
* there is internal matrix of equality constraints which is stored in the
  MCPD solver
* MCPDSetEC() replaces this matrix by another one (SET)
* MCPDAddEC() modifies one element of this matrix and leaves  other  ones
  unchanged (ADD)
* thus  MCPDAddEC()  call  preserves  all  modifications done by previous
  calls,  while  MCPDSetEC()  completely discards all changes done to the
  equality constraints.

Inputs:
    S       -   solver
    I       -   row index of element being constrained
    J       -   column index of element being constrained
    C       -   value (constraint for P[I,J]).  Can  be  either  NAN  (no
                constraint) or finite value from [0,1].

NOTES:

1. infinite values of C  will lead to exception being thrown. Values  less
than 0.0 or greater than 1.0 will lead to error code being returned  after
call to MCPDSolve().
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdaddec(mcpdstate s, ae_int_t i, ae_int_t j, <b>double</b> c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mcpd_simple2 class=nav>mcpd_simple2</a> ]</p>
<a name=sub_mcpdaddtrack></a><h6 class=pageheader>mcpdaddtrack Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  is  used to add a track - sequence of system states at the
different moments of its evolution.

You  may  add  one  or several tracks to the MCPD solver. In case you have
several tracks, they won't overwrite each other. For example,  if you pass
two tracks, A1-A2-A3 (system at t=A+1, t=A+2 and t=A+3) and B1-B2-B3, then
solver will try to model transitions from t=A+1 to t=A+2, t=A+2 to  t=A+3,
t=B+1 to t=B+2, t=B+2 to t=B+3. But it WONT mix these two tracks - i.e. it
wont try to model transition from t=A+3 to t=B+1.

Inputs:
    S       -   solver
    XY      -   track, array[K,N]:
                * I-th row is a state at t=I
                * elements of XY must be non-negative (exception will be
                  thrown on negative elements)
    K       -   number of points in a track
                * if given, only leading K rows of XY are used
                * if not given, automatically determined from size of XY

NOTES:

1. Track may contain either proportional or population data:
   * with proportional data all rows of XY must sum to 1.0, i.e. we have
     proportions instead of absolute population values
   * with population data rows of XY contain population counts and generally
     do not sum to 1.0 (although they still must be non-negative)
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdaddtrack(mcpdstate s, real_2d_array xy, ae_int_t k);
<b>void</b> mcpdaddtrack(mcpdstate s, real_2d_array xy);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mcpd_simple1 class=nav>mcpd_simple1</a> | <a href=#example_mcpd_simple2 class=nav>mcpd_simple2</a> ]</p>
<a name=sub_mcpdcreate></a><h6 class=pageheader>mcpdcreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates MCPD (Markov Chains for Population Data) solver.

This  solver  can  be  used  to find transition matrix P for N-dimensional
prediction  problem  where transition from X[i] to X[i+1] is  modelled  as
    X[i+1] = P*X[i]
where X[i] and X[i+1] are N-dimensional population vectors (components  of
each X are non-negative), and P is a N*N transition matrix (elements of  P
are non-negative, each column sums to 1.0).

Such models arise when when:
* there is some population of individuals
* individuals can have different states
* individuals can transit from one state to another
* population size is constant, i.e. there is no new individuals and no one
  leaves population
* you want to model transitions of individuals from one state into another

USAGE:

Here we give very brief outline of the MCPD. We strongly recommend you  to
read examples in the ALGLIB Reference Manual and to read ALGLIB User Guide
on data analysis which is available at http://www.alglib.net/dataanalysis/

1. User initializes algorithm state with MCPDCreate() call

2. User  adds  one  or  more  tracks -  sequences of states which describe
   evolution of a system being modelled from different starting conditions

3. User may add optional boundary, equality  and/or  linear constraints on
   the coefficients of P by calling one of the following functions:
   * MCPDSetEC() to set equality constraints
   * MCPDSetBC() to set bound constraints
   * MCPDSetLC() to set linear constraints

4. Optionally,  user  may  set  custom  weights  for prediction errors (by
   default, algorithm assigns non-equal, automatically chosen weights  for
   errors in the prediction of different components of X). It can be  done
   with a call of MCPDSetPredictionWeights() function.

5. User calls MCPDSolve() function which takes algorithm  state and
   pointer (delegate, etc.) to callback function which calculates F/G.

6. User calls MCPDResults() to get solution

Inputs:
    N       -   problem dimension, N &ge; 1

Outputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdcreate(ae_int_t n, mcpdstate &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mcpd_simple1 class=nav>mcpd_simple1</a> | <a href=#example_mcpd_simple2 class=nav>mcpd_simple2</a> ]</p>
<a name=sub_mcpdcreateentry></a><h6 class=pageheader>mcpdcreateentry Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is a specialized version of MCPDCreate()  function,  and  we
recommend  you  to read comments for this function for general information
about MCPD solver.

This  function  creates  MCPD (Markov Chains for Population  Data)  solver
for &quot;Entry-state&quot; model,  i.e. model  where transition from X[i] to X[i+1]
is modelled as
    X[i+1] = P*X[i]
where
    X[i] and X[i+1] are N-dimensional state vectors
    P is a N*N transition matrix
and  one  selected component of X[] is called &quot;entry&quot; state and is treated
in a special way:
    system state always transits from &quot;entry&quot; state to some another state
    system state can not transit from any state into &quot;entry&quot; state
Such conditions basically mean that row of P which corresponds to  &quot;entry&quot;
state is zero.

Such models arise when:
* there is some population of individuals
* individuals can have different states
* individuals can transit from one state to another
* population size is NOT constant -  at every moment of time there is some
  (unpredictable) amount of &quot;new&quot; individuals, which can transit into  one
  of the states at the next turn, but still no one leaves population
* you want to model transitions of individuals from one state into another
* but you do NOT want to predict amount of &quot;new&quot;  individuals  because  it
  does not depends on individuals already present (hence  system  can  not
  transit INTO entry state - it can only transit FROM it).

This model is discussed  in  more  details  in  the ALGLIB User Guide (see
http://www.alglib.net/dataanalysis/ for more data).

Inputs:
    N       -   problem dimension, N &ge; 2
    EntryState- index of entry state, in 0..N-1

Outputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdcreateentry(ae_int_t n, ae_int_t entrystate, mcpdstate &amp;s);
</pre>
<a name=sub_mcpdcreateentryexit></a><h6 class=pageheader>mcpdcreateentryexit Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is a specialized version of MCPDCreate()  function,  and  we
recommend  you  to read comments for this function for general information
about MCPD solver.

This  function  creates  MCPD (Markov Chains for Population  Data)  solver
for &quot;Entry-Exit-states&quot; model, i.e. model where  transition  from  X[i] to
X[i+1] is modelled as
    X[i+1] = P*X[i]
where
    X[i] and X[i+1] are N-dimensional state vectors
    P is a N*N transition matrix
one selected component of X[] is called &quot;entry&quot; state and is treated in  a
special way:
    system state always transits from &quot;entry&quot; state to some another state
    system state can not transit from any state into &quot;entry&quot; state
and another one component of X[] is called &quot;exit&quot; state and is treated  in
a special way too:
    system state can transit from any state into &quot;exit&quot; state
    system state can not transit from &quot;exit&quot; state into any other state
    transition operator discards &quot;exit&quot; state (makes it zero at each turn)
Such conditions basically mean that:
    row of P which corresponds to &quot;entry&quot; state is zero
    column of P which corresponds to &quot;exit&quot; state is zero
Multiplication by such P may decrease sum of vector components.

Such models arise when:
* there is some population of individuals
* individuals can have different states
* individuals can transit from one state to another
* population size is NOT constant
* at every moment of time there is some (unpredictable)  amount  of  &quot;new&quot;
  individuals, which can transit into one of the states at the next turn
* some  individuals  can  move  (predictably)  into &quot;exit&quot; state and leave
  population at the next turn
* you want to model transitions of individuals from one state into another,
  including transitions from the &quot;entry&quot; state and into the &quot;exit&quot; state.
* but you do NOT want to predict amount of &quot;new&quot;  individuals  because  it
  does not depends on individuals already present (hence  system  can  not
  transit INTO entry state - it can only transit FROM it).

This model is discussed  in  more  details  in  the ALGLIB User Guide (see
http://www.alglib.net/dataanalysis/ for more data).

Inputs:
    N       -   problem dimension, N &ge; 2
    EntryState- index of entry state, in 0..N-1
    ExitState-  index of exit state, in 0..N-1

Outputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdcreateentryexit(ae_int_t n, ae_int_t entrystate, ae_int_t exitstate, mcpdstate &amp;s);
</pre>
<a name=sub_mcpdcreateexit></a><h6 class=pageheader>mcpdcreateexit Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is a specialized version of MCPDCreate()  function,  and  we
recommend  you  to read comments for this function for general information
about MCPD solver.

This  function  creates  MCPD (Markov Chains for Population  Data)  solver
for &quot;Exit-state&quot; model,  i.e. model  where  transition from X[i] to X[i+1]
is modelled as
    X[i+1] = P*X[i]
where
    X[i] and X[i+1] are N-dimensional state vectors
    P is a N*N transition matrix
and  one  selected component of X[] is called &quot;exit&quot;  state and is treated
in a special way:
    system state can transit from any state into &quot;exit&quot; state
    system state can not transit from &quot;exit&quot; state into any other state
    transition operator discards &quot;exit&quot; state (makes it zero at each turn)
Such  conditions  basically  mean  that  column  of P which corresponds to
&quot;exit&quot; state is zero. Multiplication by such P may decrease sum of  vector
components.

Such models arise when:
* there is some population of individuals
* individuals can have different states
* individuals can transit from one state to another
* population size is NOT constant - individuals can move into &quot;exit&quot; state
  and leave population at the next turn, but there are no new individuals
* amount of individuals which leave population can be predicted
* you want to model transitions of individuals from one state into another
  (including transitions into the &quot;exit&quot; state)

This model is discussed  in  more  details  in  the ALGLIB User Guide (see
http://www.alglib.net/dataanalysis/ for more data).

Inputs:
    N       -   problem dimension, N &ge; 2
    ExitState-  index of exit state, in 0..N-1

Outputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdcreateexit(ae_int_t n, ae_int_t exitstate, mcpdstate &amp;s);
</pre>
<a name=sub_mcpdresults></a><h6 class=pageheader>mcpdresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
MCPD results

Inputs:
    State   -   algorithm state

Outputs:
    P       -   array[N,N], transition matrix
    Rep     -   optimization report. You should check Rep.TerminationType
                in  order  to  distinguish  successful  termination  from
                unsuccessful one. Speaking short, positive values  denote
                success, negative ones are failures.
                More information about fields of this  structure  can  be
                found in the comments on MCPDReport datatype.
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdresults(mcpdstate s, real_2d_array &amp;p, mcpdreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mcpd_simple1 class=nav>mcpd_simple1</a> | <a href=#example_mcpd_simple2 class=nav>mcpd_simple2</a> ]</p>
<a name=sub_mcpdsetbc></a><h6 class=pageheader>mcpdsetbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to add bound constraints  on  the  elements  of  the
transition matrix P.

MCPD solver has four types of constraints which can be placed on P:
* user-specified equality constraints (optional)
* user-specified bound constraints (optional)
* user-specified general linear constraints (optional)
* basic constraints (always present):
  * non-negativity: P[i,j] &ge; 0
  * consistency: every column of P sums to 1.0

Final  constraints  which  are  passed  to  the  underlying  optimizer are
calculated  as  intersection  of all present constraints. For example, you
may specify boundary constraint on P[0,0] and equality one:
    0.1 &le; P[0,0] &le; 0.9
    P[0,0]=0.5
Such  combination  of  constraints  will  be  silently  reduced  to  their
intersection, which is P[0,0]=0.5.

This  function  can  be  used  to  place bound   constraints  on arbitrary
subset  of  elements  of  P.  Set of constraints is specified by BndL/BndU
matrices, which may contain arbitrary combination  of  finite  numbers  or
infinities (like -INF &lt; x &le; 0.5 or 0.1 &le; x &lt; +INF).

You can also use MCPDAddBC() function which allows to ADD bound constraint
for one element of P without changing constraints for other elements.

These functions (MCPDSetBC and MCPDAddBC) interact as follows:
* there is internal matrix of bound constraints which is stored in the
  MCPD solver
* MCPDSetBC() replaces this matrix by another one (SET)
* MCPDAddBC() modifies one element of this matrix and  leaves  other  ones
  unchanged (ADD)
* thus  MCPDAddBC()  call  preserves  all  modifications  done by previous
  calls,  while  MCPDSetBC()  completely discards all changes  done to the
  equality constraints.

Inputs:
    S       -   solver
    BndL    -   lower bounds constraints, array[N,N]. Elements of BndL can
                be finite numbers or -INF.
    BndU    -   upper bounds constraints, array[N,N]. Elements of BndU can
                be finite numbers or +INF.
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdsetbc(mcpdstate s, real_2d_array bndl, real_2d_array bndu);
</pre>
<a name=sub_mcpdsetec></a><h6 class=pageheader>mcpdsetec Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to add equality constraints on the elements  of  the
transition matrix P.

MCPD solver has four types of constraints which can be placed on P:
* user-specified equality constraints (optional)
* user-specified bound constraints (optional)
* user-specified general linear constraints (optional)
* basic constraints (always present):
  * non-negativity: P[i,j] &ge; 0
  * consistency: every column of P sums to 1.0

Final  constraints  which  are  passed  to  the  underlying  optimizer are
calculated  as  intersection  of all present constraints. For example, you
may specify boundary constraint on P[0,0] and equality one:
    0.1 &le; P[0,0] &le; 0.9
    P[0,0]=0.5
Such  combination  of  constraints  will  be  silently  reduced  to  their
intersection, which is P[0,0]=0.5.

This  function  can  be  used  to  place equality constraints on arbitrary
subset of elements of P. Set of constraints is specified by EC, which  may
contain either NAN's or finite numbers from [0,1]. NAN denotes absence  of
constraint, finite number denotes equality constraint on specific  element
of P.

You can also  use  MCPDAddEC()  function  which  allows  to  ADD  equality
constraint  for  one  element  of P without changing constraints for other
elements.

These functions (MCPDSetEC and MCPDAddEC) interact as follows:
* there is internal matrix of equality constraints which is stored in  the
  MCPD solver
* MCPDSetEC() replaces this matrix by another one (SET)
* MCPDAddEC() modifies one element of this matrix and  leaves  other  ones
  unchanged (ADD)
* thus  MCPDAddEC()  call  preserves  all  modifications  done by previous
  calls,  while  MCPDSetEC()  completely discards all changes  done to the
  equality constraints.

Inputs:
    S       -   solver
    EC      -   equality constraints, array[N,N]. Elements of  EC  can  be
                either NAN's or finite  numbers from  [0,1].  NAN  denotes
                absence  of  constraints,  while  finite  value    denotes
                equality constraint on the corresponding element of P.

NOTES:

1. infinite values of EC will lead to exception being thrown. Values  less
than 0.0 or greater than 1.0 will lead to error code being returned  after
call to MCPDSolve().
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdsetec(mcpdstate s, real_2d_array ec);
</pre>
<a name=sub_mcpdsetlc></a><h6 class=pageheader>mcpdsetlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to set linear equality/inequality constraints on the
elements of the transition matrix P.

This function can be used to set one or several general linear constraints
on the elements of P. Two types of constraints are supported:
* equality constraints
* inequality constraints (both less-or-equal and greater-or-equal)

Coefficients  of  constraints  are  specified  by  matrix  C (one  of  the
parameters).  One  row  of  C  corresponds  to  one  constraint.   Because
transition  matrix P has N*N elements,  we  need  N*N columns to store all
coefficients  (they  are  stored row by row), and one more column to store
right part - hence C has N*N+1 columns.  Constraint  kind is stored in the
CT array.

Thus, I-th linear constraint is
    P[0,0]*C[I,0] + P[0,1]*C[I,1] + .. + P[0,N-1]*C[I,N-1] +
        + P[1,0]*C[I,N] + P[1,1]*C[I,N+1] + ... +
        + P[N-1,N-1]*C[I,N*N-1]  ?=?  C[I,N*N]
where ?=? can be either &quot;=&quot; (CT[i]=0), &quot;&le;&quot; (CT[i] &lt; 0) or &quot;&ge;&quot; (CT[i] &gt; 0).

Your constraint may involve only some subset of P (less than N*N elements).
For example it can be something like
    P[0,0] + P[0,1] = 0.5
In this case you still should pass matrix  with N*N+1 columns, but all its
elements (except for C[0,0], C[0,1] and C[0,N*N-1]) will be zero.

Inputs:
    S       -   solver
    C       -   array[K,N*N+1] - coefficients of constraints
                (see above for complete description)
    CT      -   array[K] - constraint types
                (see above for complete description)
    K       -   number of equality/inequality constraints, K &ge; 0:
                * if given, only leading K elements of C/CT are used
                * if not given, automatically determined from sizes of C/CT
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdsetlc(mcpdstate s, real_2d_array c, integer_1d_array ct, ae_int_t k);
<b>void</b> mcpdsetlc(mcpdstate s, real_2d_array c, integer_1d_array ct);
</pre>
<a name=sub_mcpdsetpredictionweights></a><h6 class=pageheader>mcpdsetpredictionweights Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to change prediction weights

MCPD solver scales prediction errors as follows
    Error(P) = ||W*(y-P*x)||^2
where
    x is a system state at time t
    y is a system state at time t+1
    P is a transition matrix
    W is a diagonal scaling matrix

By default, weights are chosen in order  to  minimize  relative prediction
error instead of absolute one. For example, if one component of  state  is
about 0.5 in magnitude and another one is about 0.05, then algorithm  will
make corresponding weights equal to 2.0 and 20.0.

Inputs:
    S       -   solver
    PW      -   array[N], weights:
                * must be non-negative values (exception will be thrown otherwise)
                * zero values will be replaced by automatically chosen values
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdsetpredictionweights(mcpdstate s, real_1d_array pw);
</pre>
<a name=sub_mcpdsetprior></a><h6 class=pageheader>mcpdsetprior Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  allows to set prior values used for regularization of your
problem.

By default, regularizing term is equal to r*||P-prior_P||^2, where r is  a
small non-zero value,  P is transition matrix, prior_P is identity matrix,
||X||^2 is a sum of squared elements of X.

This  function  allows  you to change prior values prior_P. You  can  also
change r with MCPDSetTikhonovRegularizer() function.

Inputs:
    S       -   solver
    PP      -   array[N,N], matrix of prior values:
                1. elements must be real numbers from [0,1]
                2. columns must sum to 1.0.
                First property is checked (exception is thrown otherwise),
                while second one is not checked/enforced.
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdsetprior(mcpdstate s, real_2d_array pp);
</pre>
<a name=sub_mcpdsettikhonovregularizer></a><h6 class=pageheader>mcpdsettikhonovregularizer Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function allows to  tune  amount  of  Tikhonov  regularization  being
applied to your problem.

By default, regularizing term is equal to r*||P-prior_P||^2, where r is  a
small non-zero value,  P is transition matrix, prior_P is identity matrix,
||X||^2 is a sum of squared elements of X.

This  function  allows  you to change coefficient r. You can  also  change
prior values with MCPDSetPrior() function.

Inputs:
    S       -   solver
    V       -   regularization  coefficient, finite non-negative value. It
                is  not  recommended  to specify zero value unless you are
                pretty sure that you want it.
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdsettikhonovregularizer(mcpdstate s, <b>double</b> v);
</pre>
<a name=sub_mcpdsolve></a><h6 class=pageheader>mcpdsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to start solution of the MCPD problem.

After return from this function, you can use MCPDResults() to get solution
and completion code.
ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mcpdsolve(mcpdstate s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mcpd_simple1 class=nav>mcpd_simple1</a> | <a href=#example_mcpd_simple2 class=nav>mcpd_simple2</a> ]</p>
<a name=example_mcpd_simple1></a><h6 class=pageheader>mcpd_simple1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// The very simple MCPD example</font>
<font color=navy>//</font>
<font color=navy>// We have a loan portfolio. Our loans can be in one of two states:</font>
<font color=navy>// * normal loans (<font color=blue><b>&quot;good&quot;</b></font> ones)</font>
<font color=navy>// * past due loans (<font color=blue><b>&quot;bad&quot;</b></font> ones)</font>
<font color=navy>//</font>
<font color=navy>// We assume that:</font>
<font color=navy>// * loans can transition from any state to any other state. In </font>
<font color=navy>//   particular, past due loan can become <font color=blue><b>&quot;good&quot;</b></font> one at any moment </font>
<font color=navy>//   with same (fixed) probability. Not realistic, but it is toy example :)</font>
<font color=navy>// * portfolio size does not change over time</font>
<font color=navy>//</font>
<font color=navy>// Thus, we have following model</font>
<font color=navy>//     state_new = P*state_old</font>
<font color=navy>// where</font>
<font color=navy>//         ( p00  p01 )</font>
<font color=navy>//     P = (          )</font>
<font color=navy>//         ( p10  p11 )</font>
<font color=navy>//</font>
<font color=navy>// We want to model transitions between these two states using MCPD</font>
<font color=navy>// approach (Markov Chains <b>for</b> Proportional/Population Data), i.e.</font>
<font color=navy>// to restore hidden transition matrix P using actual portfolio data.</font>
<font color=navy>// We have:</font>
<font color=navy>// * poportional data, i.e. proportion of loans in the normal and past </font>
<font color=navy>//   due states (not portfolio size measured in some currency, although </font>
<font color=navy>//   it is possible to work with population data too)</font>
<font color=navy>// * two tracks, i.e. two sequences which describe portfolio</font>
<font color=navy>//   evolution from two different starting states: [1,0] (all loans </font>
<font color=navy>//   are <font color=blue><b>&quot;good&quot;</b></font>) and [0.8,0.2] (only 80% of portfolio is in the <font color=blue><b>&quot;good&quot;</b></font></font>
<font color=navy>//   state)</font>
   mcpdstate s;
   mcpdreport rep;
   real_2d_array p;
   real_2d_array track0 = <font color=blue><b>&quot;[[1.00000,0.00000],[0.95000,0.05000],[0.92750,0.07250],[0.91738,0.08263],[0.91282,0.08718]]&quot;</b></font>;
   real_2d_array track1 = <font color=blue><b>&quot;[[0.80000,0.20000],[0.86000,0.14000],[0.88700,0.11300],[0.89915,0.10085]]&quot;</b></font>;

   mcpdcreate(2, s);
   mcpdaddtrack(s, track0);
   mcpdaddtrack(s, track1);
   mcpdsolve(s);
   mcpdresults(s, p, rep);
<font color=navy>// Hidden matrix P is equal to</font>
<font color=navy>//         ( 0.95  0.50 )</font>
<font color=navy>//         (            )</font>
<font color=navy>//         ( 0.05  0.50 )</font>
<font color=navy>// which means that <font color=blue><b>&quot;good&quot;</b></font> loans can become <font color=blue><b>&quot;bad&quot;</b></font> with 5% probability, </font>
<font color=navy>// <b>while</b> <font color=blue><b>&quot;bad&quot;</b></font> loans will <b>return</b> to good state with 50% probability.</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, p.tostring(2).c_str()); <font color=navy>// EXPECTED: [[0.95,0.50],[0.05,0.50]]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_mcpd_simple2></a><h6 class=pageheader>mcpd_simple2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Simple MCPD example</font>
<font color=navy>//</font>
<font color=navy>// We have a loan portfolio. Our loans can be in one of three states:</font>
<font color=navy>// * normal loans</font>
<font color=navy>// * past due loans</font>
<font color=navy>// * charged off loans</font>
<font color=navy>//</font>
<font color=navy>// We assume that:</font>
<font color=navy>// * normal loan can stay normal or become past due (but not charged off)</font>
<font color=navy>// * past due loan can stay past due, become normal or charged off</font>
<font color=navy>// * charged off loan will stay charged off <b>for</b> the rest of eternity</font>
<font color=navy>// * portfolio size does not change over time</font>
<font color=navy>// Not realistic, but it is toy example :)</font>
<font color=navy>//</font>
<font color=navy>// Thus, we have following model</font>
<font color=navy>//     state_new = P*state_old</font>
<font color=navy>// where</font>
<font color=navy>//         ( p00  p01    )</font>
<font color=navy>//     P = ( p10  p11    )</font>
<font color=navy>//         (      p21  1 )</font>
<font color=navy>// i.e. four elements of P are known a priori.</font>
<font color=navy>//</font>
<font color=navy>// Although it is possible (given enough data) to In order to enforce </font>
<font color=navy>// this property we set equality constraints on these elements.</font>
<font color=navy>//</font>
<font color=navy>// We want to model transitions between these two states using MCPD</font>
<font color=navy>// approach (Markov Chains <b>for</b> Proportional/Population Data), i.e.</font>
<font color=navy>// to restore hidden transition matrix P using actual portfolio data.</font>
<font color=navy>// We have:</font>
<font color=navy>// * poportional data, i.e. proportion of loans in the current and past </font>
<font color=navy>//   due states (not portfolio size measured in some currency, although </font>
<font color=navy>//   it is possible to work with population data too)</font>
<font color=navy>// * two tracks, i.e. two sequences which describe portfolio</font>
<font color=navy>//   evolution from two different starting states: [1,0,0] (all loans </font>
<font color=navy>//   are <font color=blue><b>&quot;good&quot;</b></font>) and [0.8,0.2,0.0] (only 80% of portfolio is in the <font color=blue><b>&quot;good&quot;</b></font></font>
<font color=navy>//   state)</font>
   mcpdstate s;
   mcpdreport rep;
   real_2d_array p;
   real_2d_array track0 = <font color=blue><b>&quot;[[1.000000,0.000000,0.000000],[0.950000,0.050000,0.000000],[0.927500,0.060000,0.012500],[0.911125,0.061375,0.027500],[0.896256,0.060900,0.042844]]&quot;</b></font>;
   real_2d_array track1 = <font color=blue><b>&quot;[[0.800000,0.200000,0.000000],[0.860000,0.090000,0.050000],[0.862000,0.065500,0.072500],[0.851650,0.059475,0.088875],[0.838805,0.057451,0.103744]]&quot;</b></font>;

   mcpdcreate(3, s);
   mcpdaddtrack(s, track0);
   mcpdaddtrack(s, track1);
   mcpdaddec(s, 0, 2, 0.0);
   mcpdaddec(s, 1, 2, 0.0);
   mcpdaddec(s, 2, 2, 1.0);
   mcpdaddec(s, 2, 0, 0.0);
   mcpdsolve(s);
   mcpdresults(s, p, rep);
<font color=navy>// Hidden matrix P is equal to</font>
<font color=navy>//         ( 0.95 0.50      )</font>
<font color=navy>//         ( 0.05 0.25      )</font>
<font color=navy>//         (      0.25 1.00 ) </font>
<font color=navy>// which means that <font color=blue><b>&quot;good&quot;</b></font> loans can become past due with 5% probability, </font>
<font color=navy>// <b>while</b> past due loans will become charged off with 25% probability or</font>
<font color=navy>// <b>return</b> back to normal state with 50% probability.</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, p.tostring(2).c_str()); <font color=navy>// EXPECTED: [[0.95,0.50,0.00],[0.05,0.25,0.00],[0.00,0.25,1.00]]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_mlpbase></a><h4 class=pageheader>8.2.11. mlpbase Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_modelerrors class=toc>modelerrors</a> |
<a href=#struct_multilayerperceptron class=toc>multilayerperceptron</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_mlpactivationfunction class=toc>mlpactivationfunction</a> |
<a href=#sub_mlpallerrorssparsesubset class=toc>mlpallerrorssparsesubset</a> |
<a href=#sub_mlpallerrorssubset class=toc>mlpallerrorssubset</a> |
<a href=#sub_mlpavgce class=toc>mlpavgce</a> |
<a href=#sub_mlpavgcesparse class=toc>mlpavgcesparse</a> |
<a href=#sub_mlpavgerror class=toc>mlpavgerror</a> |
<a href=#sub_mlpavgerrorsparse class=toc>mlpavgerrorsparse</a> |
<a href=#sub_mlpavgrelerror class=toc>mlpavgrelerror</a> |
<a href=#sub_mlpavgrelerrorsparse class=toc>mlpavgrelerrorsparse</a> |
<a href=#sub_mlpclserror class=toc>mlpclserror</a> |
<a href=#sub_mlpcopy class=toc>mlpcopy</a> |
<a href=#sub_mlpcopytunableparameters class=toc>mlpcopytunableparameters</a> |
<a href=#sub_mlpcreate0 class=toc>mlpcreate0</a> |
<a href=#sub_mlpcreate1 class=toc>mlpcreate1</a> |
<a href=#sub_mlpcreate2 class=toc>mlpcreate2</a> |
<a href=#sub_mlpcreateb0 class=toc>mlpcreateb0</a> |
<a href=#sub_mlpcreateb1 class=toc>mlpcreateb1</a> |
<a href=#sub_mlpcreateb2 class=toc>mlpcreateb2</a> |
<a href=#sub_mlpcreatec0 class=toc>mlpcreatec0</a> |
<a href=#sub_mlpcreatec1 class=toc>mlpcreatec1</a> |
<a href=#sub_mlpcreatec2 class=toc>mlpcreatec2</a> |
<a href=#sub_mlpcreater0 class=toc>mlpcreater0</a> |
<a href=#sub_mlpcreater1 class=toc>mlpcreater1</a> |
<a href=#sub_mlpcreater2 class=toc>mlpcreater2</a> |
<a href=#sub_mlperror class=toc>mlperror</a> |
<a href=#sub_mlperrorn class=toc>mlperrorn</a> |
<a href=#sub_mlperrorsparse class=toc>mlperrorsparse</a> |
<a href=#sub_mlperrorsparsesubset class=toc>mlperrorsparsesubset</a> |
<a href=#sub_mlperrorsubset class=toc>mlperrorsubset</a> |
<a href=#sub_mlpgetinputscaling class=toc>mlpgetinputscaling</a> |
<a href=#sub_mlpgetinputscount class=toc>mlpgetinputscount</a> |
<a href=#sub_mlpgetlayerscount class=toc>mlpgetlayerscount</a> |
<a href=#sub_mlpgetlayersize class=toc>mlpgetlayersize</a> |
<a href=#sub_mlpgetneuroninfo class=toc>mlpgetneuroninfo</a> |
<a href=#sub_mlpgetoutputscaling class=toc>mlpgetoutputscaling</a> |
<a href=#sub_mlpgetoutputscount class=toc>mlpgetoutputscount</a> |
<a href=#sub_mlpgetweight class=toc>mlpgetweight</a> |
<a href=#sub_mlpgetweightscount class=toc>mlpgetweightscount</a> |
<a href=#sub_mlpgrad class=toc>mlpgrad</a> |
<a href=#sub_mlpgradbatch class=toc>mlpgradbatch</a> |
<a href=#sub_mlpgradbatchsparse class=toc>mlpgradbatchsparse</a> |
<a href=#sub_mlpgradbatchsparsesubset class=toc>mlpgradbatchsparsesubset</a> |
<a href=#sub_mlpgradbatchsubset class=toc>mlpgradbatchsubset</a> |
<a href=#sub_mlpgradn class=toc>mlpgradn</a> |
<a href=#sub_mlpgradnbatch class=toc>mlpgradnbatch</a> |
<a href=#sub_mlphessianbatch class=toc>mlphessianbatch</a> |
<a href=#sub_mlphessiannbatch class=toc>mlphessiannbatch</a> |
<a href=#sub_mlpinitpreprocessor class=toc>mlpinitpreprocessor</a> |
<a href=#sub_mlpissoftmax class=toc>mlpissoftmax</a> |
<a href=#sub_mlpprocess class=toc>mlpprocess</a> |
<a href=#sub_mlpprocessi class=toc>mlpprocessi</a> |
<a href=#sub_mlpproperties class=toc>mlpproperties</a> |
<a href=#sub_mlprandomize class=toc>mlprandomize</a> |
<a href=#sub_mlprandomizefull class=toc>mlprandomizefull</a> |
<a href=#sub_mlprelclserror class=toc>mlprelclserror</a> |
<a href=#sub_mlprelclserrorsparse class=toc>mlprelclserrorsparse</a> |
<a href=#sub_mlprmserror class=toc>mlprmserror</a> |
<a href=#sub_mlprmserrorsparse class=toc>mlprmserrorsparse</a> |
<a href=#sub_mlpserialize class=toc>mlpserialize</a> |
<a href=#sub_mlpsetinputscaling class=toc>mlpsetinputscaling</a> |
<a href=#sub_mlpsetneuroninfo class=toc>mlpsetneuroninfo</a> |
<a href=#sub_mlpsetoutputscaling class=toc>mlpsetoutputscaling</a> |
<a href=#sub_mlpsetweight class=toc>mlpsetweight</a> |
<a href=#sub_mlpunserialize class=toc>mlpunserialize</a>
]</font>
</div>
<a name=struct_modelerrors></a><h6 class=pageheader>modelerrors Class</h6>
<hr width=600 align=left>
<pre class=narration>
Model's errors:
    * RelCLSError   -   fraction of misclassified cases.
    * AvgCE         -   acerage cross-entropy
    * RMSError      -   root-mean-square error
    * AvgError      -   average error
    * AvgRelError   -   average relative error

NOTE 1: RelCLSError/AvgCE are zero on regression problems.

NOTE 2: on classification problems  RMSError/AvgError/AvgRelError  contain
        errors in prediction of posterior probabilities
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> modelerrors {
   <b>double</b> relclserror;
   <b>double</b> avgce;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
};
</pre>
<a name=struct_multilayerperceptron></a><h6 class=pageheader>multilayerperceptron Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> multilayerperceptron {
};
</pre>
<a name=sub_mlpactivationfunction></a><h6 class=pageheader>mlpactivationfunction Function</h6>
<hr width=600 align=left>
<pre class=narration>
Neural network activation function

Inputs:
    NET         -   neuron input
    K           -   function index (zero for linear function)

Outputs:
    F           -   function
    DF          -   its derivative
    D2F         -   its second derivative
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpactivationfunction(<b>double</b> net, ae_int_t k, <b>double</b> &amp;f, <b>double</b> &amp;df, <b>double</b> &amp;d2f);
</pre>
<a name=sub_mlpallerrorssparsesubset></a><h6 class=pageheader>mlpallerrorssparsesubset Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of all types of errors on subset of dataset.

Inputs:
    Network -   network initialized with one of the network creation funcs
    XY      -   original dataset given by sparse matrix;
                one sample = one row;
                first NIn columns contain inputs,
                next NOut columns - desired outputs.
    SetSize -   real size of XY, SetSize &ge; 0;
    Subset  -   subset of SubsetSize elements, array[SubsetSize];
    SubsetSize- number of elements in Subset[] array:
                * if SubsetSize &gt; 0, rows of XY with indices Subset[0]...
                  ...Subset[SubsetSize-1] are processed
                * if SubsetSize=0, zeros are returned
                * if SubsetSize &lt; 0, entire dataset is  processed;  Subset[]
                  array is ignored in this case.

Outputs:
    Rep     -   it contains all type of errors.
ALGLIB: Copyright 04.09.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpallerrorssparsesubset(multilayerperceptron network, sparsematrix xy, ae_int_t setsize, integer_1d_array subset, ae_int_t subsetsize, modelerrors &amp;rep);
</pre>
<a name=sub_mlpallerrorssubset></a><h6 class=pageheader>mlpallerrorssubset Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of all types of errors on subset of dataset.

Inputs:
    Network -   network initialized with one of the network creation funcs
    XY      -   original dataset; one sample = one row;
                first NIn columns contain inputs,
                next NOut columns - desired outputs.
    SetSize -   real size of XY, SetSize &ge; 0;
    Subset  -   subset of SubsetSize elements, array[SubsetSize];
    SubsetSize- number of elements in Subset[] array:
                * if SubsetSize &gt; 0, rows of XY with indices Subset[0]...
                  ...Subset[SubsetSize-1] are processed
                * if SubsetSize=0, zeros are returned
                * if SubsetSize &lt; 0, entire dataset is  processed;  Subset[]
                  array is ignored in this case.

Outputs:
    Rep     -   it contains all type of errors.
ALGLIB: Copyright 04.09.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpallerrorssubset(multilayerperceptron network, real_2d_array xy, ae_int_t setsize, integer_1d_array subset, ae_int_t subsetsize, modelerrors &amp;rep);
</pre>
<a name=sub_mlpavgce></a><h6 class=pageheader>mlpavgce Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average cross-entropy  (in bits  per element) on the test set.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format;
    NPoints     -   points count.

Result:
CrossEntropy/(NPoints*LN(2)).
Zero if network solves regression task.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 08.01.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpavgce(multilayerperceptron network, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlpavgcesparse></a><h6 class=pageheader>mlpavgcesparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average  cross-entropy  (in bits  per element)  on the  test set  given by
sparse matrix.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format. This function checks  correctness
                    of  the  dataset  (no  NANs/INFs,  class  numbers  are
                    correct) and throws exception when  incorrect  dataset
                    is passed.  Sparse  matrix  must  use  CRS  format for
                    storage.
    NPoints     -   points count, &ge; 0.

Result:
CrossEntropy/(NPoints*LN(2)).
Zero if network solves regression task.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpavgcesparse(multilayerperceptron network, sparsematrix xy, ae_int_t npoints);
</pre>
<a name=sub_mlpavgerror></a><h6 class=pageheader>mlpavgerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average absolute error on the test set.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format;
    NPoints     -   points count.

Result:
Its meaning for regression task is obvious. As for classification task, it
means average error when estimating posterior probabilities.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 11.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpavgerror(multilayerperceptron network, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlpavgerrorsparse></a><h6 class=pageheader>mlpavgerrorsparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average absolute error on the test set given by sparse matrix.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format. This function checks  correctness
                    of  the  dataset  (no  NANs/INFs,  class  numbers  are
                    correct) and throws exception when  incorrect  dataset
                    is passed.  Sparse  matrix  must  use  CRS  format for
                    storage.
    NPoints     -   points count, &ge; 0.

Result:
Its meaning for regression task is obvious. As for classification task, it
means average error when estimating posterior probabilities.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpavgerrorsparse(multilayerperceptron network, sparsematrix xy, ae_int_t npoints);
</pre>
<a name=sub_mlpavgrelerror></a><h6 class=pageheader>mlpavgrelerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average relative error on the test set.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format;
    NPoints     -   points count.

Result:
Its meaning for regression task is obvious. As for classification task, it
means  average  relative  error  when  estimating posterior probability of
belonging to the correct class.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 11.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpavgrelerror(multilayerperceptron network, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlpavgrelerrorsparse></a><h6 class=pageheader>mlpavgrelerrorsparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average relative error on the test set given by sparse matrix.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format. This function checks  correctness
                    of  the  dataset  (no  NANs/INFs,  class  numbers  are
                    correct) and throws exception when  incorrect  dataset
                    is passed.  Sparse  matrix  must  use  CRS  format for
                    storage.
    NPoints     -   points count, &ge; 0.

Result:
Its meaning for regression task is obvious. As for classification task, it
means  average  relative  error  when  estimating posterior probability of
belonging to the correct class.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpavgrelerrorsparse(multilayerperceptron network, sparsematrix xy, ae_int_t npoints);
</pre>
<a name=sub_mlpclserror></a><h6 class=pageheader>mlpclserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Classification error of the neural network on dataset.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format;
    NPoints     -   points count.

Result:
    classification error (number of misclassified cases)

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t mlpclserror(multilayerperceptron network, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlpcopy></a><h6 class=pageheader>mlpcopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Copying of neural network

Inputs:
    Network1 -   original

Outputs:
    Network2 -   copy
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcopy(multilayerperceptron network1, multilayerperceptron &amp;network2);
</pre>
<a name=sub_mlpcopytunableparameters></a><h6 class=pageheader>mlpcopytunableparameters Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function copies tunable  parameters (weights/means/sigmas)  from  one
network to another with same architecture. It  performs  some  rudimentary
checks that architectures are same, and throws exception if check fails.

It is intended for fast copying of states between two  network  which  are
known to have same geometry.

Inputs:
    Network1 -   source, must be correctly initialized
    Network2 -   target, must have same architecture

Outputs:
    Network2 -   network state is copied from source to target
ALGLIB: Copyright 20.06.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcopytunableparameters(multilayerperceptron network1, multilayerperceptron network2);
</pre>
<a name=sub_mlpcreate0></a><h6 class=pageheader>mlpcreate0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
layers, with linear output layer. Network weights are  filled  with  small
random values.
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreate0(ae_int_t nin, ae_int_t nout, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreate1></a><h6 class=pageheader>mlpcreate1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same  as  MLPCreate0,  but  with  one  hidden  layer  (NHid  neurons) with
non-linear activation function. Output layer is linear.
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreate1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreate2></a><h6 class=pageheader>mlpcreate2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same as MLPCreate0, but with two hidden layers (NHid1 and  NHid2  neurons)
with non-linear activation function. Output layer is linear.
 $ALL
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreate2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreateb0></a><h6 class=pageheader>mlpcreateb0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
layers with non-linear output layer. Network weights are filled with small
random values.

Activation function of the output layer takes values:

    (B, +INF), if D &ge; 0

or

    (-INF, B), if D &lt; 0.
ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreateb0(ae_int_t nin, ae_int_t nout, <b>double</b> b, <b>double</b> d, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreateb1></a><h6 class=pageheader>mlpcreateb1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same as MLPCreateB0 but with non-linear hidden layer.
ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreateb1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, <b>double</b> b, <b>double</b> d, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreateb2></a><h6 class=pageheader>mlpcreateb2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same as MLPCreateB0 but with two non-linear hidden layers.
ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreateb2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, <b>double</b> b, <b>double</b> d, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreatec0></a><h6 class=pageheader>mlpcreatec0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Creates classifier network with NIn  inputs  and  NOut  possible  classes.
Network contains no hidden layers and linear output  layer  with  SOFTMAX-
normalization  (so  outputs  sums  up  to  1.0  and  converge to posterior
probabilities).
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreatec0(ae_int_t nin, ae_int_t nout, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreatec1></a><h6 class=pageheader>mlpcreatec1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same as MLPCreateC0, but with one non-linear hidden layer.
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreatec1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreatec2></a><h6 class=pageheader>mlpcreatec2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same as MLPCreateC0, but with two non-linear hidden layers.
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreatec2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreater0></a><h6 class=pageheader>mlpcreater0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
layers with non-linear output layer. Network weights are filled with small
random values. Activation function of the output layer takes values [A,B].
ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreater0(ae_int_t nin, ae_int_t nout, <b>double</b> a, <b>double</b> b, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreater1></a><h6 class=pageheader>mlpcreater1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same as MLPCreateR0, but with non-linear hidden layer.
ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreater1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, <b>double</b> a, <b>double</b> b, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlpcreater2></a><h6 class=pageheader>mlpcreater2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same as MLPCreateR0, but with two non-linear hidden layers.
ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreater2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, <b>double</b> a, <b>double</b> b, multilayerperceptron &amp;network);
</pre>
<a name=sub_mlperror></a><h6 class=pageheader>mlperror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Error of the neural network on dataset.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format;
    NPoints     -   points count.

Result:
    sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlperror(multilayerperceptron network, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlperrorn></a><h6 class=pageheader>mlperrorn Function</h6>
<hr width=600 align=left>
<pre class=narration>
Natural error function for neural network, internal subroutine.

NOTE: this function is single-threaded. Unlike other  error  function,  it
receives no speed-up from being executed in SMP mode.
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlperrorn(multilayerperceptron network, real_2d_array xy, ae_int_t ssize);
</pre>
<a name=sub_mlperrorsparse></a><h6 class=pageheader>mlperrorsparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Error of the neural network on dataset given by sparse matrix.

Inputs:
    Network     -   neural network
    XY          -   training  set,  see  below  for  information  on   the
                    training set format. This function checks  correctness
                    of  the  dataset  (no  NANs/INFs,  class  numbers  are
                    correct) and throws exception when  incorrect  dataset
                    is passed.  Sparse  matrix  must  use  CRS  format for
                    storage.
    NPoints     -   points count, &ge; 0

Result:
    sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlperrorsparse(multilayerperceptron network, sparsematrix xy, ae_int_t npoints);
</pre>
<a name=sub_mlperrorsparsesubset></a><h6 class=pageheader>mlperrorsparsesubset Function</h6>
<hr width=600 align=left>
<pre class=narration>
Error of the neural network on subset of sparse dataset.

Inputs:
    Network   -     neural network;
    XY        -     training  set,  see  below  for  information  on   the
                    training set format. This function checks  correctness
                    of  the  dataset  (no  NANs/INFs,  class  numbers  are
                    correct) and throws exception when  incorrect  dataset
                    is passed.  Sparse  matrix  must  use  CRS  format for
                    storage.
    SetSize   -     real size of XY, SetSize &ge; 0;
                    it is used when SubsetSize &lt; 0;
    Subset    -     subset of SubsetSize elements, array[SubsetSize];
    SubsetSize-     number of elements in Subset[] array:
                    * if SubsetSize &gt; 0, rows of XY with indices Subset[0]...
                      ...Subset[SubsetSize-1] are processed
                    * if SubsetSize=0, zeros are returned
                    * if SubsetSize &lt; 0, entire dataset is  processed;  Subset[]
                      array is ignored in this case.

Result:
    sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 04.09.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlperrorsparsesubset(multilayerperceptron network, sparsematrix xy, ae_int_t setsize, integer_1d_array subset, ae_int_t subsetsize);
</pre>
<a name=sub_mlperrorsubset></a><h6 class=pageheader>mlperrorsubset Function</h6>
<hr width=600 align=left>
<pre class=narration>
Error of the neural network on subset of dataset.

Inputs:
    Network   -     neural network;
    XY        -     training  set,  see  below  for  information  on   the
                    training set format;
    SetSize   -     real size of XY, SetSize &ge; 0;
    Subset    -     subset of SubsetSize elements, array[SubsetSize];
    SubsetSize-     number of elements in Subset[] array:
                    * if SubsetSize &gt; 0, rows of XY with indices Subset[0]...
                      ...Subset[SubsetSize-1] are processed
                    * if SubsetSize=0, zeros are returned
                    * if SubsetSize &lt; 0, entire dataset is  processed;  Subset[]
                      array is ignored in this case.

Result:
    sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 04.09.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlperrorsubset(multilayerperceptron network, real_2d_array xy, ae_int_t setsize, integer_1d_array subset, ae_int_t subsetsize);
</pre>
<a name=sub_mlpgetinputscaling></a><h6 class=pageheader>mlpgetinputscaling Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns offset/scaling coefficients for I-th input of the
network.

Inputs:
    Network     -   network
    I           -   input index

Outputs:
    Mean        -   mean term
    Sigma       -   sigma term, guaranteed to be nonzero.

I-th input is passed through linear transformation
    IN[i] = (IN[i]-Mean)/Sigma
before feeding to the network
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgetinputscaling(multilayerperceptron network, ae_int_t i, <b>double</b> &amp;mean, <b>double</b> &amp;sigma);
</pre>
<a name=sub_mlpgetinputscount></a><h6 class=pageheader>mlpgetinputscount Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns number of inputs.
ALGLIB: Copyright 19.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t mlpgetinputscount(multilayerperceptron network);
</pre>
<a name=sub_mlpgetlayerscount></a><h6 class=pageheader>mlpgetlayerscount Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns total number of layers (including input, hidden and
output layers).
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t mlpgetlayerscount(multilayerperceptron network);
</pre>
<a name=sub_mlpgetlayersize></a><h6 class=pageheader>mlpgetlayersize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns size of K-th layer.

K=0 corresponds to input layer, K=CNT-1 corresponds to output layer.

Size of the output layer is always equal to the number of outputs, although
when we have softmax-normalized network, last neuron doesn't have any
connections - it is just zero.
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t mlpgetlayersize(multilayerperceptron network, ae_int_t k);
</pre>
<a name=sub_mlpgetneuroninfo></a><h6 class=pageheader>mlpgetneuroninfo Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns information about Ith neuron of Kth layer

Inputs:
    Network     -   network
    K           -   layer index
    I           -   neuron index (within layer)

Outputs:
    FKind       -   activation function type (used by MLPActivationFunction())
                    this value is zero for input or linear neurons
    Threshold   -   also called offset, bias
                    zero for input neurons

NOTE: this function throws exception if layer or neuron with  given  index
do not exists.
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgetneuroninfo(multilayerperceptron network, ae_int_t k, ae_int_t i, ae_int_t &amp;fkind, <b>double</b> &amp;threshold);
</pre>
<a name=sub_mlpgetoutputscaling></a><h6 class=pageheader>mlpgetoutputscaling Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns offset/scaling coefficients for I-th output of the
network.

Inputs:
    Network     -   network
    I           -   input index

Outputs:
    Mean        -   mean term
    Sigma       -   sigma term, guaranteed to be nonzero.

I-th output is passed through linear transformation
    OUT[i] = OUT[i]*Sigma+Mean
before returning it to user. In case we have SOFTMAX-normalized network,
we return (Mean,Sigma)=(0.0,1.0).
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgetoutputscaling(multilayerperceptron network, ae_int_t i, <b>double</b> &amp;mean, <b>double</b> &amp;sigma);
</pre>
<a name=sub_mlpgetoutputscount></a><h6 class=pageheader>mlpgetoutputscount Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns number of outputs.
ALGLIB: Copyright 19.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t mlpgetoutputscount(multilayerperceptron network);
</pre>
<a name=sub_mlpgetweight></a><h6 class=pageheader>mlpgetweight Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns information about connection from I0-th neuron of
K0-th layer to I1-th neuron of K1-th layer.

Inputs:
    Network     -   network
    K0          -   layer index
    I0          -   neuron index (within layer)
    K1          -   layer index
    I1          -   neuron index (within layer)

Result:
    connection weight (zero for non-existent connections)

This function:
1. throws exception if layer or neuron with given index do not exists.
2. returns zero if neurons exist, but there is no connection between them
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpgetweight(multilayerperceptron network, ae_int_t k0, ae_int_t i0, ae_int_t k1, ae_int_t i1);
</pre>
<a name=sub_mlpgetweightscount></a><h6 class=pageheader>mlpgetweightscount Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns number of weights.
ALGLIB: Copyright 19.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t mlpgetweightscount(multilayerperceptron network);
</pre>
<a name=sub_mlpgrad></a><h6 class=pageheader>mlpgrad Function</h6>
<hr width=600 align=left>
<pre class=narration>
Gradient calculation

Inputs:
    Network -   network initialized with one of the network creation funcs
    X       -   input vector, length of array must be at least NIn
    DesiredY-   desired outputs, length of array must be at least NOut
    Grad    -   possibly preallocated array. If size of array is smaller
                than WCount, it will be reallocated. It is recommended to
                reuse previously allocated array to reduce allocation
                overhead.

Outputs:
    E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
    Grad    -   gradient of E with respect to weights of network, array[WCount]
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgrad(multilayerperceptron network, real_1d_array x, real_1d_array desiredy, <b>double</b> &amp;e, real_1d_array &amp;grad);
</pre>
<a name=sub_mlpgradbatch></a><h6 class=pageheader>mlpgradbatch Function</h6>
<hr width=600 align=left>
<pre class=narration>
Batch gradient calculation for a set of inputs/outputs

Inputs:
    Network -   network initialized with one of the network creation funcs
    XY      -   original dataset in dense format; one sample = one row:
                * first NIn columns contain inputs,
                * for regression problem, next NOut columns store
                  desired outputs.
                * for classification problem, next column (just one!)
                  stores class number.
    SSize   -   number of elements in XY
    Grad    -   possibly preallocated array. If size of array is smaller
                than WCount, it will be reallocated. It is recommended to
                reuse previously allocated array to reduce allocation
                overhead.

Outputs:
    E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
    Grad    -   gradient of E with respect to weights of network, array[WCount]
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgradbatch(multilayerperceptron network, real_2d_array xy, ae_int_t ssize, <b>double</b> &amp;e, real_1d_array &amp;grad);
</pre>
<a name=sub_mlpgradbatchsparse></a><h6 class=pageheader>mlpgradbatchsparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Batch gradient calculation for a set  of inputs/outputs  given  by  sparse
matrices

Inputs:
    Network -   network initialized with one of the network creation funcs
    XY      -   original dataset in sparse format; one sample = one row:
                * MATRIX MUST BE STORED IN CRS FORMAT
                * first NIn columns contain inputs.
                * for regression problem, next NOut columns store
                  desired outputs.
                * for classification problem, next column (just one!)
                  stores class number.
    SSize   -   number of elements in XY
    Grad    -   possibly preallocated array. If size of array is smaller
                than WCount, it will be reallocated. It is recommended to
                reuse previously allocated array to reduce allocation
                overhead.

Outputs:
    E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
    Grad    -   gradient of E with respect to weights of network, array[WCount]
ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgradbatchsparse(multilayerperceptron network, sparsematrix xy, ae_int_t ssize, <b>double</b> &amp;e, real_1d_array &amp;grad);
</pre>
<a name=sub_mlpgradbatchsparsesubset></a><h6 class=pageheader>mlpgradbatchsparsesubset Function</h6>
<hr width=600 align=left>
<pre class=narration>
Batch gradient calculation for a set of inputs/outputs  for  a  subset  of
dataset given by set of indexes.

Inputs:
    Network -   network initialized with one of the network creation funcs
    XY      -   original dataset in sparse format; one sample = one row:
                * MATRIX MUST BE STORED IN CRS FORMAT
                * first NIn columns contain inputs,
                * for regression problem, next NOut columns store
                  desired outputs.
                * for classification problem, next column (just one!)
                  stores class number.
    SetSize -   real size of XY, SetSize &ge; 0;
    Idx     -   subset of SubsetSize elements, array[SubsetSize]:
                * Idx[I] stores row index in the original dataset which is
                  given by XY. Gradient is calculated with respect to rows
                  whose indexes are stored in Idx[].
                * Idx[]  must store correct indexes; this function  throws
                  an  exception  in  case  incorrect index (less than 0 or
                  larger than rows(XY)) is given
                * Idx[]  may  store  indexes  in  any  order and even with
                  repetitions.
    SubsetSize- number of elements in Idx[] array:
                * positive value means that subset given by Idx[] is processed
                * zero value results in zero gradient
                * negative value means that full dataset is processed
    Grad      - possibly  preallocated array. If size of array is  smaller
                than WCount, it will be reallocated. It is  recommended to
                reuse  previously  allocated  array  to  reduce allocation
                overhead.

Outputs:
    E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
    Grad    -   gradient  of  E  with  respect   to  weights  of  network,
                array[WCount]

NOTE: when  SubsetSize &lt; 0 is used full dataset by call MLPGradBatchSparse
      function.
ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgradbatchsparsesubset(multilayerperceptron network, sparsematrix xy, ae_int_t setsize, integer_1d_array idx, ae_int_t subsetsize, <b>double</b> &amp;e, real_1d_array &amp;grad);
</pre>
<a name=sub_mlpgradbatchsubset></a><h6 class=pageheader>mlpgradbatchsubset Function</h6>
<hr width=600 align=left>
<pre class=narration>
Batch gradient calculation for a subset of dataset

Inputs:
    Network -   network initialized with one of the network creation funcs
    XY      -   original dataset in dense format; one sample = one row:
                * first NIn columns contain inputs,
                * for regression problem, next NOut columns store
                  desired outputs.
                * for classification problem, next column (just one!)
                  stores class number.
    SetSize -   real size of XY, SetSize &ge; 0;
    Idx     -   subset of SubsetSize elements, array[SubsetSize]:
                * Idx[I] stores row index in the original dataset which is
                  given by XY. Gradient is calculated with respect to rows
                  whose indexes are stored in Idx[].
                * Idx[]  must store correct indexes; this function  throws
                  an  exception  in  case  incorrect index (less than 0 or
                  larger than rows(XY)) is given
                * Idx[]  may  store  indexes  in  any  order and even with
                  repetitions.
    SubsetSize- number of elements in Idx[] array:
                * positive value means that subset given by Idx[] is processed
                * zero value results in zero gradient
                * negative value means that full dataset is processed
    Grad      - possibly  preallocated array. If size of array is  smaller
                than WCount, it will be reallocated. It is  recommended to
                reuse  previously  allocated  array  to  reduce allocation
                overhead.

Outputs:
    E         - error function, SUM(sqr(y[i]-desiredy[i])/2,i)
    Grad      - gradient  of  E  with  respect   to  weights  of  network,
                array[WCount]
ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgradbatchsubset(multilayerperceptron network, real_2d_array xy, ae_int_t setsize, integer_1d_array idx, ae_int_t subsetsize, <b>double</b> &amp;e, real_1d_array &amp;grad);
</pre>
<a name=sub_mlpgradn></a><h6 class=pageheader>mlpgradn Function</h6>
<hr width=600 align=left>
<pre class=narration>
Gradient calculation (natural error function is used)

Inputs:
    Network -   network initialized with one of the network creation funcs
    X       -   input vector, length of array must be at least NIn
    DesiredY-   desired outputs, length of array must be at least NOut
    Grad    -   possibly preallocated array. If size of array is smaller
                than WCount, it will be reallocated. It is recommended to
                reuse previously allocated array to reduce allocation
                overhead.

Outputs:
    E       -   error function, sum-of-squares for regression networks,
                cross-entropy for classification networks.
    Grad    -   gradient of E with respect to weights of network, array[WCount]
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgradn(multilayerperceptron network, real_1d_array x, real_1d_array desiredy, <b>double</b> &amp;e, real_1d_array &amp;grad);
</pre>
<a name=sub_mlpgradnbatch></a><h6 class=pageheader>mlpgradnbatch Function</h6>
<hr width=600 align=left>
<pre class=narration>
Batch gradient calculation for a set of inputs/outputs
(natural error function is used)

Inputs:
    Network -   network initialized with one of the network creation funcs
    XY      -   set of inputs/outputs; one sample = one row;
                first NIn columns contain inputs,
                next NOut columns - desired outputs.
    SSize   -   number of elements in XY
    Grad    -   possibly preallocated array. If size of array is smaller
                than WCount, it will be reallocated. It is recommended to
                reuse previously allocated array to reduce allocation
                overhead.

Outputs:
    E       -   error function, sum-of-squares for regression networks,
                cross-entropy for classification networks.
    Grad    -   gradient of E with respect to weights of network, array[WCount]
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpgradnbatch(multilayerperceptron network, real_2d_array xy, ae_int_t ssize, <b>double</b> &amp;e, real_1d_array &amp;grad);
</pre>
<a name=sub_mlphessianbatch></a><h6 class=pageheader>mlphessianbatch Function</h6>
<hr width=600 align=left>
<pre class=narration>
Batch Hessian calculation using R-algorithm.
Internal subroutine.
Hessian calculation based on R-algorithm described in
     &quot;Fast Exact Multiplication by the Hessian&quot;,
     B. A. Pearlmutter,
     Neural Computation, 1994.
ALGLIB: Copyright 26.01.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlphessianbatch(multilayerperceptron network, real_2d_array xy, ae_int_t ssize, <b>double</b> &amp;e, real_1d_array &amp;grad, real_2d_array &amp;h);
</pre>
<a name=sub_mlphessiannbatch></a><h6 class=pageheader>mlphessiannbatch Function</h6>
<hr width=600 align=left>
<pre class=narration>
Batch Hessian calculation (natural error function) using R-algorithm.
Internal subroutine.
Hessian calculation based on R-algorithm described in
     &quot;Fast Exact Multiplication by the Hessian&quot;,
     B. A. Pearlmutter,
     Neural Computation, 1994.
ALGLIB: Copyright 26.01.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlphessiannbatch(multilayerperceptron network, real_2d_array xy, ae_int_t ssize, <b>double</b> &amp;e, real_1d_array &amp;grad, real_2d_array &amp;h);
</pre>
<a name=sub_mlpinitpreprocessor></a><h6 class=pageheader>mlpinitpreprocessor Function</h6>
<hr width=600 align=left>
<pre class=narration>
Internal subroutine.
ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpinitpreprocessor(multilayerperceptron network, real_2d_array xy, ae_int_t ssize);
</pre>
<a name=sub_mlpissoftmax></a><h6 class=pageheader>mlpissoftmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
Tells whether network is SOFTMAX-normalized (i.e. classifier) or not.
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> mlpissoftmax(multilayerperceptron network);
</pre>
<a name=sub_mlpprocess></a><h6 class=pageheader>mlpprocess Function</h6>
<hr width=600 align=left>
<pre class=narration>
Procesing

Inputs:
    Network -   neural network
    X       -   input vector,  array[0..NIn-1].

Outputs:
    Y       -   result. Regression estimate when solving regression  task,
                vector of posterior probabilities for classification task.

See also MLPProcessI
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpprocess(multilayerperceptron network, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_mlpprocessi></a><h6 class=pageheader>mlpprocessi Function</h6>
<hr width=600 align=left>
<pre class=narration>
'interactive'  variant  of  MLPProcess  for  languages  like  Python which
support constructs like &quot;Y = MLPProcess(NN,X)&quot; and interactive mode of the
interpreter

This function allocates new array on each call,  so  it  is  significantly
slower than its 'non-interactive' counterpart, but it is  more  convenient
when you call it from command line.
ALGLIB: Copyright 21.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpprocessi(multilayerperceptron network, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_mlpproperties></a><h6 class=pageheader>mlpproperties Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns information about initialized network: number of inputs, outputs,
weights.
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpproperties(multilayerperceptron network, ae_int_t &amp;nin, ae_int_t &amp;nout, ae_int_t &amp;wcount);
</pre>
<a name=sub_mlprandomize></a><h6 class=pageheader>mlprandomize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Randomization of neural network weights
ALGLIB: Copyright 06.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlprandomize(multilayerperceptron network);
</pre>
<a name=sub_mlprandomizefull></a><h6 class=pageheader>mlprandomizefull Function</h6>
<hr width=600 align=left>
<pre class=narration>
Randomization of neural network weights and standartisator
ALGLIB: Copyright 10.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlprandomizefull(multilayerperceptron network);
</pre>
<a name=sub_mlprelclserror></a><h6 class=pageheader>mlprelclserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Relative classification error on the test set.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format;
    NPoints     -   points count.

Result:
Percent   of incorrectly   classified  cases.  Works  both  for classifier
networks and general purpose networks used as classifiers.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 25.12.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlprelclserror(multilayerperceptron network, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlprelclserrorsparse></a><h6 class=pageheader>mlprelclserrorsparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Relative classification error on the test set given by sparse matrix.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format. Sparse matrix must use CRS format
                    for storage.
    NPoints     -   points count, &ge; 0.

Result:
Percent   of incorrectly   classified  cases.  Works  both  for classifier
networks and general purpose networks used as classifiers.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlprelclserrorsparse(multilayerperceptron network, sparsematrix xy, ae_int_t npoints);
</pre>
<a name=sub_mlprmserror></a><h6 class=pageheader>mlprmserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
RMS error on the test set given.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format;
    NPoints     -   points count.

Result:
Root mean  square error. Its meaning for regression task is obvious. As for
classification  task,  RMS  error  means  error  when estimating  posterior
probabilities.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlprmserror(multilayerperceptron network, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlprmserrorsparse></a><h6 class=pageheader>mlprmserrorsparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
RMS error on the test set given by sparse matrix.

Inputs:
    Network     -   neural network;
    XY          -   training  set,  see  below  for  information  on   the
                    training set format. This function checks  correctness
                    of  the  dataset  (no  NANs/INFs,  class  numbers  are
                    correct) and throws exception when  incorrect  dataset
                    is passed.  Sparse  matrix  must  use  CRS  format for
                    storage.
    NPoints     -   points count, &ge; 0.

Result:
Root mean  square error. Its meaning for regression task is obvious. As for
classification  task,  RMS  error  means  error  when estimating  posterior
probabilities.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
dataset format is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlprmserrorsparse(multilayerperceptron network, sparsematrix xy, ae_int_t npoints);
</pre>
<a name=sub_mlpserialize></a><h6 class=pageheader>mlpserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: serialization
These functions serialize a data structure to a C++ string or stream.
* serialization can be freely moved across 32-bit and 64-bit systems,
  and different byte orders. For example, you can serialize a string
  on a SPARC and unserialize it on an x86.
* ALGLIB++ serialization is compatible with serialization in ALGLIB,
  in both directions.
Important properties of s_out:
* it contains alphanumeric characters, dots, underscores, minus signs
* these symbols are grouped into words, which are separated by spaces
  and Windows-style (CR+LF) newlines
ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpserialize(multilayerperceptron &amp;obj, std::string &amp;s_out);
<b>void</b> mlpserialize(multilayerperceptron &amp;obj, std::ostream &amp;s_out);
</pre>
<a name=sub_mlpsetinputscaling></a><h6 class=pageheader>mlpsetinputscaling Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets offset/scaling coefficients for I-th input of the
network.

Inputs:
    Network     -   network
    I           -   input index
    Mean        -   mean term
    Sigma       -   sigma term (if zero, will be replaced by 1.0)

NTE: I-th input is passed through linear transformation
    IN[i] = (IN[i]-Mean)/Sigma
before feeding to the network. This function sets Mean and Sigma.
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpsetinputscaling(multilayerperceptron network, ae_int_t i, <b>double</b> mean, <b>double</b> sigma);
</pre>
<a name=sub_mlpsetneuroninfo></a><h6 class=pageheader>mlpsetneuroninfo Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function modifies information about Ith neuron of Kth layer

Inputs:
    Network     -   network
    K           -   layer index
    I           -   neuron index (within layer)
    FKind       -   activation function type (used by MLPActivationFunction())
                    this value must be zero for input neurons
                    (you can not set activation function for input neurons)
    Threshold   -   also called offset, bias
                    this value must be zero for input neurons
                    (you can not set threshold for input neurons)

NOTES:
1. this function throws exception if layer or neuron with given index do
   not exists.
2. this function also throws exception when you try to set non-linear
   activation function for input neurons (any kind of network) or for output
   neurons of classifier network.
3. this function throws exception when you try to set non-zero threshold for
   input neurons (any kind of network).
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpsetneuroninfo(multilayerperceptron network, ae_int_t k, ae_int_t i, ae_int_t fkind, <b>double</b> threshold);
</pre>
<a name=sub_mlpsetoutputscaling></a><h6 class=pageheader>mlpsetoutputscaling Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets offset/scaling coefficients for I-th output of the
network.

Inputs:
    Network     -   network
    I           -   input index
    Mean        -   mean term
    Sigma       -   sigma term (if zero, will be replaced by 1.0)

Outputs:

NOTE: I-th output is passed through linear transformation
    OUT[i] = OUT[i]*Sigma+Mean
before returning it to user. This function sets Sigma/Mean. In case we
have SOFTMAX-normalized network, you can not set (Sigma,Mean) to anything
other than(0.0,1.0) - this function will throw exception.
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpsetoutputscaling(multilayerperceptron network, ae_int_t i, <b>double</b> mean, <b>double</b> sigma);
</pre>
<a name=sub_mlpsetweight></a><h6 class=pageheader>mlpsetweight Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function modifies information about connection from I0-th neuron of
K0-th layer to I1-th neuron of K1-th layer.

Inputs:
    Network     -   network
    K0          -   layer index
    I0          -   neuron index (within layer)
    K1          -   layer index
    I1          -   neuron index (within layer)
    W           -   connection weight (must be zero for non-existent
                    connections)

This function:
1. throws exception if layer or neuron with given index do not exists.
2. throws exception if you try to set non-zero weight for non-existent
   connection
ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpsetweight(multilayerperceptron network, ae_int_t k0, ae_int_t i0, ae_int_t k1, ae_int_t i1, <b>double</b> w);
</pre>
<a name=sub_mlpunserialize></a><h6 class=pageheader>mlpunserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: unserialization
These functions unserialize a data structure from a C++ string or stream.
Important properties of s_in:
* any combination of spaces, tabs, Windows or Unix stype newlines can
  be used as separators, so as to allow flexible reformatting of the
  stream or string from text or XML files.
* But you should not insert separators into the middle of the "words"
  nor you should change case of letters.
ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpunserialize(<b>const</b> std::string &amp;s_in, multilayerperceptron &amp;obj);
<b>void</b> mlpunserialize(<b>const</b> std::istream &amp;s_in, multilayerperceptron &amp;obj);
</pre>
<a name=unit_mlpe></a><h4 class=pageheader>8.2.12. mlpe Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_mlpensemble class=toc>mlpensemble</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_mlpeavgce class=toc>mlpeavgce</a> |
<a href=#sub_mlpeavgerror class=toc>mlpeavgerror</a> |
<a href=#sub_mlpeavgrelerror class=toc>mlpeavgrelerror</a> |
<a href=#sub_mlpecreate0 class=toc>mlpecreate0</a> |
<a href=#sub_mlpecreate1 class=toc>mlpecreate1</a> |
<a href=#sub_mlpecreate2 class=toc>mlpecreate2</a> |
<a href=#sub_mlpecreateb0 class=toc>mlpecreateb0</a> |
<a href=#sub_mlpecreateb1 class=toc>mlpecreateb1</a> |
<a href=#sub_mlpecreateb2 class=toc>mlpecreateb2</a> |
<a href=#sub_mlpecreatec0 class=toc>mlpecreatec0</a> |
<a href=#sub_mlpecreatec1 class=toc>mlpecreatec1</a> |
<a href=#sub_mlpecreatec2 class=toc>mlpecreatec2</a> |
<a href=#sub_mlpecreatefromnetwork class=toc>mlpecreatefromnetwork</a> |
<a href=#sub_mlpecreater0 class=toc>mlpecreater0</a> |
<a href=#sub_mlpecreater1 class=toc>mlpecreater1</a> |
<a href=#sub_mlpecreater2 class=toc>mlpecreater2</a> |
<a href=#sub_mlpeissoftmax class=toc>mlpeissoftmax</a> |
<a href=#sub_mlpeprocess class=toc>mlpeprocess</a> |
<a href=#sub_mlpeprocessi class=toc>mlpeprocessi</a> |
<a href=#sub_mlpeproperties class=toc>mlpeproperties</a> |
<a href=#sub_mlperandomize class=toc>mlperandomize</a> |
<a href=#sub_mlperelclserror class=toc>mlperelclserror</a> |
<a href=#sub_mlpermserror class=toc>mlpermserror</a> |
<a href=#sub_mlpeserialize class=toc>mlpeserialize</a> |
<a href=#sub_mlpeunserialize class=toc>mlpeunserialize</a>
]</font>
</div>
<a name=struct_mlpensemble></a><h6 class=pageheader>mlpensemble Class</h6>
<hr width=600 align=left>
<pre class=narration>
Neural networks ensemble
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> mlpensemble {
};
</pre>
<a name=sub_mlpeavgce></a><h6 class=pageheader>mlpeavgce Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average cross-entropy (in bits per element) on the test set

Inputs:
    Ensemble-   ensemble
    XY      -   test set
    NPoints -   test set size

Result:
    CrossEntropy/(NPoints*LN(2)).
    Zero if ensemble solves regression task.
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpeavgce(mlpensemble ensemble, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlpeavgerror></a><h6 class=pageheader>mlpeavgerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average error on the test set

Inputs:
    Ensemble-   ensemble
    XY      -   test set
    NPoints -   test set size

Result:
    Its meaning for regression task is obvious. As for classification task
it means average error when estimating posterior probabilities.
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpeavgerror(mlpensemble ensemble, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlpeavgrelerror></a><h6 class=pageheader>mlpeavgrelerror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Average relative error on the test set

Inputs:
    Ensemble-   ensemble
    XY      -   test set
    NPoints -   test set size

Result:
    Its meaning for regression task is obvious. As for classification task
it means average relative error when estimating posterior probabilities.
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpeavgrelerror(mlpensemble ensemble, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlpecreate0></a><h6 class=pageheader>mlpecreate0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreate0, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreate0(ae_int_t nin, ae_int_t nout, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreate1></a><h6 class=pageheader>mlpecreate1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreate1, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreate1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreate2></a><h6 class=pageheader>mlpecreate2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreate2, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreate2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreateb0></a><h6 class=pageheader>mlpecreateb0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreateB0, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreateb0(ae_int_t nin, ae_int_t nout, <b>double</b> b, <b>double</b> d, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreateb1></a><h6 class=pageheader>mlpecreateb1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreateB1, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreateb1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, <b>double</b> b, <b>double</b> d, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreateb2></a><h6 class=pageheader>mlpecreateb2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreateB2, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreateb2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, <b>double</b> b, <b>double</b> d, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreatec0></a><h6 class=pageheader>mlpecreatec0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreateC0, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreatec0(ae_int_t nin, ae_int_t nout, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreatec1></a><h6 class=pageheader>mlpecreatec1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreateC1, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreatec1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreatec2></a><h6 class=pageheader>mlpecreatec2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreateC2, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreatec2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreatefromnetwork></a><h6 class=pageheader>mlpecreatefromnetwork Function</h6>
<hr width=600 align=left>
<pre class=narration>
Creates ensemble from network. Only network geometry is copied.
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreatefromnetwork(multilayerperceptron network, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreater0></a><h6 class=pageheader>mlpecreater0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreateR0, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreater0(ae_int_t nin, ae_int_t nout, <b>double</b> a, <b>double</b> b, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreater1></a><h6 class=pageheader>mlpecreater1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreateR1, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreater1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, <b>double</b> a, <b>double</b> b, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpecreater2></a><h6 class=pageheader>mlpecreater2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Like MLPCreateR2, but for ensembles.
ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpecreater2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, <b>double</b> a, <b>double</b> b, ae_int_t ensemblesize, mlpensemble &amp;ensemble);
</pre>
<a name=sub_mlpeissoftmax></a><h6 class=pageheader>mlpeissoftmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
Return normalization type (whether ensemble is SOFTMAX-normalized or not).
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> mlpeissoftmax(mlpensemble ensemble);
</pre>
<a name=sub_mlpeprocess></a><h6 class=pageheader>mlpeprocess Function</h6>
<hr width=600 align=left>
<pre class=narration>
Procesing

Inputs:
    Ensemble-   neural networks ensemble
    X       -   input vector,  array[0..NIn-1].
    Y       -   (possibly) preallocated buffer; if size of Y is less than
                NOut, it will be reallocated. If it is large enough, it
                is NOT reallocated, so we can save some time on reallocation.

Outputs:
    Y       -   result. Regression estimate when solving regression  task,
                vector of posterior probabilities for classification task.
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpeprocess(mlpensemble ensemble, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_mlpeprocessi></a><h6 class=pageheader>mlpeprocessi Function</h6>
<hr width=600 align=left>
<pre class=narration>
'interactive'  variant  of  MLPEProcess  for  languages  like Python which
support constructs like &quot;Y = MLPEProcess(LM,X)&quot; and interactive mode of the
interpreter

This function allocates new array on each call,  so  it  is  significantly
slower than its 'non-interactive' counterpart, but it is  more  convenient
when you call it from command line.
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpeprocessi(mlpensemble ensemble, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_mlpeproperties></a><h6 class=pageheader>mlpeproperties Function</h6>
<hr width=600 align=left>
<pre class=narration>
Return ensemble properties (number of inputs and outputs).
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpeproperties(mlpensemble ensemble, ae_int_t &amp;nin, ae_int_t &amp;nout);
</pre>
<a name=sub_mlperandomize></a><h6 class=pageheader>mlperandomize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Randomization of MLP ensemble
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlperandomize(mlpensemble ensemble);
</pre>
<a name=sub_mlperelclserror></a><h6 class=pageheader>mlperelclserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
Relative classification error on the test set

Inputs:
    Ensemble-   ensemble
    XY      -   test set
    NPoints -   test set size

Result:
    percent of incorrectly classified cases.
    Works both for classifier betwork and for regression networks which
are used as classifiers.
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlperelclserror(mlpensemble ensemble, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlpermserror></a><h6 class=pageheader>mlpermserror Function</h6>
<hr width=600 align=left>
<pre class=narration>
RMS error on the test set

Inputs:
    Ensemble-   ensemble
    XY      -   test set
    NPoints -   test set size

Result:
    root mean square error.
    Its meaning for regression task is obvious. As for classification task
RMS error means error when estimating posterior probabilities.
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> mlpermserror(mlpensemble ensemble, real_2d_array xy, ae_int_t npoints);
</pre>
<a name=sub_mlpeserialize></a><h6 class=pageheader>mlpeserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: serialization
These functions serialize a data structure to a C++ string or stream.
* serialization can be freely moved across 32-bit and 64-bit systems,
  and different byte orders. For example, you can serialize a string
  on a SPARC and unserialize it on an x86.
* ALGLIB++ serialization is compatible with serialization in ALGLIB,
  in both directions.
Important properties of s_out:
* it contains alphanumeric characters, dots, underscores, minus signs
* these symbols are grouped into words, which are separated by spaces
  and Windows-style (CR+LF) newlines
ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpeserialize(mlpensemble &amp;obj, std::string &amp;s_out);
<b>void</b> mlpeserialize(mlpensemble &amp;obj, std::ostream &amp;s_out);
</pre>
<a name=sub_mlpeunserialize></a><h6 class=pageheader>mlpeunserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: unserialization
These functions unserialize a data structure from a C++ string or stream.
Important properties of s_in:
* any combination of spaces, tabs, Windows or Unix stype newlines can
  be used as separators, so as to allow flexible reformatting of the
  stream or string from text or XML files.
* But you should not insert separators into the middle of the "words"
  nor you should change case of letters.
ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpeunserialize(<b>const</b> std::string &amp;s_in, mlpensemble &amp;obj);
<b>void</b> mlpeunserialize(<b>const</b> std::istream &amp;s_in, mlpensemble &amp;obj);
</pre>
<a name=unit_mlptrain></a><h4 class=pageheader>8.2.13. mlptrain Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_mlpcvreport class=toc>mlpcvreport</a> |
<a href=#struct_mlpreport class=toc>mlpreport</a> |
<a href=#struct_mlptrainer class=toc>mlptrainer</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_mlpcontinuetraining class=toc>mlpcontinuetraining</a> |
<a href=#sub_mlpcreatetrainer class=toc>mlpcreatetrainer</a> |
<a href=#sub_mlpcreatetrainercls class=toc>mlpcreatetrainercls</a> |
<a href=#sub_mlpebagginglbfgs class=toc>mlpebagginglbfgs</a> |
<a href=#sub_mlpebagginglm class=toc>mlpebagginglm</a> |
<a href=#sub_mlpetraines class=toc>mlpetraines</a> |
<a href=#sub_mlpkfoldcv class=toc>mlpkfoldcv</a> |
<a href=#sub_mlpkfoldcvlbfgs class=toc>mlpkfoldcvlbfgs</a> |
<a href=#sub_mlpkfoldcvlm class=toc>mlpkfoldcvlm</a> |
<a href=#sub_mlpsetalgobatch class=toc>mlpsetalgobatch</a> |
<a href=#sub_mlpsetcond class=toc>mlpsetcond</a> |
<a href=#sub_mlpsetdataset class=toc>mlpsetdataset</a> |
<a href=#sub_mlpsetdecay class=toc>mlpsetdecay</a> |
<a href=#sub_mlpsetsparsedataset class=toc>mlpsetsparsedataset</a> |
<a href=#sub_mlpstarttraining class=toc>mlpstarttraining</a> |
<a href=#sub_mlptrainensemblees class=toc>mlptrainensemblees</a> |
<a href=#sub_mlptraines class=toc>mlptraines</a> |
<a href=#sub_mlptrainlbfgs class=toc>mlptrainlbfgs</a> |
<a href=#sub_mlptrainlm class=toc>mlptrainlm</a> |
<a href=#sub_mlptrainnetwork class=toc>mlptrainnetwork</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_nn_cls2 class=toc>nn_cls2</a></td><td width=15>&nbsp;</td><td>Binary classification problem</td></tr>
<tr align=left valign=top><td><a href=#example_nn_cls3 class=toc>nn_cls3</a></td><td width=15>&nbsp;</td><td>Multiclass classification problem</td></tr>
<tr align=left valign=top><td><a href=#example_nn_crossvalidation class=toc>nn_crossvalidation</a></td><td width=15>&nbsp;</td><td>Cross-validation</td></tr>
<tr align=left valign=top><td><a href=#example_nn_ensembles_es class=toc>nn_ensembles_es</a></td><td width=15>&nbsp;</td><td>Early stopping ensembles</td></tr>
<tr align=left valign=top><td><a href=#example_nn_parallel class=toc>nn_parallel</a></td><td width=15>&nbsp;</td><td>Parallel training</td></tr>
<tr align=left valign=top><td><a href=#example_nn_regr class=toc>nn_regr</a></td><td width=15>&nbsp;</td><td>Regression problem with one output (2&rArr;1)</td></tr>
<tr align=left valign=top><td><a href=#example_nn_regr_n class=toc>nn_regr_n</a></td><td width=15>&nbsp;</td><td>Regression problem with multiple outputs (2&rArr;2)</td></tr>
<tr align=left valign=top><td><a href=#example_nn_trainerobject class=toc>nn_trainerobject</a></td><td width=15>&nbsp;</td><td>Advanced example on trainer object</td></tr>
</table>
</div>
<a name=struct_mlpcvreport></a><h6 class=pageheader>mlpcvreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Cross-validation estimates of generalization error
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> mlpcvreport {
   <b>double</b> relclserror;
   <b>double</b> avgce;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
};
</pre>
<a name=struct_mlpreport></a><h6 class=pageheader>mlpreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Training report:
    * RelCLSError   -   fraction of misclassified cases.
    * AvgCE         -   acerage cross-entropy
    * RMSError      -   root-mean-square error
    * AvgError      -   average error
    * AvgRelError   -   average relative error
    * NGrad         -   number of gradient calculations
    * NHess         -   number of Hessian calculations
    * NCholesky     -   number of Cholesky decompositions

NOTE 1: RelCLSError/AvgCE are zero on regression problems.

NOTE 2: on classification problems  RMSError/AvgError/AvgRelError  contain
        errors in prediction of posterior probabilities
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> mlpreport {
   <b>double</b> relclserror;
   <b>double</b> avgce;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
   ae_int_t ngrad;
   ae_int_t nhess;
   ae_int_t ncholesky;
};
</pre>
<a name=struct_mlptrainer></a><h6 class=pageheader>mlptrainer Class</h6>
<hr width=600 align=left>
<pre class=narration>
Trainer object for neural network.
You should not try to access fields of this object directly -  use  ALGLIB
functions to work with this object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> mlptrainer {
};
</pre>
<a name=sub_mlpcontinuetraining></a><h6 class=pageheader>mlpcontinuetraining Function</h6>
<hr width=600 align=left>
<pre class=narration>
IMPORTANT: this is an &quot;expert&quot; version of the MLPTrain() function.  We  do
           not recommend you to use it unless you are pretty sure that you
           need ability to monitor training progress.

This function performs step-by-step training of the neural  network.  Here
&quot;step-by-step&quot; means that training starts  with  MLPStartTraining()  call,
and then user subsequently calls MLPContinueTraining() to perform one more
iteration of the training.

This  function  performs  one  more  iteration of the training and returns
either True (training continues) or False (training stopped). In case True
was returned, Network weights are updated according to the  current  state
of the optimization progress. In case False was  returned,  no  additional
updates is performed (previous update of  the  network weights moved us to
the final point, and no additional updates is needed).

EXAMPLE:
    &gt;
    &gt; [initialize network and trainer object]
    &gt;
    &gt; MLPStartTraining(Trainer, Network, True)
    &gt; while MLPContinueTraining(Trainer, Network) do
    &gt;     [visualize training progress]
    &gt;

Inputs:
    S           -   trainer object
    Network     -   neural  network  structure,  which  is  used to  store
                    current state of the training process.

Outputs:
    Network     -   weights of the neural network  are  rewritten  by  the
                    current approximation.

NOTE: this method uses sum-of-squares error function for training.

NOTE: it is expected that trainer object settings are NOT  changed  during
      step-by-step training, i.e. no  one  changes  stopping  criteria  or
      training set during training. It is possible and there is no defense
      against  such  actions,  but  algorithm  behavior  in  such cases is
      undefined and can be unpredictable.

NOTE: It  is  expected that Network is the same one which  was  passed  to
      MLPStartTraining() function.  However,  THIS  function  checks  only
      following:
      * that number of network inputs is consistent with trainer object
        settings
      * that number of network outputs/classes is consistent with  trainer
        object settings
      * that number of network weights is the same as number of weights in
        the network passed to MLPStartTraining() function
      Exception is thrown when these conditions are violated.

      It is also expected that you do not change state of the  network  on
      your own - the only party who has right to change network during its
      training is a trainer object. Any attempt to interfere with  trainer
      may lead to unpredictable results.
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> mlpcontinuetraining(mlptrainer s, multilayerperceptron network);
</pre>
<a name=sub_mlpcreatetrainer></a><h6 class=pageheader>mlpcreatetrainer Function</h6>
<hr width=600 align=left>
<pre class=narration>
Creation of the network trainer object for regression networks

Inputs:
    NIn         -   number of inputs, NIn &ge; 1
    NOut        -   number of outputs, NOut &ge; 1

Outputs:
    S           -   neural network trainer object.
                    This structure can be used to train any regression
                    network with NIn inputs and NOut outputs.
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreatetrainer(ae_int_t nin, ae_int_t nout, mlptrainer &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nn_regr class=nav>nn_regr</a> | <a href=#example_nn_regr_n class=nav>nn_regr_n</a> | <a href=#example_nn_trainerobject class=nav>nn_trainerobject</a> | <a href=#example_nn_crossvalidation class=nav>nn_crossvalidation</a> | <a href=#example_nn_ensembles_es class=nav>nn_ensembles_es</a> | <a href=#example_nn_parallel class=nav>nn_parallel</a> ]</p>
<a name=sub_mlpcreatetrainercls></a><h6 class=pageheader>mlpcreatetrainercls Function</h6>
<hr width=600 align=left>
<pre class=narration>
Creation of the network trainer object for classification networks

Inputs:
    NIn         -   number of inputs, NIn &ge; 1
    NClasses    -   number of classes, NClasses &ge; 2

Outputs:
    S           -   neural network trainer object.
                    This structure can be used to train any classification
                    network with NIn inputs and NOut outputs.
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpcreatetrainercls(ae_int_t nin, ae_int_t nclasses, mlptrainer &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nn_cls2 class=nav>nn_cls2</a> | <a href=#example_nn_cls3 class=nav>nn_cls3</a> ]</p>
<a name=sub_mlpebagginglbfgs></a><h6 class=pageheader>mlpebagginglbfgs Function</h6>
<hr width=600 align=left>
<pre class=narration>
Training neural networks ensemble using  bootstrap  aggregating (bagging).
L-BFGS algorithm is used as base training method.

Inputs:
    Ensemble    -   model with initialized geometry
    XY          -   training set
    NPoints     -   training set size
    Decay       -   weight decay coefficient, &ge; 0.001
    Restarts    -   restarts, &gt; 0.
    WStep       -   stopping criterion, same as in MLPTrainLBFGS
    MaxIts      -   stopping criterion, same as in MLPTrainLBFGS

Outputs:
    Ensemble    -   trained model
    Info        -   return code:
                    * -8, if both WStep=0 and MaxIts=0
                    * -2, if there is a point with class number
                          outside of [0..NClasses-1].
                    * -1, if incorrect parameters was passed
                          (NPoints &lt; 0, Restarts &lt; 1).
                    *  2, if task has been solved.
    Rep         -   training report.
    OOBErrors   -   out-of-bag generalization error estimate
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpebagginglbfgs(mlpensemble ensemble, real_2d_array xy, ae_int_t npoints, <b>double</b> decay, ae_int_t restarts, <b>double</b> wstep, ae_int_t maxits, ae_int_t &amp;info, mlpreport &amp;rep, mlpcvreport &amp;ooberrors);
</pre>
<a name=sub_mlpebagginglm></a><h6 class=pageheader>mlpebagginglm Function</h6>
<hr width=600 align=left>
<pre class=narration>
Training neural networks ensemble using  bootstrap  aggregating (bagging).
Modified Levenberg-Marquardt algorithm is used as base training method.

Inputs:
    Ensemble    -   model with initialized geometry
    XY          -   training set
    NPoints     -   training set size
    Decay       -   weight decay coefficient, &ge; 0.001
    Restarts    -   restarts, &gt; 0.

Outputs:
    Ensemble    -   trained model
    Info        -   return code:
                    * -2, if there is a point with class number
                          outside of [0..NClasses-1].
                    * -1, if incorrect parameters was passed
                          (NPoints &lt; 0, Restarts &lt; 1).
                    *  2, if task has been solved.
    Rep         -   training report.
    OOBErrors   -   out-of-bag generalization error estimate
ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpebagginglm(mlpensemble ensemble, real_2d_array xy, ae_int_t npoints, <b>double</b> decay, ae_int_t restarts, ae_int_t &amp;info, mlpreport &amp;rep, mlpcvreport &amp;ooberrors);
</pre>
<a name=sub_mlpetraines></a><h6 class=pageheader>mlpetraines Function</h6>
<hr width=600 align=left>
<pre class=narration>
Training neural networks ensemble using early stopping.

Inputs:
    Ensemble    -   model with initialized geometry
    XY          -   training set
    NPoints     -   training set size
    Decay       -   weight decay coefficient, &ge; 0.001
    Restarts    -   restarts, &gt; 0.

Outputs:
    Ensemble    -   trained model
    Info        -   return code:
                    * -2, if there is a point with class number
                          outside of [0..NClasses-1].
                    * -1, if incorrect parameters was passed
                          (NPoints &lt; 0, Restarts &lt; 1).
                    *  6, if task has been solved.
    Rep         -   training report.
    OOBErrors   -   out-of-bag generalization error estimate
ALGLIB: Copyright 10.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpetraines(mlpensemble ensemble, real_2d_array xy, ae_int_t npoints, <b>double</b> decay, ae_int_t restarts, ae_int_t &amp;info, mlpreport &amp;rep);
</pre>
<a name=sub_mlpkfoldcv></a><h6 class=pageheader>mlpkfoldcv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function estimates generalization error using cross-validation on the
current dataset with current training settings.

Inputs:
    S           -   trainer object
    Network     -   neural network. It must have same number of inputs and
                    output/classes as was specified during creation of the
                    trainer object. Network is not changed  during  cross-
                    validation and is not trained - it  is  used  only  as
                    representative of its architecture. I.e., we  estimate
                    generalization properties of  ARCHITECTURE,  not  some
                    specific network.
    NRestarts   -   number of restarts, &ge; 0:
                    * NRestarts &gt; 0  means  that  for  each cross-validation
                      round   specified  number   of  random  restarts  is
                      performed,  with  best  network  being  chosen after
                      training.
                    * NRestarts=0 is same as NRestarts=1
    FoldsCount  -   number of folds in k-fold cross-validation:
                    * 2 &le; FoldsCount &le; size of dataset
                    * recommended value: 10.
                    * values larger than dataset size will be silently
                      truncated down to dataset size

Outputs:
    Rep         -   structure which contains cross-validation estimates:
                    * Rep.RelCLSError - fraction of misclassified cases.
                    * Rep.AvgCE - acerage cross-entropy
                    * Rep.RMSError - root-mean-square error
                    * Rep.AvgError - average error
                    * Rep.AvgRelError - average relative error

NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
      or subset with only one point  was  given,  zeros  are  returned  as
      estimates.

NOTE: this method performs FoldsCount cross-validation  rounds,  each  one
      with NRestarts random starts.  Thus,  FoldsCount*NRestarts  networks
      are trained in total.

NOTE: Rep.RelCLSError/Rep.AvgCE are zero on regression problems.

NOTE: on classification problems Rep.RMSError/Rep.AvgError/Rep.AvgRelError
      contain errors in prediction of posterior probabilities.
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpkfoldcv(mlptrainer s, multilayerperceptron network, ae_int_t nrestarts, ae_int_t foldscount, mlpreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nn_crossvalidation class=nav>nn_crossvalidation</a> | <a href=#example_nn_parallel class=nav>nn_parallel</a> ]</p>
<a name=sub_mlpkfoldcvlbfgs></a><h6 class=pageheader>mlpkfoldcvlbfgs Function</h6>
<hr width=600 align=left>
<pre class=narration>
Cross-validation estimate of generalization error.

Base algorithm - L-BFGS.

Inputs:
    Network     -   neural network with initialized geometry.   Network is
                    not changed during cross-validation -  it is used only
                    as a representative of its architecture.
    XY          -   training set.
    SSize       -   training set size
    Decay       -   weight  decay, same as in MLPTrainLBFGS
    Restarts    -   number of restarts, &gt; 0.
                    restarts are counted for each partition separately, so
                    total number of restarts will be Restarts*FoldsCount.
    WStep       -   stopping criterion, same as in MLPTrainLBFGS
    MaxIts      -   stopping criterion, same as in MLPTrainLBFGS
    FoldsCount  -   number of folds in k-fold cross-validation,
                    2 &le; FoldsCount &le; SSize.
                    recommended value: 10.

Outputs:
    Info        -   return code, same as in MLPTrainLBFGS
    Rep         -   report, same as in MLPTrainLM/MLPTrainLBFGS
    CVRep       -   generalization error estimates
ALGLIB: Copyright 09.12.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpkfoldcvlbfgs(multilayerperceptron network, real_2d_array xy, ae_int_t npoints, <b>double</b> decay, ae_int_t restarts, <b>double</b> wstep, ae_int_t maxits, ae_int_t foldscount, ae_int_t &amp;info, mlpreport &amp;rep, mlpcvreport &amp;cvrep);
</pre>
<a name=sub_mlpkfoldcvlm></a><h6 class=pageheader>mlpkfoldcvlm Function</h6>
<hr width=600 align=left>
<pre class=narration>
Cross-validation estimate of generalization error.

Base algorithm - Levenberg-Marquardt.

Inputs:
    Network     -   neural network with initialized geometry.   Network is
                    not changed during cross-validation -  it is used only
                    as a representative of its architecture.
    XY          -   training set.
    SSize       -   training set size
    Decay       -   weight  decay, same as in MLPTrainLBFGS
    Restarts    -   number of restarts, &gt; 0.
                    restarts are counted for each partition separately, so
                    total number of restarts will be Restarts*FoldsCount.
    FoldsCount  -   number of folds in k-fold cross-validation,
                    2 &le; FoldsCount &le; SSize.
                    recommended value: 10.

Outputs:
    Info        -   return code, same as in MLPTrainLBFGS
    Rep         -   report, same as in MLPTrainLM/MLPTrainLBFGS
    CVRep       -   generalization error estimates
ALGLIB: Copyright 09.12.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpkfoldcvlm(multilayerperceptron network, real_2d_array xy, ae_int_t npoints, <b>double</b> decay, ae_int_t restarts, ae_int_t foldscount, ae_int_t &amp;info, mlpreport &amp;rep, mlpcvreport &amp;cvrep);
</pre>
<a name=sub_mlpsetalgobatch></a><h6 class=pageheader>mlpsetalgobatch Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets training algorithm: batch training using L-BFGS will be
used.

This algorithm:
* the most robust for small-scale problems, but may be too slow for  large
  scale ones.
* perfoms full pass through the dataset before performing step
* uses conditions specified by MLPSetCond() for stopping
* is default one used by trainer object

Inputs:
    S           -   trainer object
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpsetalgobatch(mlptrainer s);
</pre>
<a name=sub_mlpsetcond></a><h6 class=pageheader>mlpsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping criteria for the optimizer.

Inputs:
    S           -   trainer object
    WStep       -   stopping criterion. Algorithm stops if  step  size  is
                    less than WStep. Recommended value - 0.01.  Zero  step
                    size means stopping after MaxIts iterations.
                    WStep &ge; 0.
    MaxIts      -   stopping   criterion.  Algorithm  stops  after  MaxIts
                    epochs (full passes over entire dataset).  Zero MaxIts
                    means stopping when step is sufficiently small.
                    MaxIts &ge; 0.

NOTE: by default, WStep=0.005 and MaxIts=0 are used. These values are also
      used when MLPSetCond() is called with WStep=0 and MaxIts=0.

NOTE: these stopping criteria are used for all kinds of neural training  -
      from &quot;conventional&quot; networks to early stopping ensembles. When  used
      for &quot;conventional&quot; networks, they are  used  as  the  only  stopping
      criteria. When combined with early stopping, they used as ADDITIONAL
      stopping criteria which can terminate early stopping algorithm.
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpsetcond(mlptrainer s, <b>double</b> wstep, ae_int_t maxits);
</pre>
<a name=sub_mlpsetdataset></a><h6 class=pageheader>mlpsetdataset Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets &quot;current dataset&quot; of the trainer object to  one  passed
by user.

Inputs:
    S           -   trainer object
    XY          -   training  set,  see  below  for  information  on   the
                    training set format. This function checks  correctness
                    of  the  dataset  (no  NANs/INFs,  class  numbers  are
                    correct) and throws exception when  incorrect  dataset
                    is passed.
    NPoints     -   points count, &ge; 0.

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
datasetformat is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpsetdataset(mlptrainer s, real_2d_array xy, ae_int_t npoints);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nn_regr class=nav>nn_regr</a> | <a href=#example_nn_regr_n class=nav>nn_regr_n</a> | <a href=#example_nn_cls2 class=nav>nn_cls2</a> | <a href=#example_nn_cls3 class=nav>nn_cls3</a> | <a href=#example_nn_trainerobject class=nav>nn_trainerobject</a> | <a href=#example_nn_crossvalidation class=nav>nn_crossvalidation</a> | <a href=#example_nn_ensembles_es class=nav>nn_ensembles_es</a> | <a href=#example_nn_parallel class=nav>nn_parallel</a> ]</p>
<a name=sub_mlpsetdecay></a><h6 class=pageheader>mlpsetdecay Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets weight decay coefficient which is used for training.

Inputs:
    S           -   trainer object
    Decay       -   weight  decay  coefficient, &ge; 0.  Weight  decay  term
                    'Decay*||Weights||^2' is added to error  function.  If
                    you don't know what Decay to choose, use 1.0E-3.
                    Weight decay can be set to zero,  in this case network
                    is trained without weight decay.

NOTE: by default network uses some small nonzero value for weight decay.
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpsetdecay(mlptrainer s, <b>double</b> decay);
</pre>
<a name=sub_mlpsetsparsedataset></a><h6 class=pageheader>mlpsetsparsedataset Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets &quot;current dataset&quot; of the trainer object to  one  passed
by user (sparse matrix is used to store dataset).

Inputs:
    S           -   trainer object
    XY          -   training  set,  see  below  for  information  on   the
                    training set format. This function checks  correctness
                    of  the  dataset  (no  NANs/INFs,  class  numbers  are
                    correct) and throws exception when  incorrect  dataset
                    is passed. Any  sparse  storage  format  can be  used:
                    Hash-table, CRS...
    NPoints     -   points count, &ge; 0

DATASET FORMAT:

This  function  uses  two  different  dataset formats - one for regression
networks, another one for classification networks.

For regression networks with NIn inputs and NOut outputs following dataset
format is used:
* dataset is given by NPoints*(NIn+NOut) matrix
* each row corresponds to one example
* first NIn columns are inputs, next NOut columns are outputs

For classification networks with NIn inputs and NClasses clases  following
datasetformat is used:
* dataset is given by NPoints*(NIn+1) matrix
* each row corresponds to one example
* first NIn columns are inputs, last column stores class number (from 0 to
  NClasses-1).
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpsetsparsedataset(mlptrainer s, sparsematrix xy, ae_int_t npoints);
</pre>
<a name=sub_mlpstarttraining></a><h6 class=pageheader>mlpstarttraining Function</h6>
<hr width=600 align=left>
<pre class=narration>
IMPORTANT: this is an &quot;expert&quot; version of the MLPTrain() function.  We  do
           not recommend you to use it unless you are pretty sure that you
           need ability to monitor training progress.

This function performs step-by-step training of the neural  network.  Here
&quot;step-by-step&quot; means that training  starts  with  MLPStartTraining() call,
and then user subsequently calls MLPContinueTraining() to perform one more
iteration of the training.

After call to this function trainer object remembers network and  is ready
to  train  it.  However,  no  training  is  performed  until first call to
MLPContinueTraining() function. Subsequent calls  to MLPContinueTraining()
will advance training progress one iteration further.

EXAMPLE:
    &gt;
    &gt; ...initialize network and trainer object....
    &gt;
    &gt; MLPStartTraining(Trainer, Network, True)
    &gt; while MLPContinueTraining(Trainer, Network) do
    &gt;     ...visualize training progress...
    &gt;

Inputs:
    S           -   trainer object
    Network     -   neural network. It must have same number of inputs and
                    output/classes as was specified during creation of the
                    trainer object.
    RandomStart -   randomize network before training or not:
                    * True  means  that  network  is  randomized  and  its
                      initial state (one which was passed to  the  trainer
                      object) is lost.
                    * False  means  that  training  is  started  from  the
                      current state of the network

Outputs:
    Network     -   neural network which is ready to training (weights are
                    initialized, preprocessor is initialized using current
                    training set)

NOTE: this method uses sum-of-squares error function for training.

NOTE: it is expected that trainer object settings are NOT  changed  during
      step-by-step training, i.e. no  one  changes  stopping  criteria  or
      training set during training. It is possible and there is no defense
      against  such  actions,  but  algorithm  behavior  in  such cases is
      undefined and can be unpredictable.
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlpstarttraining(mlptrainer s, multilayerperceptron network, <b>bool</b> randomstart);
</pre>
<a name=sub_mlptrainensemblees></a><h6 class=pageheader>mlptrainensemblees Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function trains neural network ensemble passed to this function using
current dataset and early stopping training algorithm. Each early stopping
round performs NRestarts  random  restarts  (thus,  EnsembleSize*NRestarts
training rounds is performed in total).

Inputs:
    S           -   trainer object;
    Ensemble    -   neural network ensemble. It must have same  number  of
                    inputs and outputs/classes  as  was  specified  during
                    creation of the trainer object.
    NRestarts   -   number of restarts, &ge; 0:
                    * NRestarts &gt; 0 means that specified  number  of  random
                      restarts are performed during each ES round;
                    * NRestarts=0 is silently replaced by 1.

Outputs:
    Ensemble    -   trained ensemble;
    Rep         -   it contains all type of errors.

NOTE: this training method uses BOTH early stopping and weight decay!  So,
      you should select weight decay before starting training just as  you
      select it before training &quot;conventional&quot; networks.

NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
      or  single-point  dataset  was  passed,  ensemble  is filled by zero
      values.

NOTE: this method uses sum-of-squares error function for training.
ALGLIB: Copyright 22.08.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlptrainensemblees(mlptrainer s, mlpensemble ensemble, ae_int_t nrestarts, mlpreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nn_ensembles_es class=nav>nn_ensembles_es</a> | <a href=#example_nn_parallel class=nav>nn_parallel</a> ]</p>
<a name=sub_mlptraines></a><h6 class=pageheader>mlptraines Function</h6>
<hr width=600 align=left>
<pre class=narration>
Neural network training using early stopping (base algorithm - L-BFGS with
regularization).

Inputs:
    Network     -   neural network with initialized geometry
    TrnXY       -   training set
    TrnSize     -   training set size, TrnSize &gt; 0
    ValXY       -   validation set
    ValSize     -   validation set size, ValSize &gt; 0
    Decay       -   weight decay constant, &ge; 0.001
                    Decay term 'Decay*||Weights||^2' is added to error
                    function.
                    If you don't know what Decay to choose, use 0.001.
    Restarts    -   number of restarts, either:
                    * strictly positive number - algorithm make specified
                      number of restarts from random position.
                    * -1, in which case algorithm makes exactly one run
                      from the initial state of the network (no randomization).
                    If you don't know what Restarts to choose, choose one
                    one the following:
                    * -1 (deterministic start)
                    * +1 (one random restart)
                    * +5 (moderate amount of random restarts)

Outputs:
    Network     -   trained neural network.
    Info        -   return code:
                    * -2, if there is a point with class number
                          outside of [0..NOut-1].
                    * -1, if wrong parameters specified
                          (NPoints &lt; 0, Restarts &lt; 1, ...).
                    *  2, task has been solved, stopping  criterion  met -
                          sufficiently small step size.  Not expected  (we
                          use  EARLY  stopping)  but  possible  and not an
                          error.
                    *  6, task has been solved, stopping  criterion  met -
                          increasing of validation set error.
    Rep         -   training report

NOTE:

Algorithm stops if validation set error increases for  a  long  enough  or
step size is small enought  (there  are  task  where  validation  set  may
decrease for eternity). In any case solution returned corresponds  to  the
minimum of validation set error.
ALGLIB: Copyright 10.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlptraines(multilayerperceptron network, real_2d_array trnxy, ae_int_t trnsize, real_2d_array valxy, ae_int_t valsize, <b>double</b> decay, ae_int_t restarts, ae_int_t &amp;info, mlpreport &amp;rep);
</pre>
<a name=sub_mlptrainlbfgs></a><h6 class=pageheader>mlptrainlbfgs Function</h6>
<hr width=600 align=left>
<pre class=narration>
Neural  network  training  using  L-BFGS  algorithm  with  regularization.
Subroutine  trains  neural  network  with  restarts from random positions.
Algorithm  is  well  suited  for  problems  of  any dimensionality (memory
requirements and step complexity are linear by weights number).

Inputs:
    Network     -   neural network with initialized geometry
    XY          -   training set
    NPoints     -   training set size
    Decay       -   weight decay constant, &ge; 0.001
                    Decay term 'Decay*||Weights||^2' is added to error
                    function.
                    If you don't know what Decay to choose, use 0.001.
    Restarts    -   number of restarts from random position, &gt; 0.
                    If you don't know what Restarts to choose, use 2.
    WStep       -   stopping criterion. Algorithm stops if  step  size  is
                    less than WStep. Recommended value - 0.01.  Zero  step
                    size means stopping after MaxIts iterations.
    MaxIts      -   stopping   criterion.  Algorithm  stops  after  MaxIts
                    iterations (NOT gradient  calculations).  Zero  MaxIts
                    means stopping when step is sufficiently small.

Outputs:
    Network     -   trained neural network.
    Info        -   return code:
                    * -8, if both WStep=0 and MaxIts=0
                    * -2, if there is a point with class number
                          outside of [0..NOut-1].
                    * -1, if wrong parameters specified
                          (NPoints &lt; 0, Restarts &lt; 1).
                    *  2, if task has been solved.
    Rep         -   training report
ALGLIB: Copyright 09.12.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlptrainlbfgs(multilayerperceptron network, real_2d_array xy, ae_int_t npoints, <b>double</b> decay, ae_int_t restarts, <b>double</b> wstep, ae_int_t maxits, ae_int_t &amp;info, mlpreport &amp;rep);
</pre>
<a name=sub_mlptrainlm></a><h6 class=pageheader>mlptrainlm Function</h6>
<hr width=600 align=left>
<pre class=narration>
Neural network training  using  modified  Levenberg-Marquardt  with  exact
Hessian calculation and regularization. Subroutine trains  neural  network
with restarts from random positions. Algorithm is well  suited  for  small
and medium scale problems (hundreds of weights).

Inputs:
    Network     -   neural network with initialized geometry
    XY          -   training set
    NPoints     -   training set size
    Decay       -   weight decay constant, &ge; 0.001
                    Decay term 'Decay*||Weights||^2' is added to error
                    function.
                    If you don't know what Decay to choose, use 0.001.
    Restarts    -   number of restarts from random position, &gt; 0.
                    If you don't know what Restarts to choose, use 2.

Outputs:
    Network     -   trained neural network.
    Info        -   return code:
                    * -9, if internal matrix inverse subroutine failed
                    * -2, if there is a point with class number
                          outside of [0..NOut-1].
                    * -1, if wrong parameters specified
                          (NPoints &lt; 0, Restarts &lt; 1).
                    *  2, if task has been solved.
    Rep         -   training report
ALGLIB: Copyright 10.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlptrainlm(multilayerperceptron network, real_2d_array xy, ae_int_t npoints, <b>double</b> decay, ae_int_t restarts, ae_int_t &amp;info, mlpreport &amp;rep);
</pre>
<a name=sub_mlptrainnetwork></a><h6 class=pageheader>mlptrainnetwork Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function trains neural network passed to this function, using current
dataset (one which was passed to MLPSetDataset() or MLPSetSparseDataset())
and current training settings. Training  from  NRestarts  random  starting
positions is performed, best network is chosen.

Training is performed using current training algorithm.

Inputs:
    S           -   trainer object
    Network     -   neural network. It must have same number of inputs and
                    output/classes as was specified during creation of the
                    trainer object.
    NRestarts   -   number of restarts, &ge; 0:
                    * NRestarts &gt; 0 means that specified  number  of  random
                      restarts are performed, best network is chosen after
                      training
                    * NRestarts=0 means that current state of the  network
                      is used for training.

Outputs:
    Network     -   trained network

NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
      network  is  filled  by zero  values.  Same  behavior  for functions
      MLPStartTraining and MLPContinueTraining.

NOTE: this method uses sum-of-squares error function for training.
ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mlptrainnetwork(mlptrainer s, multilayerperceptron network, ae_int_t nrestarts, mlpreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_nn_regr class=nav>nn_regr</a> | <a href=#example_nn_regr_n class=nav>nn_regr_n</a> | <a href=#example_nn_cls2 class=nav>nn_cls2</a> | <a href=#example_nn_cls3 class=nav>nn_cls3</a> | <a href=#example_nn_trainerobject class=nav>nn_trainerobject</a> | <a href=#example_nn_parallel class=nav>nn_parallel</a> ]</p>
<a name=example_nn_cls2></a><h6 class=pageheader>nn_cls2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Suppose that we want to classify numbers as positive (<b>class</b> 0) and negative</font>
<font color=navy>// (<b>class</b> 1). We have training set which includes several strictly positive</font>
<font color=navy>// or negative numbers - and zero.</font>
<font color=navy>//</font>
<font color=navy>// The problem is that we are not sure how to classify zero, so from time to</font>
<font color=navy>// time we mark it as positive or negative (with equal probability). Other</font>
<font color=navy>// numbers are marked in pure deterministic setting. How will neural network</font>
<font color=navy>// cope with such classification task?</font>
<font color=navy>//</font>
<font color=navy>// NOTE: we use network with excessive amount of neurons, which guarantees</font>
<font color=navy>//       almost exact reproduction of the training set. Generalization ability</font>
<font color=navy>//       of such network is rather low, but we are not concerned with such</font>
<font color=navy>//       questions in this basic demo.</font>
   mlptrainer trn;
   multilayerperceptron network;
   mlpreport rep;
   real_1d_array x = <font color=blue><b>&quot;[0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
<font color=navy>// Training set. One row corresponds to one record [A &rArr; <b>class</b>(A)].</font>
<font color=navy>//</font>
<font color=navy>// Classes are denoted by numbers from 0 to 1, where 0 corresponds to positive</font>
<font color=navy>// numbers and 1 to negative numbers.</font>
<font color=navy>//</font>
<font color=navy>// [ +1  0]</font>
<font color=navy>// [ +2  0]</font>
<font color=navy>// [ -1  1]</font>
<font color=navy>// [ -2  1]</font>
<font color=navy>// [  0  0]   !! sometimes we classify 0 as positive, sometimes as negative</font>
<font color=navy>// [  0  1]   !!</font>
   real_2d_array xy = <font color=blue><b>&quot;[[+1,0],[+2,0],[-1,1],[-2,1],[0,0],[0,1]]&quot;</b></font>;
<font color=navy>//</font>
<font color=navy>// When we solve classification problems, everything is slightly different from</font>
<font color=navy>// the regression ones:</font>
<font color=navy>//</font>
<font color=navy>// 1. Network is created. Because we solve classification problem, we use</font>
<font color=navy>//    mlpcreatec1() function instead of mlpcreate1(). This function creates</font>
<font color=navy>//    classifier network with SOFTMAX-normalized outputs. This network returns</font>
<font color=navy>//    vector of <b>class</b> membership probabilities which are normalized to be</font>
<font color=navy>//    non-negative and sum to 1.0</font>
<font color=navy>//</font>
<font color=navy>// 2. We use mlpcreatetrainercls() function instead of mlpcreatetrainer() to</font>
<font color=navy>//    create trainer object. Trainer object process dataset and neural network</font>
<font color=navy>//    slightly differently to account <b>for</b> specifics of the classification</font>
<font color=navy>//    problems.</font>
<font color=navy>//</font>
<font color=navy>// 3. Dataset is attached to trainer object. Note that dataset format is slightly</font>
<font color=navy>//    different from one used <b>for</b> regression.</font>
   mlpcreatetrainercls(1, 2, trn);
   mlpcreatec1(1, 5, 2, network);
   mlpsetdataset(trn, xy, 6);
<font color=navy>// Network is trained with 5 restarts from random positions</font>
   mlptrainnetwork(trn, network, 5, rep);
<font color=navy>// Test our neural network on strictly positive and strictly negative numbers.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT! Classifier network returns <b>class</b> membership probabilities instead</font>
<font color=navy>// of <b>class</b> indexes. Network returns two values (probabilities) instead of one</font>
<font color=navy>// (<b>class</b> index).</font>
<font color=navy>//</font>
<font color=navy>// Thus, <b>for</b> +1 we expect to get [P0,P1] = [1,0], where P0 is probability that</font>
<font color=navy>// number is positive (belongs to <b>class</b> 0), and P1 is probability that number</font>
<font color=navy>// is negative (belongs to <b>class</b> 1).</font>
<font color=navy>//</font>
<font color=navy>// For -1 we expect to get [P0,P1] = [0,1]</font>
<font color=navy>//</font>
<font color=navy>// Following properties are guaranteed by network architecture:</font>
<font color=navy>// * P0 &ge; 0, P1 &ge; 0   non-negativity</font>
<font color=navy>// * P0+P1=1        normalization</font>
   x = <font color=blue><b>&quot;[1]&quot;</b></font>;
   mlpprocess(network, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.000,0.000]</font>
   x = <font color=blue><b>&quot;[-1]&quot;</b></font>;
   mlpprocess(network, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(1).c_str()); <font color=navy>// EXPECTED: [0.000,1.000]</font>
<font color=navy>//</font>
<font color=navy>// But what our network will <b>return</b> <b>for</b> 0, which is between classes 0 and 1?</font>
<font color=navy>//</font>
<font color=navy>// In our dataset it has two different marks assigned (<b>class</b> 0 AND <b>class</b> 1).</font>
<font color=navy>// So network will <b>return</b> something average between <b>class</b> 0 and <b>class</b> 1:</font>
<font color=navy>//     0 &rArr; [0.5, 0.5]</font>
   x = <font color=blue><b>&quot;[0]&quot;</b></font>;
   mlpprocess(network, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(1).c_str()); <font color=navy>// EXPECTED: [0.500,0.500]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_nn_cls3></a><h6 class=pageheader>nn_cls3 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Suppose that we want to classify numbers as positive (<b>class</b> 0) and negative</font>
<font color=navy>// (<b>class</b> 1). We also have one more <b>class</b> <b>for</b> zero (<b>class</b> 2).</font>
<font color=navy>//</font>
<font color=navy>// NOTE: we use network with excessive amount of neurons, which guarantees</font>
<font color=navy>//       almost exact reproduction of the training set. Generalization ability</font>
<font color=navy>//       of such network is rather low, but we are not concerned with such</font>
<font color=navy>//       questions in this basic demo.</font>
   mlptrainer trn;
   multilayerperceptron network;
   mlpreport rep;
   real_1d_array x = <font color=blue><b>&quot;[0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0,0,0]&quot;</b></font>;
<font color=navy>// Training set. One row corresponds to one record [A &rArr; <b>class</b>(A)].</font>
<font color=navy>//</font>
<font color=navy>// Classes are denoted by numbers from 0 to 2, where 0 corresponds to positive</font>
<font color=navy>// numbers, 1 to negative numbers, 2 to zero</font>
<font color=navy>//</font>
<font color=navy>// [ +1  0]</font>
<font color=navy>// [ +2  0]</font>
<font color=navy>// [ -1  1]</font>
<font color=navy>// [ -2  1]</font>
<font color=navy>// [  0  2]</font>
   real_2d_array xy = <font color=blue><b>&quot;[[+1,0],[+2,0],[-1,1],[-2,1],[0,2]]&quot;</b></font>;
<font color=navy>//</font>
<font color=navy>// When we solve classification problems, everything is slightly different from</font>
<font color=navy>// the regression ones:</font>
<font color=navy>//</font>
<font color=navy>// 1. Network is created. Because we solve classification problem, we use</font>
<font color=navy>//    mlpcreatec1() function instead of mlpcreate1(). This function creates</font>
<font color=navy>//    classifier network with SOFTMAX-normalized outputs. This network returns</font>
<font color=navy>//    vector of <b>class</b> membership probabilities which are normalized to be</font>
<font color=navy>//    non-negative and sum to 1.0</font>
<font color=navy>//</font>
<font color=navy>// 2. We use mlpcreatetrainercls() function instead of mlpcreatetrainer() to</font>
<font color=navy>//    create trainer object. Trainer object process dataset and neural network</font>
<font color=navy>//    slightly differently to account <b>for</b> specifics of the classification</font>
<font color=navy>//    problems.</font>
<font color=navy>//</font>
<font color=navy>// 3. Dataset is attached to trainer object. Note that dataset format is slightly</font>
<font color=navy>//    different from one used <b>for</b> regression.</font>
   mlpcreatetrainercls(1, 3, trn);
   mlpcreatec1(1, 5, 3, network);
   mlpsetdataset(trn, xy, 5);
<font color=navy>// Network is trained with 5 restarts from random positions</font>
   mlptrainnetwork(trn, network, 5, rep);
<font color=navy>// Test our neural network on strictly positive and strictly negative numbers.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT! Classifier network returns <b>class</b> membership probabilities instead</font>
<font color=navy>// of <b>class</b> indexes. Network returns three values (probabilities) instead of one</font>
<font color=navy>// (<b>class</b> index).</font>
<font color=navy>//</font>
<font color=navy>// Thus, <b>for</b> +1 we expect to get [P0,P1,P2] = [1,0,0],</font>
<font color=navy>// <b>for</b> -1 we expect to get [P0,P1,P2] = [0,1,0],</font>
<font color=navy>// and <b>for</b> 0 we will get [P0,P1,P2] = [0,0,1].</font>
<font color=navy>//</font>
<font color=navy>// Following properties are guaranteed by network architecture:</font>
<font color=navy>// * P0 &ge; 0, P1 &ge; 0, P2 &ge; 0    non-negativity</font>
<font color=navy>// * P0+P1+P2=1             normalization</font>
   x = <font color=blue><b>&quot;[1]&quot;</b></font>;
   mlpprocess(network, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.000,0.000,0.000]</font>
   x = <font color=blue><b>&quot;[-1]&quot;</b></font>;
   mlpprocess(network, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(1).c_str()); <font color=navy>// EXPECTED: [0.000,1.000,0.000]</font>
   x = <font color=blue><b>&quot;[0]&quot;</b></font>;
   mlpprocess(network, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(1).c_str()); <font color=navy>// EXPECTED: [0.000,0.000,1.000]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_nn_crossvalidation></a><h6 class=pageheader>nn_crossvalidation Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example shows how to perform cross-validation with ALGLIB</font>
   mlptrainer trn;
   multilayerperceptron network;
   mlpreport rep;
<font color=navy>// Training set: f(x)=1/(x^2+1)</font>
<font color=navy>// One row corresponds to one record [x,f(x)]</font>
   real_2d_array xy = <font color=blue><b>&quot;[[-2.0,0.2],[-1.6,0.3],[-1.3,0.4],[-1,0.5],[-0.6,0.7],[-0.3,0.9],[0,1],[2.0,0.2],[1.6,0.3],[1.3,0.4],[1,0.5],[0.6,0.7],[0.3,0.9]]&quot;</b></font>;
<font color=navy>// Trainer object is created.</font>
<font color=navy>// Dataset is attached to trainer object.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: it is not good idea to perform cross-validation on sample</font>
<font color=navy>//       as small as ours (13 examples). It is done <b>for</b> demonstration</font>
<font color=navy>//       purposes only. Generalization error estimates won't be</font>
<font color=navy>//       precise enough <b>for</b> practical purposes.</font>
   mlpcreatetrainer(1, 1, trn);
   mlpsetdataset(trn, xy, 13);
<font color=navy>// The key property of the cross-validation is that it estimates</font>
<font color=navy>// generalization properties of neural ARCHITECTURE. It does NOT</font>
<font color=navy>// estimates generalization error of some specific network which</font>
<font color=navy>// is passed to the k-fold CV routine.</font>
<font color=navy>//</font>
<font color=navy>// In our example we create 1x4x1 neural network and pass it to</font>
<font color=navy>// CV routine without training it. Original state of the network</font>
<font color=navy>// is not used <b>for</b> cross-validation - each round is restarted from</font>
<font color=navy>// random initial state. Only geometry of network matters.</font>
<font color=navy>//</font>
<font color=navy>// We perform 5 restarts from different random positions <b>for</b> each</font>
<font color=navy>// of the 10 cross-validation rounds.</font>
   mlpcreate1(1, 4, 1, network);
   mlpkfoldcv(trn, network, 5, 10, rep);
<font color=navy>// Cross-validation routine stores estimates of the generalization</font>
<font color=navy>// error to MLP report structure. You may examine its fields and</font>
<font color=navy>// see estimates of different errors (RMS, CE, Avg).</font>
<font color=navy>//</font>
<font color=navy>// Because cross-validation is non-deterministic, in our manual we</font>
<font color=navy>// can not say what values will be stored to rep after call to</font>
<font color=navy>// mlpkfoldcv(). Every CV round will <b>return</b> slightly different</font>
<font color=navy>// estimates.</font>
   <b>return</b> 0;
}
</pre>
<a name=example_nn_ensembles_es></a><h6 class=pageheader>nn_ensembles_es Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example shows how to train early stopping ensebles.</font>
   mlptrainer trn;
   mlpensemble ensemble;
   mlpreport rep;
<font color=navy>// Training set: f(x)=1/(x^2+1)</font>
<font color=navy>// One row corresponds to one record [x,f(x)]</font>
   real_2d_array xy = <font color=blue><b>&quot;[[-2.0,0.2],[-1.6,0.3],[-1.3,0.4],[-1,0.5],[-0.6,0.7],[-0.3,0.9],[0,1],[2.0,0.2],[1.6,0.3],[1.3,0.4],[1,0.5],[0.6,0.7],[0.3,0.9]]&quot;</b></font>;
<font color=navy>// Trainer object is created.</font>
<font color=navy>// Dataset is attached to trainer object.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: it is not good idea to use early stopping ensemble on sample</font>
<font color=navy>//       as small as ours (13 examples). It is done <b>for</b> demonstration</font>
<font color=navy>//       purposes only. Ensemble training algorithm won't find good</font>
<font color=navy>//       solution on such small sample.</font>
   mlpcreatetrainer(1, 1, trn);
   mlpsetdataset(trn, xy, 13);
<font color=navy>// Ensemble is created and trained. Each of 50 network is trained</font>
<font color=navy>// with 5 restarts.</font>
   mlpecreate1(1, 4, 1, 50, ensemble);
   mlptrainensemblees(trn, ensemble, 5, rep);
   <b>return</b> 0;
}
</pre>
<a name=example_nn_parallel></a><h6 class=pageheader>nn_parallel Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example shows how to use parallel functionality of ALGLIB.</font>
<font color=navy>// We generate simple 1-dimensional regression problem and show how</font>
<font color=navy>// to use parallel training, parallel cross-validation, parallel</font>
<font color=navy>// training of neural ensembles.</font>
<font color=navy>//</font>
<font color=navy>// We assume that you already know how to use ALGLIB in serial mode</font>
<font color=navy>// and concentrate on its parallel capabilities.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: it is not good idea to use parallel features on sample as small</font>
<font color=navy>//       as ours (13 examples). It is done only <b>for</b> demonstration purposes.</font>
   mlptrainer trn;
   multilayerperceptron network;
   mlpensemble ensemble;
   mlpreport rep;
   real_2d_array xy = <font color=blue><b>&quot;[[-2.0,0.2],[-1.6,0.3],[-1.3,0.4],[-1,0.5],[-0.6,0.7],[-0.3,0.9],[0,1],[2.0,0.2],[1.6,0.3],[1.3,0.4],[1,0.5],[0.6,0.7],[0.3,0.9]]&quot;</b></font>;
   mlpcreatetrainer(1, 1, trn);
   mlpsetdataset(trn, xy, 13);
   mlpcreate1(1, 4, 1, network);
   mlpecreate1(1, 4, 1, 50, ensemble);
<font color=navy>// Below we demonstrate how to perform:</font>
<font color=navy>// * parallel training of individual networks</font>
<font color=navy>// * parallel cross-validation</font>
<font color=navy>// * parallel training of neural ensembles</font>
<font color=navy>//</font>
<font color=navy>// In order to use multithreading, you have to:</font>
<font color=navy>// 1) Install SMP edition of ALGLIB.</font>
<font color=navy>// 2) This step is specific <b>for</b> C++ users: you should activate OS-specific</font>
<font color=navy>//    capabilities of ALGLIB by defining AE_OS=AE_POSIX (<b>for</b> *nix systems)</font>
<font color=navy>//    or AE_OS=AE_WINDOWS (<b>for</b> Windows systems).</font>
<font color=navy>//    C# users <b>do</b> not have to perform this step because C# programs are</font>
<font color=navy>//    portable across different systems without OS-specific tuning.</font>
<font color=navy>// 3) Tell ALGLIB that you want it to use multithreading by means of</font>
<font color=navy>//    setnworkers() call:</font>
<font color=navy>//          * setnworkers(0)  = use all cores</font>
<font color=navy>//          * setnworkers(-1) = leave one core unused</font>
<font color=navy>//          * setnworkers(-2) = leave two cores unused</font>
<font color=navy>//          * setnworkers(+2) = use 2 cores (even <b>if</b> you have more)</font>
<font color=navy>//    During runtime ALGLIB will automatically determine whether it is</font>
<font color=navy>//    feasible to start worker threads and split your task between cores.</font>
   setnworkers(+2);
<font color=navy>// First, we perform parallel training of individual network with 5</font>
<font color=navy>// restarts from random positions. These 5 rounds of  training  are</font>
<font color=navy>// executed in parallel manner,  with  best  network  chosen  after</font>
<font color=navy>// training.</font>
<font color=navy>//</font>
<font color=navy>// ALGLIB can use additional way to speed up computations -  divide</font>
<font color=navy>// dataset   into   smaller   subsets   and   process these subsets</font>
<font color=navy>// simultaneously. It allows us  to  efficiently  parallelize  even</font>
<font color=navy>// single training round. This operation is performed automatically</font>
<font color=navy>// <b>for</b> large datasets, but our toy dataset is too small.</font>
   mlptrainnetwork(trn, network, 5, rep);
<font color=navy>// Then, we perform parallel 10-fold cross-validation, with 5 random</font>
<font color=navy>// restarts per each CV round. I.e., 5*10=50  networks  are trained</font>
<font color=navy>// in total. All these operations can be parallelized.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: again, ALGLIB can parallelize  calculation   of   gradient</font>
<font color=navy>//       over entire dataset - but our dataset is too small.</font>
   mlpkfoldcv(trn, network, 5, 10, rep);
<font color=navy>// Finally, we train early stopping ensemble of 50 neural networks,</font>
<font color=navy>// each  of them is trained with 5 random restarts. I.e.,  5*50=250</font>
<font color=navy>// networks aretrained in total.</font>
   mlptrainensemblees(trn, ensemble, 5, rep);
   <b>return</b> 0;
}
</pre>
<a name=example_nn_regr></a><h6 class=pageheader>nn_regr Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// The very simple example on neural network: network is trained to reproduce</font>
<font color=navy>// small 2x2 multiplication table.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: we use network with excessive amount of neurons, which guarantees</font>
<font color=navy>//       almost exact reproduction of the training set. Generalization ability</font>
<font color=navy>//       of such network is rather low, but we are not concerned with such</font>
<font color=navy>//       questions in this basic demo.</font>
   mlptrainer trn;
   multilayerperceptron network;
   mlpreport rep;
<font color=navy>// Training set:</font>
<font color=navy>// * one row corresponds to one record A*B=C in the multiplication table</font>
<font color=navy>// * first two columns store A and B, last column stores C</font>
<font color=navy>//</font>
<font color=navy>// [1 * 1 = 1]</font>
<font color=navy>// [1 * 2 = 2]</font>
<font color=navy>// [2 * 1 = 2]</font>
<font color=navy>// [2 * 2 = 4]</font>
   real_2d_array xy = <font color=blue><b>&quot;[[1,1,1],[1,2,2],[2,1,2],[2,2,4]]&quot;</b></font>;
<font color=navy>// Network is created.</font>
<font color=navy>// Trainer object is created.</font>
<font color=navy>// Dataset is attached to trainer object.</font>
   mlpcreatetrainer(2, 1, trn);
   mlpcreate1(2, 5, 1, network);
   mlpsetdataset(trn, xy, 4);
<font color=navy>// Network is trained with 5 restarts from random positions</font>
   mlptrainnetwork(trn, network, 5, rep);
<font color=navy>// 2*2=?</font>
   real_1d_array x = <font color=blue><b>&quot;[2,2]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0]&quot;</b></font>;
   mlpprocess(network, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(1).c_str()); <font color=navy>// EXPECTED: [4.000]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_nn_regr_n></a><h6 class=pageheader>nn_regr_n Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Network with 2 inputs and 2 outputs is trained to reproduce vector function:</font>
<font color=navy>//     (x0,x1) &rArr; (x0+x1, x0*x1)</font>
<font color=navy>//</font>
<font color=navy>// Informally speaking, we want neural network to simultaneously calculate</font>
<font color=navy>// both sum of two numbers and their product.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: we use network with excessive amount of neurons, which guarantees</font>
<font color=navy>//       almost exact reproduction of the training set. Generalization ability</font>
<font color=navy>//       of such network is rather low, but we are not concerned with such</font>
<font color=navy>//       questions in this basic demo.</font>
   mlptrainer trn;
   multilayerperceptron network;
   mlpreport rep;
<font color=navy>// Training set. One row corresponds to one record [A,B,A+B,A*B].</font>
<font color=navy>//</font>
<font color=navy>// [ 1   1  1+1  1*1 ]</font>
<font color=navy>// [ 1   2  1+2  1*2 ]</font>
<font color=navy>// [ 2   1  2+1  2*1 ]</font>
<font color=navy>// [ 2   2  2+2  2*2 ]</font>
   real_2d_array xy = <font color=blue><b>&quot;[[1,1,2,1],[1,2,3,2],[2,1,3,2],[2,2,4,4]]&quot;</b></font>;
<font color=navy>// Network is created.</font>
<font color=navy>// Trainer object is created.</font>
<font color=navy>// Dataset is attached to trainer object.</font>
   mlpcreatetrainer(2, 2, trn);
   mlpcreate1(2, 5, 2, network);
   mlpsetdataset(trn, xy, 4);
<font color=navy>// Network is trained with 5 restarts from random positions</font>
   mlptrainnetwork(trn, network, 5, rep);
<font color=navy>// 2+1=?</font>
<font color=navy>// 2*1=?</font>
   real_1d_array x = <font color=blue><b>&quot;[2,1]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   mlpprocess(network, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(1).c_str()); <font color=navy>// EXPECTED: [3.000,2.000]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_nn_trainerobject></a><h6 class=pageheader>nn_trainerobject Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Trainer object is used to train network. It stores dataset, training settings,</font>
<font color=navy>// and other information which is NOT part of neural network. You should use</font>
<font color=navy>// trainer object as follows:</font>
<font color=navy>// (1) you create trainer object and specify task type (classification/regression)</font>
<font color=navy>//     and number of inputs/outputs</font>
<font color=navy>// (2) you add dataset to the trainer object</font>
<font color=navy>// (3) you may change training settings (stopping criteria or weight decay)</font>
<font color=navy>// (4) finally, you may train one or more networks</font>
<font color=navy>//</font>
<font color=navy>// You may interleave stages 2...4 and repeat them many times. Trainer object</font>
<font color=navy>// remembers its internal state and can be used several times after its creation</font>
<font color=navy>// and initialization.</font>
   mlptrainer trn;
<font color=navy>// Stage 1: object creation.</font>
<font color=navy>//</font>
<font color=navy>// We have to specify number of inputs and outputs. Trainer object can be used</font>
<font color=navy>// only <b>for</b> problems with same number of inputs/outputs as was specified during</font>
<font color=navy>// its creation.</font>
<font color=navy>//</font>
<font color=navy>// In case you want to train SOFTMAX-normalized network which solves classification</font>
<font color=navy>// problems,  you  must  use  another  function  to  create  trainer  object:</font>
<font color=navy>// mlpcreatetrainercls().</font>
<font color=navy>//</font>
<font color=navy>// Below we create trainer object which can be used to train regression networks</font>
<font color=navy>// with 2 inputs and 1 output.</font>
   mlpcreatetrainer(2, 1, trn);
<font color=navy>// Stage 2: specification of the training set</font>
<font color=navy>//</font>
<font color=navy>// By default trainer object stores empty dataset. So to solve your non-empty problem</font>
<font color=navy>// you have to set dataset by passing to trainer dense or sparse matrix.</font>
<font color=navy>//</font>
<font color=navy>// One row of the matrix corresponds to one record A*B=C in the multiplication table.</font>
<font color=navy>// First two columns store A and B, last column stores C</font>
<font color=navy>//</font>
<font color=navy>//     [1 * 1 = 1]   [ 1 1 1 ]</font>
<font color=navy>//     [1 * 2 = 2]   [ 1 2 2 ]</font>
<font color=navy>//     [2 * 1 = 2] = [ 2 1 2 ]</font>
<font color=navy>//     [2 * 2 = 4]   [ 2 2 4 ]</font>
   real_2d_array xy = <font color=blue><b>&quot;[[1,1,1],[1,2,2],[2,1,2],[2,2,4]]&quot;</b></font>;
   mlpsetdataset(trn, xy, 4);
<font color=navy>// Stage 3: modification of the training parameters.</font>
<font color=navy>//</font>
<font color=navy>// You may modify parameters like weights decay or stopping criteria:</font>
<font color=navy>// * we set moderate weight decay</font>
<font color=navy>// * we choose iterations limit as stopping condition (another condition - step size -</font>
<font color=navy>//   is zero, which means than this condition is not active)</font>
   <b>double</b> wstep = 0.000;
   ae_int_t maxits = 100;
   mlpsetdecay(trn, 0.01);
   mlpsetcond(trn, wstep, maxits);
<font color=navy>// Stage 4: training.</font>
<font color=navy>//</font>
<font color=navy>// We will train several networks with different architecture using same trainer object.</font>
<font color=navy>// We may change training parameters or even dataset, so different networks are trained</font>
<font color=navy>// differently. But in this simple example we will train all networks with same settings.</font>
<font color=navy>//</font>
<font color=navy>// We create and train three networks:</font>
<font color=navy>// * network 1 has 2x1 architecture     (2 inputs, no hidden neurons, 1 output)</font>
<font color=navy>// * network 2 has 2x5x1 architecture   (2 inputs, 5 hidden neurons, 1 output)</font>
<font color=navy>// * network 3 has 2x5x5x1 architecture (2 inputs, two hidden layers, 1 output)</font>
<font color=navy>//</font>
<font color=navy>// NOTE: these networks solve regression problems. For classification problems you</font>
<font color=navy>//       should use mlpcreatec0/c1/c2 to create neural networks which have SOFTMAX-</font>
<font color=navy>//       normalized outputs.</font>
   multilayerperceptron net1;
   multilayerperceptron net2;
   multilayerperceptron net3;
   mlpreport rep;

   mlpcreate0(2, 1, net1);
   mlpcreate1(2, 5, 1, net2);
   mlpcreate2(2, 5, 5, 1, net3);

   mlptrainnetwork(trn, net1, 5, rep);
   mlptrainnetwork(trn, net2, 5, rep);
   mlptrainnetwork(trn, net3, 5, rep);
   <b>return</b> 0;
}
</pre>
<a name=unit_pca></a><h4 class=pageheader>8.2.14. pca Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_pcabuildbasis class=toc>pcabuildbasis</a> |
<a href=#sub_pcatruncatedsubspace class=toc>pcatruncatedsubspace</a> |
<a href=#sub_pcatruncatedsubspacesparse class=toc>pcatruncatedsubspacesparse</a>
]</font>
</div>
<a name=sub_pcabuildbasis></a><h6 class=pageheader>pcabuildbasis Function</h6>
<hr width=600 align=left>
<pre class=narration>
Principal components analysis

This function builds orthogonal basis  where  first  axis  corresponds  to
direction with maximum variance, second axis  maximizes  variance  in  the
subspace orthogonal to first axis and so on.

This function builds FULL basis, i.e. returns N vectors  corresponding  to
ALL directions, no matter how informative. If you need  just a  few  (say,
10 or 50) of the most important directions, you may find it faster to  use
one of the reduced versions:
* pcatruncatedsubspace() - for subspace iteration based method

It should be noted that, unlike LDA, PCA does not use class labels.

Inputs:
    X           -   dataset, array[0..NPoints-1,0..NVars-1].
                    matrix contains ONLY INDEPENDENT VARIABLES.
    NPoints     -   dataset size, NPoints &ge; 0
    NVars       -   number of independent variables, NVars &ge; 1

Outputs:
    Info        -   return code:
                    * -4, if SVD subroutine haven't converged
                    * -1, if wrong parameters has been passed (NPoints &lt; 0,
                          NVars &lt; 1)
                    *  1, if task is solved
    S2          -   array[0..NVars-1]. variance values corresponding
                    to basis vectors.
    V           -   array[0..NVars-1,0..NVars-1]
                    matrix, whose columns store basis vectors.
ALGLIB: Copyright 25.08.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pcabuildbasis(real_2d_array x, ae_int_t npoints, ae_int_t nvars, ae_int_t &amp;info, real_1d_array &amp;s2, real_2d_array &amp;v);
</pre>
<a name=sub_pcatruncatedsubspace></a><h6 class=pageheader>pcatruncatedsubspace Function</h6>
<hr width=600 align=left>
<pre class=narration>
Principal components analysis

This function performs truncated PCA, i.e. returns just a few most important
directions.

Internally it uses iterative eigensolver which is very efficient when only
a minor fraction of full basis is required. Thus, if you need full  basis,
it is better to use pcabuildbasis() function.

It should be noted that, unlike LDA, PCA does not use class labels.

Inputs:
    X           -   dataset, array[0..NPoints-1,0..NVars-1].
                    matrix contains ONLY INDEPENDENT VARIABLES.
    NPoints     -   dataset size, NPoints &ge; 0
    NVars       -   number of independent variables, NVars &ge; 1
    NNeeded     -   number of requested components, in [1,NVars] range;
                    this function is efficient only for NNeeded &lt;&lt; NVars.
    Eps         -   desired  precision  of  vectors  returned;  underlying
                    solver will stop iterations as soon as absolute  error
                    in corresponding singular values  reduces  to  roughly
                    eps*MAX(lambda[]), with lambda[] being array of  eigen
                    values.
                    Zero value means that  algorithm  performs  number  of
                    iterations  specified  by  maxits  parameter,  without
                    paying attention to precision.
    MaxIts      -   number of iterations performed by  subspace  iteration
                    method. Zero value means that no  limit  on  iteration
                    count is placed (eps-based stopping condition is used).

Outputs:
    S2          -   array[NNeeded]. Variance values corresponding
                    to basis vectors.
    V           -   array[NVars,NNeeded]
                    matrix, whose columns store basis vectors.

NOTE: passing eps=0 and maxits=0 results in small eps  being  selected  as
stopping condition. Exact value of automatically selected eps is  version-
-dependent.
ALGLIB: Copyright 10.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pcatruncatedsubspace(real_2d_array x, ae_int_t npoints, ae_int_t nvars, ae_int_t nneeded, <b>double</b> eps, ae_int_t maxits, real_1d_array &amp;s2, real_2d_array &amp;v);
</pre>
<a name=sub_pcatruncatedsubspacesparse></a><h6 class=pageheader>pcatruncatedsubspacesparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse truncated principal components analysis

This function performs sparse truncated PCA, i.e. returns just a few  most
important principal components for a sparse input X.

Internally it uses iterative eigensolver which is very efficient when only
a minor fraction of full basis is required.

It should be noted that, unlike LDA, PCA does not use class labels.

Inputs:
    X           -   sparse dataset, sparse  npoints*nvars  matrix.  It  is
                    recommended to use CRS sparse storage format;  non-CRS
                    input will be internally converted to CRS.
                    Matrix contains ONLY INDEPENDENT VARIABLES,  and  must
                    be EXACTLY npoints*nvars.
    NPoints     -   dataset size, NPoints &ge; 0
    NVars       -   number of independent variables, NVars &ge; 1
    NNeeded     -   number of requested components, in [1,NVars] range;
                    this function is efficient only for NNeeded &lt;&lt; NVars.
    Eps         -   desired  precision  of  vectors  returned;  underlying
                    solver will stop iterations as soon as absolute  error
                    in corresponding singular values  reduces  to  roughly
                    eps*MAX(lambda[]), with lambda[] being array of  eigen
                    values.
                    Zero value means that  algorithm  performs  number  of
                    iterations  specified  by  maxits  parameter,  without
                    paying attention to precision.
    MaxIts      -   number of iterations performed by  subspace  iteration
                    method. Zero value means that no  limit  on  iteration
                    count is placed (eps-based stopping condition is used).

Outputs:
    S2          -   array[NNeeded]. Variance values corresponding
                    to basis vectors.
    V           -   array[NVars,NNeeded]
                    matrix, whose columns store basis vectors.

NOTE: passing eps=0 and maxits=0 results in small eps  being  selected  as
      a stopping condition. Exact value of automatically selected  eps  is
      version-dependent.

NOTE: zero  MaxIts  is  silently  replaced  by some reasonable value which
      prevents eternal loops (possible when inputs are degenerate and  too
      stringent stopping criteria are specified). In  current  version  it
      is 50+2*NVars.
ALGLIB: Copyright 10.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pcatruncatedsubspacesparse(sparsematrix x, ae_int_t npoints, ae_int_t nvars, ae_int_t nneeded, <b>double</b> eps, ae_int_t maxits, real_1d_array &amp;s2, real_2d_array &amp;v);
</pre>
<a name=unit_ssa></a><h4 class=pageheader>8.2.15. ssa Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_ssamodel class=toc>ssamodel</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_ssaaddsequence class=toc>ssaaddsequence</a> |
<a href=#sub_ssaanalyzelast class=toc>ssaanalyzelast</a> |
<a href=#sub_ssaanalyzelastwindow class=toc>ssaanalyzelastwindow</a> |
<a href=#sub_ssaanalyzesequence class=toc>ssaanalyzesequence</a> |
<a href=#sub_ssaappendpointandupdate class=toc>ssaappendpointandupdate</a> |
<a href=#sub_ssaappendsequenceandupdate class=toc>ssaappendsequenceandupdate</a> |
<a href=#sub_ssacleardata class=toc>ssacleardata</a> |
<a href=#sub_ssacreate class=toc>ssacreate</a> |
<a href=#sub_ssaforecastavglast class=toc>ssaforecastavglast</a> |
<a href=#sub_ssaforecastavgsequence class=toc>ssaforecastavgsequence</a> |
<a href=#sub_ssaforecastlast class=toc>ssaforecastlast</a> |
<a href=#sub_ssaforecastsequence class=toc>ssaforecastsequence</a> |
<a href=#sub_ssagetbasis class=toc>ssagetbasis</a> |
<a href=#sub_ssagetlrr class=toc>ssagetlrr</a> |
<a href=#sub_ssasetalgoprecomputed class=toc>ssasetalgoprecomputed</a> |
<a href=#sub_ssasetalgotopkdirect class=toc>ssasetalgotopkdirect</a> |
<a href=#sub_ssasetalgotopkrealtime class=toc>ssasetalgotopkrealtime</a> |
<a href=#sub_ssasetmemorylimit class=toc>ssasetmemorylimit</a> |
<a href=#sub_ssasetpoweruplength class=toc>ssasetpoweruplength</a> |
<a href=#sub_ssasetseed class=toc>ssasetseed</a> |
<a href=#sub_ssasetwindow class=toc>ssasetwindow</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_ssa_d_basic class=toc>ssa_d_basic</a></td><td width=15>&nbsp;</td><td>Simple SSA analysis demo</td></tr>
<tr align=left valign=top><td><a href=#example_ssa_d_forecast class=toc>ssa_d_forecast</a></td><td width=15>&nbsp;</td><td>Simple SSA forecasting demo</td></tr>
<tr align=left valign=top><td><a href=#example_ssa_d_realtime class=toc>ssa_d_realtime</a></td><td width=15>&nbsp;</td><td>Real-time SSA algorithm with fast incremental updates</td></tr>
</table>
</div>
<a name=struct_ssamodel></a><h6 class=pageheader>ssamodel Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores state of the SSA model.
You should use ALGLIB functions to work with this object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> ssamodel {
};
</pre>
<a name=sub_ssaaddsequence></a><h6 class=pageheader>ssaaddsequence Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function adds data sequence to SSA  model.  Only   single-dimensional
sequences are supported.

What is a sequences? Following definitions/requirements apply:
* a sequence  is  an  array of  values  measured  in  subsequent,  equally
  separated time moments (ticks).
* you may have many sequences  in your  dataset;  say,  one  sequence  may
  correspond to one trading session.
* sequence length should be larger  than current  window  length  (shorter
  sequences will be ignored during analysis).
* analysis is performed within a  sequence; different  sequences  are  NOT
  stacked together to produce one large contiguous stream of data.
* analysis is performed for all  sequences at once, i.e. same set of basis
  vectors is computed for all sequences

INCREMENTAL ANALYSIS

This function is non intended for  incremental updates of previously found
SSA basis. Calling it invalidates  all previous analysis results (basis is
reset and will be recalculated from zero during next analysis).

If  you  want  to  perform   incremental/real-time  SSA,  consider   using
following functions:
* ssaappendpointandupdate() for appending one point
* ssaappendsequenceandupdate() for appending new sequence

Inputs:
    S               -   SSA model created with ssacreate()
    X               -   array[N], data, can be larger (additional values
                        are ignored)
    N               -   data length, can be automatically determined from
                        the array length. N &ge; 0.

Outputs:
    S               -   SSA model, updated

NOTE: you can clear dataset with ssacleardata()
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaaddsequence(ssamodel s, real_1d_array x, ae_int_t n);
<b>void</b> ssaaddsequence(ssamodel s, real_1d_array x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ssa_d_basic class=nav>ssa_d_basic</a> | <a href=#example_ssa_d_forecast class=nav>ssa_d_forecast</a> | <a href=#example_ssa_d_realtime class=nav>ssa_d_realtime</a> ]</p>
<a name=sub_ssaanalyzelast></a><h6 class=pageheader>ssaanalyzelast Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function:
* builds SSA basis using internally stored (entire) dataset
* returns reconstruction for the last NTicks of the last sequence

If you want to analyze some other sequence, use ssaanalyzesequence().

Reconstruction phase involves  generation  of  NTicks-WindowWidth  sliding
windows, their decomposition using empirical orthogonal functions found by
SSA, followed by averaging of each data point across  several  overlapping
windows. Thus, every point in the output trend is reconstructed  using  up
to WindowWidth overlapping  windows  (WindowWidth windows exactly  in  the
inner points, just one window at the extremal points).

IMPORTANT: due to averaging this function returns  different  results  for
           different values of NTicks. It is expected and not a bug.

           For example:
           * Trend[NTicks-1] is always same because it is not averaged  in
             any case (same applies to Trend[0]).
           * Trend[NTicks-2] has different values  for  NTicks=WindowWidth
             and NTicks=WindowWidth+1 because former  case  means that  no
             averaging is performed, and latter  case means that averaging
             using two sliding windows  is  performed.  Larger  values  of
             NTicks produce same results as NTicks=WindowWidth+1.
           * ...and so on...

PERFORMANCE: this  function has O((NTicks-WindowWidth)*WindowWidth*NBasis)
             running time. If you work  in  time-constrained  setting  and
             have to analyze just a few last ticks, choosing NTicks  equal
             to WindowWidth+SmoothingLen, with SmoothingLen=1...WindowWidth
             will result in good compromise between noise cancellation and
             analysis speed.

Inputs:
    S               -   SSA model
    NTicks          -   number of ticks to analyze, Nticks &ge; 1.
                        * special case of NTicks &le; WindowWidth  is  handled
                          by analyzing last window and  returning   NTicks
                          last ticks.
                        * special case NTicks &gt; LastSequenceLen  is  handled
                          by prepending result with NTicks-LastSequenceLen
                          zeros.

Outputs:
    Trend           -   array[NTicks], reconstructed trend line
    Noise           -   array[NTicks], the rest of the signal;
                        it holds that ActualData = Trend+Noise.

CACHING/REUSE OF THE BASIS

Caching/reuse of previous results is performed:
* first call performs full run of SSA; basis is stored in the cache
* subsequent calls reuse previously cached basis
* if you call any function which changes model properties (window  length,
  algorithm, dataset), internal basis will be invalidated.
* the only calls which do NOT invalidate basis are listed below:
  a) ssasetwindow() with same window length
  b) ssaappendpointandupdate()
  c) ssaappendsequenceandupdate()
  d) ssasetalgotopk...() with exactly same K
  Calling these functions will result in reuse of previously found basis.

In  any  case,  only  basis  is  reused. Reconstruction is performed  from
scratch every time you call this function.

HANDLING OF DEGENERATE CASES

Following degenerate cases may happen:
* dataset is empty (no analysis can be done)
* all sequences are shorter than the window length,no analysis can be done
* no algorithm is specified (no analysis can be done)
* last sequence is shorter than the window length (analysis  can  be done,
  but we can not perform reconstruction on the last sequence)

Calling this function in degenerate cases returns following result:
* in any case, NTicks ticks is returned
* trend is assumed to be zero
* noise is initialized by the last sequence; if last sequence  is  shorter
  than the window size, it is moved to  the  end  of  the  array, and  the
  beginning of the noise array is filled by zeros

No analysis is performed in degenerate cases (we immediately return  dummy
values, no basis is constructed).
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaanalyzelast(ssamodel s, ae_int_t nticks, real_1d_array &amp;trend, real_1d_array &amp;noise);
</pre>
<a name=sub_ssaanalyzelastwindow></a><h6 class=pageheader>ssaanalyzelastwindow Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  executes  SSA  on  internally  stored  dataset and returns
analysis  for  the  last  window  of  the  last sequence. Such analysis is
an lightweight alternative for full scale reconstruction (see below).

Typical use case for this function is  real-time  setting,  when  you  are
interested in quick-and-dirty (very quick and very  dirty)  processing  of
just a few last ticks of the trend.

IMPORTANT: full  scale  SSA  involves  analysis  of  the  ENTIRE  dataset,
           with reconstruction being done for  all  positions  of  sliding
           window with subsequent hankelization  (diagonal  averaging)  of
           the resulting matrix.

           Such analysis requires O((DataLen-Window)*Window*NBasis)  FLOPs
           and can be quite costly. However, it has  nice  noise-canceling
           effects due to averaging.

           This function performs REDUCED analysis of the last window.  It
           is much faster - just O(Window*NBasis),  but  its  results  are
           DIFFERENT from that of ssaanalyzelast(). In  particular,  first
           few points of the trend are much more prone to noise.

Inputs:
    S               -   SSA model

Outputs:
    Trend           -   array[WindowSize], reconstructed trend line
    Noise           -   array[WindowSize], the rest of the signal;
                        it holds that ActualData = Trend+Noise.
    NTicks          -   current WindowSize

CACHING/REUSE OF THE BASIS

Caching/reuse of previous results is performed:
* first call performs full run of SSA; basis is stored in the cache
* subsequent calls reuse previously cached basis
* if you call any function which changes model properties (window  length,
  algorithm, dataset), internal basis will be invalidated.
* the only calls which do NOT invalidate basis are listed below:
  a) ssasetwindow() with same window length
  b) ssaappendpointandupdate()
  c) ssaappendsequenceandupdate()
  d) ssasetalgotopk...() with exactly same K
  Calling these functions will result in reuse of previously found basis.

In  any  case,  only  basis  is  reused. Reconstruction is performed  from
scratch every time you call this function.

HANDLING OF DEGENERATE CASES

Following degenerate cases may happen:
* dataset is empty (no analysis can be done)
* all sequences are shorter than the window length,no analysis can be done
* no algorithm is specified (no analysis can be done)
* last sequence is shorter than the window length (analysis can  be  done,
  but we can not perform reconstruction on the last sequence)

Calling this function in degenerate cases returns following result:
* in any case, WindowWidth ticks is returned
* trend is assumed to be zero
* noise is initialized by the last sequence; if last sequence  is  shorter
  than the window size, it is moved to  the  end  of  the  array, and  the
  beginning of the noise array is filled by zeros

No analysis is performed in degenerate cases (we immediately return  dummy
values, no basis is constructed).
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaanalyzelastwindow(ssamodel s, real_1d_array &amp;trend, real_1d_array &amp;noise, ae_int_t &amp;nticks);
</pre>
<a name=sub_ssaanalyzesequence></a><h6 class=pageheader>ssaanalyzesequence Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function:
* builds SSA basis using internally stored (entire) dataset
* returns reconstruction for the sequence being passed to this function

If  you  want  to  analyze  last  sequence  stored  in   the   model,  use
ssaanalyzelast().

Reconstruction phase involves  generation  of  NTicks-WindowWidth  sliding
windows, their decomposition using empirical orthogonal functions found by
SSA, followed by averaging of each data point across  several  overlapping
windows. Thus, every point in the output trend is reconstructed  using  up
to WindowWidth overlapping  windows  (WindowWidth windows exactly  in  the
inner points, just one window at the extremal points).

PERFORMANCE: this  function has O((NTicks-WindowWidth)*WindowWidth*NBasis)
             running time. If you work  in  time-constrained  setting  and
             have to analyze just a few last ticks, choosing NTicks  equal
             to WindowWidth+SmoothingLen, with SmoothingLen=1...WindowWidth
             will result in good compromise between noise cancellation and
             analysis speed.

Inputs:
    S               -   SSA model
    Data            -   array[NTicks], can be larger (only NTicks  leading
                        elements will be used)
    NTicks          -   number of ticks to analyze, Nticks &ge; 1.
                        * special case of NTicks &lt; WindowWidth  is   handled
                          by returning zeros as trend, and signal as noise

Outputs:
    Trend           -   array[NTicks], reconstructed trend line
    Noise           -   array[NTicks], the rest of the signal;
                        it holds that ActualData = Trend+Noise.

CACHING/REUSE OF THE BASIS

Caching/reuse of previous results is performed:
* first call performs full run of SSA; basis is stored in the cache
* subsequent calls reuse previously cached basis
* if you call any function which changes model properties (window  length,
  algorithm, dataset), internal basis will be invalidated.
* the only calls which do NOT invalidate basis are listed below:
  a) ssasetwindow() with same window length
  b) ssaappendpointandupdate()
  c) ssaappendsequenceandupdate()
  d) ssasetalgotopk...() with exactly same K
  Calling these functions will result in reuse of previously found basis.

In  any  case,  only  basis  is  reused. Reconstruction is performed  from
scratch every time you call this function.

HANDLING OF DEGENERATE CASES

Following degenerate cases may happen:
* dataset is empty (no analysis can be done)
* all sequences are shorter than the window length,no analysis can be done
* no algorithm is specified (no analysis can be done)
* sequence being passed is shorter than the window length

Calling this function in degenerate cases returns following result:
* in any case, NTicks ticks is returned
* trend is assumed to be zero
* noise is initialized by the sequence.

No analysis is performed in degenerate cases (we immediately return  dummy
values, no basis is constructed).
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaanalyzesequence(ssamodel s, real_1d_array data, ae_int_t nticks, real_1d_array &amp;trend, real_1d_array &amp;noise);
<b>void</b> ssaanalyzesequence(ssamodel s, real_1d_array data, real_1d_array &amp;trend, real_1d_array &amp;noise);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ssa_d_basic class=nav>ssa_d_basic</a> | <a href=#example_ssa_d_forecast class=nav>ssa_d_forecast</a> | <a href=#example_ssa_d_realtime class=nav>ssa_d_realtime</a> ]</p>
<a name=sub_ssaappendpointandupdate></a><h6 class=pageheader>ssaappendpointandupdate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function appends single point to last data sequence stored in the SSA
model and tries to update model in the  incremental  manner  (if  possible
with current algorithm).

If you want to add more than one point at once:
* if you want to add M points to the same sequence, perform M-1 calls with
  UpdateIts parameter set to 0.0, and last call with non-zero UpdateIts.
* if you want to add new sequence, use ssaappendsequenceandupdate()

Running time of this function does NOT depend on  dataset  size,  only  on
window width and number of singular vectors. Depending on algorithm  being
used, incremental update has complexity:
* for top-K real time   - O(UpdateIts*K*Width^2), with fractional UpdateIts
* for top-K direct      - O(Width^3) for any non-zero UpdateIts
* for precomputed basis - O(1), no update is performed

Inputs:
    S               -   SSA model created with ssacreate()
    X               -   new point
    UpdateIts       - &ge; 0,  floating  point (!)  value,  desired  update
                        frequency:
                        * zero value means that point is  stored,  but  no
                          update is performed
                        * integer part of the value means  that  specified
                          number of iterations is always performed
                        * fractional part of  the  value  means  that  one
                          iteration is performed with this probability.

                        Recommended value: 0 &lt; UpdateIts &le; 1.  Values  larger
                        than 1 are VERY seldom  needed.  If  your  dataset
                        changes slowly, you can set it  to  0.1  and  skip
                        90% of updates.

                        In any case, no information is lost even with zero
                        value of UpdateIts! It will be  incorporated  into
                        model, sooner or later.

Outputs:
    S               -   SSA model, updated

NOTE: this function uses internal  RNG  to  handle  fractional  values  of
      UpdateIts. By default it  is  initialized  with  fixed  seed  during
      initial calculation of basis. Thus subsequent calls to this function
      will result in the same sequence of pseudorandom decisions.

      However, if  you  have  several  SSA  models  which  are  calculated
      simultaneously, and if you want to reduce computational  bottlenecks
      by performing random updates at random moments, then fixed  seed  is
      not an option - all updates will fire at same moments.

      You may change it with ssasetseed() function.

NOTE: this function throws an exception if called for empty dataset (there
      is no &quot;last&quot; sequence to modify).
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaappendpointandupdate(ssamodel s, <b>double</b> x, <b>double</b> updateits);
</pre>
<a name=sub_ssaappendsequenceandupdate></a><h6 class=pageheader>ssaappendsequenceandupdate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function appends new sequence to dataset stored in the SSA  model and
tries to update model in the incremental manner (if possible  with current
algorithm).

Notes:
* if you want to add M sequences at once, perform M-1 calls with UpdateIts
  parameter set to 0.0, and last call with non-zero UpdateIts.
* if you want to add just one point, use ssaappendpointandupdate()

Running time of this function does NOT depend on  dataset  size,  only  on
sequence length, window width and number of singular vectors. Depending on
algorithm being used, incremental update has complexity:
* for top-K real time   - O(UpdateIts*K*Width^2+(NTicks-Width)*Width^2)
* for top-K direct      - O(Width^3+(NTicks-Width)*Width^2)
* for precomputed basis - O(1), no update is performed

Inputs:
    S               -   SSA model created with ssacreate()
    X               -   new sequence, array[NTicks] or larget
    NTicks          - &ge; 1, number of ticks in the sequence
    UpdateIts       - &ge; 0,  floating  point (!)  value,  desired  update
                        frequency:
                        * zero value means that point is  stored,  but  no
                          update is performed
                        * integer part of the value means  that  specified
                          number of iterations is always performed
                        * fractional part of  the  value  means  that  one
                          iteration is performed with this probability.

                        Recommended value: 0 &lt; UpdateIts &le; 1.  Values  larger
                        than 1 are VERY seldom  needed.  If  your  dataset
                        changes slowly, you can set it  to  0.1  and  skip
                        90% of updates.

                        In any case, no information is lost even with zero
                        value of UpdateIts! It will be  incorporated  into
                        model, sooner or later.

Outputs:
    S               -   SSA model, updated

NOTE: this function uses internal  RNG  to  handle  fractional  values  of
      UpdateIts. By default it  is  initialized  with  fixed  seed  during
      initial calculation of basis. Thus subsequent calls to this function
      will result in the same sequence of pseudorandom decisions.

      However, if  you  have  several  SSA  models  which  are  calculated
      simultaneously, and if you want to reduce computational  bottlenecks
      by performing random updates at random moments, then fixed  seed  is
      not an option - all updates will fire at same moments.

      You may change it with ssasetseed() function.
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaappendsequenceandupdate(ssamodel s, real_1d_array x, ae_int_t nticks, <b>double</b> updateits);
<b>void</b> ssaappendsequenceandupdate(ssamodel s, real_1d_array x, <b>double</b> updateits);
</pre>
<a name=sub_ssacleardata></a><h6 class=pageheader>ssacleardata Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function clears all data stored in the  model  and  invalidates  all
basis components found so far.

Inputs:
    S               -   SSA model created with ssacreate()

Outputs:
    S               -   SSA model, updated
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssacleardata(ssamodel s);
</pre>
<a name=sub_ssacreate></a><h6 class=pageheader>ssacreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates SSA model object.  Right after creation model is  in
&quot;dummy&quot; mode - you can add data,  but   analyzing/prediction  will  return
just zeros (it assumes that basis is empty).

HOW TO USE SSA MODEL:

1. create model with ssacreate()
2. add data with one/many ssaaddsequence() calls
3. choose SSA algorithm with one of ssasetalgo...() functions:
   * ssasetalgotopkdirect() for direct one-run analysis
   * ssasetalgotopkrealtime() for algorithm optimized for many  subsequent
     runs with warm-start capabilities
   * ssasetalgoprecomputed() for user-supplied basis
4. set window width with ssasetwindow()
5. perform one of the analysis-related activities:
   a) call ssagetbasis() to get basis
   b) call ssaanalyzelast() ssaanalyzesequence() or ssaanalyzelastwindow()
      to perform analysis (trend/noise separation)
   c) call  one  of   the   forecasting   functions  (ssaforecastlast() or
      ssaforecastsequence()) to perform prediction; alternatively, you can
      extract linear recurrence coefficients with ssagetlrr().
   SSA analysis will be performed during first  call  to  analysis-related
   function. SSA model is smart enough to track all changes in the dataset
   and  model  settings,  to  cache  previously  computed  basis  and   to
   re-evaluate basis only when necessary.

Additionally, if your setting involves constant stream  of  incoming data,
you can perform quick update already calculated  model  with  one  of  the
incremental   append-and-update   functions:  ssaappendpointandupdate() or
ssaappendsequenceandupdate().

NOTE: steps (2), (3), (4) can be performed in arbitrary order.

Inputs:
    none

Outputs:
    S               -   structure which stores model state
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssacreate(ssamodel &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ssa_d_basic class=nav>ssa_d_basic</a> | <a href=#example_ssa_d_forecast class=nav>ssa_d_forecast</a> | <a href=#example_ssa_d_realtime class=nav>ssa_d_realtime</a> ]</p>
<a name=sub_ssaforecastavglast></a><h6 class=pageheader>ssaforecastavglast Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function builds SSA basis and performs forecasting  for  a  specified
number of ticks, returning value of trend.

Forecast is performed as follows:
* SSA  trend  extraction  is  applied to last  M  sliding windows  of  the
  internally stored dataset
* for each of M sliding windows, M predictions are built
* average value of M predictions is returned

This function has following running time:
* O(NBasis*WindowWidth*M) for trend extraction phase (always performed)
* O(WindowWidth*NTicks*M) for forecast phase

NOTE: noise reduction is ALWAYS applied by this algorithm; if you want  to
      apply recurrence relation  to  raw  unprocessed  data,  use  another
      function - ssaforecastsequence() which allows to  turn  on  and  off
      noise reduction phase.

NOTE: combination of several predictions results in lesser sensitivity  to
      noise, but it may produce undesirable discontinuities  between  last
      point of the trend and first point of the prediction. The reason  is
      that  last  point  of  the  trend is usually corrupted by noise, but
      average  value of  several  predictions  is less sensitive to noise,
      thus discontinuity appears. It is not a bug.

Inputs:
    S               -   SSA model
    M               -   number  of  sliding  windows  to combine, M &ge; 1. If
                        your dataset has less than M sliding windows, this
                        parameter will be silently reduced.
    NTicks          -   number of ticks to forecast, NTicks &ge; 1

Outputs:
    Trend           -   array[NTicks], predicted trend line

CACHING/REUSE OF THE BASIS

Caching/reuse of previous results is performed:
* first call performs full run of SSA; basis is stored in the cache
* subsequent calls reuse previously cached basis
* if you call any function which changes model properties (window  length,
  algorithm, dataset), internal basis will be invalidated.
* the only calls which do NOT invalidate basis are listed below:
  a) ssasetwindow() with same window length
  b) ssaappendpointandupdate()
  c) ssaappendsequenceandupdate()
  d) ssasetalgotopk...() with exactly same K
  Calling these functions will result in reuse of previously found basis.

HANDLING OF DEGENERATE CASES

Following degenerate cases may happen:
* dataset is empty (no analysis can be done)
* all sequences are shorter than the window length,no analysis can be done
* no algorithm is specified (no analysis can be done)
* last sequence is shorter than the WindowWidth   (analysis  can  be done,
  but we can not perform forecasting on the last sequence)
* window length is 1 (impossible to use for forecasting)
* SSA analysis algorithm is  configured  to  extract  basis  whose size is
  equal to window length (impossible to use for  forecasting;  only  basis
  whose size is less than window length can be used).

Calling this function in degenerate cases returns following result:
* NTicks  copies  of  the  last  value is returned for non-empty task with
  large enough dataset, but with overcomplete  basis  (window  width=1  or
  basis size is equal to window width)
* zero trend with length=NTicks is returned for empty task

No analysis is performed in degenerate cases (we immediately return  dummy
values, no basis is ever constructed).
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaforecastavglast(ssamodel s, ae_int_t m, ae_int_t nticks, real_1d_array &amp;trend);
</pre>
<a name=sub_ssaforecastavgsequence></a><h6 class=pageheader>ssaforecastavgsequence Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function builds SSA  basis  and  performs  forecasting  for  a  user-
specified sequence, returning value of trend.

Forecasting is done in two stages:
* first,  we  extract  trend  from M last sliding windows of the sequence.
  This stage is optional, you can  turn  it  off  if  you  pass data which
  are already processed with SSA. Of course, you  can  turn  it  off  even
  for raw data, but it is not recommended  -  noise  suppression  is  very
  important for correct prediction.
* then, we apply LRR independently for M sliding windows
* average of M predictions is returned

This function has following running time:
* O(NBasis*WindowWidth*M) for trend extraction phase
* O(WindowWidth*NTicks*M) for forecast phase

NOTE: combination of several predictions results in lesser sensitivity  to
      noise, but it may produce undesirable discontinuities  between  last
      point of the trend and first point of the prediction. The reason  is
      that  last  point  of  the  trend is usually corrupted by noise, but
      average  value of  several  predictions  is less sensitive to noise,
      thus discontinuity appears. It is not a bug.

Inputs:
    S               -   SSA model
    Data            -   array[NTicks], data to forecast
    DataLen         -   number of ticks in the data, DataLen &ge; 1
    M               -   number  of  sliding  windows  to combine, M &ge; 1. If
                        your dataset has less than M sliding windows, this
                        parameter will be silently reduced.
    ForecastLen     -   number of ticks to predict, ForecastLen &ge; 1
    ApplySmoothing  -   whether to apply smoothing trend extraction or not.
                        if you do not know what to specify, pass true.

Outputs:
    Trend           -   array[ForecastLen], forecasted trend

CACHING/REUSE OF THE BASIS

Caching/reuse of previous results is performed:
* first call performs full run of SSA; basis is stored in the cache
* subsequent calls reuse previously cached basis
* if you call any function which changes model properties (window  length,
  algorithm, dataset), internal basis will be invalidated.
* the only calls which do NOT invalidate basis are listed below:
  a) ssasetwindow() with same window length
  b) ssaappendpointandupdate()
  c) ssaappendsequenceandupdate()
  d) ssasetalgotopk...() with exactly same K
  Calling these functions will result in reuse of previously found basis.

HANDLING OF DEGENERATE CASES

Following degenerate cases may happen:
* dataset is empty (no analysis can be done)
* all sequences are shorter than the window length,no analysis can be done
* no algorithm is specified (no analysis can be done)
* data sequence is shorter than the WindowWidth   (analysis  can  be done,
  but we can not perform forecasting on the last sequence)
* window length is 1 (impossible to use for forecasting)
* SSA analysis algorithm is  configured  to  extract  basis  whose size is
  equal to window length (impossible to use for  forecasting;  only  basis
  whose size is less than window length can be used).

Calling this function in degenerate cases returns following result:
* ForecastLen copies of the last value is returned for non-empty task with
  large enough dataset, but with overcomplete  basis  (window  width=1  or
  basis size is equal to window width)
* zero trend with length=ForecastLen is returned for empty task

No analysis is performed in degenerate cases (we immediately return  dummy
values, no basis is ever constructed).
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaforecastavgsequence(ssamodel s, real_1d_array data, ae_int_t datalen, ae_int_t m, ae_int_t forecastlen, <b>bool</b> applysmoothing, real_1d_array &amp;trend);
<b>void</b> ssaforecastavgsequence(ssamodel s, real_1d_array data, ae_int_t m, ae_int_t forecastlen, real_1d_array &amp;trend);
</pre>
<a name=sub_ssaforecastlast></a><h6 class=pageheader>ssaforecastlast Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function builds SSA basis and performs forecasting  for  a  specified
number of ticks, returning value of trend.

Forecast is performed as follows:
* SSA  trend  extraction  is  applied  to last WindowWidth elements of the
  internally stored dataset; this step is basically a noise reduction.
* linear recurrence relation is applied to extracted trend

This function has following running time:
* O(NBasis*WindowWidth) for trend extraction phase (always performed)
* O(WindowWidth*NTicks) for forecast phase

NOTE: noise reduction is ALWAYS applied by this algorithm; if you want  to
      apply recurrence relation  to  raw  unprocessed  data,  use  another
      function - ssaforecastsequence() which allows to  turn  on  and  off
      noise reduction phase.

NOTE: this algorithm performs prediction using only one - last  -  sliding
      window.  Predictions  produced   by   such   approach   are   smooth
      continuations of the reconstructed  trend  line,  but  they  can  be
      easily corrupted by noise. If you need  noise-resistant  prediction,
      use ssaforecastavglast() function, which averages predictions  built
      using several sliding windows.

Inputs:
    S               -   SSA model
    NTicks          -   number of ticks to forecast, NTicks &ge; 1

Outputs:
    Trend           -   array[NTicks], predicted trend line

CACHING/REUSE OF THE BASIS

Caching/reuse of previous results is performed:
* first call performs full run of SSA; basis is stored in the cache
* subsequent calls reuse previously cached basis
* if you call any function which changes model properties (window  length,
  algorithm, dataset), internal basis will be invalidated.
* the only calls which do NOT invalidate basis are listed below:
  a) ssasetwindow() with same window length
  b) ssaappendpointandupdate()
  c) ssaappendsequenceandupdate()
  d) ssasetalgotopk...() with exactly same K
  Calling these functions will result in reuse of previously found basis.

HANDLING OF DEGENERATE CASES

Following degenerate cases may happen:
* dataset is empty (no analysis can be done)
* all sequences are shorter than the window length,no analysis can be done
* no algorithm is specified (no analysis can be done)
* last sequence is shorter than the WindowWidth   (analysis  can  be done,
  but we can not perform forecasting on the last sequence)
* window length is 1 (impossible to use for forecasting)
* SSA analysis algorithm is  configured  to  extract  basis  whose size is
  equal to window length (impossible to use for  forecasting;  only  basis
  whose size is less than window length can be used).

Calling this function in degenerate cases returns following result:
* NTicks  copies  of  the  last  value is returned for non-empty task with
  large enough dataset, but with overcomplete  basis  (window  width=1  or
  basis size is equal to window width)
* zero trend with length=NTicks is returned for empty task

No analysis is performed in degenerate cases (we immediately return  dummy
values, no basis is ever constructed).
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaforecastlast(ssamodel s, ae_int_t nticks, real_1d_array &amp;trend);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ssa_d_forecast class=nav>ssa_d_forecast</a> | <a href=#example_ssa_d_realtime class=nav>ssa_d_realtime</a> ]</p>
<a name=sub_ssaforecastsequence></a><h6 class=pageheader>ssaforecastsequence Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function builds SSA  basis  and  performs  forecasting  for  a  user-
specified sequence, returning value of trend.

Forecasting is done in two stages:
* first,  we  extract  trend  from the WindowWidth  last  elements of  the
  sequence. This stage is optional, you  can  turn  it  off  if  you  pass
  data which are already processed with SSA. Of course, you  can  turn  it
  off even for raw data, but it is not recommended - noise suppression  is
  very important for correct prediction.
* then, we apply LRR for last  WindowWidth-1  elements  of  the  extracted
  trend.

This function has following running time:
* O(NBasis*WindowWidth) for trend extraction phase
* O(WindowWidth*NTicks) for forecast phase

NOTE: this algorithm performs prediction using only one - last  -  sliding
      window.  Predictions  produced   by   such   approach   are   smooth
      continuations of the reconstructed  trend  line,  but  they  can  be
      easily corrupted by noise. If you need  noise-resistant  prediction,
      use ssaforecastavgsequence() function,  which  averages  predictions
      built using several sliding windows.

Inputs:
    S               -   SSA model
    Data            -   array[DataLen], data to forecast
    DataLen         -   number of ticks in the data, DataLen &ge; 1
    ForecastLen     -   number of ticks to predict, ForecastLen &ge; 1
    ApplySmoothing  -   whether to apply smoothing trend extraction or not;
                        if you do not know what to specify, pass True.

Outputs:
    Trend           -   array[ForecastLen], forecasted trend

CACHING/REUSE OF THE BASIS

Caching/reuse of previous results is performed:
* first call performs full run of SSA; basis is stored in the cache
* subsequent calls reuse previously cached basis
* if you call any function which changes model properties (window  length,
  algorithm, dataset), internal basis will be invalidated.
* the only calls which do NOT invalidate basis are listed below:
  a) ssasetwindow() with same window length
  b) ssaappendpointandupdate()
  c) ssaappendsequenceandupdate()
  d) ssasetalgotopk...() with exactly same K
  Calling these functions will result in reuse of previously found basis.

HANDLING OF DEGENERATE CASES

Following degenerate cases may happen:
* dataset is empty (no analysis can be done)
* all sequences are shorter than the window length,no analysis can be done
* no algorithm is specified (no analysis can be done)
* data sequence is shorter than the WindowWidth   (analysis  can  be done,
  but we can not perform forecasting on the last sequence)
* window length is 1 (impossible to use for forecasting)
* SSA analysis algorithm is  configured  to  extract  basis  whose size is
  equal to window length (impossible to use for  forecasting;  only  basis
  whose size is less than window length can be used).

Calling this function in degenerate cases returns following result:
* ForecastLen copies of the last value is returned for non-empty task with
  large enough dataset, but with overcomplete  basis  (window  width=1  or
  basis size is equal to window width)
* zero trend with length=ForecastLen is returned for empty task

No analysis is performed in degenerate cases (we immediately return  dummy
values, no basis is ever constructed).
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssaforecastsequence(ssamodel s, real_1d_array data, ae_int_t datalen, ae_int_t forecastlen, <b>bool</b> applysmoothing, real_1d_array &amp;trend);
<b>void</b> ssaforecastsequence(ssamodel s, real_1d_array data, ae_int_t forecastlen, real_1d_array &amp;trend);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ssa_d_forecast class=nav>ssa_d_forecast</a> | <a href=#example_ssa_d_realtime class=nav>ssa_d_realtime</a> ]</p>
<a name=sub_ssagetbasis></a><h6 class=pageheader>ssagetbasis Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function executes SSA on internally stored dataset and returns  basis
found by current method.

Inputs:
    S               -   SSA model

Outputs:
    A               -   array[WindowWidth,NBasis],   basis;  vectors  are
                        stored in matrix columns, by descreasing variance
    SV              -   array[NBasis]:
                        * zeros - for model initialized with SSASetAlgoPrecomputed()
                        * singular values - for other algorithms
    WindowWidth     -   current window
    NBasis          -   basis size

CACHING/REUSE OF THE BASIS

Caching/reuse of previous results is performed:
* first call performs full run of SSA; basis is stored in the cache
* subsequent calls reuse previously cached basis
* if you call any function which changes model properties (window  length,
  algorithm, dataset), internal basis will be invalidated.
* the only calls which do NOT invalidate basis are listed below:
  a) ssasetwindow() with same window length
  b) ssaappendpointandupdate()
  c) ssaappendsequenceandupdate()
  d) ssasetalgotopk...() with exactly same K
  Calling these functions will result in reuse of previously found basis.

HANDLING OF DEGENERATE CASES

Calling  this  function  in  degenerate  cases  (no  data  or all data are
shorter than window size; no algorithm is specified)  returns  basis  with
just one zero vector.
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssagetbasis(ssamodel s, real_2d_array &amp;a, real_1d_array &amp;sv, ae_int_t &amp;windowwidth, ae_int_t &amp;nbasis);
</pre>
<a name=sub_ssagetlrr></a><h6 class=pageheader>ssagetlrr Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns linear recurrence relation (LRR) coefficients  found
by current SSA algorithm.

Inputs:
    S               -   SSA model

Outputs:
    A               -   array[WindowWidth-1]. Coefficients  of  the
                        linear recurrence of the form:
                        X[W-1] = X[W-2]*A[W-2] + X[W-3]*A[W-3] + ... + X[0]*A[0].
                        Empty array for WindowWidth=1.
    WindowWidth     -   current window width

CACHING/REUSE OF THE BASIS

Caching/reuse of previous results is performed:
* first call performs full run of SSA; basis is stored in the cache
* subsequent calls reuse previously cached basis
* if you call any function which changes model properties (window  length,
  algorithm, dataset), internal basis will be invalidated.
* the only calls which do NOT invalidate basis are listed below:
  a) ssasetwindow() with same window length
  b) ssaappendpointandupdate()
  c) ssaappendsequenceandupdate()
  d) ssasetalgotopk...() with exactly same K
  Calling these functions will result in reuse of previously found basis.

HANDLING OF DEGENERATE CASES

Calling  this  function  in  degenerate  cases  (no  data  or all data are
shorter than window size; no algorithm is specified) returns zeros.
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssagetlrr(ssamodel s, real_1d_array &amp;a, ae_int_t &amp;windowwidth);
</pre>
<a name=sub_ssasetalgoprecomputed></a><h6 class=pageheader>ssasetalgoprecomputed Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function sets SSA algorithm to &quot;precomputed vectors&quot; algorithm.

This  algorithm  uses  precomputed  set  of  orthonormal  (orthogonal  AND
normalized) basis vectors supplied by user. Thus, basis calculation  phase
is not performed -  we  already  have  our  basis  -  and  only  analysis/
forecasting phase requires actual calculations.

This algorithm may handle &quot;append&quot; requests which add just  one/few  ticks
to the end of the last sequence in O(1) time.

NOTE: this algorithm accepts both basis and window  width,  because  these
      two parameters are naturally aligned.  Calling  this  function  sets
      window width; if you call ssasetwindow() with  other  window  width,
      then during analysis stage algorithm will detect conflict and  reset
      to zero basis.

Inputs:
    S               -   SSA model
    A               -   array[WindowWidth,NBasis], orthonormalized  basis;
                        this function does NOT control  orthogonality  and
                        does NOT perform any kind of  renormalization.  It
                        is your responsibility to provide it with  correct
                        basis.
    WindowWidth     -   window width, &ge; 1
    NBasis          -   number of basis vectors, 1 &le; NBasis &le; WindowWidth

Outputs:
    S               -   updated model

NOTE: calling this function invalidates basis in all cases.
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssasetalgoprecomputed(ssamodel s, real_2d_array a, ae_int_t windowwidth, ae_int_t nbasis);
<b>void</b> ssasetalgoprecomputed(ssamodel s, real_2d_array a);
</pre>
<a name=sub_ssasetalgotopkdirect></a><h6 class=pageheader>ssasetalgotopkdirect Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function sets SSA algorithm to &quot;direct top-K&quot; algorithm.

&quot;Direct top-K&quot; algorithm performs full  SVD  of  the  N*WINDOW  trajectory
matrix (hence its name - direct solver  is  used),  then  extracts  top  K
components. Overall running time is O(N*WINDOW^2), where N is a number  of
ticks in the dataset, WINDOW is window width.

This algorithm may handle &quot;append&quot; requests which add just  one/few  ticks
to the end of the last sequence in O(WINDOW^3) time,  which  is  ~N/WINDOW
times faster than re-computing everything from scratch.

Inputs:
    S               -   SSA model
    TopK            -   number of components to analyze; TopK &ge; 1.

Outputs:
    S               -   updated model

NOTE: TopK &gt; WindowWidth is silently decreased to WindowWidth during analysis
      phase

NOTE: calling this function invalidates basis, except  for  the  situation
      when this algorithm was already set with same parameters.
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssasetalgotopkdirect(ssamodel s, ae_int_t topk);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ssa_d_basic class=nav>ssa_d_basic</a> | <a href=#example_ssa_d_forecast class=nav>ssa_d_forecast</a> ]</p>
<a name=sub_ssasetalgotopkrealtime></a><h6 class=pageheader>ssasetalgotopkrealtime Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets SSA algorithm to &quot;top-K real time algorithm&quot;. This algo
extracts K components with largest singular values.

It  is  real-time  version  of  top-K  algorithm  which  is  optimized for
incremental processing and  fast  start-up. Internally  it  uses  subspace
eigensolver for truncated SVD. It results  in  ability  to  perform  quick
updates of the basis when only a few points/sequences is added to dataset.

Performance profile of the algorithm is given below:
* O(K*WindowWidth^2) running time for incremental update  of  the  dataset
  with one of the &quot;append-and-update&quot; functions (ssaappendpointandupdate()
  or ssaappendsequenceandupdate()).
* O(N*WindowWidth^2) running time for initial basis evaluation (N=size  of
  dataset)
* ability  to  split  costly  initialization  across  several  incremental
  updates of the basis (so called &quot;Power-Up&quot; functionality,  activated  by
  ssasetpoweruplength() function)

Inputs:
    S               -   SSA model
    TopK            -   number of components to analyze; TopK &ge; 1.

Outputs:
    S               -   updated model

NOTE: this  algorithm  is  optimized  for  large-scale  tasks  with  large
      datasets. On toy problems with just  5-10 points it can return basis
      which is slightly different from that returned by  direct  algorithm
      (ssasetalgotopkdirect() function). However, the  difference  becomes
      negligible as dataset grows.

NOTE: TopK &gt; WindowWidth is silently decreased to WindowWidth during analysis
      phase

NOTE: calling this function invalidates basis, except  for  the  situation
      when this algorithm was already set with same parameters.
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssasetalgotopkrealtime(ssamodel s, ae_int_t topk);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ssa_d_realtime class=nav>ssa_d_realtime</a> ]</p>
<a name=sub_ssasetmemorylimit></a><h6 class=pageheader>ssasetmemorylimit Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets memory limit of SSA analysis.

Straightforward SSA with sequence length T and window width W needs O(T*W)
memory. It is possible to reduce memory consumption by splitting task into
smaller chunks.

Thus function allows you to specify approximate memory limit (measured  in
double precision numbers used for buffers). Actual memory consumption will
be comparable to the number specified by you.

Default memory limit is 50.000.000 (400Mbytes) in current version.

Inputs:
    S       -   SSA model
    MemLimit-   memory limit, &ge; 0. Zero value means no limit.
ALGLIB: Copyright 20.12.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssasetmemorylimit(ssamodel s, ae_int_t memlimit);
</pre>
<a name=sub_ssasetpoweruplength></a><h6 class=pageheader>ssasetpoweruplength Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets length of power-up cycle for real-time algorithm.

By default, this algorithm performs costly O(N*WindowWidth^2)  init  phase
followed by full run of truncated  EVD.  However,  if  you  are  ready  to
live with a bit lower-quality basis during first few iterations,  you  can
split this O(N*WindowWidth^2) initialization  between  several  subsequent
append-and-update rounds. It results in better latency of the algorithm.

This function invalidates basis/solver, next analysis call will result  in
full recalculation of everything.

Inputs:
    S       -   SSA model
    PWLen   -   length of the power-up stage:
                * 0 means that no power-up is requested
                * 1 is the same as 0
                * &gt; 1 means that delayed power-up is performed
ALGLIB: Copyright 03.11.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssasetpoweruplength(ssamodel s, ae_int_t pwlen);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ssa_d_realtime class=nav>ssa_d_realtime</a> ]</p>
<a name=sub_ssasetseed></a><h6 class=pageheader>ssasetseed Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  seed  which  is used to initialize internal RNG when
we make pseudorandom decisions on model updates.

By default, deterministic seed is used - which results in same sequence of
pseudorandom decisions every time you run SSA model. If you  specify  non-
deterministic seed value, then SSA  model  may  return  slightly different
results after each run.

This function can be useful when you have several SSA models updated  with
sseappendpointandupdate() called with 0 &lt; UpdateIts &lt; 1 (fractional value) and
due to performance limitations want them to perform updates  at  different
moments.

Inputs:
    S       -   SSA model
    Seed    -   seed:
                * positive values = use deterministic seed for each run of
                  algorithms which depend on random initialization
                * zero or negative values = use non-deterministic seed
ALGLIB: Copyright 03.11.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssasetseed(ssamodel s, ae_int_t seed);
</pre>
<a name=sub_ssasetwindow></a><h6 class=pageheader>ssasetwindow Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets window width for SSA model. You should call  it  before
analysis phase. Default window width is 1 (not for real use).

Special notes:
* this function call can be performed at any moment before  first call  to
  analysis-related functions
* changing window width invalidates internally stored basis; if you change
  window width AFTER you call analysis-related  function,  next  analysis
  phase will require re-calculation of  the  basis  according  to  current
  algorithm.
* calling this function with exactly  same window width as current one has
  no effect
* if you specify window width larger  than any data sequence stored in the
  model, analysis will return zero basis.

Inputs:
    S               -   SSA model created with ssacreate()
    WindowWidth     - &ge; 1, new window width

Outputs:
    S               -   SSA model, updated
ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ssasetwindow(ssamodel s, ae_int_t windowwidth);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ssa_d_basic class=nav>ssa_d_basic</a> | <a href=#example_ssa_d_forecast class=nav>ssa_d_forecast</a> | <a href=#example_ssa_d_realtime class=nav>ssa_d_realtime</a> ]</p>
<a name=example_ssa_d_basic></a><h6 class=pageheader>ssa_d_basic Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Here we demonstrate SSA trend/noise separation <b>for</b> some toy problem:</font>
<font color=navy>// small monotonically growing series X are analyzed with 3-tick window</font>
<font color=navy>// and <font color=blue><b>&quot;top-K&quot;</b></font> version of SSA, which selects K largest singular vectors</font>
<font color=navy>// <b>for</b> analysis, with K=1.</font>
   ssamodel s;
   real_1d_array x = <font color=blue><b>&quot;[0,0.5,1,1,1.5,2]&quot;</b></font>;
<font color=navy>// First, we create SSA model, set its properties and add dataset.</font>
<font color=navy>//</font>
<font color=navy>// We use window with width=3 and configure model to use direct SSA</font>
<font color=navy>// algorithm - one which runs exact O(N*W^2) analysis - to extract</font>
<font color=navy>// one top singular vector. Well, it is toy problem :)</font>
<font color=navy>//</font>
<font color=navy>// NOTE: SSA model may store and analyze more than one sequence</font>
<font color=navy>//       (say, different sequences may correspond to data collected</font>
<font color=navy>//       from different devices)</font>
   ssacreate(s);
   ssasetwindow(s, 3);
   ssaaddsequence(s, x);
   ssasetalgotopkdirect(s, 1);
<font color=navy>// Now we begin analysis. Internally SSA model stores everything it needs:</font>
<font color=navy>// data, settings, solvers and so on. Right after first call to analysis-</font>
<font color=navy>// related function it will analyze dataset, build basis and perform analysis.</font>
<font color=navy>//</font>
<font color=navy>// Subsequent calls to analysis functions will reuse previously computed</font>
<font color=navy>// basis, unless you invalidate it by changing model settings (or dataset).</font>
   real_1d_array trend;
   real_1d_array noise;
   ssaanalyzesequence(s, x, trend, noise);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, trend.tostring(2).c_str()); <font color=navy>// EXPECTED: [0.3815,0.5582,0.7810,1.0794,1.5041,2.0105]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_ssa_d_forecast></a><h6 class=pageheader>ssa_d_forecast Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Here we demonstrate SSA forecasting on some toy problem with clearly</font>
<font color=navy>// visible linear trend and small amount of noise.</font>
   ssamodel s;
   real_1d_array x = <font color=blue><b>&quot;[0.05,0.96,2.04,3.11,3.97,5.03,5.98,7.02,8.02]&quot;</b></font>;
<font color=navy>// First, we create SSA model, set its properties and add dataset.</font>
<font color=navy>//</font>
<font color=navy>// We use window with width=3 and configure model to use direct SSA</font>
<font color=navy>// algorithm - one which runs exact O(N*W^2) analysis - to extract</font>
<font color=navy>// two top singular vectors. Well, it is toy problem :)</font>
<font color=navy>//</font>
<font color=navy>// NOTE: SSA model may store and analyze more than one sequence</font>
<font color=navy>//       (say, different sequences may correspond to data collected</font>
<font color=navy>//       from different devices)</font>
   ssacreate(s);
   ssasetwindow(s, 3);
   ssaaddsequence(s, x);
   ssasetalgotopkdirect(s, 2);
<font color=navy>// Now we begin analysis. Internally SSA model stores everything it needs:</font>
<font color=navy>// data, settings, solvers and so on. Right after first call to analysis-</font>
<font color=navy>// related function it will analyze dataset, build basis and perform analysis.</font>
<font color=navy>//</font>
<font color=navy>// Subsequent calls to analysis functions will reuse previously computed</font>
<font color=navy>// basis, unless you invalidate it by changing model settings (or dataset).</font>
<font color=navy>//</font>
<font color=navy>// In this example we show how to use ssaforecastlast() function, which</font>
<font color=navy>// predicts changed in the last sequence of the dataset. If you want to</font>
<font color=navy>// perform prediction <b>for</b> some other sequence, use ssaforecastsequence().</font>
   real_1d_array trend;
   ssaforecastlast(s, 3, trend);
<font color=navy>// Well, we expected it to be [9,10,11]. There exists some difference,</font>
<font color=navy>// which can be explained by the artificial noise in the dataset.</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, trend.tostring(2).c_str()); <font color=navy>// EXPECTED: [9.0005,9.9322,10.8051]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_ssa_d_realtime></a><h6 class=pageheader>ssa_d_realtime Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DataAnalysis.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Suppose that you have a constant stream of incoming data, and you want</font>
<font color=navy>// to regularly perform singular spectral analysis of this stream.</font>
<font color=navy>//</font>
<font color=navy>// One full run of direct algorithm costs O(N*Width^2) operations, so</font>
<font color=navy>// the more points you have, the more it costs to rebuild basis from</font>
<font color=navy>// scratch.</font>
<font color=navy>// </font>
<font color=navy>// Luckily we have incremental SSA algorithm which can perform quick</font>
<font color=navy>// updates of already computed basis in O(K*Width^2) ops, where K</font>
<font color=navy>// is a number of singular vectors extracted. Usually it is orders of</font>
<font color=navy>// magnitude faster than full update of the basis.</font>
<font color=navy>//</font>
<font color=navy>// In this example we start from some initial dataset x0. Then we</font>
<font color=navy>// start appending elements one by one to the end of the last sequence.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: direct algorithm also supports incremental updates, but</font>
<font color=navy>//       with O(Width^3) cost. Typically K &lt;&lt; Width, so specialized</font>
<font color=navy>//       incremental algorithm is still faster.</font>
   ssamodel s1;
   real_2d_array a1;
   real_1d_array sv1;
   ae_int_t w;
   ae_int_t k;
   real_1d_array x0 = <font color=blue><b>&quot;[0.009,0.976,1.999,2.984,3.977,5.002]&quot;</b></font>;
   ssacreate(s1);
   ssasetwindow(s1, 3);
   ssaaddsequence(s1, x0);

<font color=navy>// set algorithm to the real-time version of top-K, K=2</font>
   ssasetalgotopkrealtime(s1, 2);

<font color=navy>// one more interesting feature of the incremental algorithm is <font color=blue><b>&quot;power-up&quot;</b></font> cycle.</font>
<font color=navy>// even with incremental algorithm initial basis calculation costs O(N*Width^2) ops.</font>
<font color=navy>// <b>if</b> such startup cost is too high <b>for</b> your real-time app, then you may divide</font>
<font color=navy>// initial basis calculation across several model updates. It results in better</font>
<font color=navy>// latency at the price of somewhat lesser precision during first few updates.</font>
   ssasetpoweruplength(s1, 3);

<font color=navy>// now, after we prepared everything, start to add incoming points one by one;</font>
<font color=navy>// in the real life, of course, we will perform some work between subsequent update</font>
<font color=navy>// (analyze something, predict, and so on).</font>
<font color=navy>//</font>
<font color=navy>// After each append we perform one iteration of the real-time solver. Usually</font>
<font color=navy>// one iteration is more than enough to update basis. If you have REALLY tight</font>
<font color=navy>// performance constraints, you may specify fractional amount of iterations,</font>
<font color=navy>// which means that iteration is performed with required probability.</font>
   <b>double</b> updateits = 1.0;
   ssaappendpointandupdate(s1, 5.951, updateits);
   ssagetbasis(s1, a1, sv1, w, k);

   ssaappendpointandupdate(s1, 7.074, updateits);
   ssagetbasis(s1, a1, sv1, w, k);

   ssaappendpointandupdate(s1, 7.925, updateits);
   ssagetbasis(s1, a1, sv1, w, k);

   ssaappendpointandupdate(s1, 8.992, updateits);
   ssagetbasis(s1, a1, sv1, w, k);

   ssaappendpointandupdate(s1, 9.942, updateits);
   ssagetbasis(s1, a1, sv1, w, k);

   ssaappendpointandupdate(s1, 11.051, updateits);
   ssagetbasis(s1, a1, sv1, w, k);

   ssaappendpointandupdate(s1, 11.965, updateits);
   ssagetbasis(s1, a1, sv1, w, k);

   ssaappendpointandupdate(s1, 13.047, updateits);
   ssagetbasis(s1, a1, sv1, w, k);

   ssaappendpointandupdate(s1, 13.970, updateits);
   ssagetbasis(s1, a1, sv1, w, k);

<font color=navy>// Ok, we have our basis in a1[] and singular values at sv1[].</font>
<font color=navy>// But is it good enough? Let's print it.</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a1.tostring(3).c_str()); <font color=navy>// EXPECTED: [[0.510607,0.753611],[0.575201,0.058445],[0.639081,-0.654717]]</font>

<font color=navy>// Ok, two vectors with 3 components each.</font>
<font color=navy>// But how to understand that is it really good basis?</font>
<font color=navy>// Let's compare it with direct SSA algorithm on the entire sequence.</font>
   ssamodel s2;
   real_2d_array a2;
   real_1d_array sv2;
   real_1d_array x2 = <font color=blue><b>&quot;[0.009,0.976,1.999,2.984,3.977,5.002,5.951,7.074,7.925,8.992,9.942,11.051,11.965,13.047,13.970]&quot;</b></font>;
   ssacreate(s2);
   ssasetwindow(s2, 3);
   ssaaddsequence(s2, x2);
   ssasetalgotopkdirect(s2, 2);
   ssagetbasis(s2, a2, sv2, w, k);

<font color=navy>// it is exactly the same as one calculated with incremental approach!</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a2.tostring(3).c_str()); <font color=navy>// EXPECTED: [[0.510607,0.753611],[0.575201,0.058445],[0.639081,-0.654717]]</font>
   <b>return</b> 0;
}
</pre>
</p>
<p>
<a name=pck_DiffEquations class=sheader></a><h3>8.3. DiffEquations Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_odesolver class=toc>odesolver</a></td><td>Ordinary differential equation solver</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_odesolver></a><h4 class=pageheader>8.3.1. odesolver Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_odesolverreport class=toc>odesolverreport</a> |
<a href=#struct_odesolverstate class=toc>odesolverstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_odesolverresults class=toc>odesolverresults</a> |
<a href=#sub_odesolverrkck class=toc>odesolverrkck</a> |
<a href=#sub_odesolversolve class=toc>odesolversolve</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_odesolver_d1 class=toc>odesolver_d1</a></td><td width=15>&nbsp;</td><td>Solving y'=-y with ODE solver</td></tr>
</table>
</div>
<a name=struct_odesolverreport></a><h6 class=pageheader>odesolverreport Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> odesolverreport {
   ae_int_t nfev;
   ae_int_t terminationtype;
};
</pre>
<a name=struct_odesolverstate></a><h6 class=pageheader>odesolverstate Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> odesolverstate {
   real_1d_array y;
   real_1d_array dy;
   double x;
};
</pre>
<a name=sub_odesolverresults></a><h6 class=pageheader>odesolverresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
ODE solver results

Called after OdeSolverIteration returned False.

Inputs:
    State   -   algorithm state (used by OdeSolverIteration).

Outputs:
    M       -   number of tabulated values, M &ge; 1
    XTbl    -   array[0..M-1], values of X
    YTbl    -   array[0..M-1,0..N-1], values of Y in X[i]
    Rep     -   solver report:
                * Rep.TerminationType completetion code:
                    * -2    X is not ordered  by  ascending/descending  or
                            there are non-distinct X[],  i.e.  X[i]=X[i+1]
                    * -1    incorrect parameters were specified
                    *  1    task has been solved
                * Rep.NFEV contains number of function calculations
ALGLIB: Copyright 01.09.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> odesolverresults(odesolverstate state, ae_int_t &amp;m, real_1d_array &amp;xtbl, real_2d_array &amp;ytbl, odesolverreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_odesolver_d1 class=nav>odesolver_d1</a> ]</p>
<a name=sub_odesolverrkck></a><h6 class=pageheader>odesolverrkck Function</h6>
<hr width=600 align=left>
<pre class=narration>
Cash-Karp adaptive ODE solver.

This subroutine solves ODE  Y'=f(Y,x)  with  initial  conditions  Y(xs)=Ys
(here Y may be single variable or vector of N variables).

Inputs:
    Y       -   initial conditions, array[0..N-1].
                contains values of Y[] at X[0]
    N       -   system size
    X       -   points at which Y should be tabulated, array[0..M-1]
                integrations starts at X[0], ends at X[M-1],  intermediate
                values at X[i] are returned too.
                SHOULD BE ORDERED BY ASCENDING OR BY DESCENDING!
    M       -   number of intermediate points + first point + last point:
                * M &gt; 2 means that you need both Y(X[M-1]) and M-2 values at
                  intermediate points
                * M=2 means that you want just to integrate from  X[0]  to
                  X[1] and don't interested in intermediate values.
                * M=1 means that you don't want to integrate :)
                  it is degenerate case, but it will be handled correctly.
                * M &lt; 1 means error
    Eps     -   tolerance (absolute/relative error on each  step  will  be
                less than Eps). When passing:
                * Eps &gt; 0, it means desired ABSOLUTE error
                * Eps &lt; 0, it means desired RELATIVE error.  Relative errors
                  are calculated with respect to maximum values of  Y seen
                  so far. Be careful to use this criterion  when  starting
                  from Y[] that are close to zero.
    H       -   initial  step  lenth,  it  will  be adjusted automatically
                after the first  step.  If  H=0,  step  will  be  selected
                automatically  (usualy  it  will  be  equal  to  0.001  of
                min(x[i]-x[j])).

Outputs:
    State   -   structure which stores algorithm state between  subsequent
                calls of OdeSolverIteration. Used for reverse communication.
                This structure should be passed  to the OdeSolverIteration
                subroutine.

SEE ALSO
    AutoGKSmoothW, AutoGKSingular, AutoGKIteration, AutoGKResults.
ALGLIB: Copyright 01.09.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> odesolverrkck(real_1d_array y, ae_int_t n, real_1d_array x, ae_int_t m, <b>double</b> eps, <b>double</b> h, odesolverstate &amp;state);
<b>void</b> odesolverrkck(real_1d_array y, real_1d_array x, <b>double</b> eps, <b>double</b> h, odesolverstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_odesolver_d1 class=nav>odesolver_d1</a> ]</p>
<a name=sub_odesolversolve></a><h6 class=pageheader>odesolversolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to launch iterations of ODE solver

It accepts following parameters:
    diff    -   callback which calculates dy/dx for given y and x
    ptr     -   optional pointer which is passed to diff; can be NULL
ALGLIB: Copyright 01.09.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> odesolversolve(odesolverstate &amp;state, <b>void</b> (*diff)(<b>const</b> real_1d_array &amp;y, <b>double</b> x, real_1d_array &amp;dy, <b>void</b> *ptr), <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_odesolver_d1 class=nav>odesolver_d1</a> ]</p>
<a name=example_odesolver_d1></a><h6 class=pageheader>odesolver_d1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;DiffEquations.h&quot;</b></font>

using namespace alglib;

<b>void</b> ode_function_1_diff(<b>const</b> real_1d_array &amp;y, <b>double</b> x, real_1d_array &amp;dy, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(y[],x)=-y[0]</font>
   dy[0] = -y[0];
}

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_1d_array y = <font color=blue><b>&quot;[1]&quot;</b></font>;
   real_1d_array x = <font color=blue><b>&quot;[0, 1, 2, 3]&quot;</b></font>;
   <b>double</b> eps = 0.00001;
   <b>double</b> h = 0;
   odesolverstate s;
   ae_int_t m;
   real_1d_array xtbl;
   real_2d_array ytbl;
   odesolverreport rep;
   odesolverrkck(y, x, eps, h, s);
   odesolversolve(s, ode_function_1_diff);
   odesolverresults(s, m, xtbl, ytbl, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(m)); <font color=navy>// EXPECTED: 4</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, xtbl.tostring(2).c_str()); <font color=navy>// EXPECTED: [0, 1, 2, 3]</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ytbl.tostring(2).c_str()); <font color=navy>// EXPECTED: [[1], [0.367], [0.135], [0.050]]</font>
   <b>return</b> 0;
}
</pre>
</p>
<p>
<a name=pck_FastTransforms class=sheader></a><h3>8.4. FastTransforms Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_conv class=toc>conv</a></td><td>Fast real/complex convolution</td></tr>
<tr align=left valign=top><td><a href=#unit_corr class=toc>corr</a></td><td>Fast real/complex cross-correlation</td></tr>
<tr align=left valign=top><td><a href=#unit_fft class=toc>fft</a></td><td>Real/complex FFT</td></tr>
<tr align=left valign=top><td><a href=#unit_fht class=toc>fht</a></td><td>Real Fast Hartley Transform</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_conv></a><h4 class=pageheader>8.4.1. conv Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_convc1d class=toc>convc1d</a> |
<a href=#sub_convc1dcircular class=toc>convc1dcircular</a> |
<a href=#sub_convc1dcircularinv class=toc>convc1dcircularinv</a> |
<a href=#sub_convc1dinv class=toc>convc1dinv</a> |
<a href=#sub_convr1d class=toc>convr1d</a> |
<a href=#sub_convr1dcircular class=toc>convr1dcircular</a> |
<a href=#sub_convr1dcircularinv class=toc>convr1dcircularinv</a> |
<a href=#sub_convr1dinv class=toc>convr1dinv</a>
]</font>
</div>
<a name=sub_convc1d></a><h6 class=pageheader>convc1d Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional complex convolution.

For given A/B returns conv(A,B) (non-circular). Subroutine can automatically
choose between three implementations: straightforward O(M*N)  formula  for
very small N (or M), overlap-add algorithm for  cases  where  max(M,N)  is
significantly larger than min(M,N), but O(M*N) algorithm is too slow,  and
general FFT-based formula for cases where two previois algorithms are  too
slow.

Algorithm has max(M,N)*log(max(M,N)) complexity for any M/N.

Inputs:
    A   -   array[0..M-1] - complex function to be transformed
    M   -   problem size
    B   -   array[0..N-1] - complex function to be transformed
    N   -   problem size

Outputs:
    R   -   convolution: A*B. array[0..N+M-2].

NOTE:
    It is assumed that A is zero at T &lt; 0, B is zero too.  If  one  or  both
functions have non-zero values at negative T's, you  can  still  use  this
subroutine - just shift its result correspondingly.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> convc1d(complex_1d_array a, ae_int_t m, complex_1d_array b, ae_int_t n, complex_1d_array &amp;r);
</pre>
<a name=sub_convc1dcircular></a><h6 class=pageheader>convc1dcircular Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional circular complex convolution.

For given S/R returns conv(S,R) (circular). Algorithm has linearithmic
complexity for any M/N.

IMPORTANT:  normal convolution is commutative,  i.e.   it  is symmetric  -
conv(A,B)=conv(B,A).  Cyclic convolution IS NOT.  One function - S - is  a
signal,  periodic function, and another - R - is a response,  non-periodic
function with limited length.

Inputs:
    S   -   array[0..M-1] - complex periodic signal
    M   -   problem size
    B   -   array[0..N-1] - complex non-periodic response
    N   -   problem size

Outputs:
    R   -   convolution: A*B. array[0..M-1].

NOTE:
    It is assumed that B is zero at T &lt; 0. If  it  has  non-zero  values  at
negative T's, you can still use this subroutine - just  shift  its  result
correspondingly.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> convc1dcircular(complex_1d_array s, ae_int_t m, complex_1d_array r, ae_int_t n, complex_1d_array &amp;c);
</pre>
<a name=sub_convc1dcircularinv></a><h6 class=pageheader>convc1dcircularinv Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional circular complex deconvolution (inverse of ConvC1DCircular()).

Algorithm has M*log(M)) complexity for any M (composite or prime).

Inputs:
    A   -   array[0..M-1] - convolved periodic signal, A = conv(R, B)
    M   -   convolved signal length
    B   -   array[0..N-1] - non-periodic response
    N   -   response length

Outputs:
    R   -   deconvolved signal. array[0..M-1].

NOTE:
    deconvolution is unstable process and may result in division  by  zero
(if your response function is degenerate, i.e. has zero Fourier coefficient).

NOTE:
    It is assumed that B is zero at T &lt; 0. If  it  has  non-zero  values  at
negative T's, you can still use this subroutine - just  shift  its  result
correspondingly.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> convc1dcircularinv(complex_1d_array a, ae_int_t m, complex_1d_array b, ae_int_t n, complex_1d_array &amp;r);
</pre>
<a name=sub_convc1dinv></a><h6 class=pageheader>convc1dinv Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional complex non-circular deconvolution (inverse of ConvC1D()).

Algorithm has M*log(M)) complexity for any M (composite or prime).

Inputs:
    A   -   array[0..M-1] - convolved signal, A = conv(R, B)
    M   -   convolved signal length
    B   -   array[0..N-1] - response
    N   -   response length, N &le; M

Outputs:
    R   -   deconvolved signal. array[0..M-N].

NOTE:
    deconvolution is unstable process and may result in division  by  zero
(if your response function is degenerate, i.e. has zero Fourier coefficient).

NOTE:
    It is assumed that A is zero at T &lt; 0, B is zero too.  If  one  or  both
functions have non-zero values at negative T's, you  can  still  use  this
subroutine - just shift its result correspondingly.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> convc1dinv(complex_1d_array a, ae_int_t m, complex_1d_array b, ae_int_t n, complex_1d_array &amp;r);
</pre>
<a name=sub_convr1d></a><h6 class=pageheader>convr1d Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional real convolution.

Analogous to ConvC1D(), see ConvC1D() comments for more details.

Inputs:
    A   -   array[0..M-1] - real function to be transformed
    M   -   problem size
    B   -   array[0..N-1] - real function to be transformed
    N   -   problem size

Outputs:
    R   -   convolution: A*B. array[0..N+M-2].

NOTE:
    It is assumed that A is zero at T &lt; 0, B is zero too.  If  one  or  both
functions have non-zero values at negative T's, you  can  still  use  this
subroutine - just shift its result correspondingly.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> convr1d(real_1d_array a, ae_int_t m, real_1d_array b, ae_int_t n, real_1d_array &amp;r);
</pre>
<a name=sub_convr1dcircular></a><h6 class=pageheader>convr1dcircular Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional circular real convolution.

Analogous to ConvC1DCircular(), see ConvC1DCircular() comments for more details.

Inputs:
    S   -   array[0..M-1] - real signal
    M   -   problem size
    B   -   array[0..N-1] - real response
    N   -   problem size

Outputs:
    R   -   convolution: A*B. array[0..M-1].

NOTE:
    It is assumed that B is zero at T &lt; 0. If  it  has  non-zero  values  at
negative T's, you can still use this subroutine - just  shift  its  result
correspondingly.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> convr1dcircular(real_1d_array s, ae_int_t m, real_1d_array r, ae_int_t n, real_1d_array &amp;c);
</pre>
<a name=sub_convr1dcircularinv></a><h6 class=pageheader>convr1dcircularinv Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional complex deconvolution (inverse of ConvC1D()).

Algorithm has M*log(M)) complexity for any M (composite or prime).

Inputs:
    A   -   array[0..M-1] - convolved signal, A = conv(R, B)
    M   -   convolved signal length
    B   -   array[0..N-1] - response
    N   -   response length

Outputs:
    R   -   deconvolved signal. array[0..M-N].

NOTE:
    deconvolution is unstable process and may result in division  by  zero
(if your response function is degenerate, i.e. has zero Fourier coefficient).

NOTE:
    It is assumed that B is zero at T &lt; 0. If  it  has  non-zero  values  at
negative T's, you can still use this subroutine - just  shift  its  result
correspondingly.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> convr1dcircularinv(real_1d_array a, ae_int_t m, real_1d_array b, ae_int_t n, real_1d_array &amp;r);
</pre>
<a name=sub_convr1dinv></a><h6 class=pageheader>convr1dinv Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional real deconvolution (inverse of ConvC1D()).

Algorithm has M*log(M)) complexity for any M (composite or prime).

Inputs:
    A   -   array[0..M-1] - convolved signal, A = conv(R, B)
    M   -   convolved signal length
    B   -   array[0..N-1] - response
    N   -   response length, N &le; M

Outputs:
    R   -   deconvolved signal. array[0..M-N].

NOTE:
    deconvolution is unstable process and may result in division  by  zero
(if your response function is degenerate, i.e. has zero Fourier coefficient).

NOTE:
    It is assumed that A is zero at T &lt; 0, B is zero too.  If  one  or  both
functions have non-zero values at negative T's, you  can  still  use  this
subroutine - just shift its result correspondingly.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> convr1dinv(real_1d_array a, ae_int_t m, real_1d_array b, ae_int_t n, real_1d_array &amp;r);
</pre>
<a name=unit_corr></a><h4 class=pageheader>8.4.2. corr Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_corrc1d class=toc>corrc1d</a> |
<a href=#sub_corrc1dcircular class=toc>corrc1dcircular</a> |
<a href=#sub_corrr1d class=toc>corrr1d</a> |
<a href=#sub_corrr1dcircular class=toc>corrr1dcircular</a>
]</font>
</div>
<a name=sub_corrc1d></a><h6 class=pageheader>corrc1d Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional complex cross-correlation.

For given Pattern/Signal returns corr(Pattern,Signal) (non-circular).

Correlation is calculated using reduction to  convolution.  Algorithm with
max(N,N)*log(max(N,N)) complexity is used (see  ConvC1D()  for  more  info
about performance).

IMPORTANT:
    for  historical reasons subroutine accepts its parameters in  reversed
    order: CorrC1D(Signal, Pattern) = Pattern x Signal (using  traditional
    definition of cross-correlation, denoting cross-correlation as &quot;x&quot;).

Inputs:
    Signal  -   array[0..N-1] - complex function to be transformed,
                signal containing pattern
    N       -   problem size
    Pattern -   array[0..M-1] - complex function to be transformed,
                pattern to search withing signal
    M       -   problem size

Outputs:
    R       -   cross-correlation, array[0..N+M-2]:
                * positive lags are stored in R[0..N-1],
                  R[i] = sum(conj(pattern[j])*signal[i+j]
                * negative lags are stored in R[N..N+M-2],
                  R[N+M-1-i] = sum(conj(pattern[j])*signal[-i+j]

NOTE:
    It is assumed that pattern domain is [0..M-1].  If Pattern is non-zero
on [-K..M-1],  you can still use this subroutine, just shift result by K.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> corrc1d(complex_1d_array signal, ae_int_t n, complex_1d_array pattern, ae_int_t m, complex_1d_array &amp;r);
</pre>
<a name=sub_corrc1dcircular></a><h6 class=pageheader>corrc1dcircular Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional circular complex cross-correlation.

For given Pattern/Signal returns corr(Pattern,Signal) (circular).
Algorithm has linearithmic complexity for any M/N.

IMPORTANT:
    for  historical reasons subroutine accepts its parameters in  reversed
    order:   CorrC1DCircular(Signal, Pattern) = Pattern x Signal    (using
    traditional definition of cross-correlation, denoting cross-correlation
    as &quot;x&quot;).

Inputs:
    Signal  -   array[0..N-1] - complex function to be transformed,
                periodic signal containing pattern
    N       -   problem size
    Pattern -   array[0..M-1] - complex function to be transformed,
                non-periodic pattern to search withing signal
    M       -   problem size

Outputs:
    R   -   convolution: A*B. array[0..M-1].
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> corrc1dcircular(complex_1d_array signal, ae_int_t m, complex_1d_array pattern, ae_int_t n, complex_1d_array &amp;c);
</pre>
<a name=sub_corrr1d></a><h6 class=pageheader>corrr1d Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional real cross-correlation.

For given Pattern/Signal returns corr(Pattern,Signal) (non-circular).

Correlation is calculated using reduction to  convolution.  Algorithm with
max(N,N)*log(max(N,N)) complexity is used (see  ConvC1D()  for  more  info
about performance).

IMPORTANT:
    for  historical reasons subroutine accepts its parameters in  reversed
    order: CorrR1D(Signal, Pattern) = Pattern x Signal (using  traditional
    definition of cross-correlation, denoting cross-correlation as &quot;x&quot;).

Inputs:
    Signal  -   array[0..N-1] - real function to be transformed,
                signal containing pattern
    N       -   problem size
    Pattern -   array[0..M-1] - real function to be transformed,
                pattern to search withing signal
    M       -   problem size

Outputs:
    R       -   cross-correlation, array[0..N+M-2]:
                * positive lags are stored in R[0..N-1],
                  R[i] = sum(pattern[j]*signal[i+j]
                * negative lags are stored in R[N..N+M-2],
                  R[N+M-1-i] = sum(pattern[j]*signal[-i+j]

NOTE:
    It is assumed that pattern domain is [0..M-1].  If Pattern is non-zero
on [-K..M-1],  you can still use this subroutine, just shift result by K.
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> corrr1d(real_1d_array signal, ae_int_t n, real_1d_array pattern, ae_int_t m, real_1d_array &amp;r);
</pre>
<a name=sub_corrr1dcircular></a><h6 class=pageheader>corrr1dcircular Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional circular real cross-correlation.

For given Pattern/Signal returns corr(Pattern,Signal) (circular).
Algorithm has linearithmic complexity for any M/N.

IMPORTANT:
    for  historical reasons subroutine accepts its parameters in  reversed
    order:   CorrR1DCircular(Signal, Pattern) = Pattern x Signal    (using
    traditional definition of cross-correlation, denoting cross-correlation
    as &quot;x&quot;).

Inputs:
    Signal  -   array[0..N-1] - real function to be transformed,
                periodic signal containing pattern
    N       -   problem size
    Pattern -   array[0..M-1] - real function to be transformed,
                non-periodic pattern to search withing signal
    M       -   problem size

Outputs:
    R   -   convolution: A*B. array[0..M-1].
ALGLIB: Copyright 21.07.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> corrr1dcircular(real_1d_array signal, ae_int_t m, real_1d_array pattern, ae_int_t n, real_1d_array &amp;c);
</pre>
<a name=unit_fft></a><h4 class=pageheader>8.4.3. fft Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_fftc1d class=toc>fftc1d</a> |
<a href=#sub_fftc1dinv class=toc>fftc1dinv</a> |
<a href=#sub_fftr1d class=toc>fftr1d</a> |
<a href=#sub_fftr1dinv class=toc>fftr1dinv</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_fft_complex_d1 class=toc>fft_complex_d1</a></td><td width=15>&nbsp;</td><td>Complex FFT: simple example</td></tr>
<tr align=left valign=top><td><a href=#example_fft_complex_d2 class=toc>fft_complex_d2</a></td><td width=15>&nbsp;</td><td>Complex FFT: advanced example</td></tr>
<tr align=left valign=top><td><a href=#example_fft_real_d1 class=toc>fft_real_d1</a></td><td width=15>&nbsp;</td><td>Real FFT: simple example</td></tr>
<tr align=left valign=top><td><a href=#example_fft_real_d2 class=toc>fft_real_d2</a></td><td width=15>&nbsp;</td><td>Real FFT: advanced example</td></tr>
</table>
</div>
<a name=sub_fftc1d></a><h6 class=pageheader>fftc1d Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional complex FFT.

Array size N may be arbitrary number (composite or prime).  Composite  N's
are handled with cache-oblivious variation of  a  Cooley-Tukey  algorithm.
Small prime-factors are transformed using hard coded  codelets (similar to
FFTW codelets, but without low-level  optimization),  large  prime-factors
are handled with Bluestein's algorithm.

Fastests transforms are for smooth N's (prime factors are 2, 3,  5  only),
most fast for powers of 2. When N have prime factors  larger  than  these,
but orders of magnitude smaller than N, computations will be about 4 times
slower than for nearby highly composite N's. When N itself is prime, speed
will be 6 times lower.

Algorithm has O(N*logN) complexity for any N (composite or prime).

Inputs:
    A   -   array[0..N-1] - complex function to be transformed
    N   -   problem size

Outputs:
    A   -   DFT of a input array, array[0..N-1]
            A_out[j] = SUM(A_in[k]*exp(-2*pi*sqrt(-1)*j*k/N), k = 0..N-1)
ALGLIB: Copyright 29.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fftc1d(complex_1d_array &amp;a, ae_int_t n);
<b>void</b> fftc1d(complex_1d_array &amp;a);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_fft_complex_d1 class=nav>fft_complex_d1</a> | <a href=#example_fft_complex_d2 class=nav>fft_complex_d2</a> ]</p>
<a name=sub_fftc1dinv></a><h6 class=pageheader>fftc1dinv Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional complex inverse FFT.

Array size N may be arbitrary number (composite or prime).  Algorithm  has
O(N*logN) complexity for any N (composite or prime).

See FFTC1D() description for more information about algorithm performance.

Inputs:
    A   -   array[0..N-1] - complex array to be transformed
    N   -   problem size

Outputs:
    A   -   inverse DFT of a input array, array[0..N-1]
            A_out[j] = SUM(A_in[k]/N*exp(+2*pi*sqrt(-1)*j*k/N), k = 0..N-1)
ALGLIB: Copyright 29.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fftc1dinv(complex_1d_array &amp;a, ae_int_t n);
<b>void</b> fftc1dinv(complex_1d_array &amp;a);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_fft_complex_d1 class=nav>fft_complex_d1</a> | <a href=#example_fft_complex_d2 class=nav>fft_complex_d2</a> ]</p>
<a name=sub_fftr1d></a><h6 class=pageheader>fftr1d Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional real FFT.

Algorithm has O(N*logN) complexity for any N (composite or prime).

Inputs:
    A   -   array[0..N-1] - real function to be transformed
    N   -   problem size

Outputs:
    F   -   DFT of a input array, array[0..N-1]
            F[j] = SUM(A[k]*exp(-2*pi*sqrt(-1)*j*k/N), k = 0..N-1)

NOTE:
    F[] satisfies symmetry property F[k] = conj(F[N-k]),  so just one half
of  array  is  usually needed. But for convinience subroutine returns full
complex array (with frequencies above N/2), so its result may be  used  by
other FFT-related subroutines.
ALGLIB: Copyright 01.06.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fftr1d(real_1d_array a, ae_int_t n, complex_1d_array &amp;f);
<b>void</b> fftr1d(real_1d_array a, complex_1d_array &amp;f);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_fft_real_d1 class=nav>fft_real_d1</a> | <a href=#example_fft_real_d2 class=nav>fft_real_d2</a> ]</p>
<a name=sub_fftr1dinv></a><h6 class=pageheader>fftr1dinv Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional real inverse FFT.

Algorithm has O(N*logN) complexity for any N (composite or prime).

Inputs:
    F   -   array[0..floor(N/2)] - frequencies from forward real FFT
    N   -   problem size

Outputs:
    A   -   inverse DFT of a input array, array[0..N-1]

NOTE:
    F[] should satisfy symmetry property F[k] = conj(F[N-k]), so just  one
half of frequencies array is needed - elements from 0 to floor(N/2).  F[0]
is ALWAYS real. If N is even F[floor(N/2)] is real too. If N is odd,  then
F[floor(N/2)] has no special properties.

Relying on properties noted above, FFTR1DInv subroutine uses only elements
from 0th to floor(N/2)-th. It ignores imaginary part of F[0],  and in case
N is even it ignores imaginary part of F[floor(N/2)] too.

When you call this function using full arguments list - &quot;FFTR1DInv(F,N,A)&quot;
- you can pass either either frequencies array with N elements or  reduced
array with roughly N/2 elements - subroutine will  successfully  transform
both.

If you call this function using reduced arguments list -  &quot;FFTR1DInv(F,A)&quot;
- you must pass FULL array with N elements (although higher  N/2 are still
not used) because array size is used to automatically determine FFT length
ALGLIB: Copyright 01.06.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fftr1dinv(complex_1d_array f, ae_int_t n, real_1d_array &amp;a);
<b>void</b> fftr1dinv(complex_1d_array f, real_1d_array &amp;a);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_fft_real_d1 class=nav>fft_real_d1</a> | <a href=#example_fft_real_d2 class=nav>fft_real_d2</a> ]</p>
<a name=example_fft_complex_d1></a><h6 class=pageheader>fft_complex_d1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;FastTransforms.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// first we demonstrate forward FFT:</font>
<font color=navy>// [1i,1i,1i,1i] is converted to [4i, 0, 0, 0]</font>
   complex_1d_array z = <font color=blue><b>&quot;[1i,1i,1i,1i]&quot;</b></font>;
   fftc1d(z);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, z.tostring(3).c_str()); <font color=navy>// EXPECTED: [4i,0,0,0]</font>
<font color=navy>//</font>
<font color=navy>// now we convert [4i, 0, 0, 0] back to [1i,1i,1i,1i]</font>
<font color=navy>// with backward FFT</font>
   fftc1dinv(z);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, z.tostring(3).c_str()); <font color=navy>// EXPECTED: [1i,1i,1i,1i]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_fft_complex_d2></a><h6 class=pageheader>fft_complex_d2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;FastTransforms.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// first we demonstrate forward FFT:</font>
<font color=navy>// [0,1,0,1i] is converted to [1+1i, -1-1i, -1-1i, 1+1i]</font>
   complex_1d_array z = <font color=blue><b>&quot;[0,1,0,1i]&quot;</b></font>;
   fftc1d(z);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, z.tostring(3).c_str()); <font color=navy>// EXPECTED: [1+1i, -1-1i, -1-1i, 1+1i]</font>
<font color=navy>//</font>
<font color=navy>// now we convert result back with backward FFT</font>
   fftc1dinv(z);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, z.tostring(3).c_str()); <font color=navy>// EXPECTED: [0,1,0,1i]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_fft_real_d1></a><h6 class=pageheader>fft_real_d1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;FastTransforms.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// first we demonstrate forward FFT:</font>
<font color=navy>// [1,1,1,1] is converted to [4, 0, 0, 0]</font>
   real_1d_array x = <font color=blue><b>&quot;[1,1,1,1]&quot;</b></font>;
   complex_1d_array f;
   real_1d_array x2;
   fftr1d(x, f);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, f.tostring(3).c_str()); <font color=navy>// EXPECTED: [4,0,0,0]</font>
<font color=navy>//</font>
<font color=navy>// now we convert [4, 0, 0, 0] back to [1,1,1,1]</font>
<font color=navy>// with backward FFT</font>
   fftr1dinv(f, x2);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x2.tostring(3).c_str()); <font color=navy>// EXPECTED: [1,1,1,1]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_fft_real_d2></a><h6 class=pageheader>fft_real_d2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;FastTransforms.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// first we demonstrate forward FFT:</font>
<font color=navy>// [1,2,3,4] is converted to [10, -2+2i, -2, -2-2i]</font>
<font color=navy>//</font>
<font color=navy>// note that output array is self-adjoint:</font>
<font color=navy>// * f[0] = conj(f[0])</font>
<font color=navy>// * f[1] = conj(f[3])</font>
<font color=navy>// * f[2] = conj(f[2])</font>
   real_1d_array x = <font color=blue><b>&quot;[1,2,3,4]&quot;</b></font>;
   complex_1d_array f;
   real_1d_array x2;
   fftr1d(x, f);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, f.tostring(3).c_str()); <font color=navy>// EXPECTED: [10, -2+2i, -2, -2-2i]</font>
<font color=navy>//</font>
<font color=navy>// now we convert [10, -2+2i, -2, -2-2i] back to [1,2,3,4]</font>
   fftr1dinv(f, x2);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x2.tostring(3).c_str()); <font color=navy>// EXPECTED: [1,2,3,4]</font>
<font color=navy>//</font>
<font color=navy>// remember that F is self-adjoint? It means that we can pass just half</font>
<font color=navy>// (slightly larger than half) of F to inverse real FFT and still get our result.</font>
<font color=navy>//</font>
<font color=navy>// I.e. instead [10, -2+2i, -2, -2-2i] we pass just [10, -2+2i, -2] and everything works!</font>
<font color=navy>//</font>
<font color=navy>// NOTE: in this case we should explicitly pass array length (which is 4) to ALGLIB;</font>
<font color=navy>// <b>if</b> not, it will automatically use array length to determine FFT size and</font>
<font color=navy>// will erroneously make half-length FFT.</font>
   f = <font color=blue><b>&quot;[10, -2+2i, -2]&quot;</b></font>;
   fftr1dinv(f, 4, x2);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x2.tostring(3).c_str()); <font color=navy>// EXPECTED: [1,2,3,4]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_fht></a><h4 class=pageheader>8.4.4. fht Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_fhtr1d class=toc>fhtr1d</a> |
<a href=#sub_fhtr1dinv class=toc>fhtr1dinv</a>
]</font>
</div>
<a name=sub_fhtr1d></a><h6 class=pageheader>fhtr1d Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional Fast Hartley Transform.

Algorithm has O(N*logN) complexity for any N (composite or prime).

Inputs:
    A   -   array[0..N-1] - real function to be transformed
    N   -   problem size

Outputs:
    A   -   FHT of a input array, array[0..N-1],
            A_out[k] = sum(A_in[j]*(cos(2*pi*j*k/N)+sin(2*pi*j*k/N)), j=0..N-1)
ALGLIB: Copyright 04.06.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fhtr1d(real_1d_array &amp;a, ae_int_t n);
</pre>
<a name=sub_fhtr1dinv></a><h6 class=pageheader>fhtr1dinv Function</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional inverse FHT.

Algorithm has O(N*logN) complexity for any N (composite or prime).

Inputs:
    A   -   array[0..N-1] - complex array to be transformed
    N   -   problem size

Outputs:
    A   -   inverse FHT of a input array, array[0..N-1]
ALGLIB: Copyright 29.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fhtr1dinv(real_1d_array &amp;a, ae_int_t n);
</pre>
</p>
<p>
<a name=pck_Integration class=sheader></a><h3>8.5. Integration Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_autogk class=toc>autogk</a></td><td>Adaptive 1-dimensional integration</td></tr>
<tr align=left valign=top><td><a href=#unit_gkq class=toc>gkq</a></td><td>Gauss-Kronrod quadrature generator</td></tr>
<tr align=left valign=top><td><a href=#unit_gq class=toc>gq</a></td><td>Gaussian quadrature generator</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_autogk></a><h4 class=pageheader>8.5.1. autogk Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_autogkreport class=toc>autogkreport</a> |
<a href=#struct_autogkstate class=toc>autogkstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_autogkintegrate class=toc>autogkintegrate</a> |
<a href=#sub_autogkresults class=toc>autogkresults</a> |
<a href=#sub_autogksingular class=toc>autogksingular</a> |
<a href=#sub_autogksmooth class=toc>autogksmooth</a> |
<a href=#sub_autogksmoothw class=toc>autogksmoothw</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_autogk_d1 class=toc>autogk_d1</a></td><td width=15>&nbsp;</td><td>Integrating f=exp(x) by adaptive integrator</td></tr>
</table>
</div>
<a name=struct_autogkreport></a><h6 class=pageheader>autogkreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Integration report:
* TerminationType = completetion code:
    * -5    non-convergence of Gauss-Kronrod nodes
            calculation subroutine.
    * -1    incorrect parameters were specified
    *  1    OK
* Rep.NFEV countains number of function calculations
* Rep.NIntervals contains number of intervals [a,b]
  was partitioned into.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> autogkreport {
   ae_int_t terminationtype;
   ae_int_t nfev;
   ae_int_t nintervals;
};
</pre>
<a name=struct_autogkstate></a><h6 class=pageheader>autogkstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure stores state of the integration algorithm.

Although this class has public fields,  they are not intended for external
use. You should use ALGLIB functions to work with this class:
* autogksmooth()/AutoGKSmoothW()/... to create objects
* autogkintegrate() to begin integration
* autogkresults() to get results
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> autogkstate {
   double x;
   double xminusa;
   double bminusx;
   double f;
};
</pre>
<a name=sub_autogkintegrate></a><h6 class=pageheader>autogkintegrate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to launch iterations of the 1-dimensional integrator

It accepts following parameters:
    func    -   callback which calculates f(x) for given x
    ptr     -   optional pointer which is passed to func; can be NULL
ALGLIB: Copyright 07.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> autogkintegrate(autogkstate &amp;state, <b>void</b> (*func)(<b>double</b> x, <b>double</b> xminusa, <b>double</b> bminusx, <b>double</b> &amp;y, <b>void</b> *ptr), <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_autogk_d1 class=nav>autogk_d1</a> ]</p>
<a name=sub_autogkresults></a><h6 class=pageheader>autogkresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Adaptive integration results

Called after AutoGKIteration returned False.

Inputs:
    State   -   algorithm state (used by AutoGKIteration).

Outputs:
    V       -   integral(f(x)dx,a,b)
    Rep     -   optimization report (see AutoGKReport description)
ALGLIB: Copyright 14.11.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> autogkresults(autogkstate state, <b>double</b> &amp;v, autogkreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_autogk_d1 class=nav>autogk_d1</a> ]</p>
<a name=sub_autogksingular></a><h6 class=pageheader>autogksingular Function</h6>
<hr width=600 align=left>
<pre class=narration>
Integration on a finite interval [A,B].
Integrand have integrable singularities at A/B.

F(X) must diverge as &quot;(x-A)^alpha&quot; at A, as &quot;(B-x)^beta&quot; at B,  with known
alpha/beta (alpha &gt; -1, beta &gt; -1).  If alpha/beta  are  not known,  estimates
from below can be used (but these estimates should be greater than -1 too).

One  of  alpha/beta variables (or even both alpha/beta) may be equal to 0,
which means than function F(x) is non-singular at A/B. Anyway (singular at
bounds or not), function F(x) is supposed to be continuous on (A,B).

Fast-convergent algorithm based on a Gauss-Kronrod formula is used. Result
is calculated with accuracy close to the machine precision.

Inputs:
    A, B    -   interval boundaries (A &lt; B, A = B or A &gt; B)
    Alpha   -   power-law coefficient of the F(x) at A,
                Alpha &gt; -1
    Beta    -   power-law coefficient of the F(x) at B,
                Beta &gt; -1

Outputs:
    State   -   structure which stores algorithm state

SEE ALSO
    AutoGKSmooth, AutoGKSmoothW, AutoGKResults.
ALGLIB: Copyright 06.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> autogksingular(<b>double</b> a, <b>double</b> b, <b>double</b> alpha, <b>double</b> beta, autogkstate &amp;state);
</pre>
<a name=sub_autogksmooth></a><h6 class=pageheader>autogksmooth Function</h6>
<hr width=600 align=left>
<pre class=narration>
Integration of a smooth function F(x) on a finite interval [a,b].

Fast-convergent algorithm based on a Gauss-Kronrod formula is used. Result
is calculated with accuracy close to the machine precision.

Algorithm works well only with smooth integrands.  It  may  be  used  with
continuous non-smooth integrands, but with  less  performance.

It should never be used with integrands which have integrable singularities
at lower or upper limits - algorithm may crash. Use AutoGKSingular in such
cases.

Inputs:
    A, B    -   interval boundaries (A &lt; B, A = B or A &gt; B)

Outputs:
    State   -   structure which stores algorithm state

SEE ALSO
    AutoGKSmoothW, AutoGKSingular, AutoGKResults.
ALGLIB: Copyright 06.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> autogksmooth(<b>double</b> a, <b>double</b> b, autogkstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_autogk_d1 class=nav>autogk_d1</a> ]</p>
<a name=sub_autogksmoothw></a><h6 class=pageheader>autogksmoothw Function</h6>
<hr width=600 align=left>
<pre class=narration>
Integration of a smooth function F(x) on a finite interval [a,b].

This subroutine is same as AutoGKSmooth(), but it guarantees that interval
[a,b] is partitioned into subintervals which have width at most XWidth.

Subroutine  can  be  used  when  integrating nearly-constant function with
narrow &quot;bumps&quot; (about XWidth wide). If &quot;bumps&quot; are too narrow, AutoGKSmooth
subroutine can overlook them.

Inputs:
    A, B    -   interval boundaries (A &lt; B, A = B or A &gt; B)

Outputs:
    State   -   structure which stores algorithm state

SEE ALSO
    AutoGKSmooth, AutoGKSingular, AutoGKResults.
ALGLIB: Copyright 06.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> autogksmoothw(<b>double</b> a, <b>double</b> b, <b>double</b> xwidth, autogkstate &amp;state);
</pre>
<a name=example_autogk_d1></a><h6 class=pageheader>autogk_d1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Integration.h&quot;</b></font>

using namespace alglib;

<b>void</b> int_function_1_func(<b>double</b> x, <b>double</b> xminusa, <b>double</b> bminusx, <b>double</b> &amp;y, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x)=exp(x)</font>
   y = exp(x);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates integration of f=exp(x) on [0,1]:</font>
<font color=navy>// * first, autogkstate is initialized</font>
<font color=navy>// * then we call integration function</font>
<font color=navy>// * and finally we obtain results with autogkresults() call</font>
   <b>double</b> a = 0;
   <b>double</b> b = 1;
   autogkstate s;
   <b>double</b> v;
   autogkreport rep;

   autogksmooth(a, b, s);
   autogkintegrate(s, int_function_1_func);
   autogkresults(s, v, rep);

   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.7182</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_gkq></a><h4 class=pageheader>8.5.2. gkq Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_gkqgenerategaussjacobi class=toc>gkqgenerategaussjacobi</a> |
<a href=#sub_gkqgenerategausslegendre class=toc>gkqgenerategausslegendre</a> |
<a href=#sub_gkqgeneraterec class=toc>gkqgeneraterec</a> |
<a href=#sub_gkqlegendrecalc class=toc>gkqlegendrecalc</a> |
<a href=#sub_gkqlegendretbl class=toc>gkqlegendretbl</a>
]</font>
</div>
<a name=sub_gkqgenerategaussjacobi></a><h6 class=pageheader>gkqgenerategaussjacobi Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns   Gauss   and   Gauss-Kronrod   nodes/weights   for   Gauss-Jacobi
quadrature on [-1,1] with weight function

    W(x)=power(1-x,Alpha)*power(1+x,Beta).

Inputs:
    N           -   number of Kronrod nodes, must be odd number, &ge; 3.
    Alpha       -   power-law coefficient, Alpha &gt; -1
    Beta        -   power-law coefficient, Beta &gt; -1

Outputs:
    Info        -   error code:
                    * -5    no real and positive Gauss-Kronrod formula can
                            be created for such a weight function  with  a
                            given number of nodes.
                    * -4    an  error  was   detected   when   calculating
                            weights/nodes. Alpha or  Beta  are  too  close
                            to -1 to obtain weights/nodes with high enough
                            accuracy, or, may be, N is too large.  Try  to
                            use multiple precision version.
                    * -3    internal eigenproblem solver hasn't converged
                    * -1    incorrect N was passed
                    * +1    OK
                    * +2    OK, but quadrature rule have exterior  nodes,
                            x[0] &lt; -1 or x[n-1] &gt; +1
    X           -   array[0..N-1] - array of quadrature nodes, ordered in
                    ascending order.
    WKronrod    -   array[0..N-1] - Kronrod weights
    WGauss      -   array[0..N-1] - Gauss weights (interleaved with zeros
                    corresponding to extended Kronrod nodes).
ALGLIB: Copyright 12.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gkqgenerategaussjacobi(ae_int_t n, <b>double</b> alpha, <b>double</b> beta, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;wkronrod, real_1d_array &amp;wgauss);
</pre>
<a name=sub_gkqgenerategausslegendre></a><h6 class=pageheader>gkqgenerategausslegendre Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns   Gauss   and   Gauss-Kronrod   nodes/weights  for  Gauss-Legendre
quadrature with N points.

GKQLegendreCalc (calculation) or  GKQLegendreTbl  (precomputed  table)  is
used depending on machine precision and number of nodes.

Inputs:
    N           -   number of Kronrod nodes, must be odd number, &ge; 3.

Outputs:
    Info        -   error code:
                    * -4    an  error   was   detected   when  calculating
                            weights/nodes.  N  is  too  large   to  obtain
                            weights/nodes  with  high   enough   accuracy.
                            Try  to   use   multiple   precision  version.
                    * -3    internal eigenproblem solver hasn't converged
                    * -1    incorrect N was passed
                    * +1    OK
    X           -   array[0..N-1] - array of quadrature nodes, ordered in
                    ascending order.
    WKronrod    -   array[0..N-1] - Kronrod weights
    WGauss      -   array[0..N-1] - Gauss weights (interleaved with zeros
                    corresponding to extended Kronrod nodes).
ALGLIB: Copyright 12.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gkqgenerategausslegendre(ae_int_t n, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;wkronrod, real_1d_array &amp;wgauss);
</pre>
<a name=sub_gkqgeneraterec></a><h6 class=pageheader>gkqgeneraterec Function</h6>
<hr width=600 align=left>
<pre class=narration>
Computation of nodes and weights of a Gauss-Kronrod quadrature formula

The algorithm generates the N-point Gauss-Kronrod quadrature formula  with
weight  function  given  by  coefficients  alpha  and beta of a recurrence
relation which generates a system of orthogonal polynomials:

    P-1(x)   =  0
    P0(x)    =  1
    Pn+1(x)  =  (x-alpha(n))*Pn(x)  -  beta(n)*Pn-1(x)

and zero moment Mu0

    Mu0 = integral(W(x)dx,a,b)

Inputs:
    Alpha       -   alpha coefficients, array[0..floor(3*K/2)].
    Beta        -   beta coefficients,  array[0..ceil(3*K/2)].
                    Beta[0] is not used and may be arbitrary.
                    Beta[I] &gt; 0.
    Mu0         -   zeroth moment of the weight function.
    N           -   number of nodes of the Gauss-Kronrod quadrature formula,
                    N &ge; 3,
                    N =  2*K+1.

Outputs:
    Info        -   error code:
                    * -5    no real and positive Gauss-Kronrod formula can
                            be created for such a weight function  with  a
                            given number of nodes.
                    * -4    N is too large, task may be ill  conditioned -
                            x[i]=x[i+1] found.
                    * -3    internal eigenproblem solver hasn't converged
                    * -2    Beta[i] &le; 0
                    * -1    incorrect N was passed
                    * +1    OK
    X           -   array[0..N-1] - array of quadrature nodes,
                    in ascending order.
    WKronrod    -   array[0..N-1] - Kronrod weights
    WGauss      -   array[0..N-1] - Gauss weights (interleaved with zeros
                    corresponding to extended Kronrod nodes).
ALGLIB: Copyright 08.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gkqgeneraterec(real_1d_array alpha, real_1d_array beta, <b>double</b> mu0, ae_int_t n, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;wkronrod, real_1d_array &amp;wgauss);
</pre>
<a name=sub_gkqlegendrecalc></a><h6 class=pageheader>gkqlegendrecalc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns Gauss and Gauss-Kronrod nodes for quadrature with N points.

Reduction to tridiagonal eigenproblem is used.

Inputs:
    N           -   number of Kronrod nodes, must be odd number, &ge; 3.

Outputs:
    Info        -   error code:
                    * -4    an  error   was   detected   when  calculating
                            weights/nodes.  N  is  too  large   to  obtain
                            weights/nodes  with  high   enough   accuracy.
                            Try  to   use   multiple   precision  version.
                    * -3    internal eigenproblem solver hasn't converged
                    * -1    incorrect N was passed
                    * +1    OK
    X           -   array[0..N-1] - array of quadrature nodes, ordered in
                    ascending order.
    WKronrod    -   array[0..N-1] - Kronrod weights
    WGauss      -   array[0..N-1] - Gauss weights (interleaved with zeros
                    corresponding to extended Kronrod nodes).
ALGLIB: Copyright 12.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gkqlegendrecalc(ae_int_t n, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;wkronrod, real_1d_array &amp;wgauss);
</pre>
<a name=sub_gkqlegendretbl></a><h6 class=pageheader>gkqlegendretbl Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns Gauss and Gauss-Kronrod nodes for quadrature with N  points  using
pre-calculated table. Nodes/weights were  computed  with  accuracy  up  to
1.0E-32 (if MPFR version of ALGLIB is used). In standard double  precision
accuracy reduces to something about 2.0E-16 (depending  on your compiler's
handling of long floating point constants).

Inputs:
    N           -   number of Kronrod nodes.
                    N can be 15, 21, 31, 41, 51, 61.

Outputs:
    X           -   array[0..N-1] - array of quadrature nodes, ordered in
                    ascending order.
    WKronrod    -   array[0..N-1] - Kronrod weights
    WGauss      -   array[0..N-1] - Gauss weights (interleaved with zeros
                    corresponding to extended Kronrod nodes).
ALGLIB: Copyright 12.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gkqlegendretbl(ae_int_t n, real_1d_array &amp;x, real_1d_array &amp;wkronrod, real_1d_array &amp;wgauss, <b>double</b> &amp;eps);
</pre>
<a name=unit_gq></a><h4 class=pageheader>8.5.3. gq Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_gqgenerategausshermite class=toc>gqgenerategausshermite</a> |
<a href=#sub_gqgenerategaussjacobi class=toc>gqgenerategaussjacobi</a> |
<a href=#sub_gqgenerategausslaguerre class=toc>gqgenerategausslaguerre</a> |
<a href=#sub_gqgenerategausslegendre class=toc>gqgenerategausslegendre</a> |
<a href=#sub_gqgenerategausslobattorec class=toc>gqgenerategausslobattorec</a> |
<a href=#sub_gqgenerategaussradaurec class=toc>gqgenerategaussradaurec</a> |
<a href=#sub_gqgeneraterec class=toc>gqgeneraterec</a>
]</font>
</div>
<a name=sub_gqgenerategausshermite></a><h6 class=pageheader>gqgenerategausshermite Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns  nodes/weights  for  Gauss-Hermite  quadrature on (-inf,+inf) with
weight function W(x)=exp(-x*x)

Inputs:
    N           -   number of nodes, &ge; 1

Outputs:
    Info        -   error code:
                    * -4    an  error  was   detected   when   calculating
                            weights/nodes.  May be, N is too large. Try to
                            use multiple precision version.
                    * -3    internal eigenproblem solver hasn't converged
                    * -1    incorrect N/Alpha was passed
                    * +1    OK
    X           -   array[0..N-1] - array of quadrature nodes,
                    in ascending order.
    W           -   array[0..N-1] - array of quadrature weights.
ALGLIB: Copyright 12.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gqgenerategausshermite(ae_int_t n, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;w);
</pre>
<a name=sub_gqgenerategaussjacobi></a><h6 class=pageheader>gqgenerategaussjacobi Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns  nodes/weights  for  Gauss-Jacobi quadrature on [-1,1] with weight
function W(x)=power(1-x,Alpha)*power(1+x,Beta).

Inputs:
    N           -   number of nodes, &ge; 1
    Alpha       -   power-law coefficient, Alpha &gt; -1
    Beta        -   power-law coefficient, Beta &gt; -1

Outputs:
    Info        -   error code:
                    * -4    an  error  was   detected   when   calculating
                            weights/nodes. Alpha or  Beta  are  too  close
                            to -1 to obtain weights/nodes with high enough
                            accuracy, or, may be, N is too large.  Try  to
                            use multiple precision version.
                    * -3    internal eigenproblem solver hasn't converged
                    * -1    incorrect N/Alpha/Beta was passed
                    * +1    OK
    X           -   array[0..N-1] - array of quadrature nodes,
                    in ascending order.
    W           -   array[0..N-1] - array of quadrature weights.
ALGLIB: Copyright 12.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gqgenerategaussjacobi(ae_int_t n, <b>double</b> alpha, <b>double</b> beta, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;w);
</pre>
<a name=sub_gqgenerategausslaguerre></a><h6 class=pageheader>gqgenerategausslaguerre Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns  nodes/weights  for  Gauss-Laguerre  quadrature  on  [0,+inf) with
weight function W(x)=power(x,Alpha)*exp(-x)

Inputs:
    N           -   number of nodes, &ge; 1
    Alpha       -   power-law coefficient, Alpha &gt; -1

Outputs:
    Info        -   error code:
                    * -4    an  error  was   detected   when   calculating
                            weights/nodes. Alpha is too  close  to  -1  to
                            obtain weights/nodes with high enough accuracy
                            or, may  be,  N  is  too  large.  Try  to  use
                            multiple precision version.
                    * -3    internal eigenproblem solver hasn't converged
                    * -1    incorrect N/Alpha was passed
                    * +1    OK
    X           -   array[0..N-1] - array of quadrature nodes,
                    in ascending order.
    W           -   array[0..N-1] - array of quadrature weights.
ALGLIB: Copyright 12.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gqgenerategausslaguerre(ae_int_t n, <b>double</b> alpha, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;w);
</pre>
<a name=sub_gqgenerategausslegendre></a><h6 class=pageheader>gqgenerategausslegendre Function</h6>
<hr width=600 align=left>
<pre class=narration>
Returns nodes/weights for Gauss-Legendre quadrature on [-1,1] with N
nodes.

Inputs:
    N           -   number of nodes, &ge; 1

Outputs:
    Info        -   error code:
                    * -4    an  error   was   detected   when  calculating
                            weights/nodes.  N  is  too  large   to  obtain
                            weights/nodes  with  high   enough   accuracy.
                            Try  to   use   multiple   precision  version.
                    * -3    internal eigenproblem solver hasn't  converged
                    * -1    incorrect N was passed
                    * +1    OK
    X           -   array[0..N-1] - array of quadrature nodes,
                    in ascending order.
    W           -   array[0..N-1] - array of quadrature weights.
ALGLIB: Copyright 12.05.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gqgenerategausslegendre(ae_int_t n, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;w);
</pre>
<a name=sub_gqgenerategausslobattorec></a><h6 class=pageheader>gqgenerategausslobattorec Function</h6>
<hr width=600 align=left>
<pre class=narration>
Computation of nodes and weights for a Gauss-Lobatto quadrature formula

The algorithm generates the N-point Gauss-Lobatto quadrature formula  with
weight function given by coefficients alpha and beta of a recurrence which
generates a system of orthogonal polynomials.

P-1(x)   =  0
P0(x)    =  1
Pn+1(x)  =  (x-alpha(n))*Pn(x)  -  beta(n)*Pn-1(x)

and zeroth moment Mu0

Mu0 = integral(W(x)dx,a,b)

Inputs:
    Alpha   -   array[0..N-2], alpha coefficients
    Beta    -   array[0..N-2], beta coefficients.
                Zero-indexed element is not used, may be arbitrary.
                Beta[I] &gt; 0
    Mu0     -   zeroth moment of the weighting function.
    A       -   left boundary of the integration interval.
    B       -   right boundary of the integration interval.
    N       -   number of nodes of the quadrature formula, N &ge; 3
                (including the left and right boundary nodes).

Outputs:
    Info    -   error code:
                * -3    internal eigenproblem solver hasn't converged
                * -2    Beta[i] &le; 0
                * -1    incorrect N was passed
                *  1    OK
    X       -   array[0..N-1] - array of quadrature nodes,
                in ascending order.
    W       -   array[0..N-1] - array of quadrature weights.
ALGLIB: Copyright 2005-2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gqgenerategausslobattorec(real_1d_array alpha, real_1d_array beta, <b>double</b> mu0, <b>double</b> a, <b>double</b> b, ae_int_t n, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;w);
</pre>
<a name=sub_gqgenerategaussradaurec></a><h6 class=pageheader>gqgenerategaussradaurec Function</h6>
<hr width=600 align=left>
<pre class=narration>
Computation of nodes and weights for a Gauss-Radau quadrature formula

The algorithm generates the N-point Gauss-Radau  quadrature  formula  with
weight function given by the coefficients alpha and  beta  of a recurrence
which generates a system of orthogonal polynomials.

P-1(x)   =  0
P0(x)    =  1
Pn+1(x)  =  (x-alpha(n))*Pn(x)  -  beta(n)*Pn-1(x)

and zeroth moment Mu0

Mu0 = integral(W(x)dx,a,b)

Inputs:
    Alpha   -   array[0..N-2], alpha coefficients.
    Beta    -   array[0..N-1], beta coefficients
                Zero-indexed element is not used.
                Beta[I] &gt; 0
    Mu0     -   zeroth moment of the weighting function.
    A       -   left boundary of the integration interval.
    N       -   number of nodes of the quadrature formula, N &ge; 2
                (including the left boundary node).

Outputs:
    Info    -   error code:
                * -3    internal eigenproblem solver hasn't converged
                * -2    Beta[i] &le; 0
                * -1    incorrect N was passed
                *  1    OK
    X       -   array[0..N-1] - array of quadrature nodes,
                in ascending order.
    W       -   array[0..N-1] - array of quadrature weights.
ALGLIB: Copyright 2005-2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gqgenerategaussradaurec(real_1d_array alpha, real_1d_array beta, <b>double</b> mu0, <b>double</b> a, ae_int_t n, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;w);
</pre>
<a name=sub_gqgeneraterec></a><h6 class=pageheader>gqgeneraterec Function</h6>
<hr width=600 align=left>
<pre class=narration>
Computation of nodes and weights for a Gauss quadrature formula

The algorithm generates the N-point Gauss quadrature formula  with  weight
function given by coefficients alpha and beta  of  a  recurrence  relation
which generates a system of orthogonal polynomials:

P-1(x)   =  0
P0(x)    =  1
Pn+1(x)  =  (x-alpha(n))*Pn(x)  -  beta(n)*Pn-1(x)

and zeroth moment Mu0

Mu0 = integral(W(x)dx,a,b)

Inputs:
    Alpha   -   array[0..N-1], alpha coefficients
    Beta    -   array[0..N-1], beta coefficients
                Zero-indexed element is not used and may be arbitrary.
                Beta[I] &gt; 0.
    Mu0     -   zeroth moment of the weight function.
    N       -   number of nodes of the quadrature formula, N &ge; 1

Outputs:
    Info    -   error code:
                * -3    internal eigenproblem solver hasn't converged
                * -2    Beta[i] &le; 0
                * -1    incorrect N was passed
                *  1    OK
    X       -   array[0..N-1] - array of quadrature nodes,
                in ascending order.
    W       -   array[0..N-1] - array of quadrature weights.
ALGLIB: Copyright 2005-2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> gqgeneraterec(real_1d_array alpha, real_1d_array beta, <b>double</b> mu0, ae_int_t n, ae_int_t &amp;info, real_1d_array &amp;x, real_1d_array &amp;w);
</pre>
</p>
<p>
<a name=pck_Interpolation class=sheader></a><h3>8.6. Interpolation Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_fitsphere class=toc>fitsphere</a></td><td>Fitting circle/sphere to data (least squares, minimum circumscribed, maximum inscribed, minimum zone)</td></tr>
<tr align=left valign=top><td><a href=#unit_idw class=toc>idw</a></td><td>Inverse distance weighting: interpolation/fitting with improved Shepard-like algorithm</td></tr>
<tr align=left valign=top><td><a href=#unit_intcomp class=toc>intcomp</a></td><td>Backward compatibility functions</td></tr>
<tr align=left valign=top><td><a href=#unit_lsfit class=toc>lsfit</a></td><td>Fitting with least squares target function (linear and nonlinear least-squares)</td></tr>
<tr align=left valign=top><td><a href=#unit_parametric class=toc>parametric</a></td><td>Parametric curves</td></tr>
<tr align=left valign=top><td><a href=#unit_polint class=toc>polint</a></td><td>Polynomial interpolation/fitting</td></tr>
<tr align=left valign=top><td><a href=#unit_ratint class=toc>ratint</a></td><td>Rational interpolation/fitting</td></tr>
<tr align=left valign=top><td><a href=#unit_rbf class=toc>rbf</a></td><td>Scattered N-dimensional interpolation with RBF models</td></tr>
<tr align=left valign=top><td><a href=#unit_spline1d class=toc>spline1d</a></td><td>1D spline interpolation/fitting</td></tr>
<tr align=left valign=top><td><a href=#unit_spline2d class=toc>spline2d</a></td><td>2D spline interpolation</td></tr>
<tr align=left valign=top><td><a href=#unit_spline3d class=toc>spline3d</a></td><td>3D spline interpolation</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_fitsphere></a><h4 class=pageheader>8.6.1. fitsphere Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_fitspherels class=toc>fitspherels</a> |
<a href=#sub_fitspheremc class=toc>fitspheremc</a> |
<a href=#sub_fitspheremi class=toc>fitspheremi</a> |
<a href=#sub_fitspheremz class=toc>fitspheremz</a> |
<a href=#sub_fitspherex class=toc>fitspherex</a>
]</font>
</div>
<a name=sub_fitspherels></a><h6 class=pageheader>fitspherels Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fits least squares (LS) circle (or NX-dimensional sphere) to data  (a  set
of points in NX-dimensional space).

Least squares circle minimizes sum of squared deviations between distances
from points to the center and  some  &quot;candidate&quot;  radius,  which  is  also
fitted to the data.

Inputs:
    XY      -   array[NPoints,NX] (or larger), contains dataset.
                One row = one point in NX-dimensional space.
    NPoints -   dataset size, NPoints &gt; 0
    NX      -   space dimensionality, NX &gt; 0 (1, 2, 3, 4, 5 and so on)

Outputs:
    CX      -   central point for a sphere
    R       -   radius
ALGLIB: Copyright 07.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fitspherels(real_2d_array xy, ae_int_t npoints, ae_int_t nx, real_1d_array &amp;cx, <b>double</b> &amp;r);
</pre>
<a name=sub_fitspheremc></a><h6 class=pageheader>fitspheremc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fits minimum circumscribed (MC) circle (or NX-dimensional sphere) to  data
(a set of points in NX-dimensional space).

Inputs:
    XY      -   array[NPoints,NX] (or larger), contains dataset.
                One row = one point in NX-dimensional space.
    NPoints -   dataset size, NPoints &gt; 0
    NX      -   space dimensionality, NX &gt; 0 (1, 2, 3, 4, 5 and so on)

Outputs:
    CX      -   central point for a sphere
    RHi     -   radius

NOTE: this function is an easy-to-use wrapper around more powerful &quot;expert&quot;
      function fitspherex().

      This  wrapper  is optimized  for  ease of use and stability - at the
      cost of somewhat lower  performance  (we  have  to  use  very  tight
      stopping criteria for inner optimizer because we want to  make  sure
      that it will converge on any dataset).

      If you are ready to experiment with settings of  &quot;expert&quot;  function,
      you can achieve ~2-4x speedup over standard &quot;bulletproof&quot; settings.
ALGLIB: Copyright 14.04.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fitspheremc(real_2d_array xy, ae_int_t npoints, ae_int_t nx, real_1d_array &amp;cx, <b>double</b> &amp;rhi);
</pre>
<a name=sub_fitspheremi></a><h6 class=pageheader>fitspheremi Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fits maximum inscribed circle (or NX-dimensional sphere) to data (a set of
points in NX-dimensional space).

Inputs:
    XY      -   array[NPoints,NX] (or larger), contains dataset.
                One row = one point in NX-dimensional space.
    NPoints -   dataset size, NPoints &gt; 0
    NX      -   space dimensionality, NX &gt; 0 (1, 2, 3, 4, 5 and so on)

Outputs:
    CX      -   central point for a sphere
    RLo     -   radius

NOTE: this function is an easy-to-use wrapper around more powerful &quot;expert&quot;
      function fitspherex().

      This  wrapper  is optimized  for  ease of use and stability - at the
      cost of somewhat lower  performance  (we  have  to  use  very  tight
      stopping criteria for inner optimizer because we want to  make  sure
      that it will converge on any dataset).

      If you are ready to experiment with settings of  &quot;expert&quot;  function,
      you can achieve ~2-4x speedup over standard &quot;bulletproof&quot; settings.
ALGLIB: Copyright 14.04.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fitspheremi(real_2d_array xy, ae_int_t npoints, ae_int_t nx, real_1d_array &amp;cx, <b>double</b> &amp;rlo);
</pre>
<a name=sub_fitspheremz></a><h6 class=pageheader>fitspheremz Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fits minimum zone circle (or NX-dimensional sphere)  to  data  (a  set  of
points in NX-dimensional space).

Inputs:
    XY      -   array[NPoints,NX] (or larger), contains dataset.
                One row = one point in NX-dimensional space.
    NPoints -   dataset size, NPoints &gt; 0
    NX      -   space dimensionality, NX &gt; 0 (1, 2, 3, 4, 5 and so on)

Outputs:
    CX      -   central point for a sphere
    RLo     -   radius of inscribed circle
    RHo     -   radius of circumscribed circle

NOTE: this function is an easy-to-use wrapper around more powerful &quot;expert&quot;
      function fitspherex().

      This  wrapper  is optimized  for  ease of use and stability - at the
      cost of somewhat lower  performance  (we  have  to  use  very  tight
      stopping criteria for inner optimizer because we want to  make  sure
      that it will converge on any dataset).

      If you are ready to experiment with settings of  &quot;expert&quot;  function,
      you can achieve ~2-4x speedup over standard &quot;bulletproof&quot; settings.
ALGLIB: Copyright 14.04.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fitspheremz(real_2d_array xy, ae_int_t npoints, ae_int_t nx, real_1d_array &amp;cx, <b>double</b> &amp;rlo, <b>double</b> &amp;rhi);
</pre>
<a name=sub_fitspherex></a><h6 class=pageheader>fitspherex Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fitting minimum circumscribed, maximum inscribed or minimum  zone  circles
(or NX-dimensional spheres)  to  data  (a  set of points in NX-dimensional
space).

This  is  expert  function  which  allows  to  tweak  many  parameters  of
underlying nonlinear solver:
* stopping criteria for inner iterations
* number of outer iterations
* penalty coefficient used to handle  nonlinear  constraints  (we  convert
  unconstrained nonsmooth optimization problem ivolving max() and/or min()
  operations to quadratically constrained smooth one).

You may tweak all these parameters or only some  of  them,  leaving  other
ones at their default state - just specify zero  value,  and  solver  will
fill it with appropriate default one.

These comments also include some discussion of  approach  used  to  handle
such unusual fitting problem,  its  stability,  drawbacks  of  alternative
methods, and convergence properties.

Inputs:
    XY      -   array[NPoints,NX] (or larger), contains dataset.
                One row = one point in NX-dimensional space.
    NPoints -   dataset size, NPoints &gt; 0
    NX      -   space dimensionality, NX &gt; 0 (1, 2, 3, 4, 5 and so on)
    ProblemType-used to encode problem type:
                * 0 for least squares circle
                * 1 for minimum circumscribed circle/sphere fitting (MC)
                * 2 for  maximum inscribed circle/sphere fitting (MI)
                * 3 for minimum zone circle fitting (difference between
                    Rhi and Rlo is minimized), denoted as MZ
    EpsX    -   stopping condition for NLC optimizer:
                * must be non-negative
                * use 0 to choose default value (1.0E-12 is used by default)
                * you may specify larger values, up to 1.0E-6, if you want
                  to   speed-up   solver;   NLC   solver  performs several
                  preconditioned  outer  iterations,   so   final   result
                  typically has precision much better than EpsX.
    AULIts  -   number of outer iterations performed by NLC optimizer:
                * must be non-negative
                * use 0 to choose default value (20 is used by default)
                * you may specify values smaller than 20 if you want to
                  speed up solver; 10 often results in good combination of
                  precision and speed; sometimes you may get good results
                  with just 6 outer iterations.
                Ignored for ProblemType=0.
    Penalty -   penalty coefficient for NLC optimizer:
                * must be non-negative
                * use 0 to choose default value (1.0E6 in current version)
                * it should be really large, 1.0E6...1.0E7 is a good value
                  to start from;
                * generally, default value is good enough
                Ignored for ProblemType=0.

Outputs:
    CX      -   central point for a sphere
    RLo     -   radius:
                * for ProblemType=2,3, radius of the inscribed sphere
                * for ProblemType=0 - radius of the least squares sphere
                * for ProblemType=1 - zero
    RHo     -   radius:
                * for ProblemType=1,3, radius of the circumscribed sphere
                * for ProblemType=0 - radius of the least squares sphere
                * for ProblemType=2 - zero

NOTE: ON THE UNIQUENESS OF SOLUTIONS

ALGLIB provides solution to several related circle fitting  problems:   MC
(minimum circumscribed), MI (maximum inscribed)   and   MZ  (minimum zone)
fitting, LS (least squares) fitting.

It  is  important  to  note  that  among these problems only MC and LS are
convex and have unique solution independently from starting point.

As  for MI,  it  may (or  may  not, depending on dataset properties)  have
multiple solutions, and it always  has  one degenerate solution C=infinity
which corresponds to infinitely large radius. Thus, there are no guarantees
that solution to  MI returned by this solver will be the best one (and  no
one can provide you with such guarantee because problem is  NP-hard).  The
only guarantee you have is that this solution is locally optimal, i.e.  it
can not be improved by infinitesimally small tweaks in the parameters.

It  is  also  possible  to &quot;run away&quot; to infinity when  started  from  bad
initial point located outside of point cloud (or when point cloud does not
span entire circumference/surface of the sphere).

Finally,  MZ (minimum zone circle) stands somewhere between MC  and  MI in
stability. It is somewhat regularized by &quot;circumscribed&quot; term of the merit
function; however, solutions to  MZ may be non-unique, and in some unlucky
cases it is also possible to &quot;run away to infinity&quot;.

NOTE: ON THE NONLINEARLY CONSTRAINED PROGRAMMING APPROACH

The problem formulation for MC  (minimum circumscribed   circle;  for  the
sake of simplicity we omit MZ and MI here) is:

        [     [         ]2 ]
    min [ max [ XY[i]-C ]  ]
     C  [  i  [         ]  ]

i.e. it is unconstrained nonsmooth optimization problem of finding  &quot;best&quot;
central point, with radius R being unambiguously  determined  from  C.  In
order to move away from non-smoothness we use following reformulation:

        [   ]                    [         ]2
    min [ R ] subject to R &ge; 0, [ XY[i]-C ] &le; R^2
    C,R [   ]                    [         ]

i.e. it becomes smooth quadratically constrained optimization problem with
linear target function. Such problem statement is 100% equivalent  to  the
original nonsmooth one, but much easier  to  approach.  We solve  it  with
MinNLC solver provided by ALGLIB.

NOTE: ON INSTABILITY OF SEQUENTIAL LINEARIZATION APPROACH

ALGLIB  has  nonlinearly  constrained  solver which proved to be stable on
such problems. However, some authors proposed to linearize constraints  in
the vicinity of current approximation (Ci,Ri) and to get next  approximate
solution (Ci+1,Ri+1) as solution to linear programming problem. Obviously,
LP problems are easier than nonlinearly constrained ones.

Indeed,  such approach  to   MC/MI/MZ   resulted   in  ~10-20x increase in
performance (when compared with NLC solver). However, it turned  out  that
in some cases linearized model fails to predict correct direction for next
step and tells us that we converged to solution even when we are still 2-4
digits of precision away from it.

It is important that it is not failure of LP solver - it is failure of the
linear model;  even  when  solved  exactly,  it  fails  to  handle  subtle
nonlinearities which arise near the solution. We validated it by comparing
results returned by ALGLIB linear solver with that of MATLAB.

In our experiments with linearization:
* MC failed most often, at both realistic and synthetic datasets
* MI sometimes failed, but sometimes succeeded
* MZ often  succeeded; our guess is that presence of two independent  sets
  of constraints (one set for Rlo and another one for Rhi) and  two  terms
  in the target function (Rlo and Rhi) regularizes task,  so  when  linear
  model fails to handle nonlinearities from Rlo, it uses  Rhi  as  a  hint
  (and vice versa).

Because linearization approach failed to achieve stable results, we do not
include it in ALGLIB.
ALGLIB: Copyright 14.04.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fitspherex(real_2d_array xy, ae_int_t npoints, ae_int_t nx, ae_int_t problemtype, <b>double</b> epsx, ae_int_t aulits, <b>double</b> penalty, real_1d_array &amp;cx, <b>double</b> &amp;rlo, <b>double</b> &amp;rhi);
</pre>
<a name=unit_idw></a><h4 class=pageheader>8.6.2. idw Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_idwbuilder class=toc>idwbuilder</a> |
<a href=#struct_idwcalcbuffer class=toc>idwcalcbuffer</a> |
<a href=#struct_idwmodel class=toc>idwmodel</a> |
<a href=#struct_idwreport class=toc>idwreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_idwbuildercreate class=toc>idwbuildercreate</a> |
<a href=#sub_idwbuildersetalgomstab class=toc>idwbuildersetalgomstab</a> |
<a href=#sub_idwbuildersetalgotextbookmodshepard class=toc>idwbuildersetalgotextbookmodshepard</a> |
<a href=#sub_idwbuildersetalgotextbookshepard class=toc>idwbuildersetalgotextbookshepard</a> |
<a href=#sub_idwbuildersetconstterm class=toc>idwbuildersetconstterm</a> |
<a href=#sub_idwbuildersetnlayers class=toc>idwbuildersetnlayers</a> |
<a href=#sub_idwbuildersetpoints class=toc>idwbuildersetpoints</a> |
<a href=#sub_idwbuildersetuserterm class=toc>idwbuildersetuserterm</a> |
<a href=#sub_idwbuildersetzeroterm class=toc>idwbuildersetzeroterm</a> |
<a href=#sub_idwcalc class=toc>idwcalc</a> |
<a href=#sub_idwcalc1 class=toc>idwcalc1</a> |
<a href=#sub_idwcalc2 class=toc>idwcalc2</a> |
<a href=#sub_idwcalc3 class=toc>idwcalc3</a> |
<a href=#sub_idwcalcbuf class=toc>idwcalcbuf</a> |
<a href=#sub_idwcreatecalcbuffer class=toc>idwcreatecalcbuffer</a> |
<a href=#sub_idwfit class=toc>idwfit</a> |
<a href=#sub_idwserialize class=toc>idwserialize</a> |
<a href=#sub_idwtscalcbuf class=toc>idwtscalcbuf</a> |
<a href=#sub_idwunserialize class=toc>idwunserialize</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_idw_d_mstab class=toc>idw_d_mstab</a></td><td width=15>&nbsp;</td><td>Simple model built with IDW-MSTAB algorithm</td></tr>
<tr align=left valign=top><td><a href=#example_idw_d_serialize class=toc>idw_d_serialize</a></td><td width=15>&nbsp;</td><td>IDW model serialization/unserialization</td></tr>
</table>
</div>
<a name=struct_idwbuilder></a><h6 class=pageheader>idwbuilder Class</h6>
<hr width=600 align=left>
<pre class=narration>
Builder object used to generate IDW (Inverse Distance Weighting) model.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> idwbuilder {
};
</pre>
<a name=struct_idwcalcbuffer></a><h6 class=pageheader>idwcalcbuffer Class</h6>
<hr width=600 align=left>
<pre class=narration>
Buffer  object  which  is  used  to  perform  evaluation  requests  in  the
multithreaded mode (multiple threads working with same IDW object).

This object should be created with idwcreatecalcbuffer().
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> idwcalcbuffer {
};
</pre>
<a name=struct_idwmodel></a><h6 class=pageheader>idwmodel Class</h6>
<hr width=600 align=left>
<pre class=narration>
IDW (Inverse Distance Weighting) model object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> idwmodel {
};
</pre>
<a name=struct_idwreport></a><h6 class=pageheader>idwreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
IDW fitting report:
    rmserror        RMS error
    avgerror        average error
    maxerror        maximum error
    r2              coefficient of determination,  R-squared, 1-RSS/TSS
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> idwreport {
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> maxerror;
   <b>double</b> r2;
};
</pre>
<a name=sub_idwbuildercreate></a><h6 class=pageheader>idwbuildercreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine creates builder object used  to  generate IDW  model  from
irregularly sampled (scattered) dataset.  Multidimensional  scalar/vector-
-valued are supported.

Builder object is used to fit model to data as follows:
* builder object is created with idwbuildercreate() function
* dataset is added with idwbuildersetpoints() function
* one of the modern IDW algorithms is chosen with either:
  * idwbuildersetalgomstab()            - Multilayer STABilized algorithm (interpolation)
  Alternatively, one of the textbook algorithms can be chosen (not recommended):
  * idwbuildersetalgotextbookshepard()  - textbook Shepard algorithm
  * idwbuildersetalgotextbookmodshepard()-textbook modified Shepard algorithm
* finally, model construction is performed with idwfit() function.

Inputs:
    NX  -   dimensionality of the argument, NX &ge; 1
    NY  -   dimensionality of the function being modeled, NY &ge; 1;
            NY=1 corresponds to classic scalar function, NY &ge; 1 corresponds
            to vector-valued function.

Outputs:
    State-  builder object
ALGLIB Project: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwbuildercreate(ae_int_t nx, ae_int_t ny, idwbuilder &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_idw_d_mstab class=nav>idw_d_mstab</a> | <a href=#example_idw_d_serialize class=nav>idw_d_serialize</a> ]</p>
<a name=sub_idwbuildersetalgomstab></a><h6 class=pageheader>idwbuildersetalgomstab Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets IDW model  construction  algorithm  to  the  Multilayer
Stabilized IDW method (IDW-MSTAB), a  latest  incarnation  of  the inverse
distance weighting interpolation which fixes shortcomings of  the original
and modified Shepard's variants.

The distinctive features of IDW-MSTAB are:
1) exact interpolation  is  pursued  (as  opposed  to  fitting  and  noise
   suppression)
2) improved robustness when compared with that of other algorithms:
   * MSTAB shows almost no strange  fitting  artifacts  like  ripples  and
     sharp spikes (unlike N-dimensional splines and HRBFs)
   * MSTAB does not return function values far from the  interval  spanned
     by the dataset; say, if all your points have |f| &le; 1, you  can be sure
     that model value won't deviate too much from [-1,+1]
3) good model construction time competing with that of HRBFs  and  bicubic
   splines
4) ability to work with any number of dimensions, starting from NX=1

The drawbacks of IDW-MSTAB (and all IDW algorithms in general) are:
1) dependence of the model evaluation time on the search radius
2) bad extrapolation properties, models built by this method  are  usually
   conservative in their predictions

Thus, IDW-MSTAB is  a  good  &quot;default&quot;  option  if  you  want  to  perform
scattered multidimensional interpolation. Although it has  its  drawbacks,
it is easy to use and robust, which makes it a good first step.

Inputs:
    State   -   builder object
    SRad    -   initial search radius, SRad &gt; 0 is required. A model  value
                is obtained by &quot;smart&quot; averaging  of  the  dataset  points
                within search radius.

NOTE 1: IDW interpolation can  correctly  handle  ANY  dataset,  including
        datasets with non-distinct points. In case non-distinct points are
        found, an average value for this point will be calculated.

NOTE 2: the memory requirements for model storage are O(NPoints*NLayers).
        The model construction needs twice as much memory as model storage.

NOTE 3: by default 16 IDW layers are built which is enough for most cases.
        You can change this parameter with idwbuildersetnlayers()  method.
        Larger values may be necessary if you need to reproduce  extrafine
        details at distances smaller than SRad/65536.  Smaller value   may
        be necessary if you have to save memory and  computing  time,  and
        ready to sacrifice some model quality.

ALGORITHM DESCRIPTION

ALGLIB implementation of IDW is somewhat similar to the modified Shepard's
method (one with search radius R) but overcomes several of its  drawbacks,
namely:
1) a tendency to show stepwise behavior for uniform datasets
2) a tendency to show terrible interpolation properties for highly
   nonuniform datasets which often arise in geospatial tasks
  (function values are densely sampled across multiple separated
  &quot;tracks&quot;)

IDW-MSTAB method performs several passes over dataset and builds a sequence
of progressively refined IDW models  (layers),  which starts from one with
largest search radius SRad  and continues to smaller  search  radii  until
required number of  layers  is  built.  Highest  layers  reproduce  global
behavior of the target function at larger distances  whilst  lower  layers
reproduce fine details at smaller distances.

Each layer is an IDW model built with following modifications:
* weights go to zero when distance approach to the current search radius
* an additional regularizing term is added to the distance: w=1/(d^2+lambda)
* an additional fictional term with unit weight and zero function value is
  added in order to promote continuity  properties  at  the  isolated  and
  boundary points

By default, 16 layers is built, which is enough for most  cases.  You  can
change this parameter with idwbuildersetnlayers() method.
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwbuildersetalgomstab(idwbuilder state, <b>double</b> srad);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_idw_d_mstab class=nav>idw_d_mstab</a> | <a href=#example_idw_d_serialize class=nav>idw_d_serialize</a> ]</p>
<a name=sub_idwbuildersetalgotextbookmodshepard></a><h6 class=pageheader>idwbuildersetalgotextbookmodshepard Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets  IDW  model  construction  algorithm  to the 'textbook'
modified Shepard's algorithm with user-specified search radius.

IMPORTANT: we do NOT recommend using textbook IDW algorithms because  they
           have terrible interpolation properties. Use MSTAB in all cases.

Inputs:
    State   -   builder object
    R       -   search radius

NOTE 1: IDW interpolation can  correctly  handle  ANY  dataset,  including
        datasets with non-distinct points. In case non-distinct points are
        found, an average value for this point will be calculated.
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwbuildersetalgotextbookmodshepard(idwbuilder state, <b>double</b> r);
</pre>
<a name=sub_idwbuildersetalgotextbookshepard></a><h6 class=pageheader>idwbuildersetalgotextbookshepard Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets  IDW  model  construction  algorithm  to  the  textbook
Shepard's algorithm with custom (user-specified) power parameter.

IMPORTANT: we do NOT recommend using textbook IDW algorithms because  they
           have terrible interpolation properties. Use MSTAB in all cases.

Inputs:
    State   -   builder object
    P       -   power parameter, P &gt; 0; good value to start with is 2.0

NOTE 1: IDW interpolation can  correctly  handle  ANY  dataset,  including
        datasets with non-distinct points. In case non-distinct points are
        found, an average value for this point will be calculated.
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwbuildersetalgotextbookshepard(idwbuilder state, <b>double</b> p);
</pre>
<a name=sub_idwbuildersetconstterm></a><h6 class=pageheader>idwbuildersetconstterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets constant prior term (model value at infinity).

Constant prior term is determined as mean value over dataset.

Inputs:
    S       -   spline builder
ALGLIB: Copyright 29.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwbuildersetconstterm(idwbuilder state);
</pre>
<a name=sub_idwbuildersetnlayers></a><h6 class=pageheader>idwbuildersetnlayers Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function changes number of layers used by IDW-MSTAB algorithm.

The more layers you have, the finer details can  be  reproduced  with  IDW
model. The less layers you have, the less memory and CPU time is  consumed
by the model.

Memory consumption grows linearly with layers count,  running  time  grows
sub-linearly.

The default number of layers is 16, which allows you to reproduce  details
at distance down to SRad/65536. You will rarely need to change it.

Inputs:
    State   -   builder object
    NLayers -   NLayers &ge; 1, the number of layers used by the model.
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwbuildersetnlayers(idwbuilder state, ae_int_t nlayers);
</pre>
<a name=sub_idwbuildersetpoints></a><h6 class=pageheader>idwbuildersetpoints Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function adds dataset to the builder object.

This function overrides results of the previous calls, i.e. multiple calls
of this function will result in only the last set being added.

Inputs:
    State   -   builder object
    XY      -   points, array[N,NX+NY]. One row  corresponds to  one point
                in the dataset. First NX elements  are  coordinates,  next
                NY elements are function values. Array may  be larger than
                specified, in  this  case  only leading [N,NX+NY] elements
                will be used.
    N       -   number of points in the dataset, N &ge; 0.
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwbuildersetpoints(idwbuilder state, real_2d_array xy, ae_int_t n);
<b>void</b> idwbuildersetpoints(idwbuilder state, real_2d_array xy);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_idw_d_mstab class=nav>idw_d_mstab</a> | <a href=#example_idw_d_serialize class=nav>idw_d_serialize</a> ]</p>
<a name=sub_idwbuildersetuserterm></a><h6 class=pageheader>idwbuildersetuserterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets prior term (model value at infinity) as  user-specified
value.

Inputs:
    S       -   spline builder
    V       -   value for user-defined prior

NOTE: for vector-valued models all components of the prior are set to same
      user-specified value
ALGLIB: Copyright 29.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwbuildersetuserterm(idwbuilder state, <b>double</b> v);
</pre>
<a name=sub_idwbuildersetzeroterm></a><h6 class=pageheader>idwbuildersetzeroterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets zero prior term (model value at infinity).

Inputs:
    S       -   spline builder
ALGLIB: Copyright 29.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwbuildersetzeroterm(idwbuilder state);
</pre>
<a name=sub_idwcalc></a><h6 class=pageheader>idwcalc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the IDW model at the given point.

This is general function which can be used for arbitrary NX (dimension  of
the space of arguments) and NY (dimension of the function itself). However
when  you  have  NY=1  you  may  find more convenient to  use  idwcalc1(),
idwcalc2() or idwcalc3().

NOTE: this function modifies internal temporaries of the  IDW  model, thus
      IT IS NOT  THREAD-SAFE!  If  you  want  to  perform  parallel  model
      evaluation from the multiple threads, use idwtscalcbuf()  with  per-
      thread buffer object.

Inputs:
    S       -   IDW model
    X       -   coordinates, array[NX]. X may have more than NX  elements,
                in this case only leading NX will be used.

Outputs:
    Y       -   function value, array[NY]. Y is out-parameter and will  be
                reallocated after call to this function. In case you  want
                to reuse previously allocated Y, you may use idwcalcbuf(),
                which reallocates Y only when it is too small.
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwcalc(idwmodel s, real_1d_array x, real_1d_array &amp;y);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_idw_d_mstab class=nav>idw_d_mstab</a> | <a href=#example_idw_d_serialize class=nav>idw_d_serialize</a> ]</p>
<a name=sub_idwcalc1></a><h6 class=pageheader>idwcalc1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
IDW interpolation: scalar target, 1-dimensional argument

NOTE: this function modifies internal temporaries of the  IDW  model, thus
      IT IS NOT  THREAD-SAFE!  If  you  want  to  perform  parallel  model
      evaluation from the multiple threads, use idwtscalcbuf()  with  per-
      thread buffer object.

Inputs:
    S   -   IDW interpolant built with IDW builder
    X0  -   argument value

Result:
    IDW interpolant S(X0)
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> idwcalc1(idwmodel s, <b>double</b> x0);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_idw_d_mstab class=nav>idw_d_mstab</a> | <a href=#example_idw_d_serialize class=nav>idw_d_serialize</a> ]</p>
<a name=sub_idwcalc2></a><h6 class=pageheader>idwcalc2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
IDW interpolation: scalar target, 2-dimensional argument

NOTE: this function modifies internal temporaries of the  IDW  model, thus
      IT IS NOT  THREAD-SAFE!  If  you  want  to  perform  parallel  model
      evaluation from the multiple threads, use idwtscalcbuf()  with  per-
      thread buffer object.

Inputs:
    S       -   IDW interpolant built with IDW builder
    X0, X1  -   argument value

Result:
    IDW interpolant S(X0,X1)
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> idwcalc2(idwmodel s, <b>double</b> x0, <b>double</b> x1);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_idw_d_mstab class=nav>idw_d_mstab</a> | <a href=#example_idw_d_serialize class=nav>idw_d_serialize</a> ]</p>
<a name=sub_idwcalc3></a><h6 class=pageheader>idwcalc3 Function</h6>
<hr width=600 align=left>
<pre class=narration>
IDW interpolation: scalar target, 3-dimensional argument

NOTE: this function modifies internal temporaries of the  IDW  model, thus
      IT IS NOT  THREAD-SAFE!  If  you  want  to  perform  parallel  model
      evaluation from the multiple threads, use idwtscalcbuf()  with  per-
      thread buffer object.

Inputs:
    S       -   IDW interpolant built with IDW builder
    X0,X1,X2-   argument value

Result:
    IDW interpolant S(X0,X1,X2)
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> idwcalc3(idwmodel s, <b>double</b> x0, <b>double</b> x1, <b>double</b> x2);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_idw_d_mstab class=nav>idw_d_mstab</a> | <a href=#example_idw_d_serialize class=nav>idw_d_serialize</a> ]</p>
<a name=sub_idwcalcbuf></a><h6 class=pageheader>idwcalcbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the IDW model at the given point.

Same as idwcalc(), but does not reallocate Y when in is large enough to
store function values.

NOTE: this function modifies internal temporaries of the  IDW  model, thus
      IT IS NOT  THREAD-SAFE!  If  you  want  to  perform  parallel  model
      evaluation from the multiple threads, use idwtscalcbuf()  with  per-
      thread buffer object.

Inputs:
    S       -   IDW model
    X       -   coordinates, array[NX]. X may have more than NX  elements,
                in this case only leading NX will be used.
    Y       -   possibly preallocated array

Outputs:
    Y       -   function value, array[NY]. Y is not reallocated when it
                is larger than NY.
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwcalcbuf(idwmodel s, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_idwcreatecalcbuffer></a><h6 class=pageheader>idwcreatecalcbuffer Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates buffer  structure  which  can  be  used  to  perform
parallel  IDW  model  evaluations  (with  one  IDW  model  instance  being
used from multiple threads, as long as  different  threads  use  different
instances of buffer).

This buffer object can be used with  idwtscalcbuf()  function  (here  &quot;ts&quot;
stands for &quot;thread-safe&quot;, &quot;buf&quot; is a suffix which denotes  function  which
reuses previously allocated output space).

How to use it:
* create IDW model structure or load it from file
* call idwcreatecalcbuffer(), once per thread working with IDW model  (you
  should call this function only AFTER model initialization, see below for
  more information)
* call idwtscalcbuf() from different threads,  with  each  thread  working
  with its own copy of buffer object.

Inputs:
    S           -   IDW model

Outputs:
    Buf         -   external buffer.

IMPORTANT: buffer object should be used only with  IDW model object  which
           was used to initialize buffer. Any attempt to use buffer   with
           different object is dangerous - you may  get  memory  violation
           error because sizes of internal arrays do not fit to dimensions
           of the IDW structure.

IMPORTANT: you  should  call  this function only for model which was built
           with model builder (or unserialized from file). Sizes  of  some
           internal structures are determined only after model  is  built,
           so buffer object created before model construction  stage  will
           be useless (and any attempt to use it will result in exception).
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwcreatecalcbuffer(idwmodel s, idwcalcbuffer &amp;buf);
</pre>
<a name=sub_idwfit></a><h6 class=pageheader>idwfit Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function fits IDW model to the dataset using current IDW construction
algorithm. A model being built and fitting report are returned.

Inputs:
    State   -   builder object

Outputs:
    Model   -   an IDW model built with current algorithm
    Rep     -   model fitting report, fields of this structure contain
                information about average fitting errors.

NOTE: although IDW-MSTAB algorithm is an  interpolation  method,  i.e.  it
      tries to fit the model exactly, it can  handle  datasets  with  non-
      distinct points which can not be fit exactly; in such  cases  least-
      squares fitting is performed.
ALGLIB: Copyright 22.10.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwfit(idwbuilder state, idwmodel &amp;model, idwreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_idw_d_mstab class=nav>idw_d_mstab</a> | <a href=#example_idw_d_serialize class=nav>idw_d_serialize</a> ]</p>
<a name=sub_idwserialize></a><h6 class=pageheader>idwserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: serialization
These functions serialize a data structure to a C++ string or stream.
* serialization can be freely moved across 32-bit and 64-bit systems,
  and different byte orders. For example, you can serialize a string
  on a SPARC and unserialize it on an x86.
* ALGLIB++ serialization is compatible with serialization in ALGLIB,
  in both directions.
Important properties of s_out:
* it contains alphanumeric characters, dots, underscores, minus signs
* these symbols are grouped into words, which are separated by spaces
  and Windows-style (CR+LF) newlines
ALGLIB: Copyright 28.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwserialize(idwmodel &amp;obj, std::string &amp;s_out);
<b>void</b> idwserialize(idwmodel &amp;obj, std::ostream &amp;s_out);
</pre>
<a name=sub_idwtscalcbuf></a><h6 class=pageheader>idwtscalcbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the IDW model at the given point, using
external  buffer  object  (internal  temporaries  of  IDW  model  are  not
modified).

This function allows to use same IDW model object  in  different  threads,
assuming  that  different   threads  use different instances of the buffer
structure.

Inputs:
    S       -   IDW model, may be shared between different threads
    Buf     -   buffer object created for this particular instance of  IDW
                model with idwcreatecalcbuffer().
    X       -   coordinates, array[NX]. X may have more than NX  elements,
                in this case only  leading NX will be used.
    Y       -   possibly preallocated array

Outputs:
    Y       -   function value, array[NY]. Y is not reallocated when it
                is larger than NY.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwtscalcbuf(idwmodel s, idwcalcbuffer buf, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_idwunserialize></a><h6 class=pageheader>idwunserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: unserialization
These functions unserialize a data structure from a C++ string or stream.
Important properties of s_in:
* any combination of spaces, tabs, Windows or Unix stype newlines can
  be used as separators, so as to allow flexible reformatting of the
  stream or string from text or XML files.
* But you should not insert separators into the middle of the "words"
  nor you should change case of letters.
ALGLIB: Copyright 28.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> idwunserialize(<b>const</b> std::string &amp;s_in, idwmodel &amp;obj);
<b>void</b> idwunserialize(<b>const</b> std::istream &amp;s_in, idwmodel &amp;obj);
</pre>
<a name=example_idw_d_mstab></a><h6 class=pageheader>idw_d_mstab Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example illustrates basic concepts of the IDW models:</font>
<font color=navy>// creation and evaluation.</font>
<font color=navy>// </font>
<font color=navy>// Suppose that we have set of 2-dimensional points with associated</font>
<font color=navy>// scalar function values, and we want to build an IDW model using</font>
<font color=navy>// our data.</font>
<font color=navy>// </font>
<font color=navy>// NOTE: we can work with N-dimensional models and vector-valued functions too :)</font>
<font color=navy>// </font>
<font color=navy>// Typical sequence of steps is given below:</font>
<font color=navy>// 1. we create IDW builder object</font>
<font color=navy>// 2. we attach our dataset to the IDW builder and tune algorithm settings</font>
<font color=navy>// 3. we generate IDW model</font>
<font color=navy>// 4. we use IDW model instance (evaluate, serialize, etc.)</font>
   <b>double</b> v;
<font color=navy>// Step 1: IDW builder creation.</font>
<font color=navy>//</font>
<font color=navy>// We have to specify dimensionality of the space (2 or 3) and</font>
<font color=navy>// dimensionality of the function (scalar or vector).</font>
<font color=navy>//</font>
<font color=navy>// New builder object is empty - it has not dataset and uses</font>
<font color=navy>// default model construction settings</font>
   idwbuilder builder;
   idwbuildercreate(2, 1, builder);
<font color=navy>// Step 2: dataset addition</font>
<font color=navy>//</font>
<font color=navy>// XY contains two points - x0=(-1,0) and x1=(+1,0) -</font>
<font color=navy>// and two function values f(x0)=2, f(x1)=3.</font>
   real_2d_array xy = <font color=blue><b>&quot;[[-1,0,2],[+1,0,3]]&quot;</b></font>;
   idwbuildersetpoints(builder, xy);
<font color=navy>// Step 3: choose IDW algorithm and generate model</font>
<font color=navy>//</font>
<font color=navy>// We use modified stabilized IDW algorithm with following parameters:</font>
<font color=navy>// * SRad - set to 5.0 (search radius must be large enough)</font>
<font color=navy>//</font>
<font color=navy>// IDW-MSTAB algorithm is a state-of-the-art implementation of IDW which</font>
<font color=navy>// is competitive with RBFs and bicubic splines. See comments on the</font>
<font color=navy>// idwbuildersetalgomstab() function <b>for</b> more information.</font>
   idwmodel model;
   idwreport rep;
   idwbuildersetalgomstab(builder, 5.0);
   idwfit(builder, model, rep);
<font color=navy>// Step 4: model was built, evaluate its value</font>
   v = idwcalc2(model, 1.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 3.000</font>
   <b>return</b> 0;
}
</pre>
<a name=example_idw_d_serialize></a><h6 class=pageheader>idw_d_serialize Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example shows how to serialize and unserialize IDW model.</font>
<font color=navy>// </font>
<font color=navy>// Suppose that we have set of 2-dimensional points with associated</font>
<font color=navy>// scalar function values, and we have built an IDW model using</font>
<font color=navy>// our data.</font>
<font color=navy>//</font>
<font color=navy>// This model can be serialized to string or stream. ALGLIB supports</font>
<font color=navy>// flexible (un)serialization, i.e. you can move serialized model</font>
<font color=navy>// representation between different machines (32-bit or 64-bit),</font>
<font color=navy>// different CPU architectures (x86/64, ARM) or even different</font>
<font color=navy>// programming languages supported by ALGLIB (C#, C++, ...).</font>
<font color=navy>//</font>
<font color=navy>// Our first step is to build model, evaluate it at point (1,0),</font>
<font color=navy>// and serialize it to string.</font>
   std::string s;
   <b>double</b> v;
   real_2d_array xy = <font color=blue><b>&quot;[[-1,0,2],[+1,0,3]]&quot;</b></font>;
   idwbuilder builder;
   idwmodel model;
   idwmodel model2;
   idwreport rep;
   idwbuildercreate(2, 1, builder);
   idwbuildersetpoints(builder, xy);
   idwbuildersetalgomstab(builder, 5.0);
   idwfit(builder, model, rep);
   v = idwcalc2(model, 1.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 3.000</font>
<font color=navy>//</font>
<font color=navy>// Serialization + unserialization to a different instance</font>
<font color=navy>// of the model <b>class</b>.</font>
   idwserialize(model, s);
   idwunserialize(s, model2);
<font color=navy>// Evaluate unserialized model at the same point</font>
   v = idwcalc2(model2, 1.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 3.000</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_intcomp></a><h4 class=pageheader>8.6.3. intcomp Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_nsfitspheremcc class=toc>nsfitspheremcc</a> |
<a href=#sub_nsfitspheremic class=toc>nsfitspheremic</a> |
<a href=#sub_nsfitspheremzc class=toc>nsfitspheremzc</a> |
<a href=#sub_nsfitspherex class=toc>nsfitspherex</a> |
<a href=#sub_spline1dfitpenalized class=toc>spline1dfitpenalized</a> |
<a href=#sub_spline1dfitpenalizedw class=toc>spline1dfitpenalizedw</a>
]</font>
</div>
<a name=sub_nsfitspheremcc></a><h6 class=pageheader>nsfitspheremcc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is left for backward compatibility.
Use fitspheremc() instead.
ALGLIB: Copyright 14.04.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nsfitspheremcc(real_2d_array xy, ae_int_t npoints, ae_int_t nx, real_1d_array &amp;cx, <b>double</b> &amp;rhi);
</pre>
<a name=sub_nsfitspheremic></a><h6 class=pageheader>nsfitspheremic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is left for backward compatibility.
Use fitspheremi() instead.
ALGLIB: Copyright 14.04.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nsfitspheremic(real_2d_array xy, ae_int_t npoints, ae_int_t nx, real_1d_array &amp;cx, <b>double</b> &amp;rlo);
</pre>
<a name=sub_nsfitspheremzc></a><h6 class=pageheader>nsfitspheremzc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is left for backward compatibility.
Use fitspheremz() instead.
ALGLIB: Copyright 14.04.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nsfitspheremzc(real_2d_array xy, ae_int_t npoints, ae_int_t nx, real_1d_array &amp;cx, <b>double</b> &amp;rlo, <b>double</b> &amp;rhi);
</pre>
<a name=sub_nsfitspherex></a><h6 class=pageheader>nsfitspherex Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is left for backward compatibility.
Use fitspherex() instead.
ALGLIB: Copyright 14.04.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nsfitspherex(real_2d_array xy, ae_int_t npoints, ae_int_t nx, ae_int_t problemtype, <b>double</b> epsx, ae_int_t aulits, <b>double</b> penalty, real_1d_array &amp;cx, <b>double</b> &amp;rlo, <b>double</b> &amp;rhi);
</pre>
<a name=sub_spline1dfitpenalized></a><h6 class=pageheader>spline1dfitpenalized Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is an obsolete and deprecated version of fitting by
penalized cubic spline.

It was superseded by spline1dfit(), which is an orders of magnitude faster
and more memory-efficient implementation.

Do NOT use this function in the new code!
ALGLIB Project: Copyright 18.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dfitpenalized(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t m, <b>double</b> rho, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
<b>void</b> spline1dfitpenalized(real_1d_array x, real_1d_array y, ae_int_t m, <b>double</b> rho, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_spline class=nav>lsfit_d_spline</a> ]</p>
<a name=sub_spline1dfitpenalizedw></a><h6 class=pageheader>spline1dfitpenalizedw Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is an obsolete and deprecated version of fitting by
penalized cubic spline.

It was superseded by spline1dfit(), which is an orders of magnitude faster
and more memory-efficient implementation.

Do NOT use this function in the new code!
ALGLIB Project: Copyright 19.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dfitpenalizedw(real_1d_array x, real_1d_array y, real_1d_array w, ae_int_t n, ae_int_t m, <b>double</b> rho, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
<b>void</b> spline1dfitpenalizedw(real_1d_array x, real_1d_array y, real_1d_array w, ae_int_t m, <b>double</b> rho, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_spline class=nav>lsfit_d_spline</a> ]</p>
<a name=unit_lsfit></a><h4 class=pageheader>8.6.4. lsfit Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_barycentricfitreport class=toc>barycentricfitreport</a> |
<a href=#struct_lsfitreport class=toc>lsfitreport</a> |
<a href=#struct_lsfitstate class=toc>lsfitstate</a> |
<a href=#struct_polynomialfitreport class=toc>polynomialfitreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_barycentricfitfloaterhormann class=toc>barycentricfitfloaterhormann</a> |
<a href=#sub_barycentricfitfloaterhormannwc class=toc>barycentricfitfloaterhormannwc</a> |
<a href=#sub_logisticcalc4 class=toc>logisticcalc4</a> |
<a href=#sub_logisticcalc5 class=toc>logisticcalc5</a> |
<a href=#sub_logisticfit4 class=toc>logisticfit4</a> |
<a href=#sub_logisticfit45x class=toc>logisticfit45x</a> |
<a href=#sub_logisticfit4ec class=toc>logisticfit4ec</a> |
<a href=#sub_logisticfit5 class=toc>logisticfit5</a> |
<a href=#sub_logisticfit5ec class=toc>logisticfit5ec</a> |
<a href=#sub_lsfitcreatef class=toc>lsfitcreatef</a> |
<a href=#sub_lsfitcreatefg class=toc>lsfitcreatefg</a> |
<a href=#sub_lsfitcreatefgh class=toc>lsfitcreatefgh</a> |
<a href=#sub_lsfitcreatewf class=toc>lsfitcreatewf</a> |
<a href=#sub_lsfitcreatewfg class=toc>lsfitcreatewfg</a> |
<a href=#sub_lsfitcreatewfgh class=toc>lsfitcreatewfgh</a> |
<a href=#sub_lsfitfit class=toc>lsfitfit</a> |
<a href=#sub_lsfitlinear class=toc>lsfitlinear</a> |
<a href=#sub_lsfitlinearc class=toc>lsfitlinearc</a> |
<a href=#sub_lsfitlinearw class=toc>lsfitlinearw</a> |
<a href=#sub_lsfitlinearwc class=toc>lsfitlinearwc</a> |
<a href=#sub_lsfitresults class=toc>lsfitresults</a> |
<a href=#sub_lsfitsetbc class=toc>lsfitsetbc</a> |
<a href=#sub_lsfitsetcond class=toc>lsfitsetcond</a> |
<a href=#sub_lsfitsetgradientcheck class=toc>lsfitsetgradientcheck</a> |
<a href=#sub_lsfitsetlc class=toc>lsfitsetlc</a> |
<a href=#sub_lsfitsetscale class=toc>lsfitsetscale</a> |
<a href=#sub_lsfitsetstpmax class=toc>lsfitsetstpmax</a> |
<a href=#sub_lsfitsetxrep class=toc>lsfitsetxrep</a> |
<a href=#sub_lstfitpiecewiselinearrdp class=toc>lstfitpiecewiselinearrdp</a> |
<a href=#sub_lstfitpiecewiselinearrdpfixed class=toc>lstfitpiecewiselinearrdpfixed</a> |
<a href=#sub_polynomialfit class=toc>polynomialfit</a> |
<a href=#sub_polynomialfitwc class=toc>polynomialfitwc</a> |
<a href=#sub_spline1dfitcubic class=toc>spline1dfitcubic</a> |
<a href=#sub_spline1dfitcubicwc class=toc>spline1dfitcubicwc</a> |
<a href=#sub_spline1dfithermite class=toc>spline1dfithermite</a> |
<a href=#sub_spline1dfithermitewc class=toc>spline1dfithermitewc</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_lsfit_d_lin class=toc>lsfit_d_lin</a></td><td width=15>&nbsp;</td><td>Unconstrained (general) linear least squares fitting with and without weights</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_d_linc class=toc>lsfit_d_linc</a></td><td width=15>&nbsp;</td><td>Constrained (general) linear least squares fitting with and without weights</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_d_nlf class=toc>lsfit_d_nlf</a></td><td width=15>&nbsp;</td><td>Nonlinear fitting using function value only</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_d_nlfb class=toc>lsfit_d_nlfb</a></td><td width=15>&nbsp;</td><td>Bound contstrained nonlinear fitting using function value only</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_d_nlfg class=toc>lsfit_d_nlfg</a></td><td width=15>&nbsp;</td><td>Nonlinear fitting using gradient</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_d_nlfgh class=toc>lsfit_d_nlfgh</a></td><td width=15>&nbsp;</td><td>Nonlinear fitting using gradient and Hessian</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_d_nlscale class=toc>lsfit_d_nlscale</a></td><td width=15>&nbsp;</td><td>Nonlinear fitting with custom scaling and bound constraints</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_d_pol class=toc>lsfit_d_pol</a></td><td width=15>&nbsp;</td><td>Unconstrained polynomial fitting</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_d_polc class=toc>lsfit_d_polc</a></td><td width=15>&nbsp;</td><td>Constrained polynomial fitting</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_d_spline class=toc>lsfit_d_spline</a></td><td width=15>&nbsp;</td><td>Unconstrained fitting by penalized regression spline</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_t_4pl class=toc>lsfit_t_4pl</a></td><td width=15>&nbsp;</td><td>4-parameter logistic fitting</td></tr>
<tr align=left valign=top><td><a href=#example_lsfit_t_5pl class=toc>lsfit_t_5pl</a></td><td width=15>&nbsp;</td><td>5-parameter logistic fitting</td></tr>
</table>
</div>
<a name=struct_barycentricfitreport></a><h6 class=pageheader>barycentricfitreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Barycentric fitting report:
    RMSError        RMS error
    AvgError        average error
    AvgRelError     average relative error (for non-zero Y[I])
    MaxError        maximum error
    TaskRCond       reciprocal of task's condition number
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> barycentricfitreport {
   <b>double</b> taskrcond;
   ae_int_t dbest;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
   <b>double</b> maxerror;
};
</pre>
<a name=struct_lsfitreport></a><h6 class=pageheader>lsfitreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Least squares fitting report. This structure contains informational fields
which are set by fitting functions provided by this unit.

Different functions initialize different sets of  fields,  so  you  should
read documentation on specific function you used in order  to  know  which
fields are initialized.

    TaskRCond       reciprocal of task's condition number
    IterationsCount number of internal iterations

    VarIdx          if user-supplied gradient contains errors  which  were
                    detected by nonlinear fitter, this  field  is  set  to
                    index  of  the  first  component  of gradient which is
                    suspected to be spoiled by bugs.

    RMSError        RMS error
    AvgError        average error
    AvgRelError     average relative error (for non-zero Y[I])
    MaxError        maximum error

    WRMSError       weighted RMS error

    CovPar          covariance matrix for parameters, filled by some solvers
    ErrPar          vector of errors in parameters, filled by some solvers
    ErrCurve        vector of fit errors -  variability  of  the  best-fit
                    curve, filled by some solvers.
    Noise           vector of per-point noise estimates, filled by
                    some solvers.
    R2              coefficient of determination (non-weighted, non-adjusted),
                    filled by some solvers.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> lsfitreport {
   <b>double</b> taskrcond;
   ae_int_t iterationscount;
   ae_int_t varidx;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
   <b>double</b> maxerror;
   <b>double</b> wrmserror;
   real_2d_array covpar;
   real_1d_array errpar;
   real_1d_array errcurve;
   real_1d_array noise;
   <b>double</b> r2;
};
</pre>
<a name=struct_lsfitstate></a><h6 class=pageheader>lsfitstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
Nonlinear fitter.

You should use ALGLIB functions to work with fitter.
Never try to access its fields directly!
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> lsfitstate {
   bool needf;
   bool needfg;
   bool needfgh;
   bool xupdated;
   real_1d_array c;
   double f;
   real_1d_array g;
   real_2d_array h;
   real_1d_array x;
};
</pre>
<a name=struct_polynomialfitreport></a><h6 class=pageheader>polynomialfitreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Polynomial fitting report:
    TaskRCond       reciprocal of task's condition number
    RMSError        RMS error
    AvgError        average error
    AvgRelError     average relative error (for non-zero Y[I])
    MaxError        maximum error
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> polynomialfitreport {
   <b>double</b> taskrcond;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
   <b>double</b> maxerror;
};
</pre>
<a name=sub_barycentricfitfloaterhormann></a><h6 class=pageheader>barycentricfitfloaterhormann Function</h6>
<hr width=600 align=left>
<pre class=narration>
Rational least squares fitting using  Floater-Hormann  rational  functions
with optimal D chosen from [0,9].

Equidistant  grid  with M node on [min(x),max(x)]  is  used to build basis
functions. Different values of D are tried, optimal  D  (least  root  mean
square error) is chosen.  Task  is  linear, so linear least squares solver
is used. Complexity  of  this  computational  scheme is  O(N*M^2)  (mostly
dominated by the least squares solver).

Inputs:
    X   -   points, array[0..N-1].
    Y   -   function values, array[0..N-1].
    N   -   number of points, N &gt; 0.
    M   -   number of basis functions ( = number_of_nodes), M &ge; 2.

Outputs:
    Info-   same format as in LSFitLinearWC() subroutine.
            * Info &gt; 0    task is solved
            * Info &le; 0   an error occured:
                        -4 means inconvergence of internal SVD
                        -3 means inconsistent constraints
    B   -   barycentric interpolant.
    Rep -   report, same format as in LSFitLinearWC() subroutine.
            Following fields are set:
            * DBest         best value of the D parameter
            * RMSError      rms error on the (X,Y).
            * AvgError      average error on the (X,Y).
            * AvgRelError   average relative error on the non-zero Y
            * MaxError      maximum error
                            NON-WEIGHTED ERRORS ARE CALCULATED
ALGLIB Project: Copyright 18.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> barycentricfitfloaterhormann(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t m, ae_int_t &amp;info, barycentricinterpolant &amp;b, barycentricfitreport &amp;rep);
</pre>
<a name=sub_barycentricfitfloaterhormannwc></a><h6 class=pageheader>barycentricfitfloaterhormannwc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Weghted rational least  squares  fitting  using  Floater-Hormann  rational
functions  with  optimal  D  chosen  from  [0,9],  with  constraints   and
individual weights.

Equidistant  grid  with M node on [min(x),max(x)]  is  used to build basis
functions. Different values of D are tried, optimal D (least WEIGHTED root
mean square error) is chosen.  Task  is  linear,  so  linear least squares
solver  is  used.  Complexity  of  this  computational  scheme is O(N*M^2)
(mostly dominated by the least squares solver).

SEE ALSO
* BarycentricFitFloaterHormann(), &quot;lightweight&quot; fitting without invididual
  weights and constraints.

Inputs:
    X   -   points, array[0..N-1].
    Y   -   function values, array[0..N-1].
    W   -   weights, array[0..N-1]
            Each summand in square  sum  of  approximation deviations from
            given  values  is  multiplied  by  the square of corresponding
            weight. Fill it by 1's if you don't  want  to  solve  weighted
            task.
    N   -   number of points, N &gt; 0.
    XC  -   points where function values/derivatives are constrained,
            array[0..K-1].
    YC  -   values of constraints, array[0..K-1]
    DC  -   array[0..K-1], types of constraints:
            * DC[i]=0   means that S(XC[i])=YC[i]
            * DC[i]=1   means that S'(XC[i])=YC[i]
            SEE BELOW FOR IMPORTANT INFORMATION ON CONSTRAINTS
    K   -   number of constraints, 0 &le; K &lt; M.
            K=0 means no constraints (XC/YC/DC are not used in such cases)
    M   -   number of basis functions ( = number_of_nodes), M &ge; 2.

Outputs:
    Info-   same format as in LSFitLinearWC() subroutine.
            * Info &gt; 0    task is solved
            * Info &le; 0   an error occured:
                        -4 means inconvergence of internal SVD
                        -3 means inconsistent constraints
                        -1 means another errors in parameters passed
                           (N &le; 0, for example)
    B   -   barycentric interpolant.
    Rep -   report, same format as in LSFitLinearWC() subroutine.
            Following fields are set:
            * DBest         best value of the D parameter
            * RMSError      rms error on the (X,Y).
            * AvgError      average error on the (X,Y).
            * AvgRelError   average relative error on the non-zero Y
            * MaxError      maximum error
                            NON-WEIGHTED ERRORS ARE CALCULATED

IMPORTANT:
    this subroutine doesn't calculate task's condition number for K &ne; 0.

SETTING CONSTRAINTS - DANGERS AND OPPORTUNITIES:

Setting constraints can lead  to undesired  results,  like ill-conditioned
behavior, or inconsistency being detected. From the other side,  it allows
us to improve quality of the fit. Here we summarize  our  experience  with
constrained barycentric interpolants:
* excessive  constraints  can  be  inconsistent.   Floater-Hormann   basis
  functions aren't as flexible as splines (although they are very smooth).
* the more evenly constraints are spread across [min(x),max(x)],  the more
  chances that they will be consistent
* the  greater  is  M (given  fixed  constraints),  the  more chances that
  constraints will be consistent
* in the general case, consistency of constraints IS NOT GUARANTEED.
* in the several special cases, however, we CAN guarantee consistency.
* one of this cases is constraints on the function  VALUES at the interval
  boundaries. Note that consustency of the  constraints  on  the  function
  DERIVATIVES is NOT guaranteed (you can use in such cases  cubic  splines
  which are more flexible).
* another  special  case  is ONE constraint on the function value (OR, but
  not AND, derivative) anywhere in the interval

Our final recommendation is to use constraints  WHEN  AND  ONLY  WHEN  you
can't solve your task without them. Anything beyond  special  cases  given
above is not guaranteed and may result in inconsistency.
ALGLIB Project: Copyright 18.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> barycentricfitfloaterhormannwc(real_1d_array x, real_1d_array y, real_1d_array w, ae_int_t n, real_1d_array xc, real_1d_array yc, integer_1d_array dc, ae_int_t k, ae_int_t m, ae_int_t &amp;info, barycentricinterpolant &amp;b, barycentricfitreport &amp;rep);
</pre>
<a name=sub_logisticcalc4></a><h6 class=pageheader>logisticcalc4 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates value of four-parameter logistic (4PL)  model  at
specified point X. 4PL model has following form:

    F(x|A,B,C,D) = D+(A-D)/(1+power(x/C,B))

Inputs:
    X       -   current point, X &ge; 0:
                * zero X is correctly handled even for B &le; 0
                * negative X results in exception.
    A, B, C, D- parameters of 4PL model:
                * A is unconstrained
                * B is unconstrained; zero or negative values are handled
                  correctly.
                * C &gt; 0, non-positive value results in exception
                * D is unconstrained

Result:
    model value at X

NOTE: if B=0, denominator is assumed to be equal to 2.0 even  for  zero  X
      (strictly speaking, 0^0 is undefined).

NOTE: this function also throws exception  if  all  input  parameters  are
      correct, but overflow was detected during calculations.

NOTE: this function performs a lot of checks;  if  you  need  really  high
      performance, consider evaluating model  yourself,  without  checking
      for degenerate cases.
ALGLIB Project: Copyright 14.05.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> logisticcalc4(<b>double</b> x, <b>double</b> a, <b>double</b> b, <b>double</b> c, <b>double</b> d);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_t_4pl class=nav>lsfit_t_4pl</a> ]</p>
<a name=sub_logisticcalc5></a><h6 class=pageheader>logisticcalc5 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates value of five-parameter logistic (5PL)  model  at
specified point X. 5PL model has following form:

    F(x|A,B,C,D,G) = D+(A-D)/power(1+power(x/C,B),G)

Inputs:
    X       -   current point, X &ge; 0:
                * zero X is correctly handled even for B &le; 0
                * negative X results in exception.
    A, B, C, D, G- parameters of 5PL model:
                * A is unconstrained
                * B is unconstrained; zero or negative values are handled
                  correctly.
                * C &gt; 0, non-positive value results in exception
                * D is unconstrained
                * G &gt; 0, non-positive value results in exception

Result:
    model value at X

NOTE: if B=0, denominator is assumed to be equal to power(2.0,G) even  for
      zero X (strictly speaking, 0^0 is undefined).

NOTE: this function also throws exception  if  all  input  parameters  are
      correct, but overflow was detected during calculations.

NOTE: this function performs a lot of checks;  if  you  need  really  high
      performance, consider evaluating model  yourself,  without  checking
      for degenerate cases.
ALGLIB Project: Copyright 14.05.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> logisticcalc5(<b>double</b> x, <b>double</b> a, <b>double</b> b, <b>double</b> c, <b>double</b> d, <b>double</b> g);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_t_5pl class=nav>lsfit_t_5pl</a> ]</p>
<a name=sub_logisticfit4></a><h6 class=pageheader>logisticfit4 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function fits four-parameter logistic (4PL) model  to  data  provided
by user. 4PL model has following form:

    F(x|A,B,C,D) = D+(A-D)/(1+power(x/C,B))

Here:
    * A, D - unconstrained (see LogisticFit4EC() for constrained 4PL)
    * B &ge; 0
    * C &gt; 0

IMPORTANT: output of this function is constrained in  such  way that  B &gt; 0.
           Because 4PL model is symmetric with respect to B, there  is  no
           need to explore  B &lt; 0.  Constraining  B  makes  algorithm easier
           to stabilize and debug.
           Users  who  for  some  reason  prefer to work with negative B's
           should transform output themselves (swap A and D, replace B  by
           -B).

4PL fitting is implemented as follows:
* we perform small number of restarts from random locations which helps to
  solve problem of bad local extrema. Locations are only partially  random
  - we use input data to determine good  initial  guess,  but  we  include
  controlled amount of randomness.
* we perform Levenberg-Marquardt fitting with very  tight  constraints  on
  parameters B and C - it allows us to find good  initial  guess  for  the
  second stage without risk of running into &quot;flat spot&quot;.
* second  Levenberg-Marquardt  round  is   performed   without   excessive
  constraints. Results from the previous round are used as initial guess.
* after fitting is done, we compare results with best values found so far,
  rewrite &quot;best solution&quot; if needed, and move to next random location.

Overall algorithm is very stable and is not prone to  bad  local  extrema.
Furthermore, it automatically scales when input data have  very  large  or
very small range.

Inputs:
    X       -   array[N], stores X-values.
                MUST include only non-negative numbers  (but  may  include
                zero values). Can be unsorted.
    Y       -   array[N], values to fit.
    N       -   number of points. If N is less than  length  of  X/Y, only
                leading N elements are used.

Outputs:
    A, B, C, D- parameters of 4PL model
    Rep     -   fitting report. This structure has many fields,  but  ONLY
                ONES LISTED BELOW ARE SET:
                * Rep.IterationsCount - number of iterations performed
                * Rep.RMSError - root-mean-square error
                * Rep.AvgError - average absolute error
                * Rep.AvgRelError - average relative error (calculated for
                  non-zero Y-values)
                * Rep.MaxError - maximum absolute error
                * Rep.R2 - coefficient of determination,  R-squared.  This
                  coefficient   is  calculated  as  R2=1-RSS/TSS  (in case
                  of nonlinear  regression  there  are  multiple  ways  to
                  define R2, each of them giving different results).

NOTE: for stability reasons the B parameter is restricted by [1/1000,1000]
      range. It prevents  algorithm from making trial steps  deep into the
      area of bad parameters.

NOTE: after  you  obtained  coefficients,  you  can  evaluate  model  with
      LogisticCalc4() function.

NOTE: if you need better control over fitting process than provided by this
      function, you may use LogisticFit45X().

NOTE: step is automatically scaled according to scale of parameters  being
      fitted before we compare its length with EpsX. Thus,  this  function
      can be used to fit data with very small or very large values without
      changing EpsX.
ALGLIB Project: Copyright 14.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> logisticfit4(real_1d_array x, real_1d_array y, ae_int_t n, <b>double</b> &amp;a, <b>double</b> &amp;b, <b>double</b> &amp;c, <b>double</b> &amp;d, lsfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_t_4pl class=nav>lsfit_t_4pl</a> ]</p>
<a name=sub_logisticfit45x></a><h6 class=pageheader>logisticfit45x Function</h6>
<hr width=600 align=left>
<pre class=narration>
This is &quot;expert&quot; 4PL/5PL fitting function, which can be used if  you  need
better control over fitting process than provided  by  LogisticFit4()  or
LogisticFit5().

This function fits model of the form

    F(x|A,B,C,D)   = D+(A-D)/(1+power(x/C,B))           (4PL model)

or

    F(x|A,B,C,D,G) = D+(A-D)/power(1+power(x/C,B),G)    (5PL model)

Here:
    * A, D - unconstrained
    * B &ge; 0 for 4PL, unconstrained for 5PL
    * C &gt; 0
    * G &gt; 0 (if present)

Inputs:
    X       -   array[N], stores X-values.
                MUST include only non-negative numbers  (but  may  include
                zero values). Can be unsorted.
    Y       -   array[N], values to fit.
    N       -   number of points. If N is less than  length  of  X/Y, only
                leading N elements are used.
    CnstrLeft-  optional equality constraint for model value at the   left
                boundary (at X=0). Specify NAN (Not-a-Number)  if  you  do
                not need constraint on the model value at X=0 (in C++  you
                can pass NAN as parameter, in  C#  it  will  be
                Double.NaN).
                See  below,  section  &quot;EQUALITY  CONSTRAINTS&quot;   for   more
                information about constraints.
    CnstrRight- optional equality constraint for model value at X=infinity.
                Specify NAN (Not-a-Number) if you do not  need  constraint
                on the model value (in C++  you can pass NAN as
                parameter, in  C# it will  be Double.NaN).
                See  below,  section  &quot;EQUALITY  CONSTRAINTS&quot;   for   more
                information about constraints.
    Is4PL   -   whether 4PL or 5PL models are fitted
    LambdaV -   regularization coefficient, LambdaV &ge; 0.
                Set it to zero unless you know what you are doing.
    EpsX    -   stopping condition (step size), EpsX &ge; 0.
                Zero value means that small step is automatically chosen.
                See notes below for more information.
    RsCnt   -   number of repeated restarts from  random  points.  4PL/5PL
                models are prone to problem of bad local extrema. Utilizing
                multiple random restarts allows  us  to  improve algorithm
                convergence.
                RsCnt &ge; 0.
                Zero value means that function automatically choose  small
                amount of restarts (recommended).

Outputs:
    A, B, C, D- parameters of 4PL model
    G       -   parameter of 5PL model; for Is4PL=True, G=1 is returned.
    Rep     -   fitting report. This structure has many fields,  but  ONLY
                ONES LISTED BELOW ARE SET:
                * Rep.IterationsCount - number of iterations performed
                * Rep.RMSError - root-mean-square error
                * Rep.AvgError - average absolute error
                * Rep.AvgRelError - average relative error (calculated for
                  non-zero Y-values)
                * Rep.MaxError - maximum absolute error
                * Rep.R2 - coefficient of determination,  R-squared.  This
                  coefficient   is  calculated  as  R2=1-RSS/TSS  (in case
                  of nonlinear  regression  there  are  multiple  ways  to
                  define R2, each of them giving different results).

NOTE: for better stability B  parameter is restricted by [+-1/1000,+-1000]
      range, and G is restricted by [1/10,10] range. It prevents algorithm
      from making trial steps deep into the area of bad parameters.

NOTE: after  you  obtained  coefficients,  you  can  evaluate  model  with
      LogisticCalc5() function.

NOTE: step is automatically scaled according to scale of parameters  being
      fitted before we compare its length with EpsX. Thus,  this  function
      can be used to fit data with very small or very large values without
      changing EpsX.

EQUALITY CONSTRAINTS ON PARAMETERS

4PL/5PL solver supports equality constraints on model values at  the  left
boundary (X=0) and right  boundary  (X=infinity).  These  constraints  are
completely optional and you can specify both of them, only  one  -  or  no
constraints at all.

Parameter  CnstrLeft  contains  left  constraint (or NAN for unconstrained
fitting), and CnstrRight contains right  one.  For  4PL,  left  constraint
ALWAYS corresponds to parameter A, and right one is ALWAYS  constraint  on
D. That's because 4PL model is normalized in such way that B &ge; 0.

For 5PL model things are different. Unlike  4PL  one,  5PL  model  is  NOT
symmetric with respect to  change  in  sign  of  B. Thus, negative B's are
possible, and left constraint may constrain parameter A (for positive B's)
- or parameter D (for negative B's). Similarly changes  meaning  of  right
constraint.

You do not have to decide what parameter to  constrain  -  algorithm  will
automatically determine correct parameters as fitting progresses. However,
question highlighted above is important when you interpret fitting results.
ALGLIB Project: Copyright 14.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> logisticfit45x(real_1d_array x, real_1d_array y, ae_int_t n, <b>double</b> cnstrleft, <b>double</b> cnstrright, <b>bool</b> is4pl, <b>double</b> lambdav, <b>double</b> epsx, ae_int_t rscnt, <b>double</b> &amp;a, <b>double</b> &amp;b, <b>double</b> &amp;c, <b>double</b> &amp;d, <b>double</b> &amp;g, lsfitreport &amp;rep);
</pre>
<a name=sub_logisticfit4ec></a><h6 class=pageheader>logisticfit4ec Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function fits four-parameter logistic (4PL) model  to  data  provided
by user, with optional constraints on parameters A and D.  4PL  model  has
following form:

    F(x|A,B,C,D) = D+(A-D)/(1+power(x/C,B))

Here:
    * A, D - with optional equality constraints
    * B &ge; 0
    * C &gt; 0

IMPORTANT: output of this function is constrained in  such  way that  B &gt; 0.
           Because 4PL model is symmetric with respect to B, there  is  no
           need to explore  B &lt; 0.  Constraining  B  makes  algorithm easier
           to stabilize and debug.
           Users  who  for  some  reason  prefer to work with negative B's
           should transform output themselves (swap A and D, replace B  by
           -B).

4PL fitting is implemented as follows:
* we perform small number of restarts from random locations which helps to
  solve problem of bad local extrema. Locations are only partially  random
  - we use input data to determine good  initial  guess,  but  we  include
  controlled amount of randomness.
* we perform Levenberg-Marquardt fitting with very  tight  constraints  on
  parameters B and C - it allows us to find good  initial  guess  for  the
  second stage without risk of running into &quot;flat spot&quot;.
* second  Levenberg-Marquardt  round  is   performed   without   excessive
  constraints. Results from the previous round are used as initial guess.
* after fitting is done, we compare results with best values found so far,
  rewrite &quot;best solution&quot; if needed, and move to next random location.

Overall algorithm is very stable and is not prone to  bad  local  extrema.
Furthermore, it automatically scales when input data have  very  large  or
very small range.

Inputs:
    X       -   array[N], stores X-values.
                MUST include only non-negative numbers  (but  may  include
                zero values). Can be unsorted.
    Y       -   array[N], values to fit.
    N       -   number of points. If N is less than  length  of  X/Y, only
                leading N elements are used.
    CnstrLeft-  optional equality constraint for model value at the   left
                boundary (at X=0). Specify NAN (Not-a-Number)  if  you  do
                not need constraint on the model value at X=0 (in C++  you
                can pass NAN as parameter, in  C#  it  will  be
                Double.NaN).
                See  below,  section  &quot;EQUALITY  CONSTRAINTS&quot;   for   more
                information about constraints.
    CnstrRight- optional equality constraint for model value at X=infinity.
                Specify NAN (Not-a-Number) if you do not  need  constraint
                on the model value (in C++  you can pass NAN as
                parameter, in  C# it will  be Double.NaN).
                See  below,  section  &quot;EQUALITY  CONSTRAINTS&quot;   for   more
                information about constraints.

Outputs:
    A, B, C, D- parameters of 4PL model
    Rep     -   fitting report. This structure has many fields,  but  ONLY
                ONES LISTED BELOW ARE SET:
                * Rep.IterationsCount - number of iterations performed
                * Rep.RMSError - root-mean-square error
                * Rep.AvgError - average absolute error
                * Rep.AvgRelError - average relative error (calculated for
                  non-zero Y-values)
                * Rep.MaxError - maximum absolute error
                * Rep.R2 - coefficient of determination,  R-squared.  This
                  coefficient   is  calculated  as  R2=1-RSS/TSS  (in case
                  of nonlinear  regression  there  are  multiple  ways  to
                  define R2, each of them giving different results).

NOTE: for stability reasons the B parameter is restricted by [1/1000,1000]
      range. It prevents  algorithm from making trial steps  deep into the
      area of bad parameters.

NOTE: after  you  obtained  coefficients,  you  can  evaluate  model  with
      LogisticCalc4() function.

NOTE: if you need better control over fitting process than provided by this
      function, you may use LogisticFit45X().

NOTE: step is automatically scaled according to scale of parameters  being
      fitted before we compare its length with EpsX. Thus,  this  function
      can be used to fit data with very small or very large values without
      changing EpsX.

EQUALITY CONSTRAINTS ON PARAMETERS

4PL/5PL solver supports equality constraints on model values at  the  left
boundary (X=0) and right  boundary  (X=infinity).  These  constraints  are
completely optional and you can specify both of them, only  one  -  or  no
constraints at all.

Parameter  CnstrLeft  contains  left  constraint (or NAN for unconstrained
fitting), and CnstrRight contains right  one.  For  4PL,  left  constraint
ALWAYS corresponds to parameter A, and right one is ALWAYS  constraint  on
D. That's because 4PL model is normalized in such way that B &ge; 0.
ALGLIB Project: Copyright 14.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> logisticfit4ec(real_1d_array x, real_1d_array y, ae_int_t n, <b>double</b> cnstrleft, <b>double</b> cnstrright, <b>double</b> &amp;a, <b>double</b> &amp;b, <b>double</b> &amp;c, <b>double</b> &amp;d, lsfitreport &amp;rep);
</pre>
<a name=sub_logisticfit5></a><h6 class=pageheader>logisticfit5 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function fits five-parameter logistic (5PL) model  to  data  provided
by user. 5PL model has following form:

    F(x|A,B,C,D,G) = D+(A-D)/power(1+power(x/C,B),G)

Here:
    * A, D - unconstrained
    * B - unconstrained
    * C &gt; 0
    * G &gt; 0

IMPORTANT: unlike in  4PL  fitting,  output  of  this  function   is   NOT
           constrained in  such  way that B is guaranteed to be  positive.
           Furthermore,  unlike  4PL,  5PL  model  is  NOT  symmetric with
           respect to B, so you can NOT transform model to equivalent one,
           with B having desired sign (&gt; 0 or &lt; 0).

5PL fitting is implemented as follows:
* we perform small number of restarts from random locations which helps to
  solve problem of bad local extrema. Locations are only partially  random
  - we use input data to determine good  initial  guess,  but  we  include
  controlled amount of randomness.
* we perform Levenberg-Marquardt fitting with very  tight  constraints  on
  parameters B and C - it allows us to find good  initial  guess  for  the
  second stage without risk of running into &quot;flat spot&quot;.  Parameter  G  is
  fixed at G=1.
* second  Levenberg-Marquardt  round  is   performed   without   excessive
  constraints on B and C, but with G still equal to 1.  Results  from  the
  previous round are used as initial guess.
* third Levenberg-Marquardt round relaxes constraints on G  and  tries  two
  different models - one with B &gt; 0 and one with B &lt; 0.
* after fitting is done, we compare results with best values found so far,
  rewrite &quot;best solution&quot; if needed, and move to next random location.

Overall algorithm is very stable and is not prone to  bad  local  extrema.
Furthermore, it automatically scales when input data have  very  large  or
very small range.

Inputs:
    X       -   array[N], stores X-values.
                MUST include only non-negative numbers  (but  may  include
                zero values). Can be unsorted.
    Y       -   array[N], values to fit.
    N       -   number of points. If N is less than  length  of  X/Y, only
                leading N elements are used.

Outputs:
    A,B,C,D,G-  parameters of 5PL model
    Rep     -   fitting report. This structure has many fields,  but  ONLY
                ONES LISTED BELOW ARE SET:
                * Rep.IterationsCount - number of iterations performed
                * Rep.RMSError - root-mean-square error
                * Rep.AvgError - average absolute error
                * Rep.AvgRelError - average relative error (calculated for
                  non-zero Y-values)
                * Rep.MaxError - maximum absolute error
                * Rep.R2 - coefficient of determination,  R-squared.  This
                  coefficient   is  calculated  as  R2=1-RSS/TSS  (in case
                  of nonlinear  regression  there  are  multiple  ways  to
                  define R2, each of them giving different results).

NOTE: for better stability B  parameter is restricted by [+-1/1000,+-1000]
      range, and G is restricted by [1/10,10] range. It prevents algorithm
      from making trial steps deep into the area of bad parameters.

NOTE: after  you  obtained  coefficients,  you  can  evaluate  model  with
      LogisticCalc5() function.

NOTE: if you need better control over fitting process than provided by this
      function, you may use LogisticFit45X().

NOTE: step is automatically scaled according to scale of parameters  being
      fitted before we compare its length with EpsX. Thus,  this  function
      can be used to fit data with very small or very large values without
      changing EpsX.
ALGLIB Project: Copyright 14.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> logisticfit5(real_1d_array x, real_1d_array y, ae_int_t n, <b>double</b> &amp;a, <b>double</b> &amp;b, <b>double</b> &amp;c, <b>double</b> &amp;d, <b>double</b> &amp;g, lsfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_t_5pl class=nav>lsfit_t_5pl</a> ]</p>
<a name=sub_logisticfit5ec></a><h6 class=pageheader>logisticfit5ec Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function fits five-parameter logistic (5PL) model  to  data  provided
by user, subject to optional equality constraints on parameters A  and  D.
5PL model has following form:

    F(x|A,B,C,D,G) = D+(A-D)/power(1+power(x/C,B),G)

Here:
    * A, D - with optional equality constraints
    * B - unconstrained
    * C &gt; 0
    * G &gt; 0

IMPORTANT: unlike in  4PL  fitting,  output  of  this  function   is   NOT
           constrained in  such  way that B is guaranteed to be  positive.
           Furthermore,  unlike  4PL,  5PL  model  is  NOT  symmetric with
           respect to B, so you can NOT transform model to equivalent one,
           with B having desired sign (&gt; 0 or &lt; 0).

5PL fitting is implemented as follows:
* we perform small number of restarts from random locations which helps to
  solve problem of bad local extrema. Locations are only partially  random
  - we use input data to determine good  initial  guess,  but  we  include
  controlled amount of randomness.
* we perform Levenberg-Marquardt fitting with very  tight  constraints  on
  parameters B and C - it allows us to find good  initial  guess  for  the
  second stage without risk of running into &quot;flat spot&quot;.  Parameter  G  is
  fixed at G=1.
* second  Levenberg-Marquardt  round  is   performed   without   excessive
  constraints on B and C, but with G still equal to 1.  Results  from  the
  previous round are used as initial guess.
* third Levenberg-Marquardt round relaxes constraints on G  and  tries  two
  different models - one with B &gt; 0 and one with B &lt; 0.
* after fitting is done, we compare results with best values found so far,
  rewrite &quot;best solution&quot; if needed, and move to next random location.

Overall algorithm is very stable and is not prone to  bad  local  extrema.
Furthermore, it automatically scales when input data have  very  large  or
very small range.

Inputs:
    X       -   array[N], stores X-values.
                MUST include only non-negative numbers  (but  may  include
                zero values). Can be unsorted.
    Y       -   array[N], values to fit.
    N       -   number of points. If N is less than  length  of  X/Y, only
                leading N elements are used.
    CnstrLeft-  optional equality constraint for model value at the   left
                boundary (at X=0). Specify NAN (Not-a-Number)  if  you  do
                not need constraint on the model value at X=0 (in C++  you
                can pass NAN as parameter, in  C#  it  will  be
                Double.NaN).
                See  below,  section  &quot;EQUALITY  CONSTRAINTS&quot;   for   more
                information about constraints.
    CnstrRight- optional equality constraint for model value at X=infinity.
                Specify NAN (Not-a-Number) if you do not  need  constraint
                on the model value (in C++  you can pass NAN as
                parameter, in  C# it will  be Double.NaN).
                See  below,  section  &quot;EQUALITY  CONSTRAINTS&quot;   for   more
                information about constraints.

Outputs:
    A,B,C,D,G-  parameters of 5PL model
    Rep     -   fitting report. This structure has many fields,  but  ONLY
                ONES LISTED BELOW ARE SET:
                * Rep.IterationsCount - number of iterations performed
                * Rep.RMSError - root-mean-square error
                * Rep.AvgError - average absolute error
                * Rep.AvgRelError - average relative error (calculated for
                  non-zero Y-values)
                * Rep.MaxError - maximum absolute error
                * Rep.R2 - coefficient of determination,  R-squared.  This
                  coefficient   is  calculated  as  R2=1-RSS/TSS  (in case
                  of nonlinear  regression  there  are  multiple  ways  to
                  define R2, each of them giving different results).

NOTE: for better stability B  parameter is restricted by [+-1/1000,+-1000]
      range, and G is restricted by [1/10,10] range. It prevents algorithm
      from making trial steps deep into the area of bad parameters.

NOTE: after  you  obtained  coefficients,  you  can  evaluate  model  with
      LogisticCalc5() function.

NOTE: if you need better control over fitting process than provided by this
      function, you may use LogisticFit45X().

NOTE: step is automatically scaled according to scale of parameters  being
      fitted before we compare its length with EpsX. Thus,  this  function
      can be used to fit data with very small or very large values without
      changing EpsX.

EQUALITY CONSTRAINTS ON PARAMETERS

5PL solver supports equality constraints on model  values  at   the   left
boundary (X=0) and right  boundary  (X=infinity).  These  constraints  are
completely optional and you can specify both of them, only  one  -  or  no
constraints at all.

Parameter  CnstrLeft  contains  left  constraint (or NAN for unconstrained
fitting), and CnstrRight contains right  one.

Unlike 4PL one, 5PL model is NOT symmetric with respect to  change in sign
of B. Thus, negative B's are possible, and left constraint  may  constrain
parameter A (for positive B's)  -  or  parameter  D  (for  negative  B's).
Similarly changes meaning of right constraint.

You do not have to decide what parameter to  constrain  -  algorithm  will
automatically determine correct parameters as fitting progresses. However,
question highlighted above is important when you interpret fitting results.
ALGLIB Project: Copyright 14.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> logisticfit5ec(real_1d_array x, real_1d_array y, ae_int_t n, <b>double</b> cnstrleft, <b>double</b> cnstrright, <b>double</b> &amp;a, <b>double</b> &amp;b, <b>double</b> &amp;c, <b>double</b> &amp;d, <b>double</b> &amp;g, lsfitreport &amp;rep);
</pre>
<a name=sub_lsfitcreatef></a><h6 class=pageheader>lsfitcreatef Function</h6>
<hr width=600 align=left>
<pre class=narration>
Nonlinear least squares fitting using function values only.

Combination of numerical differentiation and secant updates is used to
obtain function Jacobian.

Nonlinear task min(F(c)) is solved, where

    F(c) = (f(c,x[0])-y[0])^2 + ... + (f(c,x[n-1])-y[n-1])^2,

    * N is a number of points,
    * M is a dimension of a space points belong to,
    * K is a dimension of a space of parameters being fitted,
    * w is an N-dimensional vector of weight coefficients,
    * x is a set of N points, each of them is an M-dimensional vector,
    * c is a K-dimensional vector of parameters being fitted

This subroutine uses only f(c,x[i]).

Inputs:
    X       -   array[0..N-1,0..M-1], points (one row = one point)
    Y       -   array[0..N-1], function values.
    C       -   array[0..K-1], initial approximation to the solution,
    N       -   number of points, N &gt; 1
    M       -   dimension of space
    K       -   number of parameters being fitted
    DiffStep-   numerical differentiation step;
                should not be very small or large;
                large = loss of accuracy
                small = growth of round-off errors

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 18.10.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitcreatef(real_2d_array x, real_1d_array y, real_1d_array c, ae_int_t n, ae_int_t m, ae_int_t k, <b>double</b> diffstep, lsfitstate &amp;state);
<b>void</b> lsfitcreatef(real_2d_array x, real_1d_array y, real_1d_array c, <b>double</b> diffstep, lsfitstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlf class=nav>lsfit_d_nlf</a> | <a href=#example_lsfit_d_nlfb class=nav>lsfit_d_nlfb</a> | <a href=#example_lsfit_d_nlscale class=nav>lsfit_d_nlscale</a> ]</p>
<a name=sub_lsfitcreatefg></a><h6 class=pageheader>lsfitcreatefg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Nonlinear least squares fitting using gradient only, without individual
weights.

Nonlinear task min(F(c)) is solved, where

    F(c) = ((f(c,x[0])-y[0]))^2 + ... + ((f(c,x[n-1])-y[n-1]))^2,

    * N is a number of points,
    * M is a dimension of a space points belong to,
    * K is a dimension of a space of parameters being fitted,
    * x is a set of N points, each of them is an M-dimensional vector,
    * c is a K-dimensional vector of parameters being fitted

This subroutine uses only f(c,x[i]) and its gradient.

Inputs:
    X       -   array[0..N-1,0..M-1], points (one row = one point)
    Y       -   array[0..N-1], function values.
    C       -   array[0..K-1], initial approximation to the solution,
    N       -   number of points, N &gt; 1
    M       -   dimension of space
    K       -   number of parameters being fitted
    CheapFG -   boolean flag, which is:
                * True  if both function and gradient calculation complexity
                        are less than O(M^2).  An improved  algorithm  can
                        be  used  which corresponds  to  FGJ  scheme  from
                        MINLM unit.
                * False otherwise.
                        Standard Jacibian-bases  Levenberg-Marquardt  algo
                        will be used (FJ scheme).

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitcreatefg(real_2d_array x, real_1d_array y, real_1d_array c, ae_int_t n, ae_int_t m, ae_int_t k, <b>bool</b> cheapfg, lsfitstate &amp;state);
<b>void</b> lsfitcreatefg(real_2d_array x, real_1d_array y, real_1d_array c, <b>bool</b> cheapfg, lsfitstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlfg class=nav>lsfit_d_nlfg</a> ]</p>
<a name=sub_lsfitcreatefgh></a><h6 class=pageheader>lsfitcreatefgh Function</h6>
<hr width=600 align=left>
<pre class=narration>
Nonlinear least squares fitting using gradient/Hessian, without individial
weights.

Nonlinear task min(F(c)) is solved, where

    F(c) = ((f(c,x[0])-y[0]))^2 + ... + ((f(c,x[n-1])-y[n-1]))^2,

    * N is a number of points,
    * M is a dimension of a space points belong to,
    * K is a dimension of a space of parameters being fitted,
    * x is a set of N points, each of them is an M-dimensional vector,
    * c is a K-dimensional vector of parameters being fitted

This subroutine uses f(c,x[i]), its gradient and its Hessian.

Inputs:
    X       -   array[0..N-1,0..M-1], points (one row = one point)
    Y       -   array[0..N-1], function values.
    C       -   array[0..K-1], initial approximation to the solution,
    N       -   number of points, N &gt; 1
    M       -   dimension of space
    K       -   number of parameters being fitted

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitcreatefgh(real_2d_array x, real_1d_array y, real_1d_array c, ae_int_t n, ae_int_t m, ae_int_t k, lsfitstate &amp;state);
<b>void</b> lsfitcreatefgh(real_2d_array x, real_1d_array y, real_1d_array c, lsfitstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlfgh class=nav>lsfit_d_nlfgh</a> ]</p>
<a name=sub_lsfitcreatewf></a><h6 class=pageheader>lsfitcreatewf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Weighted nonlinear least squares fitting using function values only.

Combination of numerical differentiation and secant updates is used to
obtain function Jacobian.

Nonlinear task min(F(c)) is solved, where

    F(c) = (w[0]*(f(c,x[0])-y[0]))^2 + ... + (w[n-1]*(f(c,x[n-1])-y[n-1]))^2,

    * N is a number of points,
    * M is a dimension of a space points belong to,
    * K is a dimension of a space of parameters being fitted,
    * w is an N-dimensional vector of weight coefficients,
    * x is a set of N points, each of them is an M-dimensional vector,
    * c is a K-dimensional vector of parameters being fitted

This subroutine uses only f(c,x[i]).

Inputs:
    X       -   array[0..N-1,0..M-1], points (one row = one point)
    Y       -   array[0..N-1], function values.
    W       -   weights, array[0..N-1]
    C       -   array[0..K-1], initial approximation to the solution,
    N       -   number of points, N &gt; 1
    M       -   dimension of space
    K       -   number of parameters being fitted
    DiffStep-   numerical differentiation step;
                should not be very small or large;
                large = loss of accuracy
                small = growth of round-off errors

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 18.10.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitcreatewf(real_2d_array x, real_1d_array y, real_1d_array w, real_1d_array c, ae_int_t n, ae_int_t m, ae_int_t k, <b>double</b> diffstep, lsfitstate &amp;state);
<b>void</b> lsfitcreatewf(real_2d_array x, real_1d_array y, real_1d_array w, real_1d_array c, <b>double</b> diffstep, lsfitstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlf class=nav>lsfit_d_nlf</a> | <a href=#example_lsfit_d_nlfb class=nav>lsfit_d_nlfb</a> ]</p>
<a name=sub_lsfitcreatewfg></a><h6 class=pageheader>lsfitcreatewfg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Weighted nonlinear least squares fitting using gradient only.

Nonlinear task min(F(c)) is solved, where

    F(c) = (w[0]*(f(c,x[0])-y[0]))^2 + ... + (w[n-1]*(f(c,x[n-1])-y[n-1]))^2,

    * N is a number of points,
    * M is a dimension of a space points belong to,
    * K is a dimension of a space of parameters being fitted,
    * w is an N-dimensional vector of weight coefficients,
    * x is a set of N points, each of them is an M-dimensional vector,
    * c is a K-dimensional vector of parameters being fitted

This subroutine uses only f(c,x[i]) and its gradient.

Inputs:
    X       -   array[0..N-1,0..M-1], points (one row = one point)
    Y       -   array[0..N-1], function values.
    W       -   weights, array[0..N-1]
    C       -   array[0..K-1], initial approximation to the solution,
    N       -   number of points, N &gt; 1
    M       -   dimension of space
    K       -   number of parameters being fitted
    CheapFG -   boolean flag, which is:
                * True  if both function and gradient calculation complexity
                        are less than O(M^2).  An improved  algorithm  can
                        be  used  which corresponds  to  FGJ  scheme  from
                        MINLM unit.
                * False otherwise.
                        Standard Jacibian-bases  Levenberg-Marquardt  algo
                        will be used (FJ scheme).

Outputs:
    State   -   structure which stores algorithm state

See also:
    LSFitResults
    LSFitCreateFG (fitting without weights)
    LSFitCreateWFGH (fitting using Hessian)
    LSFitCreateFGH (fitting using Hessian, without weights)
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitcreatewfg(real_2d_array x, real_1d_array y, real_1d_array w, real_1d_array c, ae_int_t n, ae_int_t m, ae_int_t k, <b>bool</b> cheapfg, lsfitstate &amp;state);
<b>void</b> lsfitcreatewfg(real_2d_array x, real_1d_array y, real_1d_array w, real_1d_array c, <b>bool</b> cheapfg, lsfitstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlfg class=nav>lsfit_d_nlfg</a> ]</p>
<a name=sub_lsfitcreatewfgh></a><h6 class=pageheader>lsfitcreatewfgh Function</h6>
<hr width=600 align=left>
<pre class=narration>
Weighted nonlinear least squares fitting using gradient/Hessian.

Nonlinear task min(F(c)) is solved, where

    F(c) = (w[0]*(f(c,x[0])-y[0]))^2 + ... + (w[n-1]*(f(c,x[n-1])-y[n-1]))^2,

    * N is a number of points,
    * M is a dimension of a space points belong to,
    * K is a dimension of a space of parameters being fitted,
    * w is an N-dimensional vector of weight coefficients,
    * x is a set of N points, each of them is an M-dimensional vector,
    * c is a K-dimensional vector of parameters being fitted

This subroutine uses f(c,x[i]), its gradient and its Hessian.

Inputs:
    X       -   array[0..N-1,0..M-1], points (one row = one point)
    Y       -   array[0..N-1], function values.
    W       -   weights, array[0..N-1]
    C       -   array[0..K-1], initial approximation to the solution,
    N       -   number of points, N &gt; 1
    M       -   dimension of space
    K       -   number of parameters being fitted

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitcreatewfgh(real_2d_array x, real_1d_array y, real_1d_array w, real_1d_array c, ae_int_t n, ae_int_t m, ae_int_t k, lsfitstate &amp;state);
<b>void</b> lsfitcreatewfgh(real_2d_array x, real_1d_array y, real_1d_array w, real_1d_array c, lsfitstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlfgh class=nav>lsfit_d_nlfgh</a> ]</p>
<a name=sub_lsfitfit></a><h6 class=pageheader>lsfitfit Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear fitter

These functions accept following parameters:
    state   -   algorithm state
    func    -   callback which calculates function (or merit function)
                value func at given point x
    grad    -   callback which calculates function (or merit function)
                value func and gradient grad at given point x
    hess    -   callback which calculates function (or merit function)
                value func, gradient grad and Hessian hess at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL

NOTES:

1. this algorithm is somewhat unusual because it works with  parameterized
   function f(C,X), where X is a function argument (we  have  many  points
   which are characterized by different  argument  values),  and  C  is  a
   parameter to fit.

   For example, if we want to do linear fit by f(c0,c1,x) = c0*x+c1,  then
   x will be argument, and {c0,c1} will be parameters.

   It is important to understand that this algorithm finds minimum in  the
   space of function PARAMETERS (not arguments), so it  needs  derivatives
   of f() with respect to C, not X.

   In the example above it will need f=c0*x+c1 and {df/dc0,df/dc1} = {x,1}
   instead of {df/dx} = {c0}.

2. Callback functions accept C as the first parameter, and X as the second

3. If  state  was  created  with  LSFitCreateFG(),  algorithm  needs  just
   function   and   its   gradient,   but   if   state   was  created with
   LSFitCreateFGH(), algorithm will need function, gradient and Hessian.

   According  to  the  said  above,  there  ase  several  versions of this
   function, which accept different sets of callbacks.

   This flexibility opens way to subtle errors - you may create state with
   LSFitCreateFGH() (optimization using Hessian), but call function  which
   does not accept Hessian. So when algorithm will request Hessian,  there
   will be no callback to call. In this case exception will be thrown.

   Be careful to avoid such errors because there is no way to find them at
   compile time - you can see them at runtime only.
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitfit(lsfitstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;c, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> lsfitfit(lsfitstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*grad)(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;c, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> lsfitfit(lsfitstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*grad)(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr), <b>void</b> (*hess)(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, real_2d_array &amp;hess, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;c, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlf class=nav>lsfit_d_nlf</a> | <a href=#example_lsfit_d_nlfg class=nav>lsfit_d_nlfg</a> | <a href=#example_lsfit_d_nlfgh class=nav>lsfit_d_nlfgh</a> | <a href=#example_lsfit_d_nlfb class=nav>lsfit_d_nlfb</a> | <a href=#example_lsfit_d_nlscale class=nav>lsfit_d_nlscale</a> ]</p>
<a name=sub_lsfitlinear></a><h6 class=pageheader>lsfitlinear Function</h6>
<hr width=600 align=left>
<pre class=narration>
Linear least squares fitting.

QR decomposition is used to reduce task to MxM, then triangular solver  or
SVD-based solver is used depending on condition number of the  system.  It
allows to maximize speed and retain decent accuracy.

IMPORTANT: if you want to perform  polynomial  fitting,  it  may  be  more
           convenient to use PolynomialFit() function. This function gives
           best  results  on  polynomial  problems  and  solves  numerical
           stability  issues  which  arise  when   you   fit   high-degree
           polynomials to your data.

Inputs:
    Y       -   array[0..N-1] Function values in  N  points.
    FMatrix -   a table of basis functions values, array[0..N-1, 0..M-1].
                FMatrix[I, J] - value of J-th basis function in I-th point.
    N       -   number of points used. N &ge; 1.
    M       -   number of basis functions, M &ge; 1.

Outputs:
    Info    -   error code:
                * -4    internal SVD decomposition subroutine failed (very
                        rare and for degenerate systems only)
                *  1    task is solved
    C       -   decomposition coefficients, array[0..M-1]
    Rep     -   fitting report. Following fields are set:
                * Rep.TaskRCond     reciprocal of condition number
                * R2                non-adjusted coefficient of determination
                                    (non-weighted)
                * RMSError          rms error on the (X,Y).
                * AvgError          average error on the (X,Y).
                * AvgRelError       average relative error on the non-zero Y
                * MaxError          maximum error
                                    NON-WEIGHTED ERRORS ARE CALCULATED

ERRORS IN PARAMETERS

This  solver  also  calculates different kinds of errors in parameters and
fills corresponding fields of report:
* Rep.CovPar        covariance matrix for parameters, array[K,K].
* Rep.ErrPar        errors in parameters, array[K],
                    errpar = sqrt(diag(CovPar))
* Rep.ErrCurve      vector of fit errors - standard deviations of empirical
                    best-fit curve from &quot;ideal&quot; best-fit curve built  with
                    infinite number of samples, array[N].
                    errcurve = sqrt(diag(F*CovPar*F')),
                    where F is functions matrix.
* Rep.Noise         vector of per-point estimates of noise, array[N]

NOTE:       noise in the data is estimated as follows:
            * for fitting without user-supplied  weights  all  points  are
              assumed to have same level of noise, which is estimated from
              the data
            * for fitting with user-supplied weights we assume that  noise
              level in I-th point is inversely proportional to Ith weight.
              Coefficient of proportionality is estimated from the data.

NOTE:       we apply small amount of regularization when we invert squared
            Jacobian and calculate covariance matrix. It  guarantees  that
            algorithm won't divide by zero  during  inversion,  but  skews
            error estimates a bit (fractional error is about 10^-9).

            However, we believe that this difference is insignificant  for
            all practical purposes except for the situation when you  want
            to compare ALGLIB results with &quot;reference&quot;  implementation  up
            to the last significant digit.

NOTE:       covariance matrix is estimated using  correction  for  degrees
            of freedom (covariances are divided by N-M instead of dividing
            by N).
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitlinear(real_1d_array y, real_2d_array fmatrix, ae_int_t n, ae_int_t m, ae_int_t &amp;info, real_1d_array &amp;c, lsfitreport &amp;rep);
<b>void</b> lsfitlinear(real_1d_array y, real_2d_array fmatrix, ae_int_t &amp;info, real_1d_array &amp;c, lsfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_lin class=nav>lsfit_d_lin</a> ]</p>
<a name=sub_lsfitlinearc></a><h6 class=pageheader>lsfitlinearc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Constained linear least squares fitting.

This  is  variation  of LSFitLinear(),  which searchs for min|A*x=b| given
that  K  additional  constaints  C*x=bc are satisfied. It reduces original
task to modified one: min|B*y-d| WITHOUT constraints,  then  LSFitLinear()
is called.

IMPORTANT: if you want to perform  polynomial  fitting,  it  may  be  more
           convenient to use PolynomialFit() function. This function gives
           best  results  on  polynomial  problems  and  solves  numerical
           stability  issues  which  arise  when   you   fit   high-degree
           polynomials to your data.

Inputs:
    Y       -   array[0..N-1] Function values in  N  points.
    FMatrix -   a table of basis functions values, array[0..N-1, 0..M-1].
                FMatrix[I,J] - value of J-th basis function in I-th point.
    CMatrix -   a table of constaints, array[0..K-1,0..M].
                I-th row of CMatrix corresponds to I-th linear constraint:
                CMatrix[I,0]*C[0] + ... + CMatrix[I,M-1]*C[M-1] = CMatrix[I,M]
    N       -   number of points used. N &ge; 1.
    M       -   number of basis functions, M &ge; 1.
    K       -   number of constraints, 0 &le; K &lt; M
                K=0 corresponds to absence of constraints.

Outputs:
    Info    -   error code:
                * -4    internal SVD decomposition subroutine failed (very
                        rare and for degenerate systems only)
                * -3    either   too   many  constraints  (M   or   more),
                        degenerate  constraints   (some   constraints  are
                        repetead twice) or inconsistent  constraints  were
                        specified.
                *  1    task is solved
    C       -   decomposition coefficients, array[0..M-1]
    Rep     -   fitting report. Following fields are set:
                * R2                non-adjusted coefficient of determination
                                    (non-weighted)
                * RMSError          rms error on the (X,Y).
                * AvgError          average error on the (X,Y).
                * AvgRelError       average relative error on the non-zero Y
                * MaxError          maximum error
                                    NON-WEIGHTED ERRORS ARE CALCULATED

IMPORTANT:
    this subroitine doesn't calculate task's condition number for K &ne; 0.

ERRORS IN PARAMETERS

This  solver  also  calculates different kinds of errors in parameters and
fills corresponding fields of report:
* Rep.CovPar        covariance matrix for parameters, array[K,K].
* Rep.ErrPar        errors in parameters, array[K],
                    errpar = sqrt(diag(CovPar))
* Rep.ErrCurve      vector of fit errors - standard deviations of empirical
                    best-fit curve from &quot;ideal&quot; best-fit curve built  with
                    infinite number of samples, array[N].
                    errcurve = sqrt(diag(F*CovPar*F')),
                    where F is functions matrix.
* Rep.Noise         vector of per-point estimates of noise, array[N]

IMPORTANT:  errors  in  parameters  are  calculated  without  taking  into
            account boundary/linear constraints! Presence  of  constraints
            changes distribution of errors, but there is no  easy  way  to
            account for constraints when you calculate covariance matrix.

NOTE:       noise in the data is estimated as follows:
            * for fitting without user-supplied  weights  all  points  are
              assumed to have same level of noise, which is estimated from
              the data
            * for fitting with user-supplied weights we assume that  noise
              level in I-th point is inversely proportional to Ith weight.
              Coefficient of proportionality is estimated from the data.

NOTE:       we apply small amount of regularization when we invert squared
            Jacobian and calculate covariance matrix. It  guarantees  that
            algorithm won't divide by zero  during  inversion,  but  skews
            error estimates a bit (fractional error is about 10^-9).

            However, we believe that this difference is insignificant  for
            all practical purposes except for the situation when you  want
            to compare ALGLIB results with &quot;reference&quot;  implementation  up
            to the last significant digit.

NOTE:       covariance matrix is estimated using  correction  for  degrees
            of freedom (covariances are divided by N-M instead of dividing
            by N).
ALGLIB: Copyright 07.09.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitlinearc(real_1d_array y, real_2d_array fmatrix, real_2d_array cmatrix, ae_int_t n, ae_int_t m, ae_int_t k, ae_int_t &amp;info, real_1d_array &amp;c, lsfitreport &amp;rep);
<b>void</b> lsfitlinearc(real_1d_array y, real_2d_array fmatrix, real_2d_array cmatrix, ae_int_t &amp;info, real_1d_array &amp;c, lsfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_linc class=nav>lsfit_d_linc</a> ]</p>
<a name=sub_lsfitlinearw></a><h6 class=pageheader>lsfitlinearw Function</h6>
<hr width=600 align=left>
<pre class=narration>
Weighted linear least squares fitting.

QR decomposition is used to reduce task to MxM, then triangular solver  or
SVD-based solver is used depending on condition number of the  system.  It
allows to maximize speed and retain decent accuracy.

IMPORTANT: if you want to perform  polynomial  fitting,  it  may  be  more
           convenient to use PolynomialFit() function. This function gives
           best  results  on  polynomial  problems  and  solves  numerical
           stability  issues  which  arise  when   you   fit   high-degree
           polynomials to your data.

Inputs:
    Y       -   array[0..N-1] Function values in  N  points.
    W       -   array[0..N-1]  Weights  corresponding to function  values.
                Each summand in square  sum  of  approximation  deviations
                from  given  values  is  multiplied  by  the   square   of
                corresponding weight.
    FMatrix -   a table of basis functions values, array[0..N-1, 0..M-1].
                FMatrix[I, J] - value of J-th basis function in I-th point.
    N       -   number of points used. N &ge; 1.
    M       -   number of basis functions, M &ge; 1.

Outputs:
    Info    -   error code:
                * -4    internal SVD decomposition subroutine failed (very
                        rare and for degenerate systems only)
                * -1    incorrect N/M were specified
                *  1    task is solved
    C       -   decomposition coefficients, array[0..M-1]
    Rep     -   fitting report. Following fields are set:
                * Rep.TaskRCond     reciprocal of condition number
                * R2                non-adjusted coefficient of determination
                                    (non-weighted)
                * RMSError          rms error on the (X,Y).
                * AvgError          average error on the (X,Y).
                * AvgRelError       average relative error on the non-zero Y
                * MaxError          maximum error
                                    NON-WEIGHTED ERRORS ARE CALCULATED

ERRORS IN PARAMETERS

This  solver  also  calculates different kinds of errors in parameters and
fills corresponding fields of report:
* Rep.CovPar        covariance matrix for parameters, array[K,K].
* Rep.ErrPar        errors in parameters, array[K],
                    errpar = sqrt(diag(CovPar))
* Rep.ErrCurve      vector of fit errors - standard deviations of empirical
                    best-fit curve from &quot;ideal&quot; best-fit curve built  with
                    infinite number of samples, array[N].
                    errcurve = sqrt(diag(F*CovPar*F')),
                    where F is functions matrix.
* Rep.Noise         vector of per-point estimates of noise, array[N]

NOTE:       noise in the data is estimated as follows:
            * for fitting without user-supplied  weights  all  points  are
              assumed to have same level of noise, which is estimated from
              the data
            * for fitting with user-supplied weights we assume that  noise
              level in I-th point is inversely proportional to Ith weight.
              Coefficient of proportionality is estimated from the data.

NOTE:       we apply small amount of regularization when we invert squared
            Jacobian and calculate covariance matrix. It  guarantees  that
            algorithm won't divide by zero  during  inversion,  but  skews
            error estimates a bit (fractional error is about 10^-9).

            However, we believe that this difference is insignificant  for
            all practical purposes except for the situation when you  want
            to compare ALGLIB results with &quot;reference&quot;  implementation  up
            to the last significant digit.

NOTE:       covariance matrix is estimated using  correction  for  degrees
            of freedom (covariances are divided by N-M instead of dividing
            by N).
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitlinearw(real_1d_array y, real_1d_array w, real_2d_array fmatrix, ae_int_t n, ae_int_t m, ae_int_t &amp;info, real_1d_array &amp;c, lsfitreport &amp;rep);
<b>void</b> lsfitlinearw(real_1d_array y, real_1d_array w, real_2d_array fmatrix, ae_int_t &amp;info, real_1d_array &amp;c, lsfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_lin class=nav>lsfit_d_lin</a> ]</p>
<a name=sub_lsfitlinearwc></a><h6 class=pageheader>lsfitlinearwc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Weighted constained linear least squares fitting.

This  is  variation  of LSFitLinearW(), which searchs for min|A*x=b| given
that  K  additional  constaints  C*x=bc are satisfied. It reduces original
task to modified one: min|B*y-d| WITHOUT constraints,  then LSFitLinearW()
is called.

IMPORTANT: if you want to perform  polynomial  fitting,  it  may  be  more
           convenient to use PolynomialFit() function. This function gives
           best  results  on  polynomial  problems  and  solves  numerical
           stability  issues  which  arise  when   you   fit   high-degree
           polynomials to your data.

Inputs:
    Y       -   array[0..N-1] Function values in  N  points.
    W       -   array[0..N-1]  Weights  corresponding to function  values.
                Each summand in square  sum  of  approximation  deviations
                from  given  values  is  multiplied  by  the   square   of
                corresponding weight.
    FMatrix -   a table of basis functions values, array[0..N-1, 0..M-1].
                FMatrix[I,J] - value of J-th basis function in I-th point.
    CMatrix -   a table of constaints, array[0..K-1,0..M].
                I-th row of CMatrix corresponds to I-th linear constraint:
                CMatrix[I,0]*C[0] + ... + CMatrix[I,M-1]*C[M-1] = CMatrix[I,M]
    N       -   number of points used. N &ge; 1.
    M       -   number of basis functions, M &ge; 1.
    K       -   number of constraints, 0 &le; K &lt; M
                K=0 corresponds to absence of constraints.

Outputs:
    Info    -   error code:
                * -4    internal SVD decomposition subroutine failed (very
                        rare and for degenerate systems only)
                * -3    either   too   many  constraints  (M   or   more),
                        degenerate  constraints   (some   constraints  are
                        repetead twice) or inconsistent  constraints  were
                        specified.
                *  1    task is solved
    C       -   decomposition coefficients, array[0..M-1]
    Rep     -   fitting report. Following fields are set:
                * R2                non-adjusted coefficient of determination
                                    (non-weighted)
                * RMSError          rms error on the (X,Y).
                * AvgError          average error on the (X,Y).
                * AvgRelError       average relative error on the non-zero Y
                * MaxError          maximum error
                                    NON-WEIGHTED ERRORS ARE CALCULATED

IMPORTANT:
    this subroitine doesn't calculate task's condition number for K &ne; 0.

ERRORS IN PARAMETERS

This  solver  also  calculates different kinds of errors in parameters and
fills corresponding fields of report:
* Rep.CovPar        covariance matrix for parameters, array[K,K].
* Rep.ErrPar        errors in parameters, array[K],
                    errpar = sqrt(diag(CovPar))
* Rep.ErrCurve      vector of fit errors - standard deviations of empirical
                    best-fit curve from &quot;ideal&quot; best-fit curve built  with
                    infinite number of samples, array[N].
                    errcurve = sqrt(diag(F*CovPar*F')),
                    where F is functions matrix.
* Rep.Noise         vector of per-point estimates of noise, array[N]

IMPORTANT:  errors  in  parameters  are  calculated  without  taking  into
            account boundary/linear constraints! Presence  of  constraints
            changes distribution of errors, but there is no  easy  way  to
            account for constraints when you calculate covariance matrix.

NOTE:       noise in the data is estimated as follows:
            * for fitting without user-supplied  weights  all  points  are
              assumed to have same level of noise, which is estimated from
              the data
            * for fitting with user-supplied weights we assume that  noise
              level in I-th point is inversely proportional to Ith weight.
              Coefficient of proportionality is estimated from the data.

NOTE:       we apply small amount of regularization when we invert squared
            Jacobian and calculate covariance matrix. It  guarantees  that
            algorithm won't divide by zero  during  inversion,  but  skews
            error estimates a bit (fractional error is about 10^-9).

            However, we believe that this difference is insignificant  for
            all practical purposes except for the situation when you  want
            to compare ALGLIB results with &quot;reference&quot;  implementation  up
            to the last significant digit.

NOTE:       covariance matrix is estimated using  correction  for  degrees
            of freedom (covariances are divided by N-M instead of dividing
            by N).
ALGLIB: Copyright 07.09.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitlinearwc(real_1d_array y, real_1d_array w, real_2d_array fmatrix, real_2d_array cmatrix, ae_int_t n, ae_int_t m, ae_int_t k, ae_int_t &amp;info, real_1d_array &amp;c, lsfitreport &amp;rep);
<b>void</b> lsfitlinearwc(real_1d_array y, real_1d_array w, real_2d_array fmatrix, real_2d_array cmatrix, ae_int_t &amp;info, real_1d_array &amp;c, lsfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_linc class=nav>lsfit_d_linc</a> ]</p>
<a name=sub_lsfitresults></a><h6 class=pageheader>lsfitresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Nonlinear least squares fitting results.

Called after return from LSFitFit().

Inputs:
    State   -   algorithm state

Outputs:
    Info    -   completion code:
                    * -8    optimizer   detected  NAN/INF  in  the  target
                            function and/or gradient
                    * -7    gradient verification failed.
                            See LSFitSetGradientCheck() for more information.
                    * -3    inconsistent constraints
                    *  2    relative step is no more than EpsX.
                    *  5    MaxIts steps was taken
                    *  7    stopping conditions are too stringent,
                            further improvement is impossible
    C       -   array[0..K-1], solution
    Rep     -   optimization report. On success following fields are set:
                * R2                non-adjusted coefficient of determination
                                    (non-weighted)
                * RMSError          rms error on the (X,Y).
                * AvgError          average error on the (X,Y).
                * AvgRelError       average relative error on the non-zero Y
                * MaxError          maximum error
                                    NON-WEIGHTED ERRORS ARE CALCULATED
                * WRMSError         weighted rms error on the (X,Y).

ERRORS IN PARAMETERS

This  solver  also  calculates different kinds of errors in parameters and
fills corresponding fields of report:
* Rep.CovPar        covariance matrix for parameters, array[K,K].
* Rep.ErrPar        errors in parameters, array[K],
                    errpar = sqrt(diag(CovPar))
* Rep.ErrCurve      vector of fit errors - standard deviations of empirical
                    best-fit curve from &quot;ideal&quot; best-fit curve built  with
                    infinite number of samples, array[N].
                    errcurve = sqrt(diag(J*CovPar*J')),
                    where J is Jacobian matrix.
* Rep.Noise         vector of per-point estimates of noise, array[N]

IMPORTANT:  errors  in  parameters  are  calculated  without  taking  into
            account boundary/linear constraints! Presence  of  constraints
            changes distribution of errors, but there is no  easy  way  to
            account for constraints when you calculate covariance matrix.

NOTE:       noise in the data is estimated as follows:
            * for fitting without user-supplied  weights  all  points  are
              assumed to have same level of noise, which is estimated from
              the data
            * for fitting with user-supplied weights we assume that  noise
              level in I-th point is inversely proportional to Ith weight.
              Coefficient of proportionality is estimated from the data.

NOTE:       we apply small amount of regularization when we invert squared
            Jacobian and calculate covariance matrix. It  guarantees  that
            algorithm won't divide by zero  during  inversion,  but  skews
            error estimates a bit (fractional error is about 10^-9).

            However, we believe that this difference is insignificant  for
            all practical purposes except for the situation when you  want
            to compare ALGLIB results with &quot;reference&quot;  implementation  up
            to the last significant digit.

NOTE:       covariance matrix is estimated using  correction  for  degrees
            of freedom (covariances are divided by N-M instead of dividing
            by N).
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitresults(lsfitstate state, ae_int_t &amp;info, real_1d_array &amp;c, lsfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlf class=nav>lsfit_d_nlf</a> | <a href=#example_lsfit_d_nlfg class=nav>lsfit_d_nlfg</a> | <a href=#example_lsfit_d_nlfgh class=nav>lsfit_d_nlfgh</a> | <a href=#example_lsfit_d_nlfb class=nav>lsfit_d_nlfb</a> | <a href=#example_lsfit_d_nlscale class=nav>lsfit_d_nlscale</a> ]</p>
<a name=sub_lsfitsetbc></a><h6 class=pageheader>lsfitsetbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets boundary constraints for underlying optimizer

Boundary constraints are inactive by default (after initial creation).
They are preserved until explicitly turned off with another SetBC() call.

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bounds, array[K].
                If some (all) variables are unbounded, you may specify
                very small number or -INF (latter is recommended because
                it will allow solver to use better algorithm).
    BndU    -   upper bounds, array[K].
                If some (all) variables are unbounded, you may specify
                very large number or +INF (latter is recommended because
                it will allow solver to use better algorithm).

NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
variable will be &quot;frozen&quot; at X[i]=BndL[i]=BndU[i].

NOTE 2: unlike other constrained optimization algorithms, this solver  has
following useful properties:
* bound constraints are always satisfied exactly
* function is evaluated only INSIDE area specified by bound constraints
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitsetbc(lsfitstate state, real_1d_array bndl, real_1d_array bndu);
</pre>
<a name=sub_lsfitsetcond></a><h6 class=pageheader>lsfitsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Stopping conditions for nonlinear least squares fitting.

Inputs:
    State   -   structure which stores algorithm state
    EpsX    - &ge; 0
                The subroutine finishes its work if  on  k+1-th  iteration
                the condition |v| &le; EpsX is fulfilled, where:
                * |.| means Euclidian norm
                * v - scaled step vector, v[i]=dx[i]/s[i]
                * dx - ste pvector, dx=X(k+1)-X(k)
                * s - scaling coefficients set by LSFitSetScale()
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations   is    unlimited.   Only   Levenberg-Marquardt
                iterations  are  counted  (L-BFGS/CG  iterations  are  NOT
                counted because their cost is very low compared to that of
                LM).

NOTE

Passing EpsX=0  and  MaxIts=0  (simultaneously)  will  lead  to  automatic
stopping criterion selection (according to the scheme used by MINLM unit).
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitsetcond(lsfitstate state, <b>double</b> epsx, ae_int_t maxits);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlf class=nav>lsfit_d_nlf</a> | <a href=#example_lsfit_d_nlfg class=nav>lsfit_d_nlfg</a> | <a href=#example_lsfit_d_nlfgh class=nav>lsfit_d_nlfgh</a> | <a href=#example_lsfit_d_nlfb class=nav>lsfit_d_nlfb</a> | <a href=#example_lsfit_d_nlscale class=nav>lsfit_d_nlscale</a> ]</p>
<a name=sub_lsfitsetgradientcheck></a><h6 class=pageheader>lsfitsetgradientcheck Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  subroutine  turns  on  verification  of  the  user-supplied analytic
gradient:
* user calls this subroutine before fitting begins
* LSFitFit() is called
* prior to actual fitting, for  each  point  in  data  set  X_i  and  each
  component  of  parameters  being  fited C_j algorithm performs following
  steps:
  * two trial steps are made to C_j-TestStep*S[j] and C_j+TestStep*S[j],
    where C_j is j-th parameter and S[j] is a scale of j-th parameter
  * if needed, steps are bounded with respect to constraints on C[]
  * F(X_i|C) is evaluated at these trial points
  * we perform one more evaluation in the middle point of the interval
  * we  build  cubic  model using function values and derivatives at trial
    points and we compare its prediction with actual value in  the  middle
    point
  * in case difference between prediction and actual value is higher  than
    some predetermined threshold, algorithm stops with completion code -7;
    Rep.VarIdx is set to index of the parameter with incorrect derivative.
* after verification is over, algorithm proceeds to the actual optimization.

NOTE 1: verification needs N*K (points count * parameters count)  gradient
        evaluations. It is very costly and you should use it only for  low
        dimensional  problems,  when  you  want  to  be  sure  that you've
        correctly calculated analytic derivatives. You should not  use  it
        in the production code  (unless  you  want  to  check  derivatives
        provided by some third party).

NOTE 2: you  should  carefully  choose  TestStep. Value which is too large
        (so large that function behaviour is significantly non-cubic) will
        lead to false alarms. You may use  different  step  for  different
        parameters by means of setting scale with LSFitSetScale().

NOTE 3: this function may lead to false positives. In case it reports that
        I-th  derivative was calculated incorrectly, you may decrease test
        step  and  try  one  more  time  - maybe your function changes too
        sharply  and  your  step  is  too  large for such rapidly chanding
        function.

NOTE 4: this function works only for optimizers created with LSFitCreateWFG()
        or LSFitCreateFG() constructors.

Inputs:
    State       -   structure used to store algorithm state
    TestStep    -   verification step:
                    * TestStep=0 turns verification off
                    * TestStep &gt; 0 activates verification
ALGLIB: Copyright 15.06.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitsetgradientcheck(lsfitstate state, <b>double</b> teststep);
</pre>
<a name=sub_lsfitsetlc></a><h6 class=pageheader>lsfitsetlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets linear constraints for underlying optimizer

Linear constraints are inactive by default (after initial creation).
They are preserved until explicitly turned off with another SetLC() call.

Inputs:
    State   -   structure stores algorithm state
    C       -   linear constraints, array[K,N+1].
                Each row of C represents one constraint, either equality
                or inequality (see below):
                * first N elements correspond to coefficients,
                * last element corresponds to the right part.
                All elements of C (including right part) must be finite.
    CT      -   type of constraints, array[K]:
                * if CT[i] &gt; 0, then I-th constraint is C[i,*]*x &ge; C[i,n+1]
                * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
                * if CT[i] &lt; 0, then I-th constraint is C[i,*]*x &le; C[i,n+1]
    K       -   number of equality/inequality constraints, K &ge; 0:
                * if given, only leading K elements of C/CT are used
                * if not given, automatically determined from sizes of C/CT

IMPORTANT: if you have linear constraints, it is strongly  recommended  to
           set scale of variables with lsfitsetscale(). QP solver which is
           used to calculate linearly constrained steps heavily relies  on
           good scaling of input problems.

NOTE: linear  (non-box)  constraints  are  satisfied only approximately  -
      there  always  exists some violation due  to  numerical  errors  and
      algorithmic limitations.

NOTE: general linear constraints  add  significant  overhead  to  solution
      process. Although solver performs roughly same amount of  iterations
      (when compared  with  similar  box-only  constrained  problem), each
      iteration   now    involves  solution  of  linearly  constrained  QP
      subproblem, which requires ~3-5 times more Cholesky  decompositions.
      Thus, if you can reformulate your problem in such way  this  it  has
      only box constraints, it may be beneficial to do so.
ALGLIB: Copyright 29.04.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitsetlc(lsfitstate state, real_2d_array c, integer_1d_array ct, ae_int_t k);
<b>void</b> lsfitsetlc(lsfitstate state, real_2d_array c, integer_1d_array ct);
</pre>
<a name=sub_lsfitsetscale></a><h6 class=pageheader>lsfitsetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients for underlying optimizer.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison with tolerances).  Scale of
the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the function

Generally, scale is NOT considered to be a form of preconditioner.  But LM
optimizer is unique in that it uses scaling matrix both  in  the  stopping
condition tests and as Marquardt damping factor.

Proper scaling is very important for the algorithm performance. It is less
important for the quality of results, but still has some influence (it  is
easier  to  converge  when  variables  are  properly  scaled, so premature
stopping is possible when very badly scalled variables are  combined  with
relaxed stopping conditions).

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitsetscale(lsfitstate state, real_1d_array s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_nlscale class=nav>lsfit_d_nlscale</a> ]</p>
<a name=sub_lsfitsetstpmax></a><h6 class=pageheader>lsfitsetstpmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets maximum step length

Inputs:
    State   -   structure which stores algorithm state
    StpMax  -   maximum step length, &ge; 0. Set StpMax to 0.0,  if you don't
                want to limit step length.

Use this subroutine when you optimize target function which contains exp()
or  other  fast  growing  functions,  and optimization algorithm makes too
large  steps  which  leads  to overflow. This function allows us to reject
steps  that  are  too  large  (and  therefore  expose  us  to the possible
overflow) without actually calculating function value at the x+stp*d.

NOTE: non-zero StpMax leads to moderate  performance  degradation  because
intermediate  step  of  preconditioned L-BFGS optimization is incompatible
with limits on step size.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitsetstpmax(lsfitstate state, <b>double</b> stpmax);
</pre>
<a name=sub_lsfitsetxrep></a><h6 class=pageheader>lsfitsetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

When reports are needed, State.C (current parameters) and State.F (current
value of fitting function) are reported.
ALGLIB: Copyright 15.08.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lsfitsetxrep(lsfitstate state, <b>bool</b> needxrep);
</pre>
<a name=sub_lstfitpiecewiselinearrdp></a><h6 class=pageheader>lstfitpiecewiselinearrdp Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  subroutine fits piecewise linear curve to points with Ramer-Douglas-
Peucker algorithm, which stops after achieving desired precision.

IMPORTANT:
* it performs non-least-squares fitting; it builds curve, but  this  curve
  does not minimize some least squares  metric.  See  description  of  RDP
  algorithm (say, in Wikipedia) for more details on WHAT is performed.
* this function does NOT work with parametric curves  (i.e.  curves  which
  can be represented as {X(t),Y(t)}. It works with curves   which  can  be
  represented as Y(X). Thus, it is impossible to model figures like circles
  with this functions.
  If  you  want  to  work  with  parametric   curves,   you   should   use
  ParametricRDPFixed() function provided  by  &quot;Parametric&quot;  subpackage  of
  &quot;Interpolation&quot; package.

Inputs:
    X       -   array of X-coordinates:
                * at least N elements
                * can be unordered (points are automatically sorted)
                * this function may accept non-distinct X (see below for
                  more information on handling of such inputs)
    Y       -   array of Y-coordinates:
                * at least N elements
    N       -   number of elements in X/Y
    Eps     -   positive number, desired precision.

Outputs:
    X2      -   X-values of corner points for piecewise approximation,
                has length NSections+1 or zero (for NSections=0).
    Y2      -   Y-values of corner points,
                has length NSections+1 or zero (for NSections=0).
    NSections-  number of sections found by algorithm,
                NSections can be zero for degenerate datasets
                (N &le; 1 or all X[] are non-distinct).

NOTE: X2/Y2 are ordered arrays, i.e. (X2[0],Y2[0]) is  a  first  point  of
      curve, (X2[NSection-1],Y2[NSection-1]) is the last point.
ALGLIB: Copyright 02.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lstfitpiecewiselinearrdp(real_1d_array x, real_1d_array y, ae_int_t n, <b>double</b> eps, real_1d_array &amp;x2, real_1d_array &amp;y2, ae_int_t &amp;nsections);
</pre>
<a name=sub_lstfitpiecewiselinearrdpfixed></a><h6 class=pageheader>lstfitpiecewiselinearrdpfixed Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  subroutine fits piecewise linear curve to points with Ramer-Douglas-
Peucker algorithm, which stops after generating specified number of linear
sections.

IMPORTANT:
* it does NOT perform least-squares fitting; it  builds  curve,  but  this
  curve does not minimize some least squares metric.  See  description  of
  RDP algorithm (say, in Wikipedia) for more details on WHAT is performed.
* this function does NOT work with parametric curves  (i.e.  curves  which
  can be represented as {X(t),Y(t)}. It works with curves   which  can  be
  represented as Y(X). Thus,  it  is  impossible  to  model  figures  like
  circles  with  this  functions.
  If  you  want  to  work  with  parametric   curves,   you   should   use
  ParametricRDPFixed() function provided  by  &quot;Parametric&quot;  subpackage  of
  &quot;Interpolation&quot; package.

Inputs:
    X       -   array of X-coordinates:
                * at least N elements
                * can be unordered (points are automatically sorted)
                * this function may accept non-distinct X (see below for
                  more information on handling of such inputs)
    Y       -   array of Y-coordinates:
                * at least N elements
    N       -   number of elements in X/Y
    M       -   desired number of sections:
                * at most M sections are generated by this function
                * less than M sections can be generated if we have N &lt; M
                  (or some X are non-distinct).

Outputs:
    X2      -   X-values of corner points for piecewise approximation,
                has length NSections+1 or zero (for NSections=0).
    Y2      -   Y-values of corner points,
                has length NSections+1 or zero (for NSections=0).
    NSections-  number of sections found by algorithm, NSections &le; M,
                NSections can be zero for degenerate datasets
                (N &le; 1 or all X[] are non-distinct).

NOTE: X2/Y2 are ordered arrays, i.e. (X2[0],Y2[0]) is  a  first  point  of
      curve, (X2[NSection-1],Y2[NSection-1]) is the last point.
ALGLIB: Copyright 02.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lstfitpiecewiselinearrdpfixed(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t m, real_1d_array &amp;x2, real_1d_array &amp;y2, ae_int_t &amp;nsections);
</pre>
<a name=sub_polynomialfit></a><h6 class=pageheader>polynomialfit Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fitting by polynomials in barycentric form. This function provides  simple
unterface for unconstrained unweighted fitting. See  PolynomialFitWC()  if
you need constrained fitting.

Task is linear, so linear least squares solver is used. Complexity of this
computational scheme is O(N*M^2), mostly dominated by least squares solver

SEE ALSO:
    PolynomialFitWC()

NOTES:
    you can convert P from barycentric form  to  the  power  or  Chebyshev
    basis with PolynomialBar2Pow() or PolynomialBar2Cheb() functions  from
    POLINT subpackage.

Inputs:
    X   -   points, array[0..N-1].
    Y   -   function values, array[0..N-1].
    N   -   number of points, N &gt; 0
            * if given, only leading N elements of X/Y are used
            * if not given, automatically determined from sizes of X/Y
    M   -   number of basis functions (= polynomial_degree + 1), M &ge; 1

Outputs:
    Info-   same format as in LSFitLinearW() subroutine:
            * Info &gt; 0    task is solved
            * Info &le; 0   an error occured:
                        -4 means inconvergence of internal SVD
    P   -   interpolant in barycentric form.
    Rep -   report, same format as in LSFitLinearW() subroutine.
            Following fields are set:
            * RMSError      rms error on the (X,Y).
            * AvgError      average error on the (X,Y).
            * AvgRelError   average relative error on the non-zero Y
            * MaxError      maximum error
                            NON-WEIGHTED ERRORS ARE CALCULATED
ALGLIB Project: Copyright 10.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialfit(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t m, ae_int_t &amp;info, barycentricinterpolant &amp;p, polynomialfitreport &amp;rep);
<b>void</b> polynomialfit(real_1d_array x, real_1d_array y, ae_int_t m, ae_int_t &amp;info, barycentricinterpolant &amp;p, polynomialfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_pol class=nav>lsfit_d_pol</a> ]</p>
<a name=sub_polynomialfitwc></a><h6 class=pageheader>polynomialfitwc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Weighted  fitting by polynomials in barycentric form, with constraints  on
function values or first derivatives.

Small regularizing term is used when solving constrained tasks (to improve
stability).

Task is linear, so linear least squares solver is used. Complexity of this
computational scheme is O(N*M^2), mostly dominated by least squares solver

SEE ALSO:
    PolynomialFit()

NOTES:
    you can convert P from barycentric form  to  the  power  or  Chebyshev
    basis with PolynomialBar2Pow() or PolynomialBar2Cheb() functions  from
    POLINT subpackage.

Inputs:
    X   -   points, array[0..N-1].
    Y   -   function values, array[0..N-1].
    W   -   weights, array[0..N-1]
            Each summand in square  sum  of  approximation deviations from
            given  values  is  multiplied  by  the square of corresponding
            weight. Fill it by 1's if you don't  want  to  solve  weighted
            task.
    N   -   number of points, N &gt; 0.
            * if given, only leading N elements of X/Y/W are used
            * if not given, automatically determined from sizes of X/Y/W
    XC  -   points where polynomial values/derivatives are constrained,
            array[0..K-1].
    YC  -   values of constraints, array[0..K-1]
    DC  -   array[0..K-1], types of constraints:
            * DC[i]=0   means that P(XC[i])=YC[i]
            * DC[i]=1   means that P'(XC[i])=YC[i]
            SEE BELOW FOR IMPORTANT INFORMATION ON CONSTRAINTS
    K   -   number of constraints, 0 &le; K &lt; M.
            K=0 means no constraints (XC/YC/DC are not used in such cases)
    M   -   number of basis functions (= polynomial_degree + 1), M &ge; 1

Outputs:
    Info-   same format as in LSFitLinearW() subroutine:
            * Info &gt; 0    task is solved
            * Info &le; 0   an error occured:
                        -4 means inconvergence of internal SVD
                        -3 means inconsistent constraints
    P   -   interpolant in barycentric form.
    Rep -   report, same format as in LSFitLinearW() subroutine.
            Following fields are set:
            * RMSError      rms error on the (X,Y).
            * AvgError      average error on the (X,Y).
            * AvgRelError   average relative error on the non-zero Y
            * MaxError      maximum error
                            NON-WEIGHTED ERRORS ARE CALCULATED

IMPORTANT:
    this subroitine doesn't calculate task's condition number for K &ne; 0.

SETTING CONSTRAINTS - DANGERS AND OPPORTUNITIES:

Setting constraints can lead  to undesired  results,  like ill-conditioned
behavior, or inconsistency being detected. From the other side,  it allows
us to improve quality of the fit. Here we summarize  our  experience  with
constrained regression splines:
* even simple constraints can be inconsistent, see  Wikipedia  article  on
  this subject: http://en.wikipedia.org/wiki/Birkhoff_interpolation
* the  greater  is  M (given  fixed  constraints),  the  more chances that
  constraints will be consistent
* in the general case, consistency of constraints is NOT GUARANTEED.
* in the one special cases, however, we can  guarantee  consistency.  This
  case  is:  M &gt; 1  and constraints on the function values (NOT DERIVATIVES)

Our final recommendation is to use constraints  WHEN  AND  ONLY  when  you
can't solve your task without them. Anything beyond  special  cases  given
above is not guaranteed and may result in inconsistency.
ALGLIB Project: Copyright 10.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialfitwc(real_1d_array x, real_1d_array y, real_1d_array w, ae_int_t n, real_1d_array xc, real_1d_array yc, integer_1d_array dc, ae_int_t k, ae_int_t m, ae_int_t &amp;info, barycentricinterpolant &amp;p, polynomialfitreport &amp;rep);
<b>void</b> polynomialfitwc(real_1d_array x, real_1d_array y, real_1d_array w, real_1d_array xc, real_1d_array yc, integer_1d_array dc, ae_int_t m, ae_int_t &amp;info, barycentricinterpolant &amp;p, polynomialfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lsfit_d_polc class=nav>lsfit_d_polc</a> ]</p>
<a name=sub_spline1dfitcubic></a><h6 class=pageheader>spline1dfitcubic Function</h6>
<hr width=600 align=left>
<pre class=narration>
Least squares fitting by cubic spline.

This subroutine is &quot;lightweight&quot; alternative for more complex and feature-
rich Spline1DFitCubicWC().  See  Spline1DFitCubicWC() for more information
about subroutine parameters (we don't duplicate it here because of length)
ALGLIB Project: Copyright 18.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dfitcubic(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t m, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
<b>void</b> spline1dfitcubic(real_1d_array x, real_1d_array y, ae_int_t m, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
</pre>
<a name=sub_spline1dfitcubicwc></a><h6 class=pageheader>spline1dfitcubicwc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Weighted fitting by cubic  spline,  with constraints on function values or
derivatives.

Equidistant grid with M-2 nodes on [min(x,xc),max(x,xc)] is  used to build
basis functions. Basis functions are cubic splines with continuous  second
derivatives  and  non-fixed first  derivatives  at  interval  ends.  Small
regularizing term is used  when  solving  constrained  tasks  (to  improve
stability).

Task is linear, so linear least squares solver is used. Complexity of this
computational scheme is O(N*M^2), mostly dominated by least squares solver

SEE ALSO
    Spline1DFitHermiteWC()  -   fitting by Hermite splines (more flexible,
                                less smooth)
    Spline1DFitCubic()      -   &quot;lightweight&quot; fitting  by  cubic  splines,
                                without invididual weights and constraints

Inputs:
    X   -   points, array[0..N-1].
    Y   -   function values, array[0..N-1].
    W   -   weights, array[0..N-1]
            Each summand in square  sum  of  approximation deviations from
            given  values  is  multiplied  by  the square of corresponding
            weight. Fill it by 1's if you don't  want  to  solve  weighted
            task.
    N   -   number of points (optional):
            * N &gt; 0
            * if given, only first N elements of X/Y/W are processed
            * if not given, automatically determined from X/Y/W sizes
    XC  -   points where spline values/derivatives are constrained,
            array[0..K-1].
    YC  -   values of constraints, array[0..K-1]
    DC  -   array[0..K-1], types of constraints:
            * DC[i]=0   means that S(XC[i])=YC[i]
            * DC[i]=1   means that S'(XC[i])=YC[i]
            SEE BELOW FOR IMPORTANT INFORMATION ON CONSTRAINTS
    K   -   number of constraints (optional):
            * 0 &le; K &lt; M.
            * K=0 means no constraints (XC/YC/DC are not used)
            * if given, only first K elements of XC/YC/DC are used
            * if not given, automatically determined from XC/YC/DC
    M   -   number of basis functions ( = number_of_nodes+2), M &ge; 4.

Outputs:
    Info-   same format as in LSFitLinearWC() subroutine.
            * Info &gt; 0    task is solved
            * Info &le; 0   an error occured:
                        -4 means inconvergence of internal SVD
                        -3 means inconsistent constraints
    S   -   spline interpolant.
    Rep -   report, same format as in LSFitLinearWC() subroutine.
            Following fields are set:
            * RMSError      rms error on the (X,Y).
            * AvgError      average error on the (X,Y).
            * AvgRelError   average relative error on the non-zero Y
            * MaxError      maximum error
                            NON-WEIGHTED ERRORS ARE CALCULATED

IMPORTANT:
    this subroitine doesn't calculate task's condition number for K &ne; 0.

ORDER OF POINTS

Subroutine automatically sorts points, so caller may pass unsorted array.

SETTING CONSTRAINTS - DANGERS AND OPPORTUNITIES:

Setting constraints can lead  to undesired  results,  like ill-conditioned
behavior, or inconsistency being detected. From the other side,  it allows
us to improve quality of the fit. Here we summarize  our  experience  with
constrained regression splines:
* excessive constraints can be inconsistent. Splines are  piecewise  cubic
  functions, and it is easy to create an example, where  large  number  of
  constraints  concentrated  in  small  area will result in inconsistency.
  Just because spline is not flexible enough to satisfy all of  them.  And
  same constraints spread across the  [min(x),max(x)]  will  be  perfectly
  consistent.
* the more evenly constraints are spread across [min(x),max(x)],  the more
  chances that they will be consistent
* the  greater  is  M (given  fixed  constraints),  the  more chances that
  constraints will be consistent
* in the general case, consistency of constraints IS NOT GUARANTEED.
* in the several special cases, however, we CAN guarantee consistency.
* one of this cases is constraints  on  the  function  values  AND/OR  its
  derivatives at the interval boundaries.
* another  special  case  is ONE constraint on the function value (OR, but
  not AND, derivative) anywhere in the interval

Our final recommendation is to use constraints  WHEN  AND  ONLY  WHEN  you
can't solve your task without them. Anything beyond  special  cases  given
above is not guaranteed and may result in inconsistency.
ALGLIB Project: Copyright 18.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dfitcubicwc(real_1d_array x, real_1d_array y, real_1d_array w, ae_int_t n, real_1d_array xc, real_1d_array yc, integer_1d_array dc, ae_int_t k, ae_int_t m, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
<b>void</b> spline1dfitcubicwc(real_1d_array x, real_1d_array y, real_1d_array w, real_1d_array xc, real_1d_array yc, integer_1d_array dc, ae_int_t m, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
</pre>
<a name=sub_spline1dfithermite></a><h6 class=pageheader>spline1dfithermite Function</h6>
<hr width=600 align=left>
<pre class=narration>
Least squares fitting by Hermite spline.

This subroutine is &quot;lightweight&quot; alternative for more complex and feature-
rich Spline1DFitHermiteWC().  See Spline1DFitHermiteWC()  description  for
more information about subroutine parameters (we don't duplicate  it  here
because of length).
ALGLIB Project: Copyright 18.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dfithermite(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t m, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
<b>void</b> spline1dfithermite(real_1d_array x, real_1d_array y, ae_int_t m, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
</pre>
<a name=sub_spline1dfithermitewc></a><h6 class=pageheader>spline1dfithermitewc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Weighted  fitting  by Hermite spline,  with constraints on function values
or first derivatives.

Equidistant grid with M nodes on [min(x,xc),max(x,xc)] is  used  to  build
basis functions. Basis functions are Hermite splines.  Small  regularizing
term is used when solving constrained tasks (to improve stability).

Task is linear, so linear least squares solver is used. Complexity of this
computational scheme is O(N*M^2), mostly dominated by least squares solver

SEE ALSO
    Spline1DFitCubicWC()    -   fitting by Cubic splines (less flexible,
                                more smooth)
    Spline1DFitHermite()    -   &quot;lightweight&quot; Hermite fitting, without
                                invididual weights and constraints

Inputs:
    X   -   points, array[0..N-1].
    Y   -   function values, array[0..N-1].
    W   -   weights, array[0..N-1]
            Each summand in square  sum  of  approximation deviations from
            given  values  is  multiplied  by  the square of corresponding
            weight. Fill it by 1's if you don't  want  to  solve  weighted
            task.
    N   -   number of points (optional):
            * N &gt; 0
            * if given, only first N elements of X/Y/W are processed
            * if not given, automatically determined from X/Y/W sizes
    XC  -   points where spline values/derivatives are constrained,
            array[0..K-1].
    YC  -   values of constraints, array[0..K-1]
    DC  -   array[0..K-1], types of constraints:
            * DC[i]=0   means that S(XC[i])=YC[i]
            * DC[i]=1   means that S'(XC[i])=YC[i]
            SEE BELOW FOR IMPORTANT INFORMATION ON CONSTRAINTS
    K   -   number of constraints (optional):
            * 0 &le; K &lt; M.
            * K=0 means no constraints (XC/YC/DC are not used)
            * if given, only first K elements of XC/YC/DC are used
            * if not given, automatically determined from XC/YC/DC
    M   -   number of basis functions (= 2 * number of nodes),
            M &ge; 4,
            M IS EVEN!

Outputs:
    Info-   same format as in LSFitLinearW() subroutine:
            * Info &gt; 0    task is solved
            * Info &le; 0   an error occured:
                        -4 means inconvergence of internal SVD
                        -3 means inconsistent constraints
                        -2 means odd M was passed (which is not supported)
                        -1 means another errors in parameters passed
                           (N &le; 0, for example)
    S   -   spline interpolant.
    Rep -   report, same format as in LSFitLinearW() subroutine.
            Following fields are set:
            * RMSError      rms error on the (X,Y).
            * AvgError      average error on the (X,Y).
            * AvgRelError   average relative error on the non-zero Y
            * MaxError      maximum error
                            NON-WEIGHTED ERRORS ARE CALCULATED

IMPORTANT:
    this subroitine doesn't calculate task's condition number for K &ne; 0.

IMPORTANT:
    this subroitine supports only even M's

ORDER OF POINTS

Subroutine automatically sorts points, so caller may pass unsorted array.

SETTING CONSTRAINTS - DANGERS AND OPPORTUNITIES:

Setting constraints can lead  to undesired  results,  like ill-conditioned
behavior, or inconsistency being detected. From the other side,  it allows
us to improve quality of the fit. Here we summarize  our  experience  with
constrained regression splines:
* excessive constraints can be inconsistent. Splines are  piecewise  cubic
  functions, and it is easy to create an example, where  large  number  of
  constraints  concentrated  in  small  area will result in inconsistency.
  Just because spline is not flexible enough to satisfy all of  them.  And
  same constraints spread across the  [min(x),max(x)]  will  be  perfectly
  consistent.
* the more evenly constraints are spread across [min(x),max(x)],  the more
  chances that they will be consistent
* the  greater  is  M (given  fixed  constraints),  the  more chances that
  constraints will be consistent
* in the general case, consistency of constraints is NOT GUARANTEED.
* in the several special cases, however, we can guarantee consistency.
* one of this cases is  M &ge; 4  and   constraints  on   the  function  value
  (AND/OR its derivative) at the interval boundaries.
* another special case is M &ge; 4  and  ONE  constraint on the function value
  (OR, BUT NOT AND, derivative) anywhere in [min(x),max(x)]

Our final recommendation is to use constraints  WHEN  AND  ONLY  when  you
can't solve your task without them. Anything beyond  special  cases  given
above is not guaranteed and may result in inconsistency.
ALGLIB Project: Copyright 18.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dfithermitewc(real_1d_array x, real_1d_array y, real_1d_array w, ae_int_t n, real_1d_array xc, real_1d_array yc, integer_1d_array dc, ae_int_t k, ae_int_t m, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
<b>void</b> spline1dfithermitewc(real_1d_array x, real_1d_array y, real_1d_array w, real_1d_array xc, real_1d_array yc, integer_1d_array dc, ae_int_t m, ae_int_t &amp;info, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
</pre>
<a name=example_lsfit_d_lin></a><h6 class=pageheader>lsfit_d_lin Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// In this example we demonstrate linear fitting by f(x|a) = a*exp(0.5*x).</font>
<font color=navy>//</font>
<font color=navy>// We have:</font>
<font color=navy>// * y - vector of experimental data</font>
<font color=navy>// * fmatrix -  matrix of basis functions calculated at sample points</font>
<font color=navy>//              Actually, we have only one basis function F0 = exp(0.5*x).</font>
   real_2d_array fmatrix = <font color=blue><b>&quot;[[0.606531],[0.670320],[0.740818],[0.818731],[0.904837],[1.000000],[1.105171],[1.221403],[1.349859],[1.491825],[1.648721]]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[1.133719, 1.306522, 1.504604, 1.554663, 1.884638, 2.072436, 2.257285, 2.534068, 2.622017, 2.897713, 3.219371]&quot;</b></font>;
   ae_int_t info;
   real_1d_array c;
   lsfitreport rep;
<font color=navy>// Linear fitting without weights</font>
   lsfitlinear(y, fmatrix, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(4).c_str()); <font color=navy>// EXPECTED: [1.98650]</font>
<font color=navy>//</font>
<font color=navy>// Linear fitting with individual weights.</font>
<font color=navy>// Slightly different result is returned.</font>
   real_1d_array w = <font color=blue><b>&quot;[1.414213, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&quot;</b></font>;
   lsfitlinearw(y, w, fmatrix, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(4).c_str()); <font color=navy>// EXPECTED: [1.983354]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_d_linc></a><h6 class=pageheader>lsfit_d_linc Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// In this example we demonstrate linear fitting by f(x|a,b) = a*x+b</font>
<font color=navy>// with simple constraint f(0)=0.</font>
<font color=navy>//</font>
<font color=navy>// We have:</font>
<font color=navy>// * y - vector of experimental data</font>
<font color=navy>// * fmatrix -  matrix of basis functions sampled at [0,1] with step 0.2:</font>
<font color=navy>//                  [ 1.0   0.0 ]</font>
<font color=navy>//                  [ 1.0   0.2 ]</font>
<font color=navy>//                  [ 1.0   0.4 ]</font>
<font color=navy>//                  [ 1.0   0.6 ]</font>
<font color=navy>//                  [ 1.0   0.8 ]</font>
<font color=navy>//                  [ 1.0   1.0 ]</font>
<font color=navy>//              first column contains value of first basis function (constant term)</font>
<font color=navy>//              second column contains second basis function (linear term)</font>
<font color=navy>// * cmatrix -  matrix of linear constraints:</font>
<font color=navy>//                  [ 1.0  0.0  0.0 ]</font>
<font color=navy>//              first two columns contain coefficients before basis functions,</font>
<font color=navy>//              last column contains desired value of their sum.</font>
<font color=navy>//              So [1,0,0] means <font color=blue><b>&quot;1*constant_term + 0*linear_term = 0&quot;</b></font> </font>
   real_1d_array y = <font color=blue><b>&quot;[0.072436,0.246944,0.491263,0.522300,0.714064,0.921929]&quot;</b></font>;
   real_2d_array fmatrix = <font color=blue><b>&quot;[[1,0.0],[1,0.2],[1,0.4],[1,0.6],[1,0.8],[1,1.0]]&quot;</b></font>;
   real_2d_array cmatrix = <font color=blue><b>&quot;[[1,0,0]]&quot;</b></font>;
   ae_int_t info;
   real_1d_array c;
   lsfitreport rep;
<font color=navy>// Constrained fitting without weights</font>
   lsfitlinearc(y, fmatrix, cmatrix, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(3).c_str()); <font color=navy>// EXPECTED: [0,0.932933]</font>
<font color=navy>//</font>
<font color=navy>// Constrained fitting with individual weights</font>
   real_1d_array w = <font color=blue><b>&quot;[1, 1.414213, 1, 1, 1, 1]&quot;</b></font>;
   lsfitlinearwc(y, w, fmatrix, cmatrix, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(3).c_str()); <font color=navy>// EXPECTED: [0,0.938322]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_d_nlf></a><h6 class=pageheader>lsfit_d_nlf Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>void</b> function_cx_1_func(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(c,x)=exp(-c0*sqr(x0))</font>
<font color=navy>// where x is a position on X-axis and c is adjustable parameter</font>
   func = exp(-c[0]*pow(x[0],2));
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// In this example we demonstrate exponential fitting</font>
<font color=navy>// by f(x) = exp(-c*x^2)</font>
<font color=navy>// using function value only.</font>
<font color=navy>//</font>
<font color=navy>// Gradient is estimated using combination of numerical differences</font>
<font color=navy>// and secant updates. diffstep variable stores differentiation step </font>
<font color=navy>// (we have to tell algorithm what step to use).</font>
   real_2d_array x = <font color=blue><b>&quot;[[-1],[-0.8],[-0.6],[-0.4],[-0.2],[0],[0.2],[0.4],[0.6],[0.8],[1.0]]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.223130, 0.382893, 0.582748, 0.786628, 0.941765, 1.000000, 0.941765, 0.786628, 0.582748, 0.382893, 0.223130]&quot;</b></font>;
   real_1d_array c = <font color=blue><b>&quot;[0.3]&quot;</b></font>;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   ae_int_t info;
   lsfitstate state;
   lsfitreport rep;
   <b>double</b> diffstep = 0.0001;
<font color=navy>// Fitting without weights</font>
   lsfitcreatef(x, y, c, diffstep, state);
   lsfitsetcond(state, epsx, maxits);
   lsfitfit(state, function_cx_1_func);
   lsfitresults(state, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 2</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.5]</font>
<font color=navy>//</font>
<font color=navy>// Fitting with weights</font>
<font color=navy>// (you can change weights and see how it changes result)</font>
   real_1d_array w = <font color=blue><b>&quot;[1,1,1,1,1,1,1,1,1,1,1]&quot;</b></font>;
   lsfitcreatewf(x, y, w, c, diffstep, state);
   lsfitsetcond(state, epsx, maxits);
   lsfitfit(state, function_cx_1_func);
   lsfitresults(state, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 2</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.5]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_d_nlfb></a><h6 class=pageheader>lsfit_d_nlfb Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>void</b> function_cx_1_func(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(c,x)=exp(-c0*sqr(x0))</font>
<font color=navy>// where x is a position on X-axis and c is adjustable parameter</font>
   func = exp(-c[0]*pow(x[0],2));
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// In this example we demonstrate exponential fitting by</font>
<font color=navy>//     f(x) = exp(-c*x^2)</font>
<font color=navy>// subject to bound constraints</font>
<font color=navy>//     0.0 &le; c &le; 1.0</font>
<font color=navy>// using function value only.</font>
<font color=navy>//</font>
<font color=navy>// Gradient is estimated using combination of numerical differences</font>
<font color=navy>// and secant updates. diffstep variable stores differentiation step </font>
<font color=navy>// (we have to tell algorithm what step to use).</font>
<font color=navy>//</font>
<font color=navy>// Unconstrained solution is c=1.5, but because of constraints we should</font>
<font color=navy>// get c=1.0 (at the boundary).</font>
   real_2d_array x = <font color=blue><b>&quot;[[-1],[-0.8],[-0.6],[-0.4],[-0.2],[0],[0.2],[0.4],[0.6],[0.8],[1.0]]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.223130, 0.382893, 0.582748, 0.786628, 0.941765, 1.000000, 0.941765, 0.786628, 0.582748, 0.382893, 0.223130]&quot;</b></font>;
   real_1d_array c = <font color=blue><b>&quot;[0.3]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[0.0]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[1.0]&quot;</b></font>;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   ae_int_t info;
   lsfitstate state;
   lsfitreport rep;
   <b>double</b> diffstep = 0.0001;

   lsfitcreatef(x, y, c, diffstep, state);
   lsfitsetbc(state, bndl, bndu);
   lsfitsetcond(state, epsx, maxits);
   lsfitfit(state, function_cx_1_func);
   lsfitresults(state, info, c, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.0]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_d_nlfg></a><h6 class=pageheader>lsfit_d_nlfg Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>void</b> function_cx_1_func(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(c,x)=exp(-c0*sqr(x0))</font>
<font color=navy>// where x is a position on X-axis and c is adjustable parameter</font>
   func = exp(-c[0]*pow(x[0],2));
}
<b>void</b> function_cx_1_grad(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(c,x)=exp(-c0*sqr(x0)) and gradient G={df/dc[i]}</font>
<font color=navy>// where x is a position on X-axis and c is adjustable parameter.</font>
<font color=navy>// IMPORTANT: gradient is calculated with respect to C, not to X</font>
   func = exp(-c[0]*pow(x[0],2));
   grad[0] = -pow(x[0],2)*func;
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// In this example we demonstrate exponential fitting</font>
<font color=navy>// by f(x) = exp(-c*x^2)</font>
<font color=navy>// using function value and gradient (with respect to c).</font>
   real_2d_array x = <font color=blue><b>&quot;[[-1],[-0.8],[-0.6],[-0.4],[-0.2],[0],[0.2],[0.4],[0.6],[0.8],[1.0]]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.223130, 0.382893, 0.582748, 0.786628, 0.941765, 1.000000, 0.941765, 0.786628, 0.582748, 0.382893, 0.223130]&quot;</b></font>;
   real_1d_array c = <font color=blue><b>&quot;[0.3]&quot;</b></font>;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   ae_int_t info;
   lsfitstate state;
   lsfitreport rep;
<font color=navy>// Fitting without weights</font>
   lsfitcreatefg(x, y, c, true, state);
   lsfitsetcond(state, epsx, maxits);
   lsfitfit(state, function_cx_1_func, function_cx_1_grad);
   lsfitresults(state, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 2</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.5]</font>
<font color=navy>//</font>
<font color=navy>// Fitting with weights</font>
<font color=navy>// (you can change weights and see how it changes result)</font>
   real_1d_array w = <font color=blue><b>&quot;[1,1,1,1,1,1,1,1,1,1,1]&quot;</b></font>;
   lsfitcreatewfg(x, y, w, c, true, state);
   lsfitsetcond(state, epsx, maxits);
   lsfitfit(state, function_cx_1_func, function_cx_1_grad);
   lsfitresults(state, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 2</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.5]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_d_nlfgh></a><h6 class=pageheader>lsfit_d_nlfgh Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>void</b> function_cx_1_func(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(c,x)=exp(-c0*sqr(x0))</font>
<font color=navy>// where x is a position on X-axis and c is adjustable parameter</font>
   func = exp(-c[0]*pow(x[0],2));
}
<b>void</b> function_cx_1_grad(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(c,x)=exp(-c0*sqr(x0)) and gradient G={df/dc[i]}</font>
<font color=navy>// where x is a position on X-axis and c is adjustable parameter.</font>
<font color=navy>// IMPORTANT: gradient is calculated with respect to C, not to X</font>
   func = exp(-c[0]*pow(x[0],2));
   grad[0] = -pow(x[0],2)*func;
}
<b>void</b> function_cx_1_hess(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, real_2d_array &amp;hess, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(c,x)=exp(-c0*sqr(x0)), gradient G={df/dc[i]} and Hessian H={d2f/(dc[i]*dc[j])}</font>
<font color=navy>// where x is a position on X-axis and c is adjustable parameter.</font>
<font color=navy>// IMPORTANT: gradient/Hessian are calculated with respect to C, not to X</font>
   func = exp(-c[0]*pow(x[0],2));
   grad[0] = -pow(x[0],2)*func;
   hess[0][0] = pow(x[0],4)*func;
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// In this example we demonstrate exponential fitting</font>
<font color=navy>// by f(x) = exp(-c*x^2)</font>
<font color=navy>// using function value, gradient and Hessian (with respect to c)</font>
   real_2d_array x = <font color=blue><b>&quot;[[-1],[-0.8],[-0.6],[-0.4],[-0.2],[0],[0.2],[0.4],[0.6],[0.8],[1.0]]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.223130, 0.382893, 0.582748, 0.786628, 0.941765, 1.000000, 0.941765, 0.786628, 0.582748, 0.382893, 0.223130]&quot;</b></font>;
   real_1d_array c = <font color=blue><b>&quot;[0.3]&quot;</b></font>;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   ae_int_t info;
   lsfitstate state;
   lsfitreport rep;
<font color=navy>// Fitting without weights</font>
   lsfitcreatefgh(x, y, c, state);
   lsfitsetcond(state, epsx, maxits);
   lsfitfit(state, function_cx_1_func, function_cx_1_grad, function_cx_1_hess);
   lsfitresults(state, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 2</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.5]</font>
<font color=navy>//</font>
<font color=navy>// Fitting with weights</font>
<font color=navy>// (you can change weights and see how it changes result)</font>
   real_1d_array w = <font color=blue><b>&quot;[1,1,1,1,1,1,1,1,1,1,1]&quot;</b></font>;
   lsfitcreatewfgh(x, y, w, c, state);
   lsfitsetcond(state, epsx, maxits);
   lsfitfit(state, function_cx_1_func, function_cx_1_grad, function_cx_1_hess);
   lsfitresults(state, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 2</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.5]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_d_nlscale></a><h6 class=pageheader>lsfit_d_nlscale Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>void</b> function_debt_func(<b>const</b> real_1d_array &amp;c, <b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(c,x)=c[0]*(1+c[1]*(pow(x[0]-1999,c[2])-1))</font>
   func = c[0]*(1+c[1]*(pow(x[0]-1999,c[2])-1));
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// In this example we demonstrate fitting by</font>
<font color=navy>//     f(x) = c[0]*(1+c[1]*((x-1999)^c[2]-1))</font>
<font color=navy>// subject to bound constraints</font>
<font color=navy>//     -INF  &lt; c[0] &lt; +INF</font>
<font color=navy>//      -10 &le; c[1] &le; +10</font>
<font color=navy>//      0.1 &le; c[2] &le; 2.0</font>
<font color=navy>// Data we want to fit are time series of Japan national debt</font>
<font color=navy>// collected from 2000 to 2008 measured in USD (dollars, not</font>
<font color=navy>// millions of dollars).</font>
<font color=navy>//</font>
<font color=navy>// Our variables are:</font>
<font color=navy>//     c[0] - debt value at initial moment (2000),</font>
<font color=navy>//     c[1] - direction coefficient (growth or decrease),</font>
<font color=navy>//     c[2] - curvature coefficient.</font>
<font color=navy>// You may see that our variables are badly scaled - first one </font>
<font color=navy>// is order of 10^12, and next two are somewhere about 1 in </font>
<font color=navy>// magnitude. Such problem is difficult to solve without some</font>
<font color=navy>// kind of scaling.</font>
<font color=navy>// That is exactly where lsfitsetscale() function can be used.</font>
<font color=navy>// We set scale of our variables to [1.0E12, 1, 1], which allows</font>
<font color=navy>// us to easily solve this problem.</font>
<font color=navy>//</font>
<font color=navy>// You can try commenting out lsfitsetscale() call - and you will </font>
<font color=navy>// see that algorithm will fail to converge.</font>
   real_2d_array x = <font color=blue><b>&quot;[[2000],[2001],[2002],[2003],[2004],[2005],[2006],[2007],[2008]]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[4323239600000.0, 4560913100000.0, 5564091500000.0, 6743189300000.0, 7284064600000.0, 7050129600000.0, 7092221500000.0, 8483907600000.0, 8625804400000.0]&quot;</b></font>;
   real_1d_array c = <font color=blue><b>&quot;[1.0e+13, 1, 1]&quot;</b></font>;
   <b>double</b> epsx = 1.0e-5;
   real_1d_array bndl = <font color=blue><b>&quot;[-inf, -10, 0.1]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[+inf, +10, 2.0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1.0e+12, 1, 1]&quot;</b></font>;
   ae_int_t maxits = 0;
   ae_int_t info;
   lsfitstate state;
   lsfitreport rep;
   <b>double</b> diffstep = 1.0e-5;

   lsfitcreatef(x, y, c, diffstep, state);
   lsfitsetcond(state, epsx, maxits);
   lsfitsetbc(state, bndl, bndu);
   lsfitsetscale(state, s);
   lsfitfit(state, function_debt_func);
   lsfitresults(state, info, c, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 2</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(-2).c_str()); <font color=navy>// EXPECTED: [4.142560E+12, 0.434240, 0.565376]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_d_pol></a><h6 class=pageheader>lsfit_d_pol Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates polynomial fitting.</font>
<font color=navy>//</font>
<font color=navy>// Fitting is done by two (M=2) functions from polynomial basis:</font>
<font color=navy>//     f0 = 1</font>
<font color=navy>//     f1 = x</font>
<font color=navy>// Basically, it just a linear fit; more complex polynomials may be used</font>
<font color=navy>// (e.g. parabolas with M=3, cubic with M=4), but even such simple fit allows</font>
<font color=navy>// us to demonstrate polynomialfit() function in action.</font>
<font color=navy>//</font>
<font color=navy>// We have:</font>
<font color=navy>// * x      set of abscissas</font>
<font color=navy>// * y      experimental data</font>
<font color=navy>//</font>
<font color=navy>// Additionally we demonstrate weighted fitting, where second point has</font>
<font color=navy>// more weight than other ones.</font>
   real_1d_array x = <font color=blue><b>&quot;[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.00,0.05,0.26,0.32,0.33,0.43,0.60,0.60,0.77,0.98,1.02]&quot;</b></font>;
   ae_int_t m = 2;
   <b>double</b> t = 2;
   ae_int_t info;
   barycentricinterpolant p;
   polynomialfitreport rep;
   <b>double</b> v;
<font color=navy>// Fitting without individual weights</font>
<font color=navy>//</font>
<font color=navy>// NOTE: result is returned as barycentricinterpolant structure.</font>
<font color=navy>//       <b>if</b> you want to get representation in the power basis,</font>
<font color=navy>//       you can use barycentricbar2pow() function to convert</font>
<font color=navy>//       from barycentric to power representation (see docs <b>for</b> </font>
<font color=navy>//       POLINT subpackage <b>for</b> more info).</font>
   polynomialfit(x, y, m, info, p, rep);
   v = barycentriccalc(p, t);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.011</font>
<font color=navy>//</font>
<font color=navy>// Fitting with individual weights</font>
<font color=navy>//</font>
<font color=navy>// NOTE: slightly different result is returned</font>
   real_1d_array w = <font color=blue><b>&quot;[1,1.414213562,1,1,1,1,1,1,1,1,1]&quot;</b></font>;
   real_1d_array xc = <font color=blue><b>&quot;[]&quot;</b></font>;
   real_1d_array yc = <font color=blue><b>&quot;[]&quot;</b></font>;
   integer_1d_array dc = <font color=blue><b>&quot;[]&quot;</b></font>;
   polynomialfitwc(x, y, w, xc, yc, dc, m, info, p, rep);
   v = barycentriccalc(p, t);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.023</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_d_polc></a><h6 class=pageheader>lsfit_d_polc Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates polynomial fitting.</font>
<font color=navy>//</font>
<font color=navy>// Fitting is done by two (M=2) functions from polynomial basis:</font>
<font color=navy>//     f0 = 1</font>
<font color=navy>//     f1 = x</font>
<font color=navy>// with simple constraint on function value</font>
<font color=navy>//     f(0) = 0</font>
<font color=navy>// Basically, it just a linear fit; more complex polynomials may be used</font>
<font color=navy>// (e.g. parabolas with M=3, cubic with M=4), but even such simple fit allows</font>
<font color=navy>// us to demonstrate polynomialfit() function in action.</font>
<font color=navy>//</font>
<font color=navy>// We have:</font>
<font color=navy>// * x      set of abscissas</font>
<font color=navy>// * y      experimental data</font>
<font color=navy>// * xc     points where constraints are placed</font>
<font color=navy>// * yc     constraints on derivatives</font>
<font color=navy>// * dc     derivative indices</font>
<font color=navy>//          (0 means function itself, 1 means first derivative)</font>
   real_1d_array x = <font color=blue><b>&quot;[1.0,1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.9,1.1]&quot;</b></font>;
   real_1d_array w = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array xc = <font color=blue><b>&quot;[0]&quot;</b></font>;
   real_1d_array yc = <font color=blue><b>&quot;[0]&quot;</b></font>;
   integer_1d_array dc = <font color=blue><b>&quot;[0]&quot;</b></font>;
   <b>double</b> t = 2;
   ae_int_t m = 2;
   ae_int_t info;
   barycentricinterpolant p;
   polynomialfitreport rep;
   <b>double</b> v;

   polynomialfitwc(x, y, w, xc, yc, dc, m, info, p, rep);
   v = barycentriccalc(p, t);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.000</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_d_spline></a><h6 class=pageheader>lsfit_d_spline Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// In this example we demonstrate penalized spline fitting of noisy data</font>
<font color=navy>//</font>
<font color=navy>// We have:</font>
<font color=navy>// * x - abscissas</font>
<font color=navy>// * y - vector of experimental data, straight line with small noise</font>
   real_1d_array x = <font color=blue><b>&quot;[0.00,0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.10,0.00,0.30,0.40,0.30,0.40,0.62,0.68,0.75,0.95]&quot;</b></font>;
   ae_int_t info;
   <b>double</b> v;
   spline1dinterpolant s;
   spline1dfitreport rep;
   <b>double</b> rho;
<font color=navy>// Fit with VERY small amount of smoothing (rho = -5.0)</font>
<font color=navy>// and large number of basis functions (M=50).</font>
<font color=navy>//</font>
<font color=navy>// With such small regularization penalized spline almost fully reproduces function values</font>
   rho = -5.0;
   spline1dfitpenalized(x, y, 50, rho, info, s, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   v = spline1dcalc(s, 0.0);
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.10</font>
<font color=navy>//</font>
<font color=navy>// Fit with VERY large amount of smoothing (rho = 10.0)</font>
<font color=navy>// and large number of basis functions (M=50).</font>
<font color=navy>//</font>
<font color=navy>// With such regularization our spline should become close to the straight line fit.</font>
<font color=navy>// We will compare its value in x=1.0 with results obtained from such fit.</font>
   rho = +10.0;
   spline1dfitpenalized(x, y, 50, rho, info, s, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   v = spline1dcalc(s, 1.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.969</font>
<font color=navy>//</font>
<font color=navy>// In real life applications you may need some moderate degree of fitting,</font>
<font color=navy>// so we try to fit once more with rho=3.0.</font>
   rho = +3.0;
   spline1dfitpenalized(x, y, 50, rho, info, s, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_t_4pl></a><h6 class=pageheader>lsfit_t_4pl Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_1d_array x = <font color=blue><b>&quot;[1,2,3,4,5,6,7,8]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.06313223,0.44552624,0.61838364,0.71385108,0.77345838,0.81383140,0.84280033,0.86449822]&quot;</b></font>;
   ae_int_t n = 8;
   <b>double</b> a;
   <b>double</b> b;
   <b>double</b> c;
   <b>double</b> d;
   lsfitreport rep;
<font color=navy>// Test logisticfit4() on carefully designed data with a priori known answer.</font>
   logisticfit4(x, y, n, a, b, c, d, rep);
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(a)); <font color=navy>// EXPECTED: -1.000</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(b)); <font color=navy>// EXPECTED: 1.200</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(c)); <font color=navy>// EXPECTED: 0.900</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(d)); <font color=navy>// EXPECTED: 1.000</font>
<font color=navy>//</font>
<font color=navy>// Evaluate model at point x=0.5</font>
   <b>double</b> v;
   v = logisticcalc4(0.5, a, b, c, d);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: -0.33874308</font>
   <b>return</b> 0;
}
</pre>
<a name=example_lsfit_t_5pl></a><h6 class=pageheader>lsfit_t_5pl Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_1d_array x = <font color=blue><b>&quot;[1,2,3,4,5,6,7,8]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.1949776139,0.5710060208,0.726002637,0.8060434158,0.8534547965,0.8842071579,0.9054773317,0.9209088299]&quot;</b></font>;
   ae_int_t n = 8;
   <b>double</b> a;
   <b>double</b> b;
   <b>double</b> c;
   <b>double</b> d;
   <b>double</b> g;
   lsfitreport rep;
<font color=navy>// Test logisticfit5() on carefully designed data with a priori known answer.</font>
   logisticfit5(x, y, n, a, b, c, d, g, rep);
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(a)); <font color=navy>// EXPECTED: -1.000</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(b)); <font color=navy>// EXPECTED: 1.200</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(c)); <font color=navy>// EXPECTED: 0.900</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(d)); <font color=navy>// EXPECTED: 1.000</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(g)); <font color=navy>// EXPECTED: 1.200</font>
<font color=navy>//</font>
<font color=navy>// Evaluate model at point x=0.5</font>
   <b>double</b> v;
   v = logisticcalc5(0.5, a, b, c, d, g);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: -0.2354656824</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_parametric></a><h4 class=pageheader>8.6.5. parametric Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_pspline2interpolant class=toc>pspline2interpolant</a> |
<a href=#struct_pspline3interpolant class=toc>pspline3interpolant</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_parametricrdpfixed class=toc>parametricrdpfixed</a> |
<a href=#sub_pspline2arclength class=toc>pspline2arclength</a> |
<a href=#sub_pspline2build class=toc>pspline2build</a> |
<a href=#sub_pspline2buildperiodic class=toc>pspline2buildperiodic</a> |
<a href=#sub_pspline2calc class=toc>pspline2calc</a> |
<a href=#sub_pspline2diff class=toc>pspline2diff</a> |
<a href=#sub_pspline2diff2 class=toc>pspline2diff2</a> |
<a href=#sub_pspline2parametervalues class=toc>pspline2parametervalues</a> |
<a href=#sub_pspline2tangent class=toc>pspline2tangent</a> |
<a href=#sub_pspline3arclength class=toc>pspline3arclength</a> |
<a href=#sub_pspline3build class=toc>pspline3build</a> |
<a href=#sub_pspline3buildperiodic class=toc>pspline3buildperiodic</a> |
<a href=#sub_pspline3calc class=toc>pspline3calc</a> |
<a href=#sub_pspline3diff class=toc>pspline3diff</a> |
<a href=#sub_pspline3diff2 class=toc>pspline3diff2</a> |
<a href=#sub_pspline3parametervalues class=toc>pspline3parametervalues</a> |
<a href=#sub_pspline3tangent class=toc>pspline3tangent</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_parametric_rdp class=toc>parametric_rdp</a></td><td width=15>&nbsp;</td><td>Parametric Ramer-Douglas-Peucker approximation</td></tr>
</table>
</div>
<a name=struct_pspline2interpolant></a><h6 class=pageheader>pspline2interpolant Class</h6>
<hr width=600 align=left>
<pre class=narration>
Parametric spline inteprolant: 2-dimensional curve.
You should not try to access its members directly - use PSpline2XXXXXXXX()
functions instead.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> pspline2interpolant {
};
</pre>
<a name=struct_pspline3interpolant></a><h6 class=pageheader>pspline3interpolant Class</h6>
<hr width=600 align=left>
<pre class=narration>
Parametric spline inteprolant: 3-dimensional curve.
You should not try to access its members directly - use PSpline3XXXXXXXX()
functions instead.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> pspline3interpolant {
};
</pre>
<a name=sub_parametricrdpfixed></a><h6 class=pageheader>parametricrdpfixed Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  subroutine fits piecewise linear curve to points with Ramer-Douglas-
Peucker algorithm. This  function  performs PARAMETRIC fit, i.e. it can be
used to fit curves like circles.

On  input  it  accepts dataset which describes parametric multidimensional
curve X(t), with X being vector, and t taking values in [0,N), where N  is
a number of points in dataset. As result, it returns reduced  dataset  X2,
which can be used to build  parametric  curve  X2(t),  which  approximates
X(t) with desired precision (or has specified number of sections).

Inputs:
    X       -   array of multidimensional points:
                * at least N elements, leading N elements are used if more
                  than N elements were specified
                * order of points is IMPORTANT because  it  is  parametric
                  fit
                * each row of array is one point which has D coordinates
    N       -   number of elements in X
    D       -   number of dimensions (elements per row of X)
    StopM   -   stopping condition - desired number of sections:
                * at most M sections are generated by this function
                * less than M sections can be generated if we have N &lt; M
                  (or some X are non-distinct).
                * zero StopM means that algorithm does not stop after
                  achieving some pre-specified section count
    StopEps -   stopping condition - desired precision:
                * algorithm stops after error in each section is at most Eps
                * zero Eps means that algorithm does not stop after
                  achieving some pre-specified precision

Outputs:
    X2      -   array of corner points for piecewise approximation,
                has length NSections+1 or zero (for NSections=0).
    Idx2    -   array of indexes (parameter values):
                * has length NSections+1 or zero (for NSections=0).
                * each element of Idx2 corresponds to same-numbered
                  element of X2
                * each element of Idx2 is index of  corresponding  element
                  of X2 at original array X, i.e. I-th  row  of  X2  is
                  Idx2[I]-th row of X.
                * elements of Idx2 can be treated as parameter values
                  which should be used when building new parametric curve
                * Idx2[0]=0, Idx2[NSections]=N-1
    NSections-  number of sections found by algorithm, NSections &le; M,
                NSections can be zero for degenerate datasets
                (N &le; 1 or all X[] are non-distinct).

NOTE: algorithm stops after:
      a) dividing curve into StopM sections
      b) achieving required precision StopEps
      c) dividing curve into N-1 sections
      If both StopM and StopEps are non-zero, algorithm is stopped by  the
      FIRST criterion which is satisfied. In case both StopM  and  StopEps
      are zero, algorithm stops because of (c).
ALGLIB: Copyright 02.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> parametricrdpfixed(real_2d_array x, ae_int_t n, ae_int_t d, ae_int_t stopm, <b>double</b> stopeps, real_2d_array &amp;x2, integer_1d_array &amp;idx2, ae_int_t &amp;nsections);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_parametric_rdp class=nav>parametric_rdp</a> ]</p>
<a name=sub_pspline2arclength></a><h6 class=pageheader>pspline2arclength Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function  calculates  arc length, i.e. length of  curve  between  t=a
and t=b.

Inputs:
    P   -   parametric spline interpolant
    A,B -   parameter values corresponding to arc ends:
            * B &gt; A will result in positive length returned
            * B &lt; A will result in negative length returned

Result:
    length of arc starting at T=A and ending at T=B.
ALGLIB Project: Copyright 30.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> pspline2arclength(pspline2interpolant p, <b>double</b> a, <b>double</b> b);
</pre>
<a name=sub_pspline2build></a><h6 class=pageheader>pspline2build Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function  builds  non-periodic 2-dimensional parametric spline  which
starts at (X[0],Y[0]) and ends at (X[N-1],Y[N-1]).

Inputs:
    XY  -   points, array[0..N-1,0..1].
            XY[I,0:1] corresponds to the Ith point.
            Order of points is important!
    N   -   points count, N &ge; 5 for Akima splines, N &ge; 2 for other types  of
            splines.
    ST  -   spline type:
            * 0     Akima spline
            * 1     parabolically terminated Catmull-Rom spline (Tension=0)
            * 2     parabolically terminated cubic spline
    PT  -   parameterization type:
            * 0     uniform
            * 1     chord length
            * 2     centripetal

Outputs:
    P   -   parametric spline interpolant

NOTES:
* this function  assumes  that  there all consequent points  are distinct.
  I.e. (x0,y0) &ne; (x1,y1),  (x1,y1) &ne; (x2,y2),  (x2,y2) &ne; (x3,y3)  and  so on.
  However, non-consequent points may coincide, i.e. we can  have  (x0,y0)=
  =(x2,y2).
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline2build(real_2d_array xy, ae_int_t n, ae_int_t st, ae_int_t pt, pspline2interpolant &amp;p);
</pre>
<a name=sub_pspline2buildperiodic></a><h6 class=pageheader>pspline2buildperiodic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  builds  periodic  2-dimensional  parametric  spline  which
starts at (X[0],Y[0]), goes through all points to (X[N-1],Y[N-1]) and then
back to (X[0],Y[0]).

Inputs:
    XY  -   points, array[0..N-1,0..1].
            XY[I,0:1] corresponds to the Ith point.
            XY[N-1,0:1] must be different from XY[0,0:1].
            Order of points is important!
    N   -   points count, N &ge; 3 for other types of splines.
    ST  -   spline type:
            * 1     Catmull-Rom spline (Tension=0) with cyclic boundary conditions
            * 2     cubic spline with cyclic boundary conditions
    PT  -   parameterization type:
            * 0     uniform
            * 1     chord length
            * 2     centripetal

Outputs:
    P   -   parametric spline interpolant

NOTES:
* this function  assumes  that there all consequent points  are  distinct.
  I.e. (x0,y0) &ne; (x1,y1), (x1,y1) &ne; (x2,y2),  (x2,y2) &ne; (x3,y3)  and  so  on.
  However, non-consequent points may coincide, i.e. we can  have  (x0,y0)=
  =(x2,y2).
* last point of sequence is NOT equal to the first  point.  You  shouldn't
  make curve &quot;explicitly periodic&quot; by making them equal.
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline2buildperiodic(real_2d_array xy, ae_int_t n, ae_int_t st, ae_int_t pt, pspline2interpolant &amp;p);
</pre>
<a name=sub_pspline2calc></a><h6 class=pageheader>pspline2calc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function  calculates  the value of the parametric spline for a  given
value of parameter T

Inputs:
    P   -   parametric spline interpolant
    T   -   point:
            * T in [0,1] corresponds to interval spanned by points
            * for non-periodic splines T &lt; 0 (or T &gt; 1) correspond to parts of
              the curve before the first (after the last) point
            * for periodic splines T &lt; 0 (or T &gt; 1) are projected  into  [0,1]
              by making T=T-floor(T).

Outputs:
    X   -   X-position
    Y   -   Y-position
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline2calc(pspline2interpolant p, <b>double</b> t, <b>double</b> &amp;x, <b>double</b> &amp;y);
</pre>
<a name=sub_pspline2diff></a><h6 class=pageheader>pspline2diff Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates derivative, i.e. it returns (dX/dT,dY/dT).

Inputs:
    P   -   parametric spline interpolant
    T   -   point:
            * T in [0,1] corresponds to interval spanned by points
            * for non-periodic splines T &lt; 0 (or T &gt; 1) correspond to parts of
              the curve before the first (after the last) point
            * for periodic splines T &lt; 0 (or T &gt; 1) are projected  into  [0,1]
              by making T=T-floor(T).

Outputs:
    X   -   X-value
    DX  -   X-derivative
    Y   -   Y-value
    DY  -   Y-derivative
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline2diff(pspline2interpolant p, <b>double</b> t, <b>double</b> &amp;x, <b>double</b> &amp;dx, <b>double</b> &amp;y, <b>double</b> &amp;dy);
</pre>
<a name=sub_pspline2diff2></a><h6 class=pageheader>pspline2diff2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates first and second derivative with respect to T.

Inputs:
    P   -   parametric spline interpolant
    T   -   point:
            * T in [0,1] corresponds to interval spanned by points
            * for non-periodic splines T &lt; 0 (or T &gt; 1) correspond to parts of
              the curve before the first (after the last) point
            * for periodic splines T &lt; 0 (or T &gt; 1) are projected  into  [0,1]
              by making T=T-floor(T).

Outputs:
    X   -   X-value
    DX  -   derivative
    D2X -   second derivative
    Y   -   Y-value
    DY  -   derivative
    D2Y -   second derivative
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline2diff2(pspline2interpolant p, <b>double</b> t, <b>double</b> &amp;x, <b>double</b> &amp;dx, <b>double</b> &amp;d2x, <b>double</b> &amp;y, <b>double</b> &amp;dy, <b>double</b> &amp;d2y);
</pre>
<a name=sub_pspline2parametervalues></a><h6 class=pageheader>pspline2parametervalues Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns vector of parameter values correspoding to points.

I.e. for P created from (X[0],Y[0])...(X[N-1],Y[N-1]) and U=TValues(P)  we
have
    (X[0],Y[0]) = PSpline2Calc(P,U[0]),
    (X[1],Y[1]) = PSpline2Calc(P,U[1]),
    (X[2],Y[2]) = PSpline2Calc(P,U[2]),
    ...

Inputs:
    P   -   parametric spline interpolant

Outputs:
    N   -   array size
    T   -   array[0..N-1]

NOTES:
* for non-periodic splines U[0]=0, U[0] &lt; U[1] &lt; ... &lt; U[N-1], U[N-1]=1
* for periodic splines     U[0]=0, U[0] &lt; U[1] &lt; ... &lt; U[N-1], U[N-1] &lt; 1
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline2parametervalues(pspline2interpolant p, ae_int_t &amp;n, real_1d_array &amp;t);
</pre>
<a name=sub_pspline2tangent></a><h6 class=pageheader>pspline2tangent Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function  calculates  tangent vector for a given value of parameter T

Inputs:
    P   -   parametric spline interpolant
    T   -   point:
            * T in [0,1] corresponds to interval spanned by points
            * for non-periodic splines T &lt; 0 (or T &gt; 1) correspond to parts of
              the curve before the first (after the last) point
            * for periodic splines T &lt; 0 (or T &gt; 1) are projected  into  [0,1]
              by making T=T-floor(T).

Outputs:
    X    -   X-component of tangent vector (normalized)
    Y    -   Y-component of tangent vector (normalized)

NOTE:
    X^2+Y^2 is either 1 (for non-zero tangent vector) or 0.
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline2tangent(pspline2interpolant p, <b>double</b> t, <b>double</b> &amp;x, <b>double</b> &amp;y);
</pre>
<a name=sub_pspline3arclength></a><h6 class=pageheader>pspline3arclength Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function  calculates  arc length, i.e. length of  curve  between  t=a
and t=b.

Inputs:
    P   -   parametric spline interpolant
    A,B -   parameter values corresponding to arc ends:
            * B &gt; A will result in positive length returned
            * B &lt; A will result in negative length returned

Result:
    length of arc starting at T=A and ending at T=B.
ALGLIB Project: Copyright 30.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> pspline3arclength(pspline3interpolant p, <b>double</b> a, <b>double</b> b);
</pre>
<a name=sub_pspline3build></a><h6 class=pageheader>pspline3build Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function  builds  non-periodic 3-dimensional parametric spline  which
starts at (X[0],Y[0],Z[0]) and ends at (X[N-1],Y[N-1],Z[N-1]).

Same as PSpline2Build() function, but for 3D, so we  won't  duplicate  its
description here.
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline3build(real_2d_array xy, ae_int_t n, ae_int_t st, ae_int_t pt, pspline3interpolant &amp;p);
</pre>
<a name=sub_pspline3buildperiodic></a><h6 class=pageheader>pspline3buildperiodic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  builds  periodic  3-dimensional  parametric  spline  which
starts at (X[0],Y[0],Z[0]), goes through all points to (X[N-1],Y[N-1],Z[N-1])
and then back to (X[0],Y[0],Z[0]).

Same as PSpline2Build() function, but for 3D, so we  won't  duplicate  its
description here.
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline3buildperiodic(real_2d_array xy, ae_int_t n, ae_int_t st, ae_int_t pt, pspline3interpolant &amp;p);
</pre>
<a name=sub_pspline3calc></a><h6 class=pageheader>pspline3calc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function  calculates  the value of the parametric spline for a  given
value of parameter T.

Inputs:
    P   -   parametric spline interpolant
    T   -   point:
            * T in [0,1] corresponds to interval spanned by points
            * for non-periodic splines T &lt; 0 (or T &gt; 1) correspond to parts of
              the curve before the first (after the last) point
            * for periodic splines T &lt; 0 (or T &gt; 1) are projected  into  [0,1]
              by making T=T-floor(T).

Outputs:
    X   -   X-position
    Y   -   Y-position
    Z   -   Z-position
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline3calc(pspline3interpolant p, <b>double</b> t, <b>double</b> &amp;x, <b>double</b> &amp;y, <b>double</b> &amp;z);
</pre>
<a name=sub_pspline3diff></a><h6 class=pageheader>pspline3diff Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates derivative, i.e. it returns (dX/dT,dY/dT,dZ/dT).

Inputs:
    P   -   parametric spline interpolant
    T   -   point:
            * T in [0,1] corresponds to interval spanned by points
            * for non-periodic splines T &lt; 0 (or T &gt; 1) correspond to parts of
              the curve before the first (after the last) point
            * for periodic splines T &lt; 0 (or T &gt; 1) are projected  into  [0,1]
              by making T=T-floor(T).

Outputs:
    X   -   X-value
    DX  -   X-derivative
    Y   -   Y-value
    DY  -   Y-derivative
    Z   -   Z-value
    DZ  -   Z-derivative
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline3diff(pspline3interpolant p, <b>double</b> t, <b>double</b> &amp;x, <b>double</b> &amp;dx, <b>double</b> &amp;y, <b>double</b> &amp;dy, <b>double</b> &amp;z, <b>double</b> &amp;dz);
</pre>
<a name=sub_pspline3diff2></a><h6 class=pageheader>pspline3diff2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates first and second derivative with respect to T.

Inputs:
    P   -   parametric spline interpolant
    T   -   point:
            * T in [0,1] corresponds to interval spanned by points
            * for non-periodic splines T &lt; 0 (or T &gt; 1) correspond to parts of
              the curve before the first (after the last) point
            * for periodic splines T &lt; 0 (or T &gt; 1) are projected  into  [0,1]
              by making T=T-floor(T).

Outputs:
    X   -   X-value
    DX  -   derivative
    D2X -   second derivative
    Y   -   Y-value
    DY  -   derivative
    D2Y -   second derivative
    Z   -   Z-value
    DZ  -   derivative
    D2Z -   second derivative
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline3diff2(pspline3interpolant p, <b>double</b> t, <b>double</b> &amp;x, <b>double</b> &amp;dx, <b>double</b> &amp;d2x, <b>double</b> &amp;y, <b>double</b> &amp;dy, <b>double</b> &amp;d2y, <b>double</b> &amp;z, <b>double</b> &amp;dz, <b>double</b> &amp;d2z);
</pre>
<a name=sub_pspline3parametervalues></a><h6 class=pageheader>pspline3parametervalues Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns vector of parameter values correspoding to points.

Same as PSpline2ParameterValues(), but for 3D.
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline3parametervalues(pspline3interpolant p, ae_int_t &amp;n, real_1d_array &amp;t);
</pre>
<a name=sub_pspline3tangent></a><h6 class=pageheader>pspline3tangent Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function  calculates  tangent vector for a given value of parameter T

Inputs:
    P   -   parametric spline interpolant
    T   -   point:
            * T in [0,1] corresponds to interval spanned by points
            * for non-periodic splines T &lt; 0 (or T &gt; 1) correspond to parts of
              the curve before the first (after the last) point
            * for periodic splines T &lt; 0 (or T &gt; 1) are projected  into  [0,1]
              by making T=T-floor(T).

Outputs:
    X    -   X-component of tangent vector (normalized)
    Y    -   Y-component of tangent vector (normalized)
    Z    -   Z-component of tangent vector (normalized)

NOTE:
    X^2+Y^2+Z^2 is either 1 (for non-zero tangent vector) or 0.
ALGLIB Project: Copyright 28.05.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pspline3tangent(pspline3interpolant p, <b>double</b> t, <b>double</b> &amp;x, <b>double</b> &amp;y, <b>double</b> &amp;z);
</pre>
<a name=example_parametric_rdp></a><h6 class=pageheader>parametric_rdp Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use RDP algorithm to approximate parametric 2D curve given by</font>
<font color=navy>// locations in t=0,1,2,3 (see below), which form piecewise linear</font>
<font color=navy>// trajectory through D-dimensional space (2-dimensional in our example).</font>
<font color=navy>// </font>
<font color=navy>//     |</font>
<font color=navy>//     |</font>
<font color=navy>//     -     *     *     X2................X3</font>
<font color=navy>//     |                .</font>
<font color=navy>//     |               .</font>
<font color=navy>//     -     *     *  .  *     *     *     *</font>
<font color=navy>//     |             .</font>
<font color=navy>//     |            .</font>
<font color=navy>//     -     *     X1    *     *     *     *</font>
<font color=navy>//     |      .....</font>
<font color=navy>//     |  ....</font>
<font color=navy>//     X0----|-----|-----|-----|-----|-----|---</font>
   ae_int_t npoints = 4;
   ae_int_t ndimensions = 2;
   real_2d_array x = <font color=blue><b>&quot;[[0,0],[2,1],[3,3],[6,3]]&quot;</b></font>;
<font color=navy>// Approximation of parametric curve is performed by another parametric curve</font>
<font color=navy>// with lesser amount of points. It allows to work with <font color=blue><b>&quot;compressed&quot;</b></font></font>
<font color=navy>// representation, which needs smaller amount of memory. Say, in our example</font>
<font color=navy>// (we allow points with error smaller than 0.8) approximation will have</font>
<font color=navy>// just two sequential sections connecting X0 with X2, and X2 with X3.</font>
<font color=navy>// </font>
<font color=navy>//     |</font>
<font color=navy>//     |</font>
<font color=navy>//     -     *     *     X2................X3</font>
<font color=navy>//     |               . </font>
<font color=navy>//     |             .  </font>
<font color=navy>//     -     *     .     *     *     *     *</font>
<font color=navy>//     |         .    </font>
<font color=navy>//     |       .     </font>
<font color=navy>//     -     .     X1    *     *     *     *</font>
<font color=navy>//     |   .       </font>
<font color=navy>//     | .    </font>
<font color=navy>//     X0----|-----|-----|-----|-----|-----|---</font>
<font color=navy>//</font>
<font color=navy>//</font>
   real_2d_array y;
   integer_1d_array idxy;
   ae_int_t nsections;
   ae_int_t limitcnt = 0;
   <b>double</b> limiteps = 0.8;
   parametricrdpfixed(x, npoints, ndimensions, limitcnt, limiteps, y, idxy, nsections);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(nsections)); <font color=navy>// EXPECTED: 2</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, idxy.tostring().c_str()); <font color=navy>// EXPECTED: [0,2,3]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_polint></a><h4 class=pageheader>8.6.6. polint Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_polynomialbar2cheb class=toc>polynomialbar2cheb</a> |
<a href=#sub_polynomialbar2pow class=toc>polynomialbar2pow</a> |
<a href=#sub_polynomialbuild class=toc>polynomialbuild</a> |
<a href=#sub_polynomialbuildcheb1 class=toc>polynomialbuildcheb1</a> |
<a href=#sub_polynomialbuildcheb2 class=toc>polynomialbuildcheb2</a> |
<a href=#sub_polynomialbuildeqdist class=toc>polynomialbuildeqdist</a> |
<a href=#sub_polynomialcalccheb1 class=toc>polynomialcalccheb1</a> |
<a href=#sub_polynomialcalccheb2 class=toc>polynomialcalccheb2</a> |
<a href=#sub_polynomialcalceqdist class=toc>polynomialcalceqdist</a> |
<a href=#sub_polynomialcheb2bar class=toc>polynomialcheb2bar</a> |
<a href=#sub_polynomialpow2bar class=toc>polynomialpow2bar</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_polint_d_calcdiff class=toc>polint_d_calcdiff</a></td><td width=15>&nbsp;</td><td>Interpolation and differentiation using barycentric representation</td></tr>
<tr align=left valign=top><td><a href=#example_polint_d_conv class=toc>polint_d_conv</a></td><td width=15>&nbsp;</td><td>Conversion between power basis and barycentric representation</td></tr>
<tr align=left valign=top><td><a href=#example_polint_d_spec class=toc>polint_d_spec</a></td><td width=15>&nbsp;</td><td>Polynomial interpolation on special grids (equidistant, Chebyshev I/II)</td></tr>
</table>
</div>
<a name=sub_polynomialbar2cheb></a><h6 class=pageheader>polynomialbar2cheb Function</h6>
<hr width=600 align=left>
<pre class=narration>
Conversion from barycentric representation to Chebyshev basis.
This function has O(N^2) complexity.

Inputs:
    P   -   polynomial in barycentric form
    A,B -   base interval for Chebyshev polynomials (see below)
            A &ne; B

Outputs:
    T   -   coefficients of Chebyshev representation;
            P(x) = sum { T[i]*Ti(2*(x-A)/(B-A)-1), i=0..N-1 },
            where Ti - I-th Chebyshev polynomial.

NOTES:
    barycentric interpolant passed as P may be either polynomial  obtained
    from  polynomial  interpolation/ fitting or rational function which is
    NOT polynomial. We can't distinguish between these two cases, and this
    algorithm just tries to work assuming that P IS a polynomial.  If not,
    algorithm will return results, but they won't have any meaning.
ALGLIB: Copyright 30.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialbar2cheb(barycentricinterpolant p, <b>double</b> a, <b>double</b> b, real_1d_array &amp;t);
</pre>
<a name=sub_polynomialbar2pow></a><h6 class=pageheader>polynomialbar2pow Function</h6>
<hr width=600 align=left>
<pre class=narration>
Conversion from barycentric representation to power basis.
This function has O(N^2) complexity.

Inputs:
    P   -   polynomial in barycentric form
    C   -   offset (see below); 0.0 is used as default value.
    S   -   scale (see below);  1.0 is used as default value. S &ne; 0.

Outputs:
    A   -   coefficients, P(x) = sum { A[i]*((X-C)/S)^i, i=0..N-1 }
    N   -   number of coefficients (polynomial degree plus 1)

NOTES:
1.  this function accepts offset and scale, which can be  set  to  improve
    numerical properties of polynomial. For example, if P was obtained  as
    result of interpolation on [-1,+1],  you  can  set  C=0  and  S=1  and
    represent  P  as sum of 1, x, x^2, x^3 and so on. In most cases you it
    is exactly what you need.

    However, if your interpolation model was built on [999,1001], you will
    see significant growth of numerical errors when using {1, x, x^2, x^3}
    as basis. Representing P as sum of 1, (x-1000), (x-1000)^2, (x-1000)^3
    will be better option. Such representation can be  obtained  by  using
    1000.0 as offset C and 1.0 as scale S.

2.  power basis is ill-conditioned and tricks described above can't  solve
    this problem completely. This function  will  return  coefficients  in
    any  case,  but  for  N &gt; 8  they  will  become unreliable. However, N's
    less than 5 are pretty safe.

3.  barycentric interpolant passed as P may be either polynomial  obtained
    from  polynomial  interpolation/ fitting or rational function which is
    NOT polynomial. We can't distinguish between these two cases, and this
    algorithm just tries to work assuming that P IS a polynomial.  If not,
    algorithm will return results, but they won't have any meaning.
ALGLIB: Copyright 30.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialbar2pow(barycentricinterpolant p, <b>double</b> c, <b>double</b> s, real_1d_array &amp;a);
<b>void</b> polynomialbar2pow(barycentricinterpolant p, real_1d_array &amp;a);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_polint_d_conv class=nav>polint_d_conv</a> ]</p>
<a name=sub_polynomialbuild></a><h6 class=pageheader>polynomialbuild Function</h6>
<hr width=600 align=left>
<pre class=narration>
Lagrange intepolant: generation of the model on the general grid.
This function has O(N^2) complexity.

Inputs:
    X   -   abscissas, array[0..N-1]
    Y   -   function values, array[0..N-1]
    N   -   number of points, N &ge; 1

Outputs:
    P   -   barycentric model which represents Lagrange interpolant
            (see ratint unit info and BarycentricCalc() description for
            more information).
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialbuild(real_1d_array x, real_1d_array y, ae_int_t n, barycentricinterpolant &amp;p);
<b>void</b> polynomialbuild(real_1d_array x, real_1d_array y, barycentricinterpolant &amp;p);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_polint_d_calcdiff class=nav>polint_d_calcdiff</a> ]</p>
<a name=sub_polynomialbuildcheb1></a><h6 class=pageheader>polynomialbuildcheb1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Lagrange intepolant on Chebyshev grid (first kind).
This function has O(N) complexity.

Inputs:
    A   -   left boundary of [A,B]
    B   -   right boundary of [A,B]
    Y   -   function values at the nodes, array[0..N-1],
            Y[I] = Y(0.5*(B+A) + 0.5*(B-A)*cos(PI*(2*i+1)/(2*n)))
    N   -   number of points, N &ge; 1
            for N=1 a constant model is constructed.

Outputs:
    P   -   barycentric model which represents Lagrange interpolant
            (see ratint unit info and BarycentricCalc() description for
            more information).
ALGLIB: Copyright 03.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialbuildcheb1(<b>double</b> a, <b>double</b> b, real_1d_array y, ae_int_t n, barycentricinterpolant &amp;p);
<b>void</b> polynomialbuildcheb1(<b>double</b> a, <b>double</b> b, real_1d_array y, barycentricinterpolant &amp;p);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_polint_d_spec class=nav>polint_d_spec</a> ]</p>
<a name=sub_polynomialbuildcheb2></a><h6 class=pageheader>polynomialbuildcheb2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Lagrange intepolant on Chebyshev grid (second kind).
This function has O(N) complexity.

Inputs:
    A   -   left boundary of [A,B]
    B   -   right boundary of [A,B]
    Y   -   function values at the nodes, array[0..N-1],
            Y[I] = Y(0.5*(B+A) + 0.5*(B-A)*cos(PI*i/(n-1)))
    N   -   number of points, N &ge; 1
            for N=1 a constant model is constructed.

Outputs:
    P   -   barycentric model which represents Lagrange interpolant
            (see ratint unit info and BarycentricCalc() description for
            more information).
ALGLIB: Copyright 03.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialbuildcheb2(<b>double</b> a, <b>double</b> b, real_1d_array y, ae_int_t n, barycentricinterpolant &amp;p);
<b>void</b> polynomialbuildcheb2(<b>double</b> a, <b>double</b> b, real_1d_array y, barycentricinterpolant &amp;p);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_polint_d_spec class=nav>polint_d_spec</a> ]</p>
<a name=sub_polynomialbuildeqdist></a><h6 class=pageheader>polynomialbuildeqdist Function</h6>
<hr width=600 align=left>
<pre class=narration>
Lagrange intepolant: generation of the model on equidistant grid.
This function has O(N) complexity.

Inputs:
    A   -   left boundary of [A,B]
    B   -   right boundary of [A,B]
    Y   -   function values at the nodes, array[0..N-1]
    N   -   number of points, N &ge; 1
            for N=1 a constant model is constructed.

Outputs:
    P   -   barycentric model which represents Lagrange interpolant
            (see ratint unit info and BarycentricCalc() description for
            more information).
ALGLIB: Copyright 03.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialbuildeqdist(<b>double</b> a, <b>double</b> b, real_1d_array y, ae_int_t n, barycentricinterpolant &amp;p);
<b>void</b> polynomialbuildeqdist(<b>double</b> a, <b>double</b> b, real_1d_array y, barycentricinterpolant &amp;p);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_polint_d_spec class=nav>polint_d_spec</a> ]</p>
<a name=sub_polynomialcalccheb1></a><h6 class=pageheader>polynomialcalccheb1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fast polynomial interpolation function on Chebyshev points (first kind)
with O(N) complexity.

Inputs:
    A   -   left boundary of [A,B]
    B   -   right boundary of [A,B]
    F   -   function values, array[0..N-1]
    N   -   number of points on Chebyshev grid (first kind),
            X[i] = 0.5*(B+A) + 0.5*(B-A)*cos(PI*(2*i+1)/(2*n))
            for N=1 a constant model is constructed.
    T   -   position where P(x) is calculated

Result:
    value of the Lagrange interpolant at T

IMPORTANT
    this function provides fast interface which is not overflow-safe
    nor it is very precise.
    the best option is to use  PolIntBuildCheb1()/BarycentricCalc()
    subroutines unless you are pretty sure that your data will not result
    in overflow.
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> polynomialcalccheb1(<b>double</b> a, <b>double</b> b, real_1d_array f, ae_int_t n, <b>double</b> t);
<b>double</b> polynomialcalccheb1(<b>double</b> a, <b>double</b> b, real_1d_array f, <b>double</b> t);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_polint_d_spec class=nav>polint_d_spec</a> ]</p>
<a name=sub_polynomialcalccheb2></a><h6 class=pageheader>polynomialcalccheb2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fast polynomial interpolation function on Chebyshev points (second kind)
with O(N) complexity.

Inputs:
    A   -   left boundary of [A,B]
    B   -   right boundary of [A,B]
    F   -   function values, array[0..N-1]
    N   -   number of points on Chebyshev grid (second kind),
            X[i] = 0.5*(B+A) + 0.5*(B-A)*cos(PI*i/(n-1))
            for N=1 a constant model is constructed.
    T   -   position where P(x) is calculated

Result:
    value of the Lagrange interpolant at T

IMPORTANT
    this function provides fast interface which is not overflow-safe
    nor it is very precise.
    the best option is to use PolIntBuildCheb2()/BarycentricCalc()
    subroutines unless you are pretty sure that your data will not result
    in overflow.
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> polynomialcalccheb2(<b>double</b> a, <b>double</b> b, real_1d_array f, ae_int_t n, <b>double</b> t);
<b>double</b> polynomialcalccheb2(<b>double</b> a, <b>double</b> b, real_1d_array f, <b>double</b> t);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_polint_d_spec class=nav>polint_d_spec</a> ]</p>
<a name=sub_polynomialcalceqdist></a><h6 class=pageheader>polynomialcalceqdist Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fast equidistant polynomial interpolation function with O(N) complexity

Inputs:
    A   -   left boundary of [A,B]
    B   -   right boundary of [A,B]
    F   -   function values, array[0..N-1]
    N   -   number of points on equidistant grid, N &ge; 1
            for N=1 a constant model is constructed.
    T   -   position where P(x) is calculated

Result:
    value of the Lagrange interpolant at T

IMPORTANT
    this function provides fast interface which is not overflow-safe
    nor it is very precise.
    the best option is to use  PolynomialBuildEqDist()/BarycentricCalc()
    subroutines unless you are pretty sure that your data will not result
    in overflow.
ALGLIB: Copyright 02.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> polynomialcalceqdist(<b>double</b> a, <b>double</b> b, real_1d_array f, ae_int_t n, <b>double</b> t);
<b>double</b> polynomialcalceqdist(<b>double</b> a, <b>double</b> b, real_1d_array f, <b>double</b> t);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_polint_d_spec class=nav>polint_d_spec</a> ]</p>
<a name=sub_polynomialcheb2bar></a><h6 class=pageheader>polynomialcheb2bar Function</h6>
<hr width=600 align=left>
<pre class=narration>
Conversion from Chebyshev basis to barycentric representation.
This function has O(N^2) complexity.

Inputs:
    T   -   coefficients of Chebyshev representation;
            P(x) = sum { T[i]*Ti(2*(x-A)/(B-A)-1), i=0..N },
            where Ti - I-th Chebyshev polynomial.
    N   -   number of coefficients:
            * if given, only leading N elements of T are used
            * if not given, automatically determined from size of T
    A,B -   base interval for Chebyshev polynomials (see above)
            A &lt; B

Outputs:
    P   -   polynomial in barycentric form
ALGLIB: Copyright 30.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialcheb2bar(real_1d_array t, ae_int_t n, <b>double</b> a, <b>double</b> b, barycentricinterpolant &amp;p);
<b>void</b> polynomialcheb2bar(real_1d_array t, <b>double</b> a, <b>double</b> b, barycentricinterpolant &amp;p);
</pre>
<a name=sub_polynomialpow2bar></a><h6 class=pageheader>polynomialpow2bar Function</h6>
<hr width=600 align=left>
<pre class=narration>
Conversion from power basis to barycentric representation.
This function has O(N^2) complexity.

Inputs:
    A   -   coefficients, P(x) = sum { A[i]*((X-C)/S)^i, i=0..N-1 }
    N   -   number of coefficients (polynomial degree plus 1)
            * if given, only leading N elements of A are used
            * if not given, automatically determined from size of A
    C   -   offset (see below); 0.0 is used as default value.
    S   -   scale (see below);  1.0 is used as default value. S &ne; 0.

Outputs:
    P   -   polynomial in barycentric form

NOTES:
1.  this function accepts offset and scale, which can be  set  to  improve
    numerical properties of polynomial. For example, if you interpolate on
    [-1,+1],  you  can  set C=0 and S=1 and convert from sum of 1, x, x^2,
    x^3 and so on. In most cases you it is exactly what you need.

    However, if your interpolation model was built on [999,1001], you will
    see significant growth of numerical errors when using {1, x, x^2, x^3}
    as  input  basis.  Converting  from  sum  of  1, (x-1000), (x-1000)^2,
    (x-1000)^3 will be better option (you have to specify 1000.0 as offset
    C and 1.0 as scale S).

2.  power basis is ill-conditioned and tricks described above can't  solve
    this problem completely. This function  will  return barycentric model
    in any case, but for N &gt; 8 accuracy well degrade. However, N's less than
    5 are pretty safe.
ALGLIB: Copyright 30.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialpow2bar(real_1d_array a, ae_int_t n, <b>double</b> c, <b>double</b> s, barycentricinterpolant &amp;p);
<b>void</b> polynomialpow2bar(real_1d_array a, barycentricinterpolant &amp;p);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_polint_d_conv class=nav>polint_d_conv</a> ]</p>
<a name=example_polint_d_calcdiff></a><h6 class=pageheader>polint_d_calcdiff Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Here we demonstrate polynomial interpolation and differentiation</font>
<font color=navy>// of y=x^2-x sampled at [0,1,2]. Barycentric representation of polynomial is used.</font>
   real_1d_array x = <font color=blue><b>&quot;[0,1,2]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0,0,2]&quot;</b></font>;
   <b>double</b> t = -1;
   <b>double</b> v;
   <b>double</b> dv;
   <b>double</b> d2v;
   barycentricinterpolant p;

<font color=navy>// barycentric model is created</font>
   polynomialbuild(x, y, p);

<font color=navy>// barycentric interpolation is demonstrated</font>
   v = barycentriccalc(p, t);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.0</font>

<font color=navy>// barycentric differentation is demonstrated</font>
   barycentricdiff1(p, t, v, dv);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.0</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(dv)); <font color=navy>// EXPECTED: -3.0</font>

<font color=navy>// second derivatives with barycentric representation</font>
   barycentricdiff1(p, t, v, dv);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.0</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(dv)); <font color=navy>// EXPECTED: -3.0</font>
   barycentricdiff2(p, t, v, dv, d2v);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.0</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(dv)); <font color=navy>// EXPECTED: -3.0</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(d2v)); <font color=navy>// EXPECTED: 2.0</font>
   <b>return</b> 0;
}
</pre>
<a name=example_polint_d_conv></a><h6 class=pageheader>polint_d_conv Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Here we demonstrate conversion of y=x^2-x</font>
<font color=navy>// between power basis and barycentric representation.</font>
   real_1d_array a = <font color=blue><b>&quot;[0,-1,+1]&quot;</b></font>;
   <b>double</b> t = 2;
   real_1d_array a2;
   <b>double</b> v;
   barycentricinterpolant p;
<font color=navy>// a=[0,-1,+1] is decomposition of y=x^2-x in the power basis:</font>
<font color=navy>//</font>
<font color=navy>//     y = 0 - 1*x + 1*x^2</font>
<font color=navy>//</font>
<font color=navy>// We convert it to the barycentric form.</font>
   polynomialpow2bar(a, p);

<font color=navy>// now we have barycentric interpolation; we can use it <b>for</b> interpolation</font>
   v = barycentriccalc(p, t);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.0</font>

<font color=navy>// we can also convert back from barycentric representation to power basis</font>
   polynomialbar2pow(p, a2);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a2.tostring(2).c_str()); <font color=navy>// EXPECTED: [0,-1,+1]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_polint_d_spec></a><h6 class=pageheader>polint_d_spec Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Temporaries:</font>
<font color=navy>// * values of y=x^2-x sampled at three special grids:</font>
<font color=navy>//   * equdistant grid spanning [0,2],     x[i] = 2*i/(N-1), i=0..N-1</font>
<font color=navy>//   * Chebyshev-I grid spanning [-1,+1],  x[i] = 1 + cos(PI*(2*i+1)/(2*n)), i=0..N-1</font>
<font color=navy>//   * Chebyshev-II grid spanning [-1,+1], x[i] = 1 + cos(PI*i/(n-1)), i=0..N-1</font>
<font color=navy>// * barycentric interpolants <b>for</b> these three grids</font>
<font color=navy>// * vectors to store coefficients of quadratic representation</font>
   real_1d_array y_eqdist = <font color=blue><b>&quot;[0,0,2]&quot;</b></font>;
   real_1d_array y_cheb1 = <font color=blue><b>&quot;[-0.116025,0.000000,1.616025]&quot;</b></font>;
   real_1d_array y_cheb2 = <font color=blue><b>&quot;[0,0,2]&quot;</b></font>;
   barycentricinterpolant p_eqdist;
   barycentricinterpolant p_cheb1;
   barycentricinterpolant p_cheb2;
   real_1d_array a_eqdist;
   real_1d_array a_cheb1;
   real_1d_array a_cheb2;
<font color=navy>// First, we demonstrate construction of barycentric interpolants on</font>
<font color=navy>// special grids. We unpack power representation to ensure that</font>
<font color=navy>// interpolant was built correctly.</font>
<font color=navy>//</font>
<font color=navy>// In all three cases we should get same quadratic function.</font>
   polynomialbuildeqdist(0.0, 2.0, y_eqdist, p_eqdist);
   polynomialbar2pow(p_eqdist, a_eqdist);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a_eqdist.tostring(4).c_str()); <font color=navy>// EXPECTED: [0,-1,+1]</font>

   polynomialbuildcheb1(-1, +1, y_cheb1, p_cheb1);
   polynomialbar2pow(p_cheb1, a_cheb1);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a_cheb1.tostring(4).c_str()); <font color=navy>// EXPECTED: [0,-1,+1]</font>

   polynomialbuildcheb2(-1, +1, y_cheb2, p_cheb2);
   polynomialbar2pow(p_cheb2, a_cheb2);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a_cheb2.tostring(4).c_str()); <font color=navy>// EXPECTED: [0,-1,+1]</font>
<font color=navy>//</font>
<font color=navy>// Now we demonstrate polynomial interpolation without construction </font>
<font color=navy>// of the barycentricinterpolant structure.</font>
<font color=navy>//</font>
<font color=navy>// We calculate interpolant value at x=-2.</font>
<font color=navy>// In all three cases we should get same f=6</font>
   <b>double</b> t = -2;
   <b>double</b> v;
   v = polynomialcalceqdist(0.0, 2.0, y_eqdist, t);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 6.0</font>

   v = polynomialcalccheb1(-1, +1, y_cheb1, t);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 6.0</font>

   v = polynomialcalccheb2(-1, +1, y_cheb2, t);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 6.0</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_ratint></a><h4 class=pageheader>8.6.7. ratint Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_barycentricinterpolant class=toc>barycentricinterpolant</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_barycentricbuildfloaterhormann class=toc>barycentricbuildfloaterhormann</a> |
<a href=#sub_barycentricbuildxyw class=toc>barycentricbuildxyw</a> |
<a href=#sub_barycentriccalc class=toc>barycentriccalc</a> |
<a href=#sub_barycentricdiff1 class=toc>barycentricdiff1</a> |
<a href=#sub_barycentricdiff2 class=toc>barycentricdiff2</a> |
<a href=#sub_barycentriclintransx class=toc>barycentriclintransx</a> |
<a href=#sub_barycentriclintransy class=toc>barycentriclintransy</a> |
<a href=#sub_barycentricunpack class=toc>barycentricunpack</a>
]</font>
</div>
<a name=struct_barycentricinterpolant></a><h6 class=pageheader>barycentricinterpolant Class</h6>
<hr width=600 align=left>
<pre class=narration>
Barycentric interpolant.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> barycentricinterpolant {
};
</pre>
<a name=sub_barycentricbuildfloaterhormann></a><h6 class=pageheader>barycentricbuildfloaterhormann Function</h6>
<hr width=600 align=left>
<pre class=narration>
Rational interpolant without poles

The subroutine constructs the rational interpolating function without real
poles  (see  'Barycentric rational interpolation with no  poles  and  high
rates of approximation', Michael S. Floater. and  Kai  Hormann,  for  more
information on this subject).

Inputs:
    X   -   interpolation nodes, array[0..N-1].
    Y   -   function values, array[0..N-1].
    N   -   number of nodes, N &gt; 0.
    D   -   order of the interpolation scheme, 0 &le; D &le; N-1.
            D &lt; 0 will cause an error.
            D &ge; N it will be replaced with D=N-1.
            if you don't know what D to choose, use small value about 3-5.

Outputs:
    B   -   barycentric interpolant.

Note:
    this algorithm always succeeds and calculates the weights  with  close
    to machine precision.
ALGLIB Project: Copyright 17.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> barycentricbuildfloaterhormann(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t d, barycentricinterpolant &amp;b);
</pre>
<a name=sub_barycentricbuildxyw></a><h6 class=pageheader>barycentricbuildxyw Function</h6>
<hr width=600 align=left>
<pre class=narration>
Rational interpolant from X/Y/W arrays

F(t) = SUM(i=0,n-1,w[i]*f[i]/(t-x[i])) / SUM(i=0,n-1,w[i]/(t-x[i]))

Inputs:
    X   -   interpolation nodes, array[0..N-1]
    F   -   function values, array[0..N-1]
    W   -   barycentric weights, array[0..N-1]
    N   -   nodes count, N &gt; 0

Outputs:
    B   -   barycentric interpolant built from (X, Y, W)
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> barycentricbuildxyw(real_1d_array x, real_1d_array y, real_1d_array w, ae_int_t n, barycentricinterpolant &amp;b);
</pre>
<a name=sub_barycentriccalc></a><h6 class=pageheader>barycentriccalc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Rational interpolation using barycentric formula

F(t) = SUM(i=0,n-1,w[i]*f[i]/(t-x[i])) / SUM(i=0,n-1,w[i]/(t-x[i]))

Inputs:
    B   -   barycentric interpolant built with one of model building
            subroutines.
    T   -   interpolation point

Result:
    barycentric interpolant F(t)
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> barycentriccalc(barycentricinterpolant b, <b>double</b> t);
</pre>
<a name=sub_barycentricdiff1></a><h6 class=pageheader>barycentricdiff1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Differentiation of barycentric interpolant: first derivative.

Algorithm used in this subroutine is very robust and should not fail until
provided with values too close to MaxRealNumber  (usually  MaxRealNumber/N
or greater will overflow).

Inputs:
    B   -   barycentric interpolant built with one of model building
            subroutines.
    T   -   interpolation point

Outputs:
    F   -   barycentric interpolant at T
    DF  -   first derivative
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> barycentricdiff1(barycentricinterpolant b, <b>double</b> t, <b>double</b> &amp;f, <b>double</b> &amp;df);
</pre>
<a name=sub_barycentricdiff2></a><h6 class=pageheader>barycentricdiff2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Differentiation of barycentric interpolant: first/second derivatives.

Inputs:
    B   -   barycentric interpolant built with one of model building
            subroutines.
    T   -   interpolation point

Outputs:
    F   -   barycentric interpolant at T
    DF  -   first derivative
    D2F -   second derivative

NOTE: this algorithm may fail due to overflow/underflor if  used  on  data
whose values are close to MaxRealNumber or MinRealNumber.  Use more robust
BarycentricDiff1() subroutine in such cases.
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> barycentricdiff2(barycentricinterpolant b, <b>double</b> t, <b>double</b> &amp;f, <b>double</b> &amp;df, <b>double</b> &amp;d2f);
</pre>
<a name=sub_barycentriclintransx></a><h6 class=pageheader>barycentriclintransx Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine performs linear transformation of the argument.

Inputs:
    B       -   rational interpolant in barycentric form
    CA, CB  -   transformation coefficients: x = CA*t + CB

Outputs:
    B       -   transformed interpolant with X replaced by T
ALGLIB Project: Copyright 19.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> barycentriclintransx(barycentricinterpolant b, <b>double</b> ca, <b>double</b> cb);
</pre>
<a name=sub_barycentriclintransy></a><h6 class=pageheader>barycentriclintransy Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  subroutine   performs   linear  transformation  of  the  barycentric
interpolant.

Inputs:
    B       -   rational interpolant in barycentric form
    CA, CB  -   transformation coefficients: B2(x) = CA*B(x) + CB

Outputs:
    B       -   transformed interpolant
ALGLIB Project: Copyright 19.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> barycentriclintransy(barycentricinterpolant b, <b>double</b> ca, <b>double</b> cb);
</pre>
<a name=sub_barycentricunpack></a><h6 class=pageheader>barycentricunpack Function</h6>
<hr width=600 align=left>
<pre class=narration>
Extracts X/Y/W arrays from rational interpolant

Inputs:
    B   -   barycentric interpolant

Outputs:
    N   -   nodes count, N &gt; 0
    X   -   interpolation nodes, array[0..N-1]
    F   -   function values, array[0..N-1]
    W   -   barycentric weights, array[0..N-1]
ALGLIB: Copyright 17.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> barycentricunpack(barycentricinterpolant b, ae_int_t &amp;n, real_1d_array &amp;x, real_1d_array &amp;y, real_1d_array &amp;w);
</pre>
<a name=unit_rbf></a><h4 class=pageheader>8.6.8. rbf Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_rbfcalcbuffer class=toc>rbfcalcbuffer</a> |
<a href=#struct_rbfmodel class=toc>rbfmodel</a> |
<a href=#struct_rbfreport class=toc>rbfreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_rbfbuildmodel class=toc>rbfbuildmodel</a> |
<a href=#sub_rbfcalc class=toc>rbfcalc</a> |
<a href=#sub_rbfcalc1 class=toc>rbfcalc1</a> |
<a href=#sub_rbfcalc2 class=toc>rbfcalc2</a> |
<a href=#sub_rbfcalc3 class=toc>rbfcalc3</a> |
<a href=#sub_rbfcalcbuf class=toc>rbfcalcbuf</a> |
<a href=#sub_rbfcreate class=toc>rbfcreate</a> |
<a href=#sub_rbfcreatecalcbuffer class=toc>rbfcreatecalcbuffer</a> |
<a href=#sub_rbfgetmodelversion class=toc>rbfgetmodelversion</a> |
<a href=#sub_rbfgridcalc2 class=toc>rbfgridcalc2</a> |
<a href=#sub_rbfgridcalc2v class=toc>rbfgridcalc2v</a> |
<a href=#sub_rbfgridcalc2vsubset class=toc>rbfgridcalc2vsubset</a> |
<a href=#sub_rbfgridcalc3v class=toc>rbfgridcalc3v</a> |
<a href=#sub_rbfgridcalc3vsubset class=toc>rbfgridcalc3vsubset</a> |
<a href=#sub_rbfpeekprogress class=toc>rbfpeekprogress</a> |
<a href=#sub_rbfrequesttermination class=toc>rbfrequesttermination</a> |
<a href=#sub_rbfserialize class=toc>rbfserialize</a> |
<a href=#sub_rbfsetalgohierarchical class=toc>rbfsetalgohierarchical</a> |
<a href=#sub_rbfsetalgomultilayer class=toc>rbfsetalgomultilayer</a> |
<a href=#sub_rbfsetalgoqnn class=toc>rbfsetalgoqnn</a> |
<a href=#sub_rbfsetconstterm class=toc>rbfsetconstterm</a> |
<a href=#sub_rbfsetlinterm class=toc>rbfsetlinterm</a> |
<a href=#sub_rbfsetpoints class=toc>rbfsetpoints</a> |
<a href=#sub_rbfsetpointsandscales class=toc>rbfsetpointsandscales</a> |
<a href=#sub_rbfsetv2bf class=toc>rbfsetv2bf</a> |
<a href=#sub_rbfsetv2its class=toc>rbfsetv2its</a> |
<a href=#sub_rbfsetv2supportr class=toc>rbfsetv2supportr</a> |
<a href=#sub_rbfsetzeroterm class=toc>rbfsetzeroterm</a> |
<a href=#sub_rbftscalcbuf class=toc>rbftscalcbuf</a> |
<a href=#sub_rbfunpack class=toc>rbfunpack</a> |
<a href=#sub_rbfunserialize class=toc>rbfunserialize</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_rbf_d_hrbf class=toc>rbf_d_hrbf</a></td><td width=15>&nbsp;</td><td>Simple model built with HRBF algorithm</td></tr>
<tr align=left valign=top><td><a href=#example_rbf_d_polterm class=toc>rbf_d_polterm</a></td><td width=15>&nbsp;</td><td>RBF models - working with polynomial term</td></tr>
<tr align=left valign=top><td><a href=#example_rbf_d_serialize class=toc>rbf_d_serialize</a></td><td width=15>&nbsp;</td><td>Serialization/unserialization</td></tr>
<tr align=left valign=top><td><a href=#example_rbf_d_vector class=toc>rbf_d_vector</a></td><td width=15>&nbsp;</td><td>Working with vector functions</td></tr>
</table>
</div>
<a name=struct_rbfcalcbuffer></a><h6 class=pageheader>rbfcalcbuffer Class</h6>
<hr width=600 align=left>
<pre class=narration>
Buffer object which is used to perform nearest neighbor  requests  in  the
multithreaded mode (multiple threads working with same KD-tree object).

This object should be created with KDTreeCreateBuffer().
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> rbfcalcbuffer {
};
</pre>
<a name=struct_rbfmodel></a><h6 class=pageheader>rbfmodel Class</h6>
<hr width=600 align=left>
<pre class=narration>
RBF model.

Never try to directly work with fields of this object - always use  ALGLIB
functions to use this object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> rbfmodel {
};
</pre>
<a name=struct_rbfreport></a><h6 class=pageheader>rbfreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
RBF solution report:
* TerminationType   -   termination type, positive values - success,
                        non-positive - failure.

Fields which are set by modern RBF solvers (hierarchical):
* RMSError          -   root-mean-square error; NAN for old solvers (ML, QNN)
* MaxError          -   maximum error; NAN for old solvers (ML, QNN)
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> rbfreport {
   <b>double</b> rmserror;
   <b>double</b> maxerror;
   ae_int_t arows;
   ae_int_t acols;
   ae_int_t annz;
   ae_int_t iterationscount;
   ae_int_t nmv;
   ae_int_t terminationtype;
};
</pre>
<a name=sub_rbfbuildmodel></a><h6 class=pageheader>rbfbuildmodel Function</h6>
<hr width=600 align=left>
<pre class=narration>
This   function  builds  RBF  model  and  returns  report  (contains  some
information which can be used for evaluation of the algorithm properties).

Call to this function modifies RBF model by calculating its centers/radii/
weights  and  saving  them  into  RBFModel  structure.  Initially RBFModel
contain zero coefficients, but after call to this function  we  will  have
coefficients which were calculated in order to fit our dataset.

After you called this function you can call RBFCalc(),  RBFGridCalc()  and
other model calculation functions.

Inputs:
    S       -   RBF model, initialized by RBFCreate() call
    Rep     -   report:
                * Rep.TerminationType:
                  * -5 - non-distinct basis function centers were detected,
                         interpolation  aborted;  only  QNN  returns  this
                         error   code, other  algorithms  can  handle non-
                         distinct nodes.
                  * -4 - nonconvergence of the internal SVD solver
                  * -3   incorrect model construction algorithm was chosen:
                         QNN or RBF-ML, combined with one of the incompatible
                         features - NX=1 or NX &gt; 3; points with per-dimension
                         scales.
                  *  1 - successful termination
                  *  8 - a termination request was submitted via
                         rbfrequesttermination() function.

                Fields which are set only by modern RBF solvers (hierarchical
                or nonnegative; older solvers like QNN and ML initialize these
                fields by NANs):
                * rep.rmserror - root-mean-square error at nodes
                * rep.maxerror - maximum error at nodes

                Fields are used for debugging purposes:
                * Rep.IterationsCount - iterations count of the LSQR solver
                * Rep.NMV - number of matrix-vector products
                * Rep.ARows - rows count for the system matrix
                * Rep.ACols - columns count for the system matrix
                * Rep.ANNZ - number of significantly non-zero elements
                  (elements above some algorithm-determined threshold)

NOTE:  failure  to  build  model will leave current state of the structure
unchanged.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfbuildmodel(rbfmodel s, rbfreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_hrbf class=nav>rbf_d_hrbf</a> | <a href=#example_rbf_d_vector class=nav>rbf_d_vector</a> | <a href=#example_rbf_d_polterm class=nav>rbf_d_polterm</a> ]</p>
<a name=sub_rbfcalc></a><h6 class=pageheader>rbfcalc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the RBF model at the given point.

This is general function which can be used for arbitrary NX (dimension  of
the space of arguments) and NY (dimension of the function itself). However
when  you  have  NY=1  you  may  find more convenient to use rbfcalc2() or
rbfcalc3().

If you want to perform parallel model evaluation  from  multiple  threads,
use rbftscalcbuf() with per-thread buffer object.

This function returns 0.0 when model is not initialized.

Inputs:
    S       -   RBF model
    X       -   coordinates, array[NX].
                X may have more than NX elements, in this case only
                leading NX will be used.

Outputs:
    Y       -   function value, array[NY]. Y is out-parameter and
                reallocated after call to this function. In case you  want
                to reuse previously allocated Y, you may use RBFCalcBuf(),
                which reallocates Y only when it is too small.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfcalc(rbfmodel s, real_1d_array x, real_1d_array &amp;y);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_vector class=nav>rbf_d_vector</a> ]</p>
<a name=sub_rbfcalc1></a><h6 class=pageheader>rbfcalc1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the RBF model in the given point.

IMPORTANT: this function works only with modern  (hierarchical)  RBFs.  It
           can not be used with legacy (version 1) RBFs because older  RBF
           code does not support 1-dimensional models.

This function should be used when we have NY=1 (scalar function) and  NX=1
(1-dimensional space). If you have 3-dimensional space, use rbfcalc3(). If
you  have  2-dimensional  space,  use  rbfcalc3().  If  you  have  general
situation (NX-dimensional space, NY-dimensional function)  you  should use
generic rbfcalc().

If you want to perform parallel model evaluation  from  multiple  threads,
use rbftscalcbuf() with per-thread buffer object.

This function returns 0.0 when:
* model is not initialized
* NX &ne; 1
* NY &ne; 1

Inputs:
    S       -   RBF model
    X0      -   X-coordinate, finite number

Result:
    value of the model or 0.0 (as defined above)
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rbfcalc1(rbfmodel s, <b>double</b> x0);
</pre>
<a name=sub_rbfcalc2></a><h6 class=pageheader>rbfcalc2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the RBF model in the given point.

This function should be used when we have NY=1 (scalar function) and  NX=2
(2-dimensional space). If you have 3-dimensional space, use rbfcalc3(). If
you have general situation (NX-dimensional space, NY-dimensional function)
you should use generic rbfcalc().

If  you  want  to  calculate  function  values  many times, consider using
rbfgridcalc2v(), which is far more efficient than many subsequent calls to
rbfcalc2().

If you want to perform parallel model evaluation  from  multiple  threads,
use rbftscalcbuf() with per-thread buffer object.

This function returns 0.0 when:
* model is not initialized
* NX &ne; 2
 *NY &ne; 1

Inputs:
    S       -   RBF model
    X0      -   first coordinate, finite number
    X1      -   second coordinate, finite number

Result:
    value of the model or 0.0 (as defined above)
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rbfcalc2(rbfmodel s, <b>double</b> x0, <b>double</b> x1);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_hrbf class=nav>rbf_d_hrbf</a> | <a href=#example_rbf_d_polterm class=nav>rbf_d_polterm</a> ]</p>
<a name=sub_rbfcalc3></a><h6 class=pageheader>rbfcalc3 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates value of the RBF model in the given point.

This function should be used when we have NY=1 (scalar function) and  NX=3
(3-dimensional space). If you have 2-dimensional space, use rbfcalc2(). If
you have general situation (NX-dimensional space, NY-dimensional function)
you should use generic rbfcalc().

If  you  want  to  calculate  function  values  many times, consider using
rbfgridcalc3v(), which is far more efficient than many subsequent calls to
rbfcalc3().

If you want to perform parallel model evaluation  from  multiple  threads,
use rbftscalcbuf() with per-thread buffer object.

This function returns 0.0 when:
* model is not initialized
* NX &ne; 3
 *NY &ne; 1

Inputs:
    S       -   RBF model
    X0      -   first coordinate, finite number
    X1      -   second coordinate, finite number
    X2      -   third coordinate, finite number

Result:
    value of the model or 0.0 (as defined above)
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rbfcalc3(rbfmodel s, <b>double</b> x0, <b>double</b> x1, <b>double</b> x2);
</pre>
<a name=sub_rbfcalcbuf></a><h6 class=pageheader>rbfcalcbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the RBF model at the given point.

Same as rbfcalc(), but does not reallocate Y when in is large enough to
store function values.

If you want to perform parallel model evaluation  from  multiple  threads,
use rbftscalcbuf() with per-thread buffer object.

Inputs:
    S       -   RBF model
    X       -   coordinates, array[NX].
                X may have more than NX elements, in this case only
                leading NX will be used.
    Y       -   possibly preallocated array

Outputs:
    Y       -   function value, array[NY]. Y is not reallocated when it
                is larger than NY.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfcalcbuf(rbfmodel s, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_rbfcreate></a><h6 class=pageheader>rbfcreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates RBF  model  for  a  scalar (NY=1)  or  vector (NY &gt; 1)
function in a NX-dimensional space (NX &ge; 1).

Newly created model is empty. It can be used for interpolation right after
creation, but it just returns zeros. You have to add points to the  model,
tune interpolation settings, and then  call  model  construction  function
rbfbuildmodel() which will update model according to your specification.

USAGE:
1. User creates model with rbfcreate()
2. User adds dataset with rbfsetpoints() (points do NOT have to  be  on  a
   regular grid) or rbfsetpointsandscales().
3. (OPTIONAL) User chooses polynomial term by calling:
   * rbflinterm() to set linear term
   * rbfconstterm() to set constant term
   * rbfzeroterm() to set zero term
   By default, linear term is used.
4. User tweaks algorithm properties with  rbfsetalgohierarchical()  method
   (or chooses one of the legacy algorithms - QNN  (rbfsetalgoqnn)  or  ML
   (rbfsetalgomultilayer)).
5. User calls rbfbuildmodel() function which rebuilds model  according  to
   the specification
6. User may call rbfcalc() to calculate model value at the specified point,
   rbfgridcalc() to  calculate   model  values at the points of the regular
   grid. User may extract model coefficients with rbfunpack() call.

IMPORTANT: we recommend you to use latest model construction  algorithm  -
           hierarchical RBFs, which is activated by rbfsetalgohierarchical()
           function. This algorithm is the fastest one, and  most  memory-
           efficient.
           However,  it  is  incompatible  with older versions  of  ALGLIB
           (pre-3.11). So, if you serialize hierarchical model,  you  will
           be unable to load it in pre-3.11 ALGLIB. Other model types (QNN
           and RBF-ML) are still backward-compatible.

Inputs:
    NX      -   dimension of the space, NX &ge; 1
    NY      -   function dimension, NY &ge; 1

Outputs:
    S       -   RBF model (initially equals to zero)

NOTE 1: memory requirements. RBF models require amount of memory  which is
        proportional  to the number of data points. Some additional memory
        is allocated during model construction, but most of this memory is
        freed after model coefficients  are  calculated.  Amount  of  this
        additional memory depends on model  construction  algorithm  being
        used.

NOTE 2: prior to ALGLIB version 3.11, RBF models supported  only  NX=2  or
        NX=3. Any  attempt  to  create  single-dimensional  or  more  than
        3-dimensional RBF model resulted in exception.

        ALGLIB 3.11 supports any NX &gt; 0, but models created with  NX != 2  and
        NX != 3 are incompatible with (a) older versions of ALGLIB, (b)  old
        model construction algorithms (QNN or RBF-ML).

        So, if you create a model with NX=2 or NX=3,  then,  depending  on
        specific  model construction algorithm being chosen, you will (QNN
        and RBF-ML) or will not (HierarchicalRBF) get backward compatibility
        with older versions of ALGLIB. You have a choice here.

        However, if you create a model with NX neither 2 nor 3,  you  have
        no backward compatibility from the start, and you  are  forced  to
        use hierarchical RBFs and ALGLIB 3.11 or later.
ALGLIB: Copyright 13.12.2011, 20.06.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfcreate(ae_int_t nx, ae_int_t ny, rbfmodel &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_hrbf class=nav>rbf_d_hrbf</a> | <a href=#example_rbf_d_vector class=nav>rbf_d_vector</a> | <a href=#example_rbf_d_polterm class=nav>rbf_d_polterm</a> | <a href=#example_rbf_d_serialize class=nav>rbf_d_serialize</a> ]</p>
<a name=sub_rbfcreatecalcbuffer></a><h6 class=pageheader>rbfcreatecalcbuffer Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates buffer  structure  which  can  be  used  to  perform
parallel  RBF  model  evaluations  (with  one  RBF  model  instance  being
used from multiple threads, as long as  different  threads  use  different
instances of buffer).

This buffer object can be used with  rbftscalcbuf()  function  (here  &quot;ts&quot;
stands for &quot;thread-safe&quot;, &quot;buf&quot; is a suffix which denotes  function  which
reuses previously allocated output space).

How to use it:
* create RBF model structure with rbfcreate()
* load data, tune parameters
* call rbfbuildmodel()
* call rbfcreatecalcbuffer(), once per thread working with RBF model  (you
  should call this function only AFTER call to rbfbuildmodel(), see  below
  for more information)
* call rbftscalcbuf() from different threads,  with  each  thread  working
  with its own copy of buffer object.

Inputs:
    S           -   RBF model

Outputs:
    Buf         -   external buffer.

IMPORTANT: buffer object should be used only with  RBF model object  which
           was used to initialize buffer. Any attempt to use buffer   with
           different object is dangerous - you may  get  memory  violation
           error because sizes of internal arrays do not fit to dimensions
           of RBF structure.

IMPORTANT: you  should  call  this function only for model which was built
           with rbfbuildmodel() function, after successful  invocation  of
           rbfbuildmodel().  Sizes   of   some   internal  structures  are
           determined only after model is built, so buffer object  created
           before model  construction  stage  will  be  useless  (and  any
           attempt to use it will result in exception).
ALGLIB: Copyright 02.04.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfcreatecalcbuffer(rbfmodel s, rbfcalcbuffer &amp;buf);
</pre>
<a name=sub_rbfgetmodelversion></a><h6 class=pageheader>rbfgetmodelversion Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns model version.

Inputs:
    S       -   RBF model

Result:
    * 1 - for models created by QNN and RBF-ML algorithms,
      compatible with ALGLIB 3.10 or earlier.
    * 2 - for models created by HierarchicalRBF, requires
      ALGLIB 3.11 or later
ALGLIB: Copyright 06.07.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t rbfgetmodelversion(rbfmodel s);
</pre>
<a name=sub_rbfgridcalc2></a><h6 class=pageheader>rbfgridcalc2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This is legacy function for gridded calculation of RBF model.

It is superseded by rbfgridcalc2v() and  rbfgridcalc2vsubset()  functions.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfgridcalc2(rbfmodel s, real_1d_array x0, ae_int_t n0, real_1d_array x1, ae_int_t n1, real_2d_array &amp;y);
</pre>
<a name=sub_rbfgridcalc2v></a><h6 class=pageheader>rbfgridcalc2v Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the RBF  model  at  the  regular  grid,
which  has  N0*N1 points, with Point[I,J] = (X0[I], X1[J]).  Vector-valued
RBF models are supported.

This function returns 0.0 when:
* model is not initialized
* NX &ne; 2

NOTE: Parallel  processing  is  implemented only for modern (hierarchical)
      RBFs. Legacy version 1 RBFs (created  by  QNN  or  RBF-ML) are still
      processed serially.

Inputs:
    S       -   RBF model, used in read-only mode, can be  shared  between
                multiple   invocations  of  this  function  from  multiple
                threads.

    X0      -   array of grid nodes, first coordinates, array[N0].
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N0      -   grid size (number of nodes) in the first dimension

    X1      -   array of grid nodes, second coordinates, array[N1]
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N1      -   grid size (number of nodes) in the second dimension

Outputs:
    Y       -   function values, array[NY*N0*N1], where NY is a  number of
                &quot;output&quot; vector values (this  function   supports  vector-
                valued RBF models). Y is out-variable and  is  reallocated
                by this function.
                Y[K+NY*(I0+I1*N0)]=F_k(X0[I0],X1[I1]), for:
                *  K=0...NY-1
                * I0=0...N0-1
                * I1=0...N1-1

NOTE: this function supports weakly ordered grid nodes, i.e. you may  have
      X[i]=X[i+1] for some i. It does  not  provide  you  any  performance
      benefits  due  to   duplication  of  points,  just  convenience  and
      flexibility.

NOTE: this  function  is  re-entrant,  i.e.  you  may  use  same  rbfmodel
      structure in multiple threads calling  this function  for  different
      grids.

NOTE: if you need function values on some subset  of  regular  grid, which
      may be described as &quot;several compact and  dense  islands&quot;,  you  may
      use rbfgridcalc2vsubset().
ALGLIB: Copyright 27.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfgridcalc2v(rbfmodel s, real_1d_array x0, ae_int_t n0, real_1d_array x1, ae_int_t n1, real_1d_array &amp;y);
</pre>
<a name=sub_rbfgridcalc2vsubset></a><h6 class=pageheader>rbfgridcalc2vsubset Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the RBF model at some subset of regular
grid:
* grid has N0*N1 points, with Point[I,J] = (X0[I], X1[J])
* only values at some subset of this grid are required
Vector-valued RBF models are supported.

This function returns 0.0 when:
* model is not initialized
* NX &ne; 2

NOTE: Parallel  processing  is  implemented only for modern (hierarchical)
      RBFs. Legacy version 1 RBFs (created  by  QNN  or  RBF-ML) are still
      processed serially.

Inputs:
    S       -   RBF model, used in read-only mode, can be  shared  between
                multiple   invocations  of  this  function  from  multiple
                threads.

    X0      -   array of grid nodes, first coordinates, array[N0].
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N0      -   grid size (number of nodes) in the first dimension

    X1      -   array of grid nodes, second coordinates, array[N1]
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N1      -   grid size (number of nodes) in the second dimension

    FlagY   -   array[N0*N1]:
                * Y[I0+I1*N0] corresponds to node (X0[I0],X1[I1])
                * it is a &quot;bitmap&quot; array which contains  False  for  nodes
                  which are NOT calculated, and True for nodes  which  are
                  required.

Outputs:
    Y       -   function values, array[NY*N0*N1*N2], where NY is a  number
                of &quot;output&quot; vector values (this function  supports vector-
                valued RBF models):
                * Y[K+NY*(I0+I1*N0)]=F_k(X0[I0],X1[I1]),
                  for K=0...NY-1, I0=0...N0-1, I1=0...N1-1.
                * elements of Y[] which correspond  to  FlagY[]=True   are
                  loaded by model values (which may be  exactly  zero  for
                  some nodes).
                * elements of Y[] which correspond to FlagY[]=False MAY be
                  initialized by zeros OR may be calculated. This function
                  processes  grid  as  a  hierarchy  of  nested blocks and
                  micro-rows. If just one element of micro-row is required,
                  entire micro-row (up to 8 nodes in the current  version,
                  but no promises) is calculated.

NOTE: this function supports weakly ordered grid nodes, i.e. you may  have
      X[i]=X[i+1] for some i. It does  not  provide  you  any  performance
      benefits  due  to   duplication  of  points,  just  convenience  and
      flexibility.

NOTE: this  function  is  re-entrant,  i.e.  you  may  use  same  rbfmodel
      structure in multiple threads calling  this function  for  different
      grids.
ALGLIB: Copyright 04.03.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfgridcalc2vsubset(rbfmodel s, real_1d_array x0, ae_int_t n0, real_1d_array x1, ae_int_t n1, boolean_1d_array flagy, real_1d_array &amp;y);
</pre>
<a name=sub_rbfgridcalc3v></a><h6 class=pageheader>rbfgridcalc3v Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the RBF  model  at  the  regular  grid,
which  has  N0*N1*N2  points,  with  Point[I,J,K] = (X0[I], X1[J], X2[K]).
Vector-valued RBF models are supported.

This function returns 0.0 when:
* model is not initialized
* NX &ne; 3

NOTE: Parallel  processing  is  implemented only for modern (hierarchical)
      RBFs. Legacy version 1 RBFs (created  by  QNN  or  RBF-ML) are still
      processed serially.

Inputs:
    S       -   RBF model, used in read-only mode, can be  shared  between
                multiple   invocations  of  this  function  from  multiple
                threads.

    X0      -   array of grid nodes, first coordinates, array[N0].
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N0      -   grid size (number of nodes) in the first dimension

    X1      -   array of grid nodes, second coordinates, array[N1]
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N1      -   grid size (number of nodes) in the second dimension

    X2      -   array of grid nodes, third coordinates, array[N2]
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N2      -   grid size (number of nodes) in the third dimension

Outputs:
    Y       -   function values, array[NY*N0*N1*N2], where NY is a  number
                of &quot;output&quot; vector values (this function  supports vector-
                valued RBF models). Y is out-variable and  is  reallocated
                by this function.
                Y[K+NY*(I0+I1*N0+I2*N0*N1)]=F_k(X0[I0],X1[I1],X2[I2]), for:
                *  K=0...NY-1
                * I0=0...N0-1
                * I1=0...N1-1
                * I2=0...N2-1

NOTE: this function supports weakly ordered grid nodes, i.e. you may  have
      X[i]=X[i+1] for some i. It does  not  provide  you  any  performance
      benefits  due  to   duplication  of  points,  just  convenience  and
      flexibility.

NOTE: this  function  is  re-entrant,  i.e.  you  may  use  same  rbfmodel
      structure in multiple threads calling  this function  for  different
      grids.

NOTE: if you need function values on some subset  of  regular  grid, which
      may be described as &quot;several compact and  dense  islands&quot;,  you  may
      use rbfgridcalc3vsubset().
ALGLIB: Copyright 04.03.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfgridcalc3v(rbfmodel s, real_1d_array x0, ae_int_t n0, real_1d_array x1, ae_int_t n1, real_1d_array x2, ae_int_t n2, real_1d_array &amp;y);
</pre>
<a name=sub_rbfgridcalc3vsubset></a><h6 class=pageheader>rbfgridcalc3vsubset Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the RBF model at some subset of regular
grid:
* grid has N0*N1*N2 points, with Point[I,J,K] = (X0[I], X1[J], X2[K])
* only values at some subset of this grid are required
Vector-valued RBF models are supported.

This function returns 0.0 when:
* model is not initialized
* NX &ne; 3

NOTE: Parallel  processing  is  implemented only for modern (hierarchical)
      RBFs. Legacy version 1 RBFs (created  by  QNN  or  RBF-ML) are still
      processed serially.

Inputs:
    S       -   RBF model, used in read-only mode, can be  shared  between
                multiple   invocations  of  this  function  from  multiple
                threads.

    X0      -   array of grid nodes, first coordinates, array[N0].
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N0      -   grid size (number of nodes) in the first dimension

    X1      -   array of grid nodes, second coordinates, array[N1]
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N1      -   grid size (number of nodes) in the second dimension

    X2      -   array of grid nodes, third coordinates, array[N2]
                Must be ordered by ascending. Exception is generated
                if the array is not correctly ordered.
    N2      -   grid size (number of nodes) in the third dimension

    FlagY   -   array[N0*N1*N2]:
                * Y[I0+I1*N0+I2*N0*N1] corresponds to node (X0[I0],X1[I1],X2[I2])
                * it is a &quot;bitmap&quot; array which contains  False  for  nodes
                  which are NOT calculated, and True for nodes  which  are
                  required.

Outputs:
    Y       -   function values, array[NY*N0*N1*N2], where NY is a  number
                of &quot;output&quot; vector values (this function  supports vector-
                valued RBF models):
                * Y[K+NY*(I0+I1*N0+I2*N0*N1)]=F_k(X0[I0],X1[I1],X2[I2]),
                  for K=0...NY-1, I0=0...N0-1, I1=0...N1-1, I2=0...N2-1.
                * elements of Y[] which correspond  to  FlagY[]=True   are
                  loaded by model values (which may be  exactly  zero  for
                  some nodes).
                * elements of Y[] which correspond to FlagY[]=False MAY be
                  initialized by zeros OR may be calculated. This function
                  processes  grid  as  a  hierarchy  of  nested blocks and
                  micro-rows. If just one element of micro-row is required,
                  entire micro-row (up to 8 nodes in the current  version,
                  but no promises) is calculated.

NOTE: this function supports weakly ordered grid nodes, i.e. you may  have
      X[i]=X[i+1] for some i. It does  not  provide  you  any  performance
      benefits  due  to   duplication  of  points,  just  convenience  and
      flexibility.

NOTE: this  function  is  re-entrant,  i.e.  you  may  use  same  rbfmodel
      structure in multiple threads calling  this function  for  different
      grids.
ALGLIB: Copyright 04.03.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfgridcalc3vsubset(rbfmodel s, real_1d_array x0, ae_int_t n0, real_1d_array x1, ae_int_t n1, real_1d_array x2, ae_int_t n2, boolean_1d_array flagy, real_1d_array &amp;y);
</pre>
<a name=sub_rbfpeekprogress></a><h6 class=pageheader>rbfpeekprogress Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to peek into hierarchical RBF  construction  process
from  some  other  thread  and  get current progress indicator. It returns
value in [0,1].

IMPORTANT: only HRBFs (hierarchical RBFs) support  peeking  into  progress
           indicator. Legacy RBF-ML and RBF-QNN do  not  support  it.  You
           will always get 0 value.

Inputs:
    S           -   RBF model object

Result:
    progress value, in [0,1]
ALGLIB: Copyright 17.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rbfpeekprogress(rbfmodel s);
</pre>
<a name=sub_rbfrequesttermination></a><h6 class=pageheader>rbfrequesttermination Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function  is  used  to  submit  a  request  for  termination  of  the
hierarchical RBF construction process from some other thread.  As  result,
RBF construction is terminated smoothly (with proper deallocation  of  all
necessary resources) and resultant model is filled by zeros.

A rep.terminationtype=8 will be returned upon receiving such request.

IMPORTANT: only  HRBFs  (hierarchical  RBFs) support termination requests.
           Legacy RBF-ML and RBF-QNN do not  support  it.  An  attempt  to
           terminate their construction will be ignored.

IMPORTANT: termination request flag is cleared when the model construction
           starts. Thus, any pre-construction termination requests will be
           silently ignored - only ones submitted AFTER  construction  has
           actually began will be handled.

Inputs:
    S           -   RBF model object
ALGLIB: Copyright 17.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfrequesttermination(rbfmodel s);
</pre>
<a name=sub_rbfserialize></a><h6 class=pageheader>rbfserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: serialization
These functions serialize a data structure to a C++ string or stream.
* serialization can be freely moved across 32-bit and 64-bit systems,
  and different byte orders. For example, you can serialize a string
  on a SPARC and unserialize it on an x86.
* ALGLIB++ serialization is compatible with serialization in ALGLIB,
  in both directions.
Important properties of s_out:
* it contains alphanumeric characters, dots, underscores, minus signs
* these symbols are grouped into words, which are separated by spaces
  and Windows-style (CR+LF) newlines
ALGLIB: Copyright 02.02.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfserialize(rbfmodel &amp;obj, std::string &amp;s_out);
<b>void</b> rbfserialize(rbfmodel &amp;obj, std::ostream &amp;s_out);
</pre>
<a name=sub_rbfsetalgohierarchical></a><h6 class=pageheader>rbfsetalgohierarchical Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  RBF interpolation algorithm. ALGLIB supports several
RBF algorithms with different properties.

This  algorithm is called Hierarchical RBF. It  similar  to  its  previous
incarnation, RBF-ML, i.e.  it  also  builds  a  sequence  of  models  with
decreasing radii. However, it uses more economical way of  building  upper
layers (ones with large radii), which results in faster model construction
and evaluation, as well as smaller memory footprint during construction.

This algorithm has following important features:
* ability to handle millions of points
* controllable smoothing via nonlinearity penalization
* support for NX-dimensional models with NX=1 or NX &gt; 3 (unlike QNN or RBF-ML)
* support for specification of per-dimensional  radii  via  scale  vector,
  which is set by means of rbfsetpointsandscales() function. This  feature
  is useful if you solve  spatio-temporal  interpolation  problems,  where
  different radii are required for spatial and temporal dimensions.

Running times are roughly proportional to:
* N*log(N)*NLayers - for model construction
* N*NLayers - for model evaluation
You may see that running time does not depend on search radius  or  points
density, just on number of layers in the hierarchy.

IMPORTANT: this model construction algorithm was introduced in ALGLIB 3.11
           and  produces  models  which  are  INCOMPATIBLE  with  previous
           versions of ALGLIB. You can  not  unserialize  models  produced
           with this function in ALGLIB 3.10 or earlier.

Inputs:
    S       -   RBF model, initialized by rbfcreate() call
    RBase   -   RBase parameter, RBase &gt; 0
    NLayers -   NLayers parameter, NLayers &gt; 0, recommended value  to  start
                with - about 5.
    LambdaNS- &ge; 0, nonlinearity penalty coefficient, negative values are
                not allowed. This parameter adds controllable smoothing to
                the problem, which may reduce noise. Specification of non-
                zero lambda means that in addition to fitting error solver
                will  also  minimize   LambdaNS*|S''(x)|^2  (appropriately
                generalized to multiple dimensions.

                Specification of exactly zero value means that no  penalty
                is added  (we  do  not  even  evaluate  matrix  of  second
                derivatives which is necessary for smoothing).

                Calculation of nonlinearity penalty is costly - it results
                in  several-fold  increase  of  model  construction  time.
                Evaluation time remains the same.

                Optimal  lambda  is  problem-dependent and requires  trial
                and  error.  Good  value to  start  from  is  1e-5...1e-6,
                which corresponds to slightly noticeable smoothing  of the
                function.  Value  1e-2  usually  means  that  quite  heavy
                smoothing is applied.

TUNING ALGORITHM

In order to use this algorithm you have to choose three parameters:
* initial radius RBase
* number of layers in the model NLayers
* penalty coefficient LambdaNS

Initial radius is easy to choose - you can pick any number  several  times
larger  than  the  average  distance between points. Algorithm won't break
down if you choose radius which is too large (model construction time will
increase, but model will be built correctly).

Choose such number of layers that RLast=RBase/2^(NLayers-1)  (radius  used
by  the  last  layer)  will  be  smaller than the typical distance between
points.  In  case  model  error  is  too large, you can increase number of
layers.  Having  more  layers  will make model construction and evaluation
proportionally slower, but it will allow you to have model which precisely
fits your data. From the other side, if you want to  suppress  noise,  you
can DECREASE number of layers to make your model less flexible (or specify
non-zero LambdaNS).

TYPICAL ERRORS

1. Using too small number of layers - RBF models with large radius are not
   flexible enough to reproduce small variations in the  target  function.
   You  need  many  layers  with  different radii, from large to small, in
   order to have good model.

2. Using  initial  radius  which  is  too  small.  You will get model with
   &quot;holes&quot; in the areas which are too far away from interpolation centers.
   However, algorithm will work correctly (and quickly) in this case.
ALGLIB: Copyright 20.06.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetalgohierarchical(rbfmodel s, <b>double</b> rbase, ae_int_t nlayers, <b>double</b> lambdans);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_hrbf class=nav>rbf_d_hrbf</a> | <a href=#example_rbf_d_vector class=nav>rbf_d_vector</a> | <a href=#example_rbf_d_polterm class=nav>rbf_d_polterm</a> ]</p>
<a name=sub_rbfsetalgomultilayer></a><h6 class=pageheader>rbfsetalgomultilayer Function</h6>
<hr width=600 align=left>
<pre class=narration>
DEPRECATED:since version 3.11 ALGLIB includes new RBF  model  construction
           algorithm, Hierarchical  RBF.  This  algorithm  is  faster  and
           requires less memory than QNN and RBF-ML. It is especially good
           for large-scale interpolation problems. So, we recommend you to
           consider Hierarchical RBF as default option.

This  function  sets  RBF interpolation algorithm. ALGLIB supports several
RBF algorithms with different properties.

This  algorithm is called RBF-ML. It builds  multilayer  RBF  model,  i.e.
model with subsequently decreasing  radii,  which  allows  us  to  combine
smoothness (due to  large radii of  the first layers) with  exactness (due
to small radii of the last layers) and fast convergence.

Internally RBF-ML uses many different  means  of acceleration, from sparse
matrices  to  KD-trees,  which  results in algorithm whose working time is
roughly proportional to N*log(N)*Density*RBase^2*NLayers,  where  N  is  a
number of points, Density is an average density if points per unit of  the
interpolation space, RBase is an initial radius, NLayers is  a  number  of
layers.

RBF-ML is good for following kinds of interpolation problems:
1. &quot;exact&quot; problems (perfect fit) with well separated points
2. least squares problems with arbitrary distribution of points (algorithm
   gives  perfect  fit  where it is possible, and resorts to least squares
   fit in the hard areas).
3. noisy problems where  we  want  to  apply  some  controlled  amount  of
   smoothing.

Inputs:
    S       -   RBF model, initialized by RBFCreate() call
    RBase   -   RBase parameter, RBase &gt; 0
    NLayers -   NLayers parameter, NLayers &gt; 0, recommended value  to  start
                with - about 5.
    LambdaV -   regularization value, can be useful when  solving  problem
                in the least squares sense.  Optimal  lambda  is  problem-
                dependent and require trial and error. In our  experience,
                good lambda can be as large as 0.1, and you can use  0.001
                as initial guess.
                Default  value  - 0.01, which is used when LambdaV is  not
                given.  You  can  specify  zero  value,  but  it  is   not
                recommended to do so.

TUNING ALGORITHM

In order to use this algorithm you have to choose three parameters:
* initial radius RBase
* number of layers in the model NLayers
* regularization coefficient LambdaV

Initial radius is easy to choose - you can pick any number  several  times
larger  than  the  average  distance between points. Algorithm won't break
down if you choose radius which is too large (model construction time will
increase, but model will be built correctly).

Choose such number of layers that RLast=RBase/2^(NLayers-1)  (radius  used
by  the  last  layer)  will  be  smaller than the typical distance between
points.  In  case  model  error  is  too large, you can increase number of
layers.  Having  more  layers  will make model construction and evaluation
proportionally slower, but it will allow you to have model which precisely
fits your data. From the other side, if you want to  suppress  noise,  you
can DECREASE number of layers to make your model less flexible.

Regularization coefficient LambdaV controls smoothness of  the  individual
models built for each layer. We recommend you to use default value in case
you don't want to tune this parameter,  because  having  non-zero  LambdaV
accelerates and stabilizes internal iterative algorithm. In case you  want
to suppress noise you can use  LambdaV  as  additional  parameter  (larger
value = more smoothness) to tune.

TYPICAL ERRORS

1. Using  initial  radius  which is too large. Memory requirements  of the
   RBF-ML are roughly proportional to N*Density*RBase^2 (where Density  is
   an average density of points per unit of the interpolation  space).  In
   the extreme case of the very large RBase we will need O(N^2)  units  of
   memory - and many layers in order to decrease radius to some reasonably
   small value.

2. Using too small number of layers - RBF models with large radius are not
   flexible enough to reproduce small variations in the  target  function.
   You  need  many  layers  with  different radii, from large to small, in
   order to have good model.

3. Using  initial  radius  which  is  too  small.  You will get model with
   &quot;holes&quot; in the areas which are too far away from interpolation centers.
   However, algorithm will work correctly (and quickly) in this case.

4. Using too many layers - you will get too large and too slow model. This
   model  will  perfectly  reproduce  your function, but maybe you will be
   able to achieve similar results with less layers (and less memory).
ALGLIB: Copyright 02.03.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetalgomultilayer(rbfmodel s, <b>double</b> rbase, ae_int_t nlayers, <b>double</b> lambdav);
<b>void</b> rbfsetalgomultilayer(rbfmodel s, <b>double</b> rbase, ae_int_t nlayers);
</pre>
<a name=sub_rbfsetalgoqnn></a><h6 class=pageheader>rbfsetalgoqnn Function</h6>
<hr width=600 align=left>
<pre class=narration>
DEPRECATED:since version 3.11 ALGLIB includes new RBF  model  construction
           algorithm, Hierarchical  RBF.  This  algorithm  is  faster  and
           requires less memory than QNN and RBF-ML. It is especially good
           for large-scale interpolation problems. So, we recommend you to
           consider Hierarchical RBF as default option.

This  function  sets  RBF interpolation algorithm. ALGLIB supports several
RBF algorithms with different properties.

This algorithm is called RBF-QNN and  it  is  good  for  point  sets  with
following properties:
a) all points are distinct
b) all points are well separated.
c) points  distribution  is  approximately  uniform.  There is no &quot;contour
   lines&quot;, clusters of points, or other small-scale structures.

Algorithm description:
1) interpolation centers are allocated to data points
2) interpolation radii are calculated as distances to the  nearest centers
   times Q coefficient (where Q is a value from [0.75,1.50]).
3) after  performing (2) radii are transformed in order to avoid situation
   when single outlier has very large radius and  influences  many  points
   across all dataset. Transformation has following form:
       new_r[i] = min(r[i],Z*median(r[]))
   where r[i] is I-th radius, median()  is a median  radius across  entire
   dataset, Z is user-specified value which controls amount  of  deviation
   from median radius.

When (a) is violated,  we  will  be unable to build RBF model. When (b) or
(c) are violated, model will be built, but interpolation quality  will  be
low. See http://www.alglib.net/interpolation/ for more information on this
subject.

This algorithm is used by default.

Additional Q parameter controls smoothness properties of the RBF basis:
* Q &lt; 0.75 will give perfectly conditioned basis,  but  terrible  smoothness
  properties (RBF interpolant will have sharp peaks around function values)
* Q around 1.0 gives good balance between smoothness and condition number
* Q &gt; 1.5 will lead to badly conditioned systems and slow convergence of the
  underlying linear solver (although smoothness will be very good)
* Q &gt; 2.0 will effectively make optimizer useless because it won't  converge
  within reasonable amount of iterations. It is possible to set such large
  Q, but it is advised not to do so.

Inputs:
    S       -   RBF model, initialized by RBFCreate() call
    Q       -   Q parameter, Q &gt; 0, recommended value - 1.0
    Z       -   Z parameter, Z &gt; 0, recommended value - 5.0

NOTE: this   function  has   some   serialization-related  subtleties.  We
      recommend you to study serialization examples from ALGLIB  Reference
      Manual if you want to perform serialization of your models.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetalgoqnn(rbfmodel s, <b>double</b> q, <b>double</b> z);
<b>void</b> rbfsetalgoqnn(rbfmodel s);
</pre>
<a name=sub_rbfsetconstterm></a><h6 class=pageheader>rbfsetconstterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets constant term (model is a sum of radial basis functions
plus constant).  This  function  won't  have  effect  until  next  call to
RBFBuildModel().

Inputs:
    S       -   RBF model, initialized by RBFCreate() call

NOTE: this   function  has   some   serialization-related  subtleties.  We
      recommend you to study serialization examples from ALGLIB  Reference
      Manual if you want to perform serialization of your models.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetconstterm(rbfmodel s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_polterm class=nav>rbf_d_polterm</a> ]</p>
<a name=sub_rbfsetlinterm></a><h6 class=pageheader>rbfsetlinterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets linear term (model is a sum of radial  basis  functions
plus linear polynomial). This function won't have effect until  next  call
to RBFBuildModel().

Inputs:
    S       -   RBF model, initialized by RBFCreate() call

NOTE: this   function  has   some   serialization-related  subtleties.  We
      recommend you to study serialization examples from ALGLIB  Reference
      Manual if you want to perform serialization of your models.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetlinterm(rbfmodel s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_polterm class=nav>rbf_d_polterm</a> ]</p>
<a name=sub_rbfsetpoints></a><h6 class=pageheader>rbfsetpoints Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function adds dataset.

This function overrides results of the previous calls, i.e. multiple calls
of this function will result in only the last set being added.

IMPORTANT: ALGLIB version 3.11 and later allows you to specify  a  set  of
           per-dimension scales. Interpolation radii are multiplied by the
           scale vector. It may be useful if you have mixed spatio-temporal
           data (say, a set of 3D slices recorded at different times).
           You should call rbfsetpointsandscales() function  to  use  this
           feature.

Inputs:
    S       -   RBF model, initialized by rbfcreate() call.
    XY      -   points, array[N,NX+NY]. One row corresponds to  one  point
                in the dataset. First NX elements  are  coordinates,  next
                NY elements are function values. Array may  be larger than
                specified, in  this  case  only leading [N,NX+NY] elements
                will be used.
    N       -   number of points in the dataset

After you've added dataset and (optionally) tuned algorithm  settings  you
should call rbfbuildmodel() in order to build a model for you.

NOTE: dataset added by this function is not saved during model serialization.
      MODEL ITSELF is serialized, but data used to build it are not.

      So, if you 1) add dataset to  empty  RBF  model,  2)  serialize  and
      unserialize it, then you will get an empty RBF model with no dataset
      being attached.

      From the other side, if you call rbfbuildmodel() between (1) and (2),
      then after (2) you will get your fully constructed RBF model  -  but
      again with no dataset attached, so subsequent calls to rbfbuildmodel()
      will produce empty model.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetpoints(rbfmodel s, real_2d_array xy, ae_int_t n);
<b>void</b> rbfsetpoints(rbfmodel s, real_2d_array xy);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_hrbf class=nav>rbf_d_hrbf</a> | <a href=#example_rbf_d_vector class=nav>rbf_d_vector</a> | <a href=#example_rbf_d_polterm class=nav>rbf_d_polterm</a> ]</p>
<a name=sub_rbfsetpointsandscales></a><h6 class=pageheader>rbfsetpointsandscales Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function adds dataset and a vector of per-dimension scales.

It may be useful if you have mixed spatio-temporal data - say, a set of 3D
slices recorded at different times. Such data typically require  different
RBF radii for spatial and temporal dimensions. ALGLIB solves this  problem
by specifying single RBF radius, which is (optionally) multiplied  by  the
scale vector.

This function overrides results of the previous calls, i.e. multiple calls
of this function will result in only the last set being added.

IMPORTANT: only HierarchicalRBF algorithm can work with scaled points. So,
           using this function results in RBF models which can be used  in
           ALGLIB 3.11 or later. Previous versions of the library will  be
           unable  to unserialize models produced by HierarchicalRBF algo.

           Any attempt to use this function with RBF-ML or QNN  algorithms
           will result  in  -3  error  code   being   returned  (incorrect
           algorithm).

Inputs:
    R       -   RBF model, initialized by rbfcreate() call.
    XY      -   points, array[N,NX+NY]. One row corresponds to  one  point
                in the dataset. First NX elements  are  coordinates,  next
                NY elements are function values. Array may  be larger than
                specified, in  this  case  only leading [N,NX+NY] elements
                will be used.
    N       -   number of points in the dataset
    S       -   array[NX], scale vector, S[i] &gt; 0.

After you've added dataset and (optionally) tuned algorithm  settings  you
should call rbfbuildmodel() in order to build a model for you.

NOTE: dataset added by this function is not saved during model serialization.
      MODEL ITSELF is serialized, but data used to build it are not.

      So, if you 1) add dataset to  empty  RBF  model,  2)  serialize  and
      unserialize it, then you will get an empty RBF model with no dataset
      being attached.

      From the other side, if you call rbfbuildmodel() between (1) and (2),
      then after (2) you will get your fully constructed RBF model  -  but
      again with no dataset attached, so subsequent calls to rbfbuildmodel()
      will produce empty model.
ALGLIB: Copyright 20.06.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetpointsandscales(rbfmodel r, real_2d_array xy, ae_int_t n, real_1d_array s);
<b>void</b> rbfsetpointsandscales(rbfmodel r, real_2d_array xy, real_1d_array s);
</pre>
<a name=sub_rbfsetv2bf></a><h6 class=pageheader>rbfsetv2bf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets basis function type, which can be:
* 0 for classic Gaussian
* 1 for fast and compact bell-like basis function, which  becomes  exactly
  zero at distance equal to 3*R (default option).

Inputs:
    S       -   RBF model, initialized by RBFCreate() call
    BF      -   basis function type:
                * 0 - classic Gaussian
                * 1 - fast and compact one
ALGLIB: Copyright 01.02.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetv2bf(rbfmodel s, ae_int_t bf);
</pre>
<a name=sub_rbfsetv2its></a><h6 class=pageheader>rbfsetv2its Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping criteria of the underlying linear  solver  for
hierarchical (version 2) RBF constructor.

Inputs:
    S       -   RBF model, initialized by RBFCreate() call
    MaxIts  -   this criterion will stop algorithm after MaxIts iterations.
                Typically a few hundreds iterations is required,  with 400
                being a good default value to start experimentation.
                Zero value means that default value will be selected.
ALGLIB: Copyright 01.02.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetv2its(rbfmodel s, ae_int_t maxits);
</pre>
<a name=sub_rbfsetv2supportr></a><h6 class=pageheader>rbfsetv2supportr Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets support radius parameter  of  hierarchical  (version 2)
RBF constructor.

Hierarchical RBF model achieves great speed-up  by removing from the model
excessive (too dense) nodes. Say, if you have RBF radius equal to 1 meter,
and two nodes are just 1 millimeter apart, you  may  remove  one  of  them
without reducing model quality.

Support radius parameter is used to justify which points need removal, and
which do not. If two points are less than  SUPPORT_R*CUR_RADIUS  units  of
distance apart, one of them is removed from the model. The larger  support
radius  is, the faster model  construction  AND  evaluation are.  However,
too large values result in &quot;bumpy&quot; models.

Inputs:
    S       -   RBF model, initialized by RBFCreate() call
    R       -   support radius coefficient, &ge; 0.
                Recommended values are [0.1,0.4] range, with 0.1 being
                default value.
ALGLIB: Copyright 01.02.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetv2supportr(rbfmodel s, <b>double</b> r);
</pre>
<a name=sub_rbfsetzeroterm></a><h6 class=pageheader>rbfsetzeroterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  zero  term (model is a sum of radial basis functions
without polynomial term). This function won't have effect until next  call
to RBFBuildModel().

Inputs:
    S       -   RBF model, initialized by RBFCreate() call

NOTE: this   function  has   some   serialization-related  subtleties.  We
      recommend you to study serialization examples from ALGLIB  Reference
      Manual if you want to perform serialization of your models.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfsetzeroterm(rbfmodel s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_polterm class=nav>rbf_d_polterm</a> ]</p>
<a name=sub_rbftscalcbuf></a><h6 class=pageheader>rbftscalcbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates values of the RBF model at the given point, using
external  buffer  object  (internal  temporaries  of  RBF  model  are  not
modified).

This function allows to use same RBF model object  in  different  threads,
assuming  that  different   threads  use  different  instances  of  buffer
structure.

Inputs:
    S       -   RBF model, may be shared between different threads
    Buf     -   buffer object created for this particular instance of  RBF
                model with rbfcreatecalcbuffer().
    X       -   coordinates, array[NX].
                X may have more than NX elements, in this case only
                leading NX will be used.
    Y       -   possibly preallocated array

Outputs:
    Y       -   function value, array[NY]. Y is not reallocated when it
                is larger than NY.
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbftscalcbuf(rbfmodel s, rbfcalcbuffer buf, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_rbfunpack></a><h6 class=pageheader>rbfunpack Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function &quot;unpacks&quot; RBF model by extracting its coefficients.

Inputs:
    S       -   RBF model

Outputs:
    NX      -   dimensionality of argument
    NY      -   dimensionality of the target function
    XWR     -   model information, array[NC,NX+NY+1].
                One row of the array corresponds to one basis function:
                * first NX columns  - coordinates of the center
                * next NY columns   - weights, one per dimension of the
                                      function being modelled
                For ModelVersion=1:
                * last column       - radius, same for all dimensions of
                                      the function being modelled
                For ModelVersion=2:
                * last NX columns   - radii, one per dimension
    NC      -   number of the centers
    V       -   polynomial  term , array[NY,NX+1]. One row per one
                dimension of the function being modelled. First NX
                elements are linear coefficients, V[NX] is equal to the
                constant part.
    ModelVersion-version of the RBF model:
                * 1 - for models created by QNN and RBF-ML algorithms,
                  compatible with ALGLIB 3.10 or earlier.
                * 2 - for models created by HierarchicalRBF, requires
                  ALGLIB 3.11 or later
ALGLIB: Copyright 13.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfunpack(rbfmodel s, ae_int_t &amp;nx, ae_int_t &amp;ny, real_2d_array &amp;xwr, ae_int_t &amp;nc, real_2d_array &amp;v, ae_int_t &amp;modelversion);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_rbf_d_polterm class=nav>rbf_d_polterm</a> ]</p>
<a name=sub_rbfunserialize></a><h6 class=pageheader>rbfunserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: unserialization
These functions unserialize a data structure from a C++ string or stream.
Important properties of s_in:
* any combination of spaces, tabs, Windows or Unix stype newlines can
  be used as separators, so as to allow flexible reformatting of the
  stream or string from text or XML files.
* But you should not insert separators into the middle of the "words"
  nor you should change case of letters.
ALGLIB: Copyright 02.02.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rbfunserialize(<b>const</b> std::string &amp;s_in, rbfmodel &amp;obj);
<b>void</b> rbfunserialize(<b>const</b> std::istream &amp;s_in, rbfmodel &amp;obj);
</pre>
<a name=example_rbf_d_hrbf></a><h6 class=pageheader>rbf_d_hrbf Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example illustrates basic concepts of the RBF models: creation, modification,</font>
<font color=navy>// evaluation.</font>
<font color=navy>// </font>
<font color=navy>// Suppose that we have set of 2-dimensional points with associated</font>
<font color=navy>// scalar function values, and we want to build a RBF model using</font>
<font color=navy>// our data.</font>
<font color=navy>// </font>
<font color=navy>// NOTE: we can work with 3D models too :)</font>
<font color=navy>// </font>
<font color=navy>// Typical sequence of steps is given below:</font>
<font color=navy>// 1. we create RBF model object</font>
<font color=navy>// 2. we attach our dataset to the RBF model and tune algorithm settings</font>
<font color=navy>// 3. we rebuild RBF model using QNN algorithm on new data</font>
<font color=navy>// 4. we use RBF model (evaluate, serialize, etc.)</font>
   <b>double</b> v;
<font color=navy>// Step 1: RBF model creation.</font>
<font color=navy>//</font>
<font color=navy>// We have to specify dimensionality of the space (2 or 3) and</font>
<font color=navy>// dimensionality of the function (scalar or vector).</font>
<font color=navy>//</font>
<font color=navy>// New model is empty - it can be evaluated,</font>
<font color=navy>// but we just get zero value at any point.</font>
   rbfmodel model;
   rbfcreate(2, 1, model);

   v = rbfcalc2(model, 0.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.000</font>
<font color=navy>//</font>
<font color=navy>// Step 2: we add dataset.</font>
<font color=navy>//</font>
<font color=navy>// XY contains two points - x0=(-1,0) and x1=(+1,0) -</font>
<font color=navy>// and two function values f(x0)=2, f(x1)=3.</font>
<font color=navy>//</font>
<font color=navy>// We added points, but model was not rebuild yet.</font>
<font color=navy>// If we call rbfcalc2(), we still will get 0.0 as result.</font>
   real_2d_array xy = <font color=blue><b>&quot;[[-1,0,2],[+1,0,3]]&quot;</b></font>;
   rbfsetpoints(model, xy);

   v = rbfcalc2(model, 0.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.000</font>
<font color=navy>//</font>
<font color=navy>// Step 3: rebuild model</font>
<font color=navy>//</font>
<font color=navy>// After we've configured model, we should rebuild it -</font>
<font color=navy>// it will change coefficients stored internally in the</font>
<font color=navy>// rbfmodel structure.</font>
<font color=navy>//</font>
<font color=navy>// We use hierarchical RBF algorithm with following parameters:</font>
<font color=navy>// * RBase - set to 1.0</font>
<font color=navy>// * NLayers - three layers are used (although such simple problem</font>
<font color=navy>//   does not need more than 1 layer)</font>
<font color=navy>// * LambdaReg - is set to zero value, no smoothing is required</font>
   rbfreport rep;
   rbfsetalgohierarchical(model, 1.0, 3, 0.0);
   rbfbuildmodel(model, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 1</font>
<font color=navy>//</font>
<font color=navy>// Step 4: model was built</font>
<font color=navy>//</font>
<font color=navy>// After call of rbfbuildmodel(), rbfcalc2() will <b>return</b></font>
<font color=navy>// value of the new model.</font>
   v = rbfcalc2(model, 0.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.500</font>
   <b>return</b> 0;
}
</pre>
<a name=example_rbf_d_polterm></a><h6 class=pageheader>rbf_d_polterm Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example show how to work with polynomial term</font>
<font color=navy>// </font>
<font color=navy>// Suppose that we have set of 2-dimensional points with associated</font>
<font color=navy>// scalar function values, and we want to build a RBF model using</font>
<font color=navy>// our data.</font>
<font color=navy>//</font>
<font color=navy>// We use hierarchical RBF algorithm with following parameters:</font>
<font color=navy>// * RBase - set to 1.0</font>
<font color=navy>// * NLayers - three layers are used (although such simple problem</font>
<font color=navy>//   does not need more than 1 layer)</font>
<font color=navy>// * LambdaReg - is set to zero value, no smoothing is required</font>
   <b>double</b> v;
   rbfmodel model;
   real_2d_array xy = <font color=blue><b>&quot;[[-1,0,2],[+1,0,3]]&quot;</b></font>;
   rbfreport rep;

   rbfcreate(2, 1, model);
   rbfsetpoints(model, xy);
   rbfsetalgohierarchical(model, 1.0, 3, 0.0);
<font color=navy>// By default, RBF model uses linear term. It means that model</font>
<font color=navy>// looks like</font>
<font color=navy>//     f(x,y) = SUM(RBF[i]) + a*x + b*y + c</font>
<font color=navy>// where RBF[i] is I-th radial basis function and a*x+by+c is a</font>
<font color=navy>// linear term. Having linear terms in a model gives us:</font>
<font color=navy>// (1) improved extrapolation properties</font>
<font color=navy>// (2) linearity of the model when data can be perfectly fitted</font>
<font color=navy>//     by the linear function</font>
<font color=navy>// (3) linear asymptotic behavior</font>
<font color=navy>//</font>
<font color=navy>// Our simple dataset can be modelled by the linear function</font>
<font color=navy>//     f(x,y) = 0.5*x + 2.5</font>
<font color=navy>// and rbfbuildmodel() with default settings should preserve this</font>
<font color=navy>// linearity.</font>
   ae_int_t nx;
   ae_int_t ny;
   ae_int_t nc;
   ae_int_t modelversion;
   real_2d_array xwr;
   real_2d_array c;
   rbfbuildmodel(model, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 1</font>
   rbfunpack(model, nx, ny, xwr, nc, c, modelversion);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(2).c_str()); <font color=navy>// EXPECTED: [[0.500,0.000,2.500]]</font>

<font color=navy>// asymptotic behavior of our function is linear</font>
   v = rbfcalc2(model, 1000.0, 0.0);
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 502.50</font>
<font color=navy>//</font>
<font color=navy>// Instead of linear term we can use constant term. In this case</font>
<font color=navy>// we will get model which has form</font>
<font color=navy>//     f(x,y) = SUM(RBF[i]) + c</font>
<font color=navy>// where RBF[i] is I-th radial basis function and c is a constant,</font>
<font color=navy>// which is equal to the average function value on the dataset.</font>
<font color=navy>//</font>
<font color=navy>// Because we've already attached dataset to the model the only</font>
<font color=navy>// thing we have to <b>do</b> is to call rbfsetconstterm() and then</font>
<font color=navy>// rebuild model with rbfbuildmodel().</font>
   rbfsetconstterm(model);
   rbfbuildmodel(model, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 1</font>
   rbfunpack(model, nx, ny, xwr, nc, c, modelversion);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(2).c_str()); <font color=navy>// EXPECTED: [[0.000,0.000,2.500]]</font>

<font color=navy>// asymptotic behavior of our function is constant</font>
   v = rbfcalc2(model, 1000.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.500</font>
<font color=navy>//</font>
<font color=navy>// Finally, we can use zero term. Just plain RBF without polynomial</font>
<font color=navy>// part:</font>
<font color=navy>//     f(x,y) = SUM(RBF[i])</font>
<font color=navy>// where RBF[i] is I-th radial basis function.</font>
   rbfsetzeroterm(model);
   rbfbuildmodel(model, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 1</font>
   rbfunpack(model, nx, ny, xwr, nc, c, modelversion);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(2).c_str()); <font color=navy>// EXPECTED: [[0.000,0.000,0.000]]</font>

<font color=navy>// asymptotic behavior of our function is just zero constant</font>
   v = rbfcalc2(model, 1000.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.000</font>
   <b>return</b> 0;
}
</pre>
<a name=example_rbf_d_serialize></a><h6 class=pageheader>rbf_d_serialize Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example show how to serialize and unserialize RBF model</font>
<font color=navy>// </font>
<font color=navy>// Suppose that we have set of 2-dimensional points with associated</font>
<font color=navy>// scalar function values, and we want to build a RBF model using</font>
<font color=navy>// our data. Then we want to serialize it to string and to unserialize</font>
<font color=navy>// from string, loading to another instance of RBF model.</font>
<font color=navy>//</font>
<font color=navy>// Here we assume that you already know how to create RBF models.</font>
   std::string s;
   <b>double</b> v;
   rbfmodel model0;
   rbfmodel model1;
   real_2d_array xy = <font color=blue><b>&quot;[[-1,0,2],[+1,0,3]]&quot;</b></font>;
   rbfreport rep;

<font color=navy>// model initialization</font>
   rbfcreate(2, 1, model0);
   rbfsetpoints(model0, xy);
   rbfsetalgohierarchical(model0, 1.0, 3, 0.0);
   rbfbuildmodel(model0, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 1</font>
<font color=navy>//</font>
<font color=navy>// Serialization - it looks easy,</font>
<font color=navy>// but you should carefully read next section.</font>
   rbfserialize(model0, s);
   rbfunserialize(s, model1);

<font color=navy>// both models <b>return</b> same value</font>
   v = rbfcalc2(model0, 0.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.500</font>
   v = rbfcalc2(model1, 0.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.500</font>
<font color=navy>//</font>
<font color=navy>// Previous section shows that model state is saved/restored during</font>
<font color=navy>// serialization. However, some properties are NOT serialized.</font>
<font color=navy>//</font>
<font color=navy>// Serialization saves/restores RBF model, but it does NOT saves/restores</font>
<font color=navy>// settings which were used to build current model. In particular, dataset</font>
<font color=navy>// which was used to build model, is not preserved.</font>
<font color=navy>//</font>
<font color=navy>// What does it mean in <b>for</b> us?</font>
<font color=navy>//</font>
<font color=navy>// Do you remember this sequence: rbfcreate-rbfsetpoints-rbfbuildmodel?</font>
<font color=navy>// First step creates model, second step adds dataset and tunes model</font>
<font color=navy>// settings, third step builds model using current dataset and model</font>
<font color=navy>// construction settings.</font>
<font color=navy>//</font>
<font color=navy>// If you call rbfbuildmodel() without calling rbfsetpoints() first, you</font>
<font color=navy>// will get empty (zero) RBF model. In our example, model0 contains</font>
<font color=navy>// dataset which was added by rbfsetpoints() call. However, model1 does</font>
<font color=navy>// NOT contain dataset - because dataset is NOT serialized.</font>
<font color=navy>//</font>
<font color=navy>// This, <b>if</b> we call rbfbuildmodel(model0,rep), we will get same model,</font>
<font color=navy>// which returns 2.5 at (x,y)=(0,0). However, after same call model1 will</font>
<font color=navy>// <b>return</b> zero - because it contains RBF model (coefficients), but does NOT</font>
<font color=navy>// contain dataset which was used to build this model.</font>
<font color=navy>//</font>
<font color=navy>// Basically, it means that:</font>
<font color=navy>// * serialization of the RBF model preserves anything related to the model</font>
<font color=navy>//   EVALUATION</font>
<font color=navy>// * but it does NOT creates perfect copy of the original object.</font>
   rbfbuildmodel(model0, rep);
   v = rbfcalc2(model0, 0.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.500</font>

   rbfbuildmodel(model1, rep);
   v = rbfcalc2(model1, 0.0, 0.0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.000</font>
   <b>return</b> 0;
}
</pre>
<a name=example_rbf_d_vector></a><h6 class=pageheader>rbf_d_vector Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Suppose that we have set of 2-dimensional points with associated VECTOR</font>
<font color=navy>// function values, and we want to build a RBF model using our data.</font>
<font color=navy>// </font>
<font color=navy>// Typical sequence of steps is given below:</font>
<font color=navy>// 1. we create RBF model object</font>
<font color=navy>// 2. we attach our dataset to the RBF model and tune algorithm settings</font>
<font color=navy>// 3. we rebuild RBF model using new data</font>
<font color=navy>// 4. we use RBF model (evaluate, serialize, etc.)</font>
   real_1d_array x;
   real_1d_array y;
<font color=navy>// Step 1: RBF model creation.</font>
<font color=navy>//</font>
<font color=navy>// We have to specify dimensionality of the space (equal to 2) and</font>
<font color=navy>// dimensionality of the function (2-dimensional vector function).</font>
<font color=navy>//</font>
<font color=navy>// New model is empty - it can be evaluated,</font>
<font color=navy>// but we just get zero value at any point.</font>
   rbfmodel model;
   rbfcreate(2, 2, model);

   x = <font color=blue><b>&quot;[+1,+1]&quot;</b></font>;
   rbfcalc(model, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(2).c_str()); <font color=navy>// EXPECTED: [0.000,0.000]</font>
<font color=navy>//</font>
<font color=navy>// Step 2: we add dataset.</font>
<font color=navy>//</font>
<font color=navy>// XY arrays containt four points:</font>
<font color=navy>// * (x0,y0) = (+1,+1), f(x0,y0)=(0,-1)</font>
<font color=navy>// * (x1,y1) = (+1,-1), f(x1,y1)=(-1,0)</font>
<font color=navy>// * (x2,y2) = (-1,-1), f(x2,y2)=(0,+1)</font>
<font color=navy>// * (x3,y3) = (-1,+1), f(x3,y3)=(+1,0)</font>
   real_2d_array xy = <font color=blue><b>&quot;[[+1,+1,0,-1],[+1,-1,-1,0],[-1,-1,0,+1],[-1,+1,+1,0]]&quot;</b></font>;
   rbfsetpoints(model, xy);

<font color=navy>// We added points, but model was not rebuild yet.</font>
<font color=navy>// If we call rbfcalc(), we still will get 0.0 as result.</font>
   rbfcalc(model, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(2).c_str()); <font color=navy>// EXPECTED: [0.000,0.000]</font>
<font color=navy>//</font>
<font color=navy>// Step 3: rebuild model</font>
<font color=navy>//</font>
<font color=navy>// We use hierarchical RBF algorithm with following parameters:</font>
<font color=navy>// * RBase - set to 1.0</font>
<font color=navy>// * NLayers - three layers are used (although such simple problem</font>
<font color=navy>//   does not need more than 1 layer)</font>
<font color=navy>// * LambdaReg - is set to zero value, no smoothing is required</font>
<font color=navy>//</font>
<font color=navy>// After we've configured model, we should rebuild it -</font>
<font color=navy>// it will change coefficients stored internally in the</font>
<font color=navy>// rbfmodel structure.</font>
   rbfreport rep;
   rbfsetalgohierarchical(model, 1.0, 3, 0.0);
   rbfbuildmodel(model, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 1</font>
<font color=navy>//</font>
<font color=navy>// Step 4: model was built</font>
<font color=navy>//</font>
<font color=navy>// After call of rbfbuildmodel(), rbfcalc() will <b>return</b></font>
<font color=navy>// value of the new model.</font>
   rbfcalc(model, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(2).c_str()); <font color=navy>// EXPECTED: [0.000,-1.000]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_spline1d></a><h4 class=pageheader>8.6.9. spline1d Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_spline1dfitreport class=toc>spline1dfitreport</a> |
<a href=#struct_spline1dinterpolant class=toc>spline1dinterpolant</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_spline1dbuildakima class=toc>spline1dbuildakima</a> |
<a href=#sub_spline1dbuildcatmullrom class=toc>spline1dbuildcatmullrom</a> |
<a href=#sub_spline1dbuildcubic class=toc>spline1dbuildcubic</a> |
<a href=#sub_spline1dbuildhermite class=toc>spline1dbuildhermite</a> |
<a href=#sub_spline1dbuildlinear class=toc>spline1dbuildlinear</a> |
<a href=#sub_spline1dbuildmonotone class=toc>spline1dbuildmonotone</a> |
<a href=#sub_spline1dcalc class=toc>spline1dcalc</a> |
<a href=#sub_spline1dconvcubic class=toc>spline1dconvcubic</a> |
<a href=#sub_spline1dconvdiff2cubic class=toc>spline1dconvdiff2cubic</a> |
<a href=#sub_spline1dconvdiffcubic class=toc>spline1dconvdiffcubic</a> |
<a href=#sub_spline1ddiff class=toc>spline1ddiff</a> |
<a href=#sub_spline1dfit class=toc>spline1dfit</a> |
<a href=#sub_spline1dgriddiff2cubic class=toc>spline1dgriddiff2cubic</a> |
<a href=#sub_spline1dgriddiffcubic class=toc>spline1dgriddiffcubic</a> |
<a href=#sub_spline1dintegrate class=toc>spline1dintegrate</a> |
<a href=#sub_spline1dlintransx class=toc>spline1dlintransx</a> |
<a href=#sub_spline1dlintransy class=toc>spline1dlintransy</a> |
<a href=#sub_spline1dunpack class=toc>spline1dunpack</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_spline1d_d_convdiff class=toc>spline1d_d_convdiff</a></td><td width=15>&nbsp;</td><td>Resampling using cubic splines</td></tr>
<tr align=left valign=top><td><a href=#example_spline1d_d_cubic class=toc>spline1d_d_cubic</a></td><td width=15>&nbsp;</td><td>Cubic spline interpolation</td></tr>
<tr align=left valign=top><td><a href=#example_spline1d_d_griddiff class=toc>spline1d_d_griddiff</a></td><td width=15>&nbsp;</td><td>Differentiation on the grid using cubic splines</td></tr>
<tr align=left valign=top><td><a href=#example_spline1d_d_linear class=toc>spline1d_d_linear</a></td><td width=15>&nbsp;</td><td>Piecewise linear spline interpolation</td></tr>
<tr align=left valign=top><td><a href=#example_spline1d_d_monotone class=toc>spline1d_d_monotone</a></td><td width=15>&nbsp;</td><td>Monotone interpolation</td></tr>
</table>
</div>
<a name=struct_spline1dfitreport></a><h6 class=pageheader>spline1dfitreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Spline fitting report:
    RMSError        RMS error
    AvgError        average error
    AvgRelError     average relative error (for non-zero Y[I])
    MaxError        maximum error

Fields  below are  filled  by   obsolete    functions   (Spline1DFitCubic,
Spline1DFitHermite). Modern fitting functions do NOT fill these fields:
    TaskRCond       reciprocal of task's condition number
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> spline1dfitreport {
   <b>double</b> taskrcond;
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> avgrelerror;
   <b>double</b> maxerror;
};
</pre>
<a name=struct_spline1dinterpolant></a><h6 class=pageheader>spline1dinterpolant Class</h6>
<hr width=600 align=left>
<pre class=narration>
1-dimensional spline interpolant
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> spline1dinterpolant {
};
</pre>
<a name=sub_spline1dbuildakima></a><h6 class=pageheader>spline1dbuildakima Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds Akima spline interpolant

Inputs:
    X           -   spline nodes, array[0..N-1]
    Y           -   function values, array[0..N-1]
    N           -   points count (optional):
                    * N &ge; 2
                    * if given, only first N points are used to build spline
                    * if not given, automatically detected from X/Y sizes
                      (len(X) must be equal to len(Y))

Outputs:
    C           -   spline interpolant

ORDER OF POINTS

Subroutine automatically sorts points, so caller may pass unsorted array.
ALGLIB Project: Copyright 24.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dbuildakima(real_1d_array x, real_1d_array y, ae_int_t n, spline1dinterpolant &amp;c);
<b>void</b> spline1dbuildakima(real_1d_array x, real_1d_array y, spline1dinterpolant &amp;c);
</pre>
<a name=sub_spline1dbuildcatmullrom></a><h6 class=pageheader>spline1dbuildcatmullrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds Catmull-Rom spline interpolant.

Inputs:
    X           -   spline nodes, array[0..N-1].
    Y           -   function values, array[0..N-1].

OPTIONAL PARAMETERS:
    N           -   points count:
                    * N &ge; 2
                    * if given, only first N points are used to build spline
                    * if not given, automatically detected from X/Y sizes
                      (len(X) must be equal to len(Y))
    BoundType   -   boundary condition type:
                    * -1 for periodic boundary condition
                    *  0 for parabolically terminated spline (default)
    Tension     -   tension parameter:
                    * tension=0   corresponds to classic Catmull-Rom spline (default)
                    * 0 &lt; tension &lt; 1 corresponds to more general form - cardinal spline

Outputs:
    C           -   spline interpolant

ORDER OF POINTS

Subroutine automatically sorts points, so caller may pass unsorted array.

PROBLEMS WITH PERIODIC BOUNDARY CONDITIONS:

Problems with periodic boundary conditions have Y[first_point]=Y[last_point].
However, this subroutine doesn't require you to specify equal  values  for
the first and last points - it automatically forces them  to  be  equal by
copying  Y[first_point]  (corresponds  to the leftmost,  minimal  X[])  to
Y[last_point]. However it is recommended to pass consistent values of Y[],
i.e. to make Y[first_point]=Y[last_point].
ALGLIB Project: Copyright 23.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dbuildcatmullrom(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t boundtype, <b>double</b> tension, spline1dinterpolant &amp;c);
<b>void</b> spline1dbuildcatmullrom(real_1d_array x, real_1d_array y, spline1dinterpolant &amp;c);
</pre>
<a name=sub_spline1dbuildcubic></a><h6 class=pageheader>spline1dbuildcubic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds cubic spline interpolant.

Inputs:
    X           -   spline nodes, array[0..N-1].
    Y           -   function values, array[0..N-1].

OPTIONAL PARAMETERS:
    N           -   points count:
                    * N &ge; 2
                    * if given, only first N points are used to build spline
                    * if not given, automatically detected from X/Y sizes
                      (len(X) must be equal to len(Y))
    BoundLType  -   boundary condition type for the left boundary
    BoundL      -   left boundary condition (first or second derivative,
                    depending on the BoundLType)
    BoundRType  -   boundary condition type for the right boundary
    BoundR      -   right boundary condition (first or second derivative,
                    depending on the BoundRType)

Outputs:
    C           -   spline interpolant

ORDER OF POINTS

Subroutine automatically sorts points, so caller may pass unsorted array.

SETTING BOUNDARY VALUES:

The BoundLType/BoundRType parameters can have the following values:
    * -1, which corresonds to the periodic (cyclic) boundary conditions.
          In this case:
          * both BoundLType and BoundRType must be equal to -1.
          * BoundL/BoundR are ignored
          * Y[last] is ignored (it is assumed to be equal to Y[first]).
    *  0, which  corresponds  to  the  parabolically   terminated  spline
          (BoundL and/or BoundR are ignored).
    *  1, which corresponds to the first derivative boundary condition
    *  2, which corresponds to the second derivative boundary condition
    *  by default, BoundType=0 is used

PROBLEMS WITH PERIODIC BOUNDARY CONDITIONS:

Problems with periodic boundary conditions have Y[first_point]=Y[last_point].
However, this subroutine doesn't require you to specify equal  values  for
the first and last points - it automatically forces them  to  be  equal by
copying  Y[first_point]  (corresponds  to the leftmost,  minimal  X[])  to
Y[last_point]. However it is recommended to pass consistent values of Y[],
i.e. to make Y[first_point]=Y[last_point].
ALGLIB Project: Copyright 23.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dbuildcubic(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t boundltype, <b>double</b> boundl, ae_int_t boundrtype, <b>double</b> boundr, spline1dinterpolant &amp;c);
<b>void</b> spline1dbuildcubic(real_1d_array x, real_1d_array y, spline1dinterpolant &amp;c);
</pre>
<a name=sub_spline1dbuildhermite></a><h6 class=pageheader>spline1dbuildhermite Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds Hermite spline interpolant.

Inputs:
    X           -   spline nodes, array[0..N-1]
    Y           -   function values, array[0..N-1]
    D           -   derivatives, array[0..N-1]
    N           -   points count (optional):
                    * N &ge; 2
                    * if given, only first N points are used to build spline
                    * if not given, automatically detected from X/Y sizes
                      (len(X) must be equal to len(Y))

Outputs:
    C           -   spline interpolant.

ORDER OF POINTS

Subroutine automatically sorts points, so caller may pass unsorted array.
ALGLIB Project: Copyright 23.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dbuildhermite(real_1d_array x, real_1d_array y, real_1d_array d, ae_int_t n, spline1dinterpolant &amp;c);
<b>void</b> spline1dbuildhermite(real_1d_array x, real_1d_array y, real_1d_array d, spline1dinterpolant &amp;c);
</pre>
<a name=sub_spline1dbuildlinear></a><h6 class=pageheader>spline1dbuildlinear Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds linear spline interpolant

Inputs:
    X   -   spline nodes, array[0..N-1]
    Y   -   function values, array[0..N-1]
    N   -   points count (optional):
            * N &ge; 2
            * if given, only first N points are used to build spline
            * if not given, automatically detected from X/Y sizes
              (len(X) must be equal to len(Y))

Outputs:
    C   -   spline interpolant

ORDER OF POINTS

Subroutine automatically sorts points, so caller may pass unsorted array.
ALGLIB Project: Copyright 24.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dbuildlinear(real_1d_array x, real_1d_array y, ae_int_t n, spline1dinterpolant &amp;c);
<b>void</b> spline1dbuildlinear(real_1d_array x, real_1d_array y, spline1dinterpolant &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline1d_d_linear class=nav>spline1d_d_linear</a> | <a href=#example_spline1d_d_cubic class=nav>spline1d_d_cubic</a> ]</p>
<a name=sub_spline1dbuildmonotone></a><h6 class=pageheader>spline1dbuildmonotone Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function builds monotone cubic Hermite interpolant. This interpolant
is monotonic in [x(0),x(n-1)] and is constant outside of this interval.

In  case  y[]  form  non-monotonic  sequence,  interpolant  is  piecewise
monotonic.  Say, for x=(0,1,2,3,4)  and  y=(0,1,2,1,0)  interpolant  will
monotonically grow at [0..2] and monotonically decrease at [2..4].

Inputs:
    X           -   spline nodes, array[0..N-1]. Subroutine automatically
                    sorts points, so caller may pass unsorted array.
    Y           -   function values, array[0..N-1]
    N           -   the number of points(N &ge; 2).

Outputs:
    C           -   spline interpolant.
ALGLIB Project: Copyright 21.06.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dbuildmonotone(real_1d_array x, real_1d_array y, ae_int_t n, spline1dinterpolant &amp;c);
<b>void</b> spline1dbuildmonotone(real_1d_array x, real_1d_array y, spline1dinterpolant &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline1d_d_monotone class=nav>spline1d_d_monotone</a> ]</p>
<a name=sub_spline1dcalc></a><h6 class=pageheader>spline1dcalc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates the value of the spline at the given point X.

Inputs:
    C   -   spline interpolant
    X   -   point

Result:
    S(x)
ALGLIB Project: Copyright 23.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spline1dcalc(spline1dinterpolant c, <b>double</b> x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline1d_d_linear class=nav>spline1d_d_linear</a> | <a href=#example_spline1d_d_cubic class=nav>spline1d_d_cubic</a> | <a href=#example_spline1d_d_monotone class=nav>spline1d_d_monotone</a> ]</p>
<a name=sub_spline1dconvcubic></a><h6 class=pageheader>spline1dconvcubic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function solves following problem: given table y[] of function values
at old nodes x[]  and new nodes  x2[],  it calculates and returns table of
function values y2[] (calculated at x2[]).

This function yields same result as Spline1DBuildCubic() call followed  by
sequence of Spline1DDiff() calls, but it can be several times faster  when
called for ordered X[] and X2[].

Inputs:
    X           -   old spline nodes
    Y           -   function values
    X2           -  new spline nodes

OPTIONAL PARAMETERS:
    N           -   points count:
                    * N &ge; 2
                    * if given, only first N points from X/Y are used
                    * if not given, automatically detected from X/Y sizes
                      (len(X) must be equal to len(Y))
    BoundLType  -   boundary condition type for the left boundary
    BoundL      -   left boundary condition (first or second derivative,
                    depending on the BoundLType)
    BoundRType  -   boundary condition type for the right boundary
    BoundR      -   right boundary condition (first or second derivative,
                    depending on the BoundRType)
    N2          -   new points count:
                    * N2 &ge; 2
                    * if given, only first N2 points from X2 are used
                    * if not given, automatically detected from X2 size

Outputs:
    F2          -   function values at X2[]

ORDER OF POINTS

Subroutine automatically sorts points, so caller  may pass unsorted array.
Function  values  are correctly reordered on  return, so F2[I]  is  always
equal to S(X2[I]) independently of points order.

SETTING BOUNDARY VALUES:

The BoundLType/BoundRType parameters can have the following values:
    * -1, which corresonds to the periodic (cyclic) boundary conditions.
          In this case:
          * both BoundLType and BoundRType must be equal to -1.
          * BoundL/BoundR are ignored
          * Y[last] is ignored (it is assumed to be equal to Y[first]).
    *  0, which  corresponds  to  the  parabolically   terminated  spline
          (BoundL and/or BoundR are ignored).
    *  1, which corresponds to the first derivative boundary condition
    *  2, which corresponds to the second derivative boundary condition
    *  by default, BoundType=0 is used

PROBLEMS WITH PERIODIC BOUNDARY CONDITIONS:

Problems with periodic boundary conditions have Y[first_point]=Y[last_point].
However, this subroutine doesn't require you to specify equal  values  for
the first and last points - it automatically forces them  to  be  equal by
copying  Y[first_point]  (corresponds  to the leftmost,  minimal  X[])  to
Y[last_point]. However it is recommended to pass consistent values of Y[],
i.e. to make Y[first_point]=Y[last_point].
ALGLIB Project: Copyright 03.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dconvcubic(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t boundltype, <b>double</b> boundl, ae_int_t boundrtype, <b>double</b> boundr, real_1d_array x2, ae_int_t n2, real_1d_array &amp;y2);
<b>void</b> spline1dconvcubic(real_1d_array x, real_1d_array y, real_1d_array x2, real_1d_array &amp;y2);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline1d_d_convdiff class=nav>spline1d_d_convdiff</a> ]</p>
<a name=sub_spline1dconvdiff2cubic></a><h6 class=pageheader>spline1dconvdiff2cubic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function solves following problem: given table y[] of function values
at old nodes x[]  and new nodes  x2[],  it calculates and returns table of
function  values  y2[],  first  and  second  derivatives  d2[]  and  dd2[]
(calculated at x2[]).

This function yields same result as Spline1DBuildCubic() call followed  by
sequence of Spline1DDiff() calls, but it can be several times faster  when
called for ordered X[] and X2[].

Inputs:
    X           -   old spline nodes
    Y           -   function values
    X2           -  new spline nodes

OPTIONAL PARAMETERS:
    N           -   points count:
                    * N &ge; 2
                    * if given, only first N points from X/Y are used
                    * if not given, automatically detected from X/Y sizes
                      (len(X) must be equal to len(Y))
    BoundLType  -   boundary condition type for the left boundary
    BoundL      -   left boundary condition (first or second derivative,
                    depending on the BoundLType)
    BoundRType  -   boundary condition type for the right boundary
    BoundR      -   right boundary condition (first or second derivative,
                    depending on the BoundRType)
    N2          -   new points count:
                    * N2 &ge; 2
                    * if given, only first N2 points from X2 are used
                    * if not given, automatically detected from X2 size

Outputs:
    F2          -   function values at X2[]
    D2          -   first derivatives at X2[]
    DD2         -   second derivatives at X2[]

ORDER OF POINTS

Subroutine automatically sorts points, so caller  may pass unsorted array.
Function  values  are correctly reordered on  return, so F2[I]  is  always
equal to S(X2[I]) independently of points order.

SETTING BOUNDARY VALUES:

The BoundLType/BoundRType parameters can have the following values:
    * -1, which corresonds to the periodic (cyclic) boundary conditions.
          In this case:
          * both BoundLType and BoundRType must be equal to -1.
          * BoundL/BoundR are ignored
          * Y[last] is ignored (it is assumed to be equal to Y[first]).
    *  0, which  corresponds  to  the  parabolically   terminated  spline
          (BoundL and/or BoundR are ignored).
    *  1, which corresponds to the first derivative boundary condition
    *  2, which corresponds to the second derivative boundary condition
    *  by default, BoundType=0 is used

PROBLEMS WITH PERIODIC BOUNDARY CONDITIONS:

Problems with periodic boundary conditions have Y[first_point]=Y[last_point].
However, this subroutine doesn't require you to specify equal  values  for
the first and last points - it automatically forces them  to  be  equal by
copying  Y[first_point]  (corresponds  to the leftmost,  minimal  X[])  to
Y[last_point]. However it is recommended to pass consistent values of Y[],
i.e. to make Y[first_point]=Y[last_point].
ALGLIB Project: Copyright 03.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dconvdiff2cubic(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t boundltype, <b>double</b> boundl, ae_int_t boundrtype, <b>double</b> boundr, real_1d_array x2, ae_int_t n2, real_1d_array &amp;y2, real_1d_array &amp;d2, real_1d_array &amp;dd2);
<b>void</b> spline1dconvdiff2cubic(real_1d_array x, real_1d_array y, real_1d_array x2, real_1d_array &amp;y2, real_1d_array &amp;d2, real_1d_array &amp;dd2);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline1d_d_convdiff class=nav>spline1d_d_convdiff</a> ]</p>
<a name=sub_spline1dconvdiffcubic></a><h6 class=pageheader>spline1dconvdiffcubic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function solves following problem: given table y[] of function values
at old nodes x[]  and new nodes  x2[],  it calculates and returns table of
function values y2[] and derivatives d2[] (calculated at x2[]).

This function yields same result as Spline1DBuildCubic() call followed  by
sequence of Spline1DDiff() calls, but it can be several times faster  when
called for ordered X[] and X2[].

Inputs:
    X           -   old spline nodes
    Y           -   function values
    X2           -  new spline nodes

OPTIONAL PARAMETERS:
    N           -   points count:
                    * N &ge; 2
                    * if given, only first N points from X/Y are used
                    * if not given, automatically detected from X/Y sizes
                      (len(X) must be equal to len(Y))
    BoundLType  -   boundary condition type for the left boundary
    BoundL      -   left boundary condition (first or second derivative,
                    depending on the BoundLType)
    BoundRType  -   boundary condition type for the right boundary
    BoundR      -   right boundary condition (first or second derivative,
                    depending on the BoundRType)
    N2          -   new points count:
                    * N2 &ge; 2
                    * if given, only first N2 points from X2 are used
                    * if not given, automatically detected from X2 size

Outputs:
    F2          -   function values at X2[]
    D2          -   first derivatives at X2[]

ORDER OF POINTS

Subroutine automatically sorts points, so caller  may pass unsorted array.
Function  values  are correctly reordered on  return, so F2[I]  is  always
equal to S(X2[I]) independently of points order.

SETTING BOUNDARY VALUES:

The BoundLType/BoundRType parameters can have the following values:
    * -1, which corresonds to the periodic (cyclic) boundary conditions.
          In this case:
          * both BoundLType and BoundRType must be equal to -1.
          * BoundL/BoundR are ignored
          * Y[last] is ignored (it is assumed to be equal to Y[first]).
    *  0, which  corresponds  to  the  parabolically   terminated  spline
          (BoundL and/or BoundR are ignored).
    *  1, which corresponds to the first derivative boundary condition
    *  2, which corresponds to the second derivative boundary condition
    *  by default, BoundType=0 is used

PROBLEMS WITH PERIODIC BOUNDARY CONDITIONS:

Problems with periodic boundary conditions have Y[first_point]=Y[last_point].
However, this subroutine doesn't require you to specify equal  values  for
the first and last points - it automatically forces them  to  be  equal by
copying  Y[first_point]  (corresponds  to the leftmost,  minimal  X[])  to
Y[last_point]. However it is recommended to pass consistent values of Y[],
i.e. to make Y[first_point]=Y[last_point].
ALGLIB Project: Copyright 03.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dconvdiffcubic(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t boundltype, <b>double</b> boundl, ae_int_t boundrtype, <b>double</b> boundr, real_1d_array x2, ae_int_t n2, real_1d_array &amp;y2, real_1d_array &amp;d2);
<b>void</b> spline1dconvdiffcubic(real_1d_array x, real_1d_array y, real_1d_array x2, real_1d_array &amp;y2, real_1d_array &amp;d2);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline1d_d_convdiff class=nav>spline1d_d_convdiff</a> ]</p>
<a name=sub_spline1ddiff></a><h6 class=pageheader>spline1ddiff Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine differentiates the spline.

Inputs:
    C   -   spline interpolant.
    X   -   point

Result:
    S   -   S(x)
    DS  -   S'(x)
    D2S -   S''(x)
ALGLIB Project: Copyright 24.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1ddiff(spline1dinterpolant c, <b>double</b> x, <b>double</b> &amp;s, <b>double</b> &amp;ds, <b>double</b> &amp;d2s);
</pre>
<a name=sub_spline1dfit></a><h6 class=pageheader>spline1dfit Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fitting by smoothing (penalized) cubic spline.

This function approximates N scattered points (some of X[] may be equal to
each other) by cubic spline with M  nodes  at  equidistant  grid  spanning
interval [min(x,xc),max(x,xc)].

The problem is regularized by adding nonlinearity penalty to  usual  least
squares penalty function:

    MERIT_FUNC = F_LS + F_NL

where F_LS is a least squares error  term,  and  F_NL  is  a  nonlinearity
penalty which is roughly proportional to LambdaNS*integral{ S''(x)^2*dx }.
Algorithm applies automatic renormalization of F_NL  which  makes  penalty
term roughly invariant to scaling of X[] and changes in M.

This function is a new edition  of  penalized  regression  spline fitting,
a fast and compact one which needs much less resources that  its  previous
version: just O(maxMN) memory and O(maxMN*log(maxMN)) time.

NOTE: it is OK to run this function with both M &lt;&lt; N and M &gt;&gt; N;  say,  it  is
      possible to process 100 points with 1000-node spline.

Inputs:
    X           -   points, array[0..N-1].
    Y           -   function values, array[0..N-1].
    N           -   number of points (optional):
                    * N &gt; 0
                    * if given, only first N elements of X/Y are processed
                    * if not given, automatically determined from lengths
    M           -   number of basis functions ( = number_of_nodes), M &ge; 4.
    LambdaNS    -   LambdaNS &ge; 0, regularization  constant  passed by user.
                    It penalizes nonlinearity in the regression spline.
                    Possible values to start from are 0.00001, 0.1, 1

Outputs:
    S   -   spline interpolant.
    Rep -   Following fields are set:
            * RMSError      rms error on the (X,Y).
            * AvgError      average error on the (X,Y).
            * AvgRelError   average relative error on the non-zero Y
            * MaxError      maximum error
ALGLIB Project: Copyright 27.08.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dfit(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t m, <b>double</b> lambdans, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
<b>void</b> spline1dfit(real_1d_array x, real_1d_array y, ae_int_t m, <b>double</b> lambdans, spline1dinterpolant &amp;s, spline1dfitreport &amp;rep);
</pre>
<a name=sub_spline1dgriddiff2cubic></a><h6 class=pageheader>spline1dgriddiff2cubic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function solves following problem: given table y[] of function values
at  nodes  x[],  it  calculates  and  returns  tables  of first and second
function derivatives d1[] and d2[] (calculated at the same nodes x[]).

This function yields same result as Spline1DBuildCubic() call followed  by
sequence of Spline1DDiff() calls, but it can be several times faster  when
called for ordered X[] and X2[].

Inputs:
    X           -   spline nodes
    Y           -   function values

OPTIONAL PARAMETERS:
    N           -   points count:
                    * N &ge; 2
                    * if given, only first N points are used
                    * if not given, automatically detected from X/Y sizes
                      (len(X) must be equal to len(Y))
    BoundLType  -   boundary condition type for the left boundary
    BoundL      -   left boundary condition (first or second derivative,
                    depending on the BoundLType)
    BoundRType  -   boundary condition type for the right boundary
    BoundR      -   right boundary condition (first or second derivative,
                    depending on the BoundRType)

Outputs:
    D1          -   S' values at X[]
    D2          -   S'' values at X[]

ORDER OF POINTS

Subroutine automatically sorts points, so caller may pass unsorted array.
Derivative values are correctly reordered on return, so  D[I]  is  always
equal to S'(X[I]) independently of points order.

SETTING BOUNDARY VALUES:

The BoundLType/BoundRType parameters can have the following values:
    * -1, which corresonds to the periodic (cyclic) boundary conditions.
          In this case:
          * both BoundLType and BoundRType must be equal to -1.
          * BoundL/BoundR are ignored
          * Y[last] is ignored (it is assumed to be equal to Y[first]).
    *  0, which  corresponds  to  the  parabolically   terminated  spline
          (BoundL and/or BoundR are ignored).
    *  1, which corresponds to the first derivative boundary condition
    *  2, which corresponds to the second derivative boundary condition
    *  by default, BoundType=0 is used

PROBLEMS WITH PERIODIC BOUNDARY CONDITIONS:

Problems with periodic boundary conditions have Y[first_point]=Y[last_point].
However, this subroutine doesn't require you to specify equal  values  for
the first and last points - it automatically forces them  to  be  equal by
copying  Y[first_point]  (corresponds  to the leftmost,  minimal  X[])  to
Y[last_point]. However it is recommended to pass consistent values of Y[],
i.e. to make Y[first_point]=Y[last_point].
ALGLIB Project: Copyright 03.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dgriddiff2cubic(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t boundltype, <b>double</b> boundl, ae_int_t boundrtype, <b>double</b> boundr, real_1d_array &amp;d1, real_1d_array &amp;d2);
<b>void</b> spline1dgriddiff2cubic(real_1d_array x, real_1d_array y, real_1d_array &amp;d1, real_1d_array &amp;d2);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline1d_d_griddiff class=nav>spline1d_d_griddiff</a> ]</p>
<a name=sub_spline1dgriddiffcubic></a><h6 class=pageheader>spline1dgriddiffcubic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function solves following problem: given table y[] of function values
at nodes x[], it calculates and returns table of function derivatives  d[]
(calculated at the same nodes x[]).

This function yields same result as Spline1DBuildCubic() call followed  by
sequence of Spline1DDiff() calls, but it can be several times faster  when
called for ordered X[] and X2[].

Inputs:
    X           -   spline nodes
    Y           -   function values

OPTIONAL PARAMETERS:
    N           -   points count:
                    * N &ge; 2
                    * if given, only first N points are used
                    * if not given, automatically detected from X/Y sizes
                      (len(X) must be equal to len(Y))
    BoundLType  -   boundary condition type for the left boundary
    BoundL      -   left boundary condition (first or second derivative,
                    depending on the BoundLType)
    BoundRType  -   boundary condition type for the right boundary
    BoundR      -   right boundary condition (first or second derivative,
                    depending on the BoundRType)

Outputs:
    D           -   derivative values at X[]

ORDER OF POINTS

Subroutine automatically sorts points, so caller may pass unsorted array.
Derivative values are correctly reordered on return, so  D[I]  is  always
equal to S'(X[I]) independently of points order.

SETTING BOUNDARY VALUES:

The BoundLType/BoundRType parameters can have the following values:
    * -1, which corresonds to the periodic (cyclic) boundary conditions.
          In this case:
          * both BoundLType and BoundRType must be equal to -1.
          * BoundL/BoundR are ignored
          * Y[last] is ignored (it is assumed to be equal to Y[first]).
    *  0, which  corresponds  to  the  parabolically   terminated  spline
          (BoundL and/or BoundR are ignored).
    *  1, which corresponds to the first derivative boundary condition
    *  2, which corresponds to the second derivative boundary condition
    *  by default, BoundType=0 is used

PROBLEMS WITH PERIODIC BOUNDARY CONDITIONS:

Problems with periodic boundary conditions have Y[first_point]=Y[last_point].
However, this subroutine doesn't require you to specify equal  values  for
the first and last points - it automatically forces them  to  be  equal by
copying  Y[first_point]  (corresponds  to the leftmost,  minimal  X[])  to
Y[last_point]. However it is recommended to pass consistent values of Y[],
i.e. to make Y[first_point]=Y[last_point].
ALGLIB Project: Copyright 03.09.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dgriddiffcubic(real_1d_array x, real_1d_array y, ae_int_t n, ae_int_t boundltype, <b>double</b> boundl, ae_int_t boundrtype, <b>double</b> boundr, real_1d_array &amp;d);
<b>void</b> spline1dgriddiffcubic(real_1d_array x, real_1d_array y, real_1d_array &amp;d);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline1d_d_griddiff class=nav>spline1d_d_griddiff</a> ]</p>
<a name=sub_spline1dintegrate></a><h6 class=pageheader>spline1dintegrate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine integrates the spline.

Inputs:
    C   -   spline interpolant.
    X   -   right bound of the integration interval [a, x],
            here 'a' denotes min(x[])
Result:
    integral(S(t)dt,a,x)
ALGLIB Project: Copyright 23.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spline1dintegrate(spline1dinterpolant c, <b>double</b> x);
</pre>
<a name=sub_spline1dlintransx></a><h6 class=pageheader>spline1dlintransx Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine performs linear transformation of the spline argument.

Inputs:
    C   -   spline interpolant.
    A, B-   transformation coefficients: x = A*t + B
Result:
    C   -   transformed spline
ALGLIB Project: Copyright 30.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dlintransx(spline1dinterpolant c, <b>double</b> a, <b>double</b> b);
</pre>
<a name=sub_spline1dlintransy></a><h6 class=pageheader>spline1dlintransy Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine performs linear transformation of the spline.

Inputs:
    C   -   spline interpolant.
    A, B-   transformation coefficients: S2(x) = A*S(x) + B
Result:
    C   -   transformed spline
ALGLIB Project: Copyright 30.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dlintransy(spline1dinterpolant c, <b>double</b> a, <b>double</b> b);
</pre>
<a name=sub_spline1dunpack></a><h6 class=pageheader>spline1dunpack Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine unpacks the spline into the coefficients table.

Inputs:
    C   -   spline interpolant.
    X   -   point

Outputs:
    Tbl -   coefficients table, unpacked format, array[0..N-2, 0..5].
            For I = 0...N-2:
                Tbl[I,0] = X[i]
                Tbl[I,1] = X[i+1]
                Tbl[I,2] = C0
                Tbl[I,3] = C1
                Tbl[I,4] = C2
                Tbl[I,5] = C3
            On [x[i], x[i+1]] spline is equals to:
                S(x) = C0 + C1*t + C2*t^2 + C3*t^3
                t = x-x[i]

NOTE:
    You  can rebuild spline with  Spline1DBuildHermite()  function,  which
    accepts as inputs function values and derivatives at nodes, which  are
    easy to calculate when you have coefficients.
ALGLIB Project: Copyright 29.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline1dunpack(spline1dinterpolant c, ae_int_t &amp;n, real_2d_array &amp;tbl);
</pre>
<a name=example_spline1d_d_convdiff></a><h6 class=pageheader>spline1d_d_convdiff Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use cubic spline to <b>do</b> resampling, i.e. having</font>
<font color=navy>// values of f(x)=x^2 sampled at 5 equidistant nodes on [-1,+1]</font>
<font color=navy>// we calculate values/derivatives of cubic spline on </font>
<font color=navy>// another grid (equidistant with 9 nodes on [-1,+1])</font>
<font color=navy>// WITHOUT CONSTRUCTION OF SPLINE OBJECT.</font>
<font color=navy>//</font>
<font color=navy>// There are efficient functions spline1dconvcubic(),</font>
<font color=navy>// spline1dconvdiffcubic() and spline1dconvdiff2cubic() </font>
<font color=navy>// <b>for</b> such calculations.</font>
<font color=navy>//</font>
<font color=navy>// We use default boundary conditions (&quot;parabolically terminated</font>
<font color=navy>// spline&quot;) because cubic spline built with such boundary conditions </font>
<font color=navy>// will exactly reproduce any quadratic f(x).</font>
<font color=navy>//</font>
<font color=navy>// Actually, we could use natural conditions, but we feel that </font>
<font color=navy>// spline which exactly reproduces f() will show us more </font>
<font color=navy>// understandable results.</font>
   real_1d_array x_old = <font color=blue><b>&quot;[-1.0,-0.5,0.0,+0.5,+1.0]&quot;</b></font>;
   real_1d_array y_old = <font color=blue><b>&quot;[+1.0,0.25,0.0,0.25,+1.0]&quot;</b></font>;
   real_1d_array x_new = <font color=blue><b>&quot;[-1.00,-0.75,-0.50,-0.25,0.00,+0.25,+0.50,+0.75,+1.00]&quot;</b></font>;
   real_1d_array y_new;
   real_1d_array d1_new;
   real_1d_array d2_new;
<font color=navy>// First, conversion without differentiation.</font>
<font color=navy>//</font>
<font color=navy>//</font>
   spline1dconvcubic(x_old, y_old, x_new, y_new);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y_new.tostring(3).c_str()); <font color=navy>// EXPECTED: [1.0000, 0.5625, 0.2500, 0.0625, 0.0000, 0.0625, 0.2500, 0.5625, 1.0000]</font>
<font color=navy>//</font>
<font color=navy>// Then, conversion with differentiation (first derivatives only)</font>
<font color=navy>//</font>
<font color=navy>//</font>
   spline1dconvdiffcubic(x_old, y_old, x_new, y_new, d1_new);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y_new.tostring(3).c_str()); <font color=navy>// EXPECTED: [1.0000, 0.5625, 0.2500, 0.0625, 0.0000, 0.0625, 0.2500, 0.5625, 1.0000]</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, d1_new.tostring(3).c_str()); <font color=navy>// EXPECTED: [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]</font>
<font color=navy>//</font>
<font color=navy>// Finally, conversion with first and second derivatives</font>
<font color=navy>//</font>
<font color=navy>//</font>
   spline1dconvdiff2cubic(x_old, y_old, x_new, y_new, d1_new, d2_new);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y_new.tostring(3).c_str()); <font color=navy>// EXPECTED: [1.0000, 0.5625, 0.2500, 0.0625, 0.0000, 0.0625, 0.2500, 0.5625, 1.0000]</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, d1_new.tostring(3).c_str()); <font color=navy>// EXPECTED: [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, d2_new.tostring(3).c_str()); <font color=navy>// EXPECTED: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline1d_d_cubic></a><h6 class=pageheader>spline1d_d_cubic Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use cubic spline to interpolate f(x)=x^2 sampled </font>
<font color=navy>// at 5 equidistant nodes on [-1,+1].</font>
<font color=navy>//</font>
<font color=navy>// First, we use default boundary conditions (&quot;parabolically terminated</font>
<font color=navy>// spline&quot;) because cubic spline built with such boundary conditions </font>
<font color=navy>// will exactly reproduce any quadratic f(x).</font>
<font color=navy>//</font>
<font color=navy>// Then we try to use natural boundary conditions</font>
<font color=navy>//     d2S(-1)/dx^2 = 0.0</font>
<font color=navy>//     d2S(+1)/dx^2 = 0.0</font>
<font color=navy>// and see that such spline interpolated f(x) with small error.</font>
   real_1d_array x = <font color=blue><b>&quot;[-1.0,-0.5,0.0,+0.5,+1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[+1.0,0.25,0.0,0.25,+1.0]&quot;</b></font>;
   <b>double</b> t = 0.25;
   <b>double</b> v;
   spline1dinterpolant s;
   ae_int_t natural_bound_type = 2;
<font color=navy>// Test exact boundary conditions: build S(x), calculare S(0.25)</font>
<font color=navy>// (almost same as original function)</font>
   spline1dbuildcubic(x, y, s);
   v = spline1dcalc(s, t);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.0625</font>
<font color=navy>//</font>
<font color=navy>// Test natural boundary conditions: build S(x), calculare S(0.25)</font>
<font color=navy>// (small interpolation error)</font>
   spline1dbuildcubic(x, y, 5, natural_bound_type, 0.0, natural_bound_type, 0.0, s);
   v = spline1dcalc(s, t);
   printf(<font color=blue><b>&quot;%.3f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.0580</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline1d_d_griddiff></a><h6 class=pageheader>spline1d_d_griddiff Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use cubic spline to <b>do</b> grid differentiation, i.e. having</font>
<font color=navy>// values of f(x)=x^2 sampled at 5 equidistant nodes on [-1,+1]</font>
<font color=navy>// we calculate derivatives of cubic spline at nodes WITHOUT</font>
<font color=navy>// CONSTRUCTION OF SPLINE OBJECT.</font>
<font color=navy>//</font>
<font color=navy>// There are efficient functions spline1dgriddiffcubic() and</font>
<font color=navy>// spline1dgriddiff2cubic() <b>for</b> such calculations.</font>
<font color=navy>//</font>
<font color=navy>// We use default boundary conditions (&quot;parabolically terminated</font>
<font color=navy>// spline&quot;) because cubic spline built with such boundary conditions </font>
<font color=navy>// will exactly reproduce any quadratic f(x).</font>
<font color=navy>//</font>
<font color=navy>// Actually, we could use natural conditions, but we feel that </font>
<font color=navy>// spline which exactly reproduces f() will show us more </font>
<font color=navy>// understandable results.</font>
   real_1d_array x = <font color=blue><b>&quot;[-1.0,-0.5,0.0,+0.5,+1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[+1.0,0.25,0.0,0.25,+1.0]&quot;</b></font>;
   real_1d_array d1;
   real_1d_array d2;
<font color=navy>// We calculate first derivatives: they must be equal to 2*x</font>
   spline1dgriddiffcubic(x, y, d1);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, d1.tostring(3).c_str()); <font color=navy>// EXPECTED: [-2.0, -1.0, 0.0, +1.0, +2.0]</font>
<font color=navy>//</font>
<font color=navy>// Now test griddiff2, which returns first AND second derivatives.</font>
<font color=navy>// First derivative is 2*x, second is equal to 2.0</font>
   spline1dgriddiff2cubic(x, y, d1, d2);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, d1.tostring(3).c_str()); <font color=navy>// EXPECTED: [-2.0, -1.0, 0.0, +1.0, +2.0]</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, d2.tostring(3).c_str()); <font color=navy>// EXPECTED: [ 2.0,  2.0, 2.0,  2.0,  2.0]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline1d_d_linear></a><h6 class=pageheader>spline1d_d_linear Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use piecewise linear spline to interpolate f(x)=x^2 sampled </font>
<font color=navy>// at 5 equidistant nodes on [-1,+1].</font>
   real_1d_array x = <font color=blue><b>&quot;[-1.0,-0.5,0.0,+0.5,+1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[+1.0,0.25,0.0,0.25,+1.0]&quot;</b></font>;
   <b>double</b> t = 0.25;
   <b>double</b> v;
   spline1dinterpolant s;

<font color=navy>// build spline</font>
   spline1dbuildlinear(x, y, s);

<font color=navy>// calculate S(0.25) - it is quite different from 0.25^2=0.0625</font>
   v = spline1dcalc(s, t);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.125</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline1d_d_monotone></a><h6 class=pageheader>spline1d_d_monotone Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// Spline built witn spline1dbuildcubic() can be non-monotone even when</font>
<font color=navy>// Y-values form monotone sequence. Say, <b>for</b> x=[0,1,2] and y=[0,1,1]</font>
<font color=navy>// cubic spline will monotonically grow until x=1.5 and then start</font>
<font color=navy>// decreasing.</font>
<font color=navy>//</font>
<font color=navy>// That's why ALGLIB provides special spline construction function</font>
<font color=navy>// which builds spline which preserves monotonicity of the original</font>
<font color=navy>// dataset.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: in case original dataset is non-monotonic, ALGLIB splits it</font>
<font color=navy>// into monotone subsequences and builds piecewise monotonic spline.</font>
   real_1d_array x = <font color=blue><b>&quot;[0,1,2]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0,1,1]&quot;</b></font>;
   spline1dinterpolant s;

<font color=navy>// build spline</font>
   spline1dbuildmonotone(x, y, s);

<font color=navy>// calculate S at x = [-0.5, 0.0, 0.5, 1.0, 1.5, 2.0]</font>
<font color=navy>// you may see that spline is really monotonic</font>
   <b>double</b> v;
   v = spline1dcalc(s, -0.5);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.0000</font>
   v = spline1dcalc(s, 0.0);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.0000</font>
   v = spline1dcalc(s, +0.5);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.5000</font>
   v = spline1dcalc(s, 1.0);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.0000</font>
   v = spline1dcalc(s, 1.5);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.0000</font>
   v = spline1dcalc(s, 2.0);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.0000</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_spline2d></a><h4 class=pageheader>8.6.10. spline2d Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_spline2dbuilder class=toc>spline2dbuilder</a> |
<a href=#struct_spline2dfitreport class=toc>spline2dfitreport</a> |
<a href=#struct_spline2dinterpolant class=toc>spline2dinterpolant</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_spline2dbuildbicubic class=toc>spline2dbuildbicubic</a> |
<a href=#sub_spline2dbuildbicubicv class=toc>spline2dbuildbicubicv</a> |
<a href=#sub_spline2dbuildbilinear class=toc>spline2dbuildbilinear</a> |
<a href=#sub_spline2dbuildbilinearv class=toc>spline2dbuildbilinearv</a> |
<a href=#sub_spline2dbuildercreate class=toc>spline2dbuildercreate</a> |
<a href=#sub_spline2dbuildersetalgoblocklls class=toc>spline2dbuildersetalgoblocklls</a> |
<a href=#sub_spline2dbuildersetalgofastddm class=toc>spline2dbuildersetalgofastddm</a> |
<a href=#sub_spline2dbuildersetalgonaivells class=toc>spline2dbuildersetalgonaivells</a> |
<a href=#sub_spline2dbuildersetarea class=toc>spline2dbuildersetarea</a> |
<a href=#sub_spline2dbuildersetareaauto class=toc>spline2dbuildersetareaauto</a> |
<a href=#sub_spline2dbuildersetconstterm class=toc>spline2dbuildersetconstterm</a> |
<a href=#sub_spline2dbuildersetgrid class=toc>spline2dbuildersetgrid</a> |
<a href=#sub_spline2dbuildersetlinterm class=toc>spline2dbuildersetlinterm</a> |
<a href=#sub_spline2dbuildersetpoints class=toc>spline2dbuildersetpoints</a> |
<a href=#sub_spline2dbuildersetuserterm class=toc>spline2dbuildersetuserterm</a> |
<a href=#sub_spline2dbuildersetzeroterm class=toc>spline2dbuildersetzeroterm</a> |
<a href=#sub_spline2dcalc class=toc>spline2dcalc</a> |
<a href=#sub_spline2dcalcv class=toc>spline2dcalcv</a> |
<a href=#sub_spline2dcalcvbuf class=toc>spline2dcalcvbuf</a> |
<a href=#sub_spline2dcalcvi class=toc>spline2dcalcvi</a> |
<a href=#sub_spline2dcopy class=toc>spline2dcopy</a> |
<a href=#sub_spline2ddiff class=toc>spline2ddiff</a> |
<a href=#sub_spline2ddiffvi class=toc>spline2ddiffvi</a> |
<a href=#sub_spline2dfit class=toc>spline2dfit</a> |
<a href=#sub_spline2dlintransf class=toc>spline2dlintransf</a> |
<a href=#sub_spline2dlintransxy class=toc>spline2dlintransxy</a> |
<a href=#sub_spline2dresamplebicubic class=toc>spline2dresamplebicubic</a> |
<a href=#sub_spline2dresamplebilinear class=toc>spline2dresamplebilinear</a> |
<a href=#sub_spline2dserialize class=toc>spline2dserialize</a> |
<a href=#sub_spline2dunpack class=toc>spline2dunpack</a> |
<a href=#sub_spline2dunpackv class=toc>spline2dunpackv</a> |
<a href=#sub_spline2dunserialize class=toc>spline2dunserialize</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_spline2d_bicubic class=toc>spline2d_bicubic</a></td><td width=15>&nbsp;</td><td>Bilinear spline interpolation</td></tr>
<tr align=left valign=top><td><a href=#example_spline2d_bilinear class=toc>spline2d_bilinear</a></td><td width=15>&nbsp;</td><td>Bilinear spline interpolation</td></tr>
<tr align=left valign=top><td><a href=#example_spline2d_copytrans class=toc>spline2d_copytrans</a></td><td width=15>&nbsp;</td><td>Copy and transform</td></tr>
<tr align=left valign=top><td><a href=#example_spline2d_fit_blocklls class=toc>spline2d_fit_blocklls</a></td><td width=15>&nbsp;</td><td>Fitting bicubic spline to irregular data</td></tr>
<tr align=left valign=top><td><a href=#example_spline2d_unpack class=toc>spline2d_unpack</a></td><td width=15>&nbsp;</td><td>Unpacking bilinear spline</td></tr>
<tr align=left valign=top><td><a href=#example_spline2d_vector class=toc>spline2d_vector</a></td><td width=15>&nbsp;</td><td>Copy and transform</td></tr>
</table>
</div>
<a name=struct_spline2dbuilder></a><h6 class=pageheader>spline2dbuilder Class</h6>
<hr width=600 align=left>
<pre class=narration>
Nonlinear least squares solver used to fit 2D splines to data
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> spline2dbuilder {
};
</pre>
<a name=struct_spline2dfitreport></a><h6 class=pageheader>spline2dfitreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Spline 2D fitting report:
    rmserror        RMS error
    avgerror        average error
    maxerror        maximum error
    r2              coefficient of determination,  R-squared, 1-RSS/TSS
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> spline2dfitreport {
   <b>double</b> rmserror;
   <b>double</b> avgerror;
   <b>double</b> maxerror;
   <b>double</b> r2;
};
</pre>
<a name=struct_spline2dinterpolant></a><h6 class=pageheader>spline2dinterpolant Class</h6>
<hr width=600 align=left>
<pre class=narration>
2-dimensional spline inteprolant
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> spline2dinterpolant {
};
</pre>
<a name=sub_spline2dbuildbicubic></a><h6 class=pageheader>spline2dbuildbicubic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine was deprecated in ALGLIB 3.6.0

We recommend you to switch  to  Spline2DBuildBicubicV(),  which  is  more
flexible and accepts its arguments in more convenient order.
ALGLIB Project: Copyright 05.07.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildbicubic(real_1d_array x, real_1d_array y, real_2d_array f, ae_int_t m, ae_int_t n, spline2dinterpolant &amp;c);
</pre>
<a name=sub_spline2dbuildbicubicv></a><h6 class=pageheader>spline2dbuildbicubicv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds bicubic vector-valued spline.

Inputs:
    X   -   spline abscissas, array[0..N-1]
    Y   -   spline ordinates, array[0..M-1]
    F   -   function values, array[0..M*N*D-1]:
            * first D elements store D values at (X[0],Y[0])
            * next D elements store D values at (X[1],Y[0])
            * general form - D function values at (X[i],Y[j]) are stored
              at F[D*(J*N+I)...D*(J*N+I)+D-1].
    M,N -   grid size, M &ge; 2, N &ge; 2
    D   -   vector dimension, D &ge; 1

Outputs:
    C   -   spline interpolant
ALGLIB Project: Copyright 16.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildbicubicv(real_1d_array x, ae_int_t n, real_1d_array y, ae_int_t m, real_1d_array f, ae_int_t d, spline2dinterpolant &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_bicubic class=nav>spline2d_bicubic</a> ]</p>
<a name=sub_spline2dbuildbilinear></a><h6 class=pageheader>spline2dbuildbilinear Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine was deprecated in ALGLIB 3.6.0

We recommend you to switch  to  Spline2DBuildBilinearV(),  which  is  more
flexible and accepts its arguments in more convenient order.
ALGLIB Project: Copyright 05.07.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildbilinear(real_1d_array x, real_1d_array y, real_2d_array f, ae_int_t m, ae_int_t n, spline2dinterpolant &amp;c);
</pre>
<a name=sub_spline2dbuildbilinearv></a><h6 class=pageheader>spline2dbuildbilinearv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds bilinear vector-valued spline.

Inputs:
    X   -   spline abscissas, array[0..N-1]
    Y   -   spline ordinates, array[0..M-1]
    F   -   function values, array[0..M*N*D-1]:
            * first D elements store D values at (X[0],Y[0])
            * next D elements store D values at (X[1],Y[0])
            * general form - D function values at (X[i],Y[j]) are stored
              at F[D*(J*N+I)...D*(J*N+I)+D-1].
    M,N -   grid size, M &ge; 2, N &ge; 2
    D   -   vector dimension, D &ge; 1

Outputs:
    C   -   spline interpolant
ALGLIB Project: Copyright 16.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildbilinearv(real_1d_array x, ae_int_t n, real_1d_array y, ae_int_t m, real_1d_array f, ae_int_t d, spline2dinterpolant &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_bilinear class=nav>spline2d_bilinear</a> | <a href=#example_spline2d_vector class=nav>spline2d_vector</a> ]</p>
<a name=sub_spline2dbuildercreate></a><h6 class=pageheader>spline2dbuildercreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine creates least squares solver used to  fit  2D  splines  to
irregularly sampled (scattered) data.

Solver object is used to perform spline fits as follows:
* solver object is created with spline2dbuildercreate() function
* dataset is added with spline2dbuildersetpoints() function
* fit area is chosen:
  * spline2dbuildersetarea()     - for user-defined area
  * spline2dbuildersetareaauto() - for automatically chosen area
* number of grid nodes is chosen with spline2dbuildersetgrid()
* prior term is chosen with one of the following functions:
  * spline2dbuildersetlinterm()   to set linear prior
  * spline2dbuildersetconstterm() to set constant prior
  * spline2dbuildersetzeroterm()  to set zero prior
  * spline2dbuildersetuserterm()  to set user-defined constant prior
* solver algorithm is chosen with either:
  * spline2dbuildersetalgoblocklls() - BlockLLS algorithm, medium-scale problems
  * spline2dbuildersetalgofastddm()  - FastDDM algorithm, large-scale problems
* finally, fitting itself is performed with spline2dfit() function.

Most of the steps above can be omitted,  solver  is  configured with  good
defaults. The minimum is to call:
* spline2dbuildercreate() to create solver object
* spline2dbuildersetpoints() to specify dataset
* spline2dbuildersetgrid() to tell how many nodes you need
* spline2dfit() to perform fit

Inputs:
    D   -   positive number, number of Y-components: D=1 for simple scalar
            fit, D &gt; 1 for vector-valued spline fitting.

Outputs:
    S   -   solver object
ALGLIB Project: Copyright 29.01.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildercreate(ae_int_t d, spline2dbuilder &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_fit_blocklls class=nav>spline2d_fit_blocklls</a> ]</p>
<a name=sub_spline2dbuildersetalgoblocklls></a><h6 class=pageheader>spline2dbuildersetalgoblocklls Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  allows  you to choose least squares solver used to perform
fitting. This function sets solver algorithm to &quot;BlockLLS&quot;, which performs
least squares fitting  with  fast  sparse  direct  solver,  with  optional
nonsmoothness penalty being applied.

Nonlinearity penalty has the following form:

                          [                                            ]
    P() ~ Lambda* integral[ (d2S/dx2)^2 + 2*(d2S/dxdy)^2 + (d2S/dy2)^2 ]dxdy
                          [                                            ]

here integral is calculated over entire grid, and &quot;~&quot; means &quot;proportional&quot;
because integral is normalized after calcilation. Extremely  large  values
of Lambda result in linear fit being performed.

NOTE: this algorithm is the most robust and controllable one,  but  it  is
      limited by 512x512 grids and (say) up to 1.000.000 points.  However,
      ALGLIB has one more  spline  solver:  FastDDM  algorithm,  which  is
      intended for really large-scale problems (in 10M-100M range). FastDDM
      algorithm also has better parallelism properties.

More information on BlockLLS solver:
* memory requirements: ~[32*K^3+256*NPoints]  bytes  for  KxK  grid   with
  NPoints-sized dataset
* serial running time: O(K^4+NPoints)
* parallelism potential: limited. You may get some sublinear gain when
  working with large grids (K's in 256..512 range)

Inputs:
    S       -   spline 2D builder object
    LambdaNS-   non-negative value:
                * positive value means that some smoothing is applied
                * zero value means  that  no  smoothing  is  applied,  and
                  corresponding entries of design matrix  are  numerically
                  zero and dropped from consideration.
ALGLIB: Copyright 05.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetalgoblocklls(spline2dbuilder state, <b>double</b> lambdans);
</pre>
<a name=sub_spline2dbuildersetalgofastddm></a><h6 class=pageheader>spline2dbuildersetalgofastddm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  allows  you to choose least squares solver used to perform
fitting. This function sets solver algorithm to &quot;FastDDM&quot;, which  performs
fast parallel fitting by splitting problem into smaller chunks and merging
results together.

This solver is optimized for large-scale problems, starting  from  256x256
grids, and up to 10000x10000 grids. Of course, it will  work  for  smaller
grids too.

More detailed description of the algorithm is given below:
* algorithm generates hierarchy  of  nested  grids,  ranging  from  ~16x16
  (topmost &quot;layer&quot; of the model) to ~KX*KY one (final layer). Upper layers
  model global behavior of the function, lower layers are  used  to  model
  fine details. Moving from layer to layer doubles grid density.
* fitting  is  started  from  topmost  layer, subsequent layers are fitted
  using residuals from previous ones.
* user may choose to skip generation of upper layers and generate  only  a
  few bottom ones, which  will  result  in  much  better  performance  and
  parallelization efficiency, at the cost of algorithm inability to &quot;patch&quot;
  large holes in the dataset.
* every layer is regularized using progressively increasing regularization
  coefficient; thus, increasing  LambdaV  penalizes  fine  details  first,
  leaving lower frequencies almost intact for a while.
* after fitting is done, all layers are merged together into  one  bicubic
  spline

IMPORTANT: regularization coefficient used by  this  solver  is  different
           from the one used by  BlockLLS.  Latter  utilizes  nonlinearity
           penalty,  which  is  global  in  nature  (large  regularization
           results in global linear trend being  extracted);  this  solver
           uses another, localized form of penalty, which is suitable  for
           parallel processing.

Notes on memory and performance:
* memory requirements: most memory is consumed  during  modeling   of  the
  higher layers; ~[512*NPoints] bytes is required for a  model  with  full
  hierarchy of grids being generated. However, if you skip a  few  topmost
  layers, you will get nearly constant (wrt. points count and  grid  size)
  memory consumption.
* serial running time: O(K*K)+O(NPoints) for a KxK grid
* parallelism potential: good. You may get  nearly  linear  speed-up  when
  performing fitting with just a few layers. Adding more layers results in
  model becoming more global, which somewhat  reduces  efficiency  of  the
  parallel code.

Inputs:
    S       -   spline 2D builder object
    NLayers -   number of layers in the model:
                * NLayers &ge; 1 means that up  to  chosen  number  of  bottom
                  layers is fitted
                * NLayers=0 means that maximum number of layers is  chosen
                  (according to current grid size)
                * NLayers &le; -1 means that up to |NLayers| topmost layers is
                  skipped
                Recommendations:
                * good &quot;default&quot; value is 2 layers
                * you may need  more  layers,  if  your  dataset  is  very
                  irregular and you want to &quot;patch&quot;  large  holes.  For  a
                  grid step H (equal to AreaWidth/GridSize) you may expect
                  that last layer reproduces variations at distance H (and
                  can patch holes that wide); that higher  layers  operate
                  at distances 2*H, 4*H, 8*H and so on.
                * good value for &quot;bullletproof&quot; mode is  NLayers=0,  which
                  results in complete hierarchy of layers being generated.
    LambdaV -   regularization coefficient, chosen in such a way  that  it
                penalizes bottom layers (fine details) first.
                LambdaV &ge; 0, zero value means that no penalty is applied.
ALGLIB: Copyright 05.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetalgofastddm(spline2dbuilder state, ae_int_t nlayers, <b>double</b> lambdav);
</pre>
<a name=sub_spline2dbuildersetalgonaivells></a><h6 class=pageheader>spline2dbuildersetalgonaivells Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  allows  you to choose least squares solver used to perform
fitting. This function sets solver algorithm to &quot;NaiveLLS&quot;.

IMPORTANT: NaiveLLS is NOT intended to be used in  real  life  code!  This
           algorithm solves problem by generated dense (K^2)x(K^2+NPoints)
           matrix and solves  linear  least  squares  problem  with  dense
           solver.

           It is here just  to  test  BlockLLS  against  reference  solver
           (and maybe for someone trying to compare well optimized  solver
           against straightforward approach to the LLS problem).

More information on naive LLS solver:
* memory requirements: ~[8*K^4+256*NPoints] bytes for KxK grid.
* serial running time: O(K^6+NPoints) for KxK grid
* when compared with BlockLLS,  NaiveLLS  has ~K  larger memory demand and
  ~K^2  larger running time.

Inputs:
    S       -   spline 2D builder object
    LambdaNS-   nonsmoothness penalty
ALGLIB: Copyright 05.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetalgonaivells(spline2dbuilder state, <b>double</b> lambdans);
</pre>
<a name=sub_spline2dbuildersetarea></a><h6 class=pageheader>spline2dbuildersetarea Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  area  where  2D  spline  interpolant  is   built  to
user-defined one: [XA,XB]*[YA,YB]

Inputs:
    S       -   spline 2D builder object
    XA,XB   -   spatial extent in the first (X) dimension, XA &lt; XB
    YA,YB   -   spatial extent in the second (Y) dimension, YA &lt; YB
ALGLIB: Copyright 05.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetarea(spline2dbuilder state, <b>double</b> xa, <b>double</b> xb, <b>double</b> ya, <b>double</b> yb);
</pre>
<a name=sub_spline2dbuildersetareaauto></a><h6 class=pageheader>spline2dbuildersetareaauto Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets area where 2D spline interpolant is built. &quot;Auto&quot; means
that area extent is determined automatically from dataset extent.

Inputs:
    S       -   spline 2D builder object
ALGLIB: Copyright 05.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetareaauto(spline2dbuilder state);
</pre>
<a name=sub_spline2dbuildersetconstterm></a><h6 class=pageheader>spline2dbuildersetconstterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets constant prior term (model is a sum of  bicubic  spline
and global prior, which can be linear, constant, user-defined  constant or
zero).

Constant prior term is determined by least squares fitting.

Inputs:
    S       -   spline builder
ALGLIB: Copyright 01.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetconstterm(spline2dbuilder state);
</pre>
<a name=sub_spline2dbuildersetgrid></a><h6 class=pageheader>spline2dbuildersetgrid Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  nodes  count  for  2D spline interpolant. Fitting is
performed on area defined with one of the &quot;setarea&quot;  functions;  this  one
sets number of nodes placed upon the fitting area.

Inputs:
    S       -   spline 2D builder object
    KX      -   nodes count for the first (X) dimension; fitting  interval
                [XA,XB] is separated into KX-1 subintervals, with KX nodes
                created at the boundaries.
    KY      -   nodes count for the first (Y) dimension; fitting  interval
                [YA,YB] is separated into KY-1 subintervals, with KY nodes
                created at the boundaries.

NOTE: at  least  4  nodes  is  created in each dimension, so KX and KY are
      silently increased if needed.
ALGLIB: Copyright 05.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetgrid(spline2dbuilder state, ae_int_t kx, ae_int_t ky);
</pre>
<a name=sub_spline2dbuildersetlinterm></a><h6 class=pageheader>spline2dbuildersetlinterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets linear prior term (model is a sum of bicubic spline and
global  prior,  which  can  be  linear, constant, user-defined constant or
zero).

Linear prior term is determined by least squares fitting.

Inputs:
    S       -   spline builder
ALGLIB: Copyright 01.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetlinterm(spline2dbuilder state);
</pre>
<a name=sub_spline2dbuildersetpoints></a><h6 class=pageheader>spline2dbuildersetpoints Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function adds dataset to the builder object.

This function overrides results of the previous calls, i.e. multiple calls
of this function will result in only the last set being added.

Inputs:
    S       -   spline 2D builder object
    XY      -   points, array[N,2+D]. One  row  corresponds to  one  point
                in the dataset. First 2  elements  are  coordinates,  next
                D  elements are function values. Array may  be larger than
                specified, in  this  case  only leading [N,NX+NY] elements
                will be used.
    N       -   number of points in the dataset
ALGLIB: Copyright 05.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetpoints(spline2dbuilder state, real_2d_array xy, ae_int_t n);
</pre>
<a name=sub_spline2dbuildersetuserterm></a><h6 class=pageheader>spline2dbuildersetuserterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets constant prior term (model is a sum of  bicubic  spline
and global prior, which can be linear, constant, user-defined  constant or
zero).

Constant prior term is determined by least squares fitting.

Inputs:
    S       -   spline builder
    V       -   value for user-defined prior
ALGLIB: Copyright 01.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetuserterm(spline2dbuilder state, <b>double</b> v);
</pre>
<a name=sub_spline2dbuildersetzeroterm></a><h6 class=pageheader>spline2dbuildersetzeroterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets zero prior term (model is a sum of bicubic  spline  and
global  prior,  which  can  be  linear, constant, user-defined constant or
zero).

Inputs:
    S       -   spline builder
ALGLIB: Copyright 01.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dbuildersetzeroterm(spline2dbuilder state);
</pre>
<a name=sub_spline2dcalc></a><h6 class=pageheader>spline2dcalc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates the value of the bilinear or bicubic spline  at
the given point X.

Inputs:
    C   -   2D spline object.
            Built by spline2dbuildbilinearv or spline2dbuildbicubicv.
    X, Y-   point

Result:
    S(x,y)
ALGLIB Project: Copyright 05.07.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spline2dcalc(spline2dinterpolant c, <b>double</b> x, <b>double</b> y);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_bilinear class=nav>spline2d_bilinear</a> | <a href=#example_spline2d_bicubic class=nav>spline2d_bicubic</a> ]</p>
<a name=sub_spline2dcalcv></a><h6 class=pageheader>spline2dcalcv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates bilinear or bicubic vector-valued spline at the
given point (X,Y).

Inputs:
    C   -   spline interpolant.
    X, Y-   point

Outputs:
    F   -   array[D] which stores function values.  F is out-parameter and
            it  is  reallocated  after  call to this function. In case you
            want  to    reuse  previously  allocated  F,   you   may   use
            Spline2DCalcVBuf(),  which  reallocates  F only when it is too
            small.
ALGLIB Project: Copyright 16.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dcalcv(spline2dinterpolant c, <b>double</b> x, <b>double</b> y, real_1d_array &amp;f);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_vector class=nav>spline2d_vector</a> ]</p>
<a name=sub_spline2dcalcvbuf></a><h6 class=pageheader>spline2dcalcvbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates bilinear or bicubic vector-valued spline at the
given point (X,Y).

If you need just some specific component of vector-valued spline, you  can
use spline2dcalcvi() function.

Inputs:
    C   -   spline interpolant.
    X, Y-   point
    F   -   output buffer, possibly preallocated array. In case array size
            is large enough to store result, it is not reallocated.  Array
            which is too short will be reallocated

Outputs:
    F   -   array[D] (or larger) which stores function values
ALGLIB Project: Copyright 01.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dcalcvbuf(spline2dinterpolant c, <b>double</b> x, <b>double</b> y, real_1d_array &amp;f);
</pre>
<a name=sub_spline2dcalcvi></a><h6 class=pageheader>spline2dcalcvi Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates specific component of vector-valued bilinear or
bicubic spline at the given point (X,Y).

Inputs:
    C   -   spline interpolant.
    X, Y-   point
    I   -   component index, in [0,D). An exception is generated for out
            of range values.

Result:
    value of I-th component
ALGLIB Project: Copyright 01.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spline2dcalcvi(spline2dinterpolant c, <b>double</b> x, <b>double</b> y, ae_int_t i);
</pre>
<a name=sub_spline2dcopy></a><h6 class=pageheader>spline2dcopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine makes the copy of the spline model.

Inputs:
    C   -   spline interpolant

Outputs:
    CC  -   spline copy
ALGLIB Project: Copyright 29.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dcopy(spline2dinterpolant c, spline2dinterpolant &amp;cc);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_copytrans class=nav>spline2d_copytrans</a> ]</p>
<a name=sub_spline2ddiff></a><h6 class=pageheader>spline2ddiff Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates the value of the bilinear or bicubic spline  at
the given point X and its derivatives.

Inputs:
    C   -   spline interpolant.
    X, Y-   point

Outputs:
    F   -   S(x,y)
    FX  -   dS(x,y)/dX
    FY  -   dS(x,y)/dY
    FXY -   d2S(x,y)/dXdY
ALGLIB Project: Copyright 05.07.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2ddiff(spline2dinterpolant c, <b>double</b> x, <b>double</b> y, <b>double</b> &amp;f, <b>double</b> &amp;fx, <b>double</b> &amp;fy, <b>double</b> &amp;fxy);
</pre>
<a name=sub_spline2ddiffvi></a><h6 class=pageheader>spline2ddiffvi Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates value of  specific  component  of  bilinear  or
bicubic vector-valued spline and its derivatives.

Inputs:
    C   -   spline interpolant.
    X, Y-   point
    I   -   component index, in [0,D)

Outputs:
    F   -   S(x,y)
    FX  -   dS(x,y)/dX
    FY  -   dS(x,y)/dY
    FXY -   d2S(x,y)/dXdY
ALGLIB Project: Copyright 05.07.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2ddiffvi(spline2dinterpolant c, <b>double</b> x, <b>double</b> y, ae_int_t i, <b>double</b> &amp;f, <b>double</b> &amp;fx, <b>double</b> &amp;fy, <b>double</b> &amp;fxy);
</pre>
<a name=sub_spline2dfit></a><h6 class=pageheader>spline2dfit Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function fits bicubic spline to current dataset, using current  area/
grid and current LLS solver.

Inputs:
    State   -   spline 2D builder object

Outputs:
    S       -   2D spline, fit result
    Rep     -   fitting report, which provides some additional info  about
                errors, R2 coefficient and so on.
ALGLIB: Copyright 05.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dfit(spline2dbuilder state, spline2dinterpolant &amp;s, spline2dfitreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_fit_blocklls class=nav>spline2d_fit_blocklls</a> ]</p>
<a name=sub_spline2dlintransf></a><h6 class=pageheader>spline2dlintransf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine performs linear transformation of the spline.

Inputs:
    C   -   spline interpolant.
    A, B-   transformation coefficients: S2(x,y) = A*S(x,y) + B

Outputs:
    C   -   transformed spline
ALGLIB Project: Copyright 30.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dlintransf(spline2dinterpolant c, <b>double</b> a, <b>double</b> b);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_copytrans class=nav>spline2d_copytrans</a> ]</p>
<a name=sub_spline2dlintransxy></a><h6 class=pageheader>spline2dlintransxy Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine performs linear transformation of the spline argument.

Inputs:
    C       -   spline interpolant
    AX, BX  -   transformation coefficients: x = A*t + B
    AY, BY  -   transformation coefficients: y = A*u + B
Result:
    C   -   transformed spline
ALGLIB Project: Copyright 30.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dlintransxy(spline2dinterpolant c, <b>double</b> ax, <b>double</b> bx, <b>double</b> ay, <b>double</b> by);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_copytrans class=nav>spline2d_copytrans</a> ]</p>
<a name=sub_spline2dresamplebicubic></a><h6 class=pageheader>spline2dresamplebicubic Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bicubic spline resampling

Inputs:
    A           -   function values at the old grid,
                    array[0..OldHeight-1, 0..OldWidth-1]
    OldHeight   -   old grid height, OldHeight &gt; 1
    OldWidth    -   old grid width, OldWidth &gt; 1
    NewHeight   -   new grid height, NewHeight &gt; 1
    NewWidth    -   new grid width, NewWidth &gt; 1

Outputs:
    B           -   function values at the new grid,
                    array[0..NewHeight-1, 0..NewWidth-1]
ALGLIB Routine: Copyright 2007 May 15 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dresamplebicubic(real_2d_array a, ae_int_t oldheight, ae_int_t oldwidth, real_2d_array &amp;b, ae_int_t newheight, ae_int_t newwidth);
</pre>
<a name=sub_spline2dresamplebilinear></a><h6 class=pageheader>spline2dresamplebilinear Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bilinear spline resampling

Inputs:
    A           -   function values at the old grid,
                    array[0..OldHeight-1, 0..OldWidth-1]
    OldHeight   -   old grid height, OldHeight &gt; 1
    OldWidth    -   old grid width, OldWidth &gt; 1
    NewHeight   -   new grid height, NewHeight &gt; 1
    NewWidth    -   new grid width, NewWidth &gt; 1

Outputs:
    B           -   function values at the new grid,
                    array[0..NewHeight-1, 0..NewWidth-1]
ALGLIB Routine: Copyright 09.07.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dresamplebilinear(real_2d_array a, ae_int_t oldheight, ae_int_t oldwidth, real_2d_array &amp;b, ae_int_t newheight, ae_int_t newwidth);
</pre>
<a name=sub_spline2dserialize></a><h6 class=pageheader>spline2dserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: serialization
These functions serialize a data structure to a C++ string or stream.
* serialization can be freely moved across 32-bit and 64-bit systems,
  and different byte orders. For example, you can serialize a string
  on a SPARC and unserialize it on an x86.
* ALGLIB++ serialization is compatible with serialization in ALGLIB,
  in both directions.
Important properties of s_out:
* it contains alphanumeric characters, dots, underscores, minus signs
* these symbols are grouped into words, which are separated by spaces
  and Windows-style (CR+LF) newlines
ALGLIB: Copyright 28.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dserialize(spline2dinterpolant &amp;obj, std::string &amp;s_out);
<b>void</b> spline2dserialize(spline2dinterpolant &amp;obj, std::ostream &amp;s_out);
</pre>
<a name=sub_spline2dunpack></a><h6 class=pageheader>spline2dunpack Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine was deprecated in ALGLIB 3.6.0

We recommend you to switch  to  Spline2DUnpackV(),  which is more flexible
and accepts its arguments in more convenient order.
ALGLIB Project: Copyright 29.06.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dunpack(spline2dinterpolant c, ae_int_t &amp;m, ae_int_t &amp;n, real_2d_array &amp;tbl);
</pre>
<a name=sub_spline2dunpackv></a><h6 class=pageheader>spline2dunpackv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine unpacks two-dimensional spline into the coefficients table

Inputs:
    C   -   spline interpolant.

Result:
    M, N-   grid size (x-axis and y-axis)
    D   -   number of components
    Tbl -   coefficients table, unpacked format,
            D - components: [0..(N-1)*(M-1)*D-1, 0..19].
            For T=0..D-1 (component index), I = 0...N-2 (x index),
            J=0..M-2 (y index):
                K :=  T + I*D + J*D*(N-1)

                K-th row stores decomposition for T-th component of the
                vector-valued function

                Tbl[K,0] = X[i]
                Tbl[K,1] = X[i+1]
                Tbl[K,2] = Y[j]
                Tbl[K,3] = Y[j+1]
                Tbl[K,4] = C00
                Tbl[K,5] = C01
                Tbl[K,6] = C02
                Tbl[K,7] = C03
                Tbl[K,8] = C10
                Tbl[K,9] = C11
                ...
                Tbl[K,19] = C33
            On each grid square spline is equals to:
                S(x) = SUM(c[i,j]*(t^i)*(u^j), i=0..3, j=0..3)
                t = x-x[j]
                u = y-y[i]
ALGLIB Project: Copyright 16.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dunpackv(spline2dinterpolant c, ae_int_t &amp;m, ae_int_t &amp;n, ae_int_t &amp;d, real_2d_array &amp;tbl);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline2d_unpack class=nav>spline2d_unpack</a> ]</p>
<a name=sub_spline2dunserialize></a><h6 class=pageheader>spline2dunserialize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Serializer: unserialization
These functions unserialize a data structure from a C++ string or stream.
Important properties of s_in:
* any combination of spaces, tabs, Windows or Unix stype newlines can
  be used as separators, so as to allow flexible reformatting of the
  stream or string from text or XML files.
* But you should not insert separators into the middle of the "words"
  nor you should change case of letters.
ALGLIB: Copyright 28.02.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline2dunserialize(<b>const</b> std::string &amp;s_in, spline2dinterpolant &amp;obj);
<b>void</b> spline2dunserialize(<b>const</b> std::istream &amp;s_in, spline2dinterpolant &amp;obj);
</pre>
<a name=example_spline2d_bicubic></a><h6 class=pageheader>spline2d_bicubic Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use bilinear spline to interpolate f(x,y)=x^2+2*y^2 sampled </font>
<font color=navy>// at (x,y) from [0.0, 0.5, 1.0] X [0.0, 1.0].</font>
   real_1d_array x = <font color=blue><b>&quot;[0.0, 0.5, 1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array f = <font color=blue><b>&quot;[0.00,0.25,1.00,2.00,2.25,3.00]&quot;</b></font>;
   <b>double</b> vx = 0.25;
   <b>double</b> vy = 0.50;
   <b>double</b> v;
   <b>double</b> dx;
   <b>double</b> dy;
   <b>double</b> dxy;
   spline2dinterpolant s;

<font color=navy>// build spline</font>
   spline2dbuildbicubicv(x, 3, y, 2, f, 1, s);

<font color=navy>// calculate S(0.25,0.50)</font>
   v = spline2dcalc(s, vx, vy);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.0625</font>

<font color=navy>// calculate derivatives</font>
   spline2ddiff(s, vx, vy, v, dx, dy, dxy);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.0625</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(dx)); <font color=navy>// EXPECTED: 0.5000</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(dy)); <font color=navy>// EXPECTED: 2.0000</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline2d_bilinear></a><h6 class=pageheader>spline2d_bilinear Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use bilinear spline to interpolate f(x,y)=x^2+2*y^2 sampled </font>
<font color=navy>// at (x,y) from [0.0, 0.5, 1.0] X [0.0, 1.0].</font>
   real_1d_array x = <font color=blue><b>&quot;[0.0, 0.5, 1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array f = <font color=blue><b>&quot;[0.00,0.25,1.00,2.00,2.25,3.00]&quot;</b></font>;
   <b>double</b> vx = 0.25;
   <b>double</b> vy = 0.50;
   <b>double</b> v;
   spline2dinterpolant s;

<font color=navy>// build spline</font>
   spline2dbuildbilinearv(x, 3, y, 2, f, 1, s);

<font color=navy>// calculate S(0.25,0.50)</font>
   v = spline2dcalc(s, vx, vy);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.1250</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline2d_copytrans></a><h6 class=pageheader>spline2d_copytrans Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We build bilinear spline <b>for</b> f(x,y)=x+2*y <b>for</b> (x,y) in [0,1].</font>
<font color=navy>// Then we apply several transformations to this spline.</font>
   real_1d_array x = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array f = <font color=blue><b>&quot;[0.00,1.00,2.00,3.00]&quot;</b></font>;
   spline2dinterpolant s;
   spline2dinterpolant snew;
   <b>double</b> v;
   spline2dbuildbilinearv(x, 2, y, 2, f, 1, s);

<font color=navy>// copy spline, apply transformation x:=2*xnew, y:=4*ynew</font>
<font color=navy>// evaluate at (xnew,ynew) = (0.25,0.25) - should be same as (x,y)=(0.5,1.0)</font>
   spline2dcopy(s, snew);
   spline2dlintransxy(snew, 2.0, 0.0, 4.0, 0.0);
   v = spline2dcalc(snew, 0.25, 0.25);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.500</font>

<font color=navy>// copy spline, apply transformation SNew:=2*S+3</font>
   spline2dcopy(s, snew);
   spline2dlintransf(snew, 2.0, 3.0);
   v = spline2dcalc(snew, 0.5, 1.0);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 8.000</font>
<font color=navy>//</font>
<font color=navy>// Same example, but <b>for</b> vector spline (f0,f1) = {x+2*y, 2*x+y}</font>
   real_1d_array f2 = <font color=blue><b>&quot;[0.00,0.00, 1.00,2.00, 2.00,1.00, 3.00,3.00]&quot;</b></font>;
   real_1d_array vr;
   spline2dbuildbilinearv(x, 2, y, 2, f2, 2, s);

<font color=navy>// copy spline, apply transformation x:=2*xnew, y:=4*ynew</font>
   spline2dcopy(s, snew);
   spline2dlintransxy(snew, 2.0, 0.0, 4.0, 0.0);
   spline2dcalcv(snew, 0.25, 0.25, vr);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, vr.tostring(4).c_str()); <font color=navy>// EXPECTED: [2.500,2.000]</font>

<font color=navy>// copy spline, apply transformation SNew:=2*S+3</font>
   spline2dcopy(s, snew);
   spline2dlintransf(snew, 2.0, 3.0);
   spline2dcalcv(snew, 0.5, 1.0, vr);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, vr.tostring(4).c_str()); <font color=navy>// EXPECTED: [8.000,7.000]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline2d_fit_blocklls></a><h6 class=pageheader>spline2d_fit_blocklls Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use bicubic spline to reproduce f(x,y)=1/(1+x^2+2*y^2) sampled</font>
<font color=navy>// at irregular points (x,y) from [-1,+1]*[-1,+1]</font>
<font color=navy>//</font>
<font color=navy>// We have 5 such points, located approximately at corners of the area</font>
<font color=navy>// and its center -  but not exactly at the grid. Thus, we have to FIT</font>
<font color=navy>// the spline, i.e. to solve least squares problem</font>
   real_2d_array xy = <font color=blue><b>&quot;[[-0.987,-0.902,0.359],[0.948,-0.992,0.347],[-1.000,1.000,0.333],[1.000,0.973,0.339],[0.017,0.180,0.968]]&quot;</b></font>;
<font color=navy>// First step is to create spline2dbuilder object and set its properties:</font>
<font color=navy>// * d=1 means that we create vector-valued spline with 1 component</font>
<font color=navy>// * we specify dataset xy</font>
<font color=navy>// * we rely on automatic selection of interpolation area</font>
<font color=navy>// * we tell builder that we want to use 5x5 grid <b>for</b> an underlying spline</font>
<font color=navy>// * we choose least squares solver named BlockLLS and configure it by</font>
<font color=navy>//   telling that we want to apply zero nonlinearity penalty.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: you can specify non-zero lambdav <b>if</b> you want to make your spline</font>
<font color=navy>//       more <font color=blue><b>&quot;rigid&quot;</b></font>, i.e. to penalize nonlinearity.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: ALGLIB has two solvers which fit bicubic splines to irregular data,</font>
<font color=navy>//       one of them is BlockLLS and another one is FastDDM. Former is</font>
<font color=navy>//       intended <b>for</b> moderately sized grids (up to 512x512 nodes, although</font>
<font color=navy>//       it may take up to few minutes); it is the most easy to use and</font>
<font color=navy>//       control spline fitting function in the library. Latter, FastDDM,</font>
<font color=navy>//       is intended <b>for</b> efficient solution of large-scale problems</font>
<font color=navy>//       (up to 100.000.000 nodes). Both solvers can be parallelized, but</font>
<font color=navy>//       FastDDM is much more efficient. See comments <b>for</b> more information.</font>
   spline2dbuilder builder;
   ae_int_t d = 1;
   <b>double</b> lambdav = 0.000;
   spline2dbuildercreate(d, builder);
   spline2dbuildersetpoints(builder, xy, 5);
   spline2dbuildersetgrid(builder, 5, 5);
   spline2dbuildersetalgoblocklls(builder, lambdav);
<font color=navy>// Now we are ready to fit and evaluate our results</font>
   spline2dinterpolant s;
   spline2dfitreport rep;
   spline2dfit(builder, s, rep);

<font color=navy>// evaluate results - function value at the grid is reproduced exactly</font>
   <b>double</b> v;
   v = spline2dcalc(s, -1, 1);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.333000</font>

<font color=navy>// check maximum error - it must be nearly zero</font>
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(rep.maxerror)); <font color=navy>// EXPECTED: 0.000</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline2d_unpack></a><h6 class=pageheader>spline2d_unpack Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We build bilinear spline <b>for</b> f(x,y)=x+2*y+3*xy <b>for</b> (x,y) in [0,1].</font>
<font color=navy>// Then we demonstrate how to unpack it.</font>
   real_1d_array x = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array f = <font color=blue><b>&quot;[0.00,1.00,2.00,6.00]&quot;</b></font>;
   real_2d_array c;
   ae_int_t m;
   ae_int_t n;
   ae_int_t d;
   spline2dinterpolant s;

<font color=navy>// build spline</font>
   spline2dbuildbilinearv(x, 2, y, 2, f, 1, s);

<font color=navy>// unpack and test</font>
   spline2dunpackv(s, m, n, d, c);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(4).c_str()); <font color=navy>// EXPECTED: [[0, 1, 0, 1, 0,2,0,0, 1,3,0,0, 0,0,0,0, 0,0,0,0 ]]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline2d_vector></a><h6 class=pageheader>spline2d_vector Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We build bilinear vector-valued spline (f0,f1) = {x+2*y, 2*x+y}</font>
<font color=navy>// Spline is built using function values at 2x2 grid: (x,y)=[0,1]*[0,1]</font>
<font color=navy>// Then we perform evaluation at (x,y)=(0.1,0.3)</font>
   real_1d_array x = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array f = <font color=blue><b>&quot;[0.00,0.00, 1.00,2.00, 2.00,1.00, 3.00,3.00]&quot;</b></font>;
   spline2dinterpolant s;
   real_1d_array vr;
   spline2dbuildbilinearv(x, 2, y, 2, f, 2, s);
   spline2dcalcv(s, 0.1, 0.3, vr);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, vr.tostring(4).c_str()); <font color=navy>// EXPECTED: [0.700,0.500]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_spline3d></a><h4 class=pageheader>8.6.11. spline3d Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_spline3dinterpolant class=toc>spline3dinterpolant</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_spline3dbuildtrilinearv class=toc>spline3dbuildtrilinearv</a> |
<a href=#sub_spline3dcalc class=toc>spline3dcalc</a> |
<a href=#sub_spline3dcalcv class=toc>spline3dcalcv</a> |
<a href=#sub_spline3dcalcvbuf class=toc>spline3dcalcvbuf</a> |
<a href=#sub_spline3dlintransf class=toc>spline3dlintransf</a> |
<a href=#sub_spline3dlintransxyz class=toc>spline3dlintransxyz</a> |
<a href=#sub_spline3dresampletrilinear class=toc>spline3dresampletrilinear</a> |
<a href=#sub_spline3dunpackv class=toc>spline3dunpackv</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_spline3d_trilinear class=toc>spline3d_trilinear</a></td><td width=15>&nbsp;</td><td>Trilinear spline interpolation</td></tr>
<tr align=left valign=top><td><a href=#example_spline3d_vector class=toc>spline3d_vector</a></td><td width=15>&nbsp;</td><td>Vector-valued trilinear spline interpolation</td></tr>
</table>
</div>
<a name=struct_spline3dinterpolant></a><h6 class=pageheader>spline3dinterpolant Class</h6>
<hr width=600 align=left>
<pre class=narration>
3-dimensional spline inteprolant
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> spline3dinterpolant {
};
</pre>
<a name=sub_spline3dbuildtrilinearv></a><h6 class=pageheader>spline3dbuildtrilinearv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine builds trilinear vector-valued spline.

Inputs:
    X   -   spline abscissas,  array[0..N-1]
    Y   -   spline ordinates,  array[0..M-1]
    Z   -   spline applicates, array[0..L-1]
    F   -   function values, array[0..M*N*L*D-1]:
            * first D elements store D values at (X[0],Y[0],Z[0])
            * next D elements store D values at (X[1],Y[0],Z[0])
            * next D elements store D values at (X[2],Y[0],Z[0])
            * ...
            * next D elements store D values at (X[0],Y[1],Z[0])
            * next D elements store D values at (X[1],Y[1],Z[0])
            * next D elements store D values at (X[2],Y[1],Z[0])
            * ...
            * next D elements store D values at (X[0],Y[0],Z[1])
            * next D elements store D values at (X[1],Y[0],Z[1])
            * next D elements store D values at (X[2],Y[0],Z[1])
            * ...
            * general form - D function values at (X[i],Y[j]) are stored
              at F[D*(N*(M*K+J)+I)...D*(N*(M*K+J)+I)+D-1].
    M,N,
    L   -   grid size, M &ge; 2, N &ge; 2, L &ge; 2
    D   -   vector dimension, D &ge; 1

Outputs:
    C   -   spline interpolant
ALGLIB Project: Copyright 26.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline3dbuildtrilinearv(real_1d_array x, ae_int_t n, real_1d_array y, ae_int_t m, real_1d_array z, ae_int_t l, real_1d_array f, ae_int_t d, spline3dinterpolant &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline3d_trilinear class=nav>spline3d_trilinear</a> | <a href=#example_spline3d_vector class=nav>spline3d_vector</a> ]</p>
<a name=sub_spline3dcalc></a><h6 class=pageheader>spline3dcalc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates the value of the trilinear or tricubic spline at
the given point (X,Y,Z).

Inputs:
    C   -   coefficients table.
            Built by BuildBilinearSpline or BuildBicubicSpline.
    X, Y,
    Z   -   point

Result:
    S(x,y,z)
ALGLIB Project: Copyright 26.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spline3dcalc(spline3dinterpolant c, <b>double</b> x, <b>double</b> y, <b>double</b> z);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline3d_trilinear class=nav>spline3d_trilinear</a> ]</p>
<a name=sub_spline3dcalcv></a><h6 class=pageheader>spline3dcalcv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates trilinear or tricubic vector-valued spline at the
given point (X,Y,Z).

Inputs:
    C   -   spline interpolant.
    X, Y,
    Z   -   point

Outputs:
    F   -   array[D] which stores function values.  F is out-parameter and
            it  is  reallocated  after  call to this function. In case you
            want  to    reuse  previously  allocated  F,   you   may   use
            Spline2DCalcVBuf(),  which  reallocates  F only when it is too
            small.
ALGLIB Project: Copyright 26.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline3dcalcv(spline3dinterpolant c, <b>double</b> x, <b>double</b> y, <b>double</b> z, real_1d_array &amp;f);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_spline3d_vector class=nav>spline3d_vector</a> ]</p>
<a name=sub_spline3dcalcvbuf></a><h6 class=pageheader>spline3dcalcvbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates bilinear or bicubic vector-valued spline at the
given point (X,Y,Z).

Inputs:
    C   -   spline interpolant.
    X, Y,
    Z   -   point
    F   -   output buffer, possibly preallocated array. In case array size
            is large enough to store result, it is not reallocated.  Array
            which is too short will be reallocated

Outputs:
    F   -   array[D] (or larger) which stores function values
ALGLIB Project: Copyright 26.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline3dcalcvbuf(spline3dinterpolant c, <b>double</b> x, <b>double</b> y, <b>double</b> z, real_1d_array &amp;f);
</pre>
<a name=sub_spline3dlintransf></a><h6 class=pageheader>spline3dlintransf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine performs linear transformation of the spline.

Inputs:
    C   -   spline interpolant.
    A, B-   transformation coefficients: S2(x,y) = A*S(x,y,z) + B

Outputs:
    C   -   transformed spline
ALGLIB Project: Copyright 26.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline3dlintransf(spline3dinterpolant c, <b>double</b> a, <b>double</b> b);
</pre>
<a name=sub_spline3dlintransxyz></a><h6 class=pageheader>spline3dlintransxyz Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine performs linear transformation of the spline argument.

Inputs:
    C       -   spline interpolant
    AX, BX  -   transformation coefficients: x = A*u + B
    AY, BY  -   transformation coefficients: y = A*v + B
    AZ, BZ  -   transformation coefficients: z = A*w + B

Outputs:
    C   -   transformed spline
ALGLIB Project: Copyright 26.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline3dlintransxyz(spline3dinterpolant c, <b>double</b> ax, <b>double</b> bx, <b>double</b> ay, <b>double</b> by, <b>double</b> az, <b>double</b> bz);
</pre>
<a name=sub_spline3dresampletrilinear></a><h6 class=pageheader>spline3dresampletrilinear Function</h6>
<hr width=600 align=left>
<pre class=narration>
Trilinear spline resampling

Inputs:
    A           -   array[0..OldXCount*OldYCount*OldZCount-1], function
                    values at the old grid, :
                        A[0]        x=0,y=0,z=0
                        A[1]        x=1,y=0,z=0
                        A[..]       ...
                        A[..]       x=oldxcount-1,y=0,z=0
                        A[..]       x=0,y=1,z=0
                        A[..]       ...
                        ...
    OldZCount   -   old Z-count, OldZCount &gt; 1
    OldYCount   -   old Y-count, OldYCount &gt; 1
    OldXCount   -   old X-count, OldXCount &gt; 1
    NewZCount   -   new Z-count, NewZCount &gt; 1
    NewYCount   -   new Y-count, NewYCount &gt; 1
    NewXCount   -   new X-count, NewXCount &gt; 1

Outputs:
    B           -   array[0..NewXCount*NewYCount*NewZCount-1], function
                    values at the new grid:
                        B[0]        x=0,y=0,z=0
                        B[1]        x=1,y=0,z=0
                        B[..]       ...
                        B[..]       x=newxcount-1,y=0,z=0
                        B[..]       x=0,y=1,z=0
                        B[..]       ...
                        ...
ALGLIB Routine: Copyright 26.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline3dresampletrilinear(real_1d_array a, ae_int_t oldzcount, ae_int_t oldycount, ae_int_t oldxcount, ae_int_t newzcount, ae_int_t newycount, ae_int_t newxcount, real_1d_array &amp;b);
</pre>
<a name=sub_spline3dunpackv></a><h6 class=pageheader>spline3dunpackv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine unpacks tri-dimensional spline into the coefficients table

Inputs:
    C   -   spline interpolant.

Result:
    N   -   grid size (X)
    M   -   grid size (Y)
    L   -   grid size (Z)
    D   -   number of components
    SType-  spline type. Currently, only one spline type is supported:
            trilinear spline, as indicated by SType=1.
    Tbl -   spline coefficients: [0..(N-1)*(M-1)*(L-1)*D-1, 0..13].
            For T=0..D-1 (component index), I = 0...N-2 (x index),
            J=0..M-2 (y index), K=0..L-2 (z index):
                Q := T + I*D + J*D*(N-1) + K*D*(N-1)*(M-1),

                Q-th row stores decomposition for T-th component of the
                vector-valued function

                Tbl[Q,0] = X[i]
                Tbl[Q,1] = X[i+1]
                Tbl[Q,2] = Y[j]
                Tbl[Q,3] = Y[j+1]
                Tbl[Q,4] = Z[k]
                Tbl[Q,5] = Z[k+1]

                Tbl[Q,6] = C000
                Tbl[Q,7] = C100
                Tbl[Q,8] = C010
                Tbl[Q,9] = C110
                Tbl[Q,10]= C001
                Tbl[Q,11]= C101
                Tbl[Q,12]= C011
                Tbl[Q,13]= C111
            On each grid square spline is equals to:
                S(x) = SUM(c[i,j,k]*(x^i)*(y^j)*(z^k), i=0..1, j=0..1, k=0..1)
                t = x-x[j]
                u = y-y[i]
                v = z-z[k]

            NOTE: format of Tbl is given for SType=1. Future versions of
                  ALGLIB can use different formats for different values of
                  SType.
ALGLIB Project: Copyright 26.04.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spline3dunpackv(spline3dinterpolant c, ae_int_t &amp;n, ae_int_t &amp;m, ae_int_t &amp;l, ae_int_t &amp;d, ae_int_t &amp;stype, real_2d_array &amp;tbl);
</pre>
<a name=example_spline3d_trilinear></a><h6 class=pageheader>spline3d_trilinear Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use trilinear spline to interpolate f(x,y,z)=x+xy+z sampled </font>
<font color=navy>// at (x,y,z) from [0.0, 1.0] X [0.0, 1.0] X [0.0, 1.0].</font>
<font color=navy>//</font>
<font color=navy>// We store x, y and z-values at local arrays with same names.</font>
<font color=navy>// Function values are stored in the array F as follows:</font>
<font color=navy>//     f[0]     (x,y,z) = (0,0,0)</font>
<font color=navy>//     f[1]     (x,y,z) = (1,0,0)</font>
<font color=navy>//     f[2]     (x,y,z) = (0,1,0)</font>
<font color=navy>//     f[3]     (x,y,z) = (1,1,0)</font>
<font color=navy>//     f[4]     (x,y,z) = (0,0,1)</font>
<font color=navy>//     f[5]     (x,y,z) = (1,0,1)</font>
<font color=navy>//     f[6]     (x,y,z) = (0,1,1)</font>
<font color=navy>//     f[7]     (x,y,z) = (1,1,1)</font>
   real_1d_array x = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array z = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array f = <font color=blue><b>&quot;[0,1,0,2,1,2,1,3]&quot;</b></font>;
   <b>double</b> vx = 0.50;
   <b>double</b> vy = 0.50;
   <b>double</b> vz = 0.50;
   <b>double</b> v;
   spline3dinterpolant s;

<font color=navy>// build spline</font>
   spline3dbuildtrilinearv(x, 2, y, 2, z, 2, f, 1, s);

<font color=navy>// calculate S(0.5,0.5,0.5)</font>
   v = spline3dcalc(s, vx, vy, vz);
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.2500</font>
   <b>return</b> 0;
}
</pre>
<a name=example_spline3d_vector></a><h6 class=pageheader>spline3d_vector Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Interpolation.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We use trilinear vector-valued spline to interpolate {f0,f1}={x+xy+z,x+xy+yz+z}</font>
<font color=navy>// sampled at (x,y,z) from [0.0, 1.0] X [0.0, 1.0] X [0.0, 1.0].</font>
<font color=navy>//</font>
<font color=navy>// We store x, y and z-values at local arrays with same names.</font>
<font color=navy>// Function values are stored in the array F as follows:</font>
<font color=navy>//     f[0]     f0, (x,y,z) = (0,0,0)</font>
<font color=navy>//     f[1]     f1, (x,y,z) = (0,0,0)</font>
<font color=navy>//     f[2]     f0, (x,y,z) = (1,0,0)</font>
<font color=navy>//     f[3]     f1, (x,y,z) = (1,0,0)</font>
<font color=navy>//     f[4]     f0, (x,y,z) = (0,1,0)</font>
<font color=navy>//     f[5]     f1, (x,y,z) = (0,1,0)</font>
<font color=navy>//     f[6]     f0, (x,y,z) = (1,1,0)</font>
<font color=navy>//     f[7]     f1, (x,y,z) = (1,1,0)</font>
<font color=navy>//     f[8]     f0, (x,y,z) = (0,0,1)</font>
<font color=navy>//     f[9]     f1, (x,y,z) = (0,0,1)</font>
<font color=navy>//     f[10]    f0, (x,y,z) = (1,0,1)</font>
<font color=navy>//     f[11]    f1, (x,y,z) = (1,0,1)</font>
<font color=navy>//     f[12]    f0, (x,y,z) = (0,1,1)</font>
<font color=navy>//     f[13]    f1, (x,y,z) = (0,1,1)</font>
<font color=navy>//     f[14]    f0, (x,y,z) = (1,1,1)</font>
<font color=navy>//     f[15]    f1, (x,y,z) = (1,1,1)</font>
   real_1d_array x = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array z = <font color=blue><b>&quot;[0.0, 1.0]&quot;</b></font>;
   real_1d_array f = <font color=blue><b>&quot;[0,0, 1,1, 0,0, 2,2, 1,1, 2,2, 1,2, 3,4]&quot;</b></font>;
   <b>double</b> vx = 0.50;
   <b>double</b> vy = 0.50;
   <b>double</b> vz = 0.50;
   spline3dinterpolant s;

<font color=navy>// build spline</font>
   spline3dbuildtrilinearv(x, 2, y, 2, z, 2, f, 2, s);

<font color=navy>// calculate S(0.5,0.5,0.5) - we have vector of values instead of single value</font>
   real_1d_array v;
   spline3dcalcv(s, vx, vy, vz, v);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, v.tostring(4).c_str()); <font color=navy>// EXPECTED: [1.2500,1.5000]</font>
   <b>return</b> 0;
}
</pre>
</p>
<p>
<a name=pck_LinAlg class=sheader></a><h3>8.7. LinAlg Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_ablas class=toc>ablas</a></td><td>Level 2 and Level 3 BLAS operations</td></tr>
<tr align=left valign=top><td><a href=#unit_bdsvd class=toc>bdsvd</a></td><td>Bidiagonal SVD</td></tr>
<tr align=left valign=top><td><a href=#unit_evd class=toc>evd</a></td><td>Direct and iterative eigensolvers</td></tr>
<tr align=left valign=top><td><a href=#unit_inverseupdate class=toc>inverseupdate</a></td><td>Sherman-Morrison update of the inverse matrix</td></tr>
<tr align=left valign=top><td><a href=#unit_matdet class=toc>matdet</a></td><td>Determinant calculation</td></tr>
<tr align=left valign=top><td><a href=#unit_matgen class=toc>matgen</a></td><td>Random matrix generation</td></tr>
<tr align=left valign=top><td><a href=#unit_matinv class=toc>matinv</a></td><td>Matrix inverse</td></tr>
<tr align=left valign=top><td><a href=#unit_normestimator class=toc>normestimator</a></td><td>Estimates norm of the sparse matrix (from below)</td></tr>
<tr align=left valign=top><td><a href=#unit_ortfac class=toc>ortfac</a></td><td>Real/complex QR/LQ, bi(tri)diagonal, Hessenberg decompositions</td></tr>
<tr align=left valign=top><td><a href=#unit_rcond class=toc>rcond</a></td><td>Condition number estimate</td></tr>
<tr align=left valign=top><td><a href=#unit_schur class=toc>schur</a></td><td>Schur decomposition</td></tr>
<tr align=left valign=top><td><a href=#unit_sparse class=toc>sparse</a></td><td>Sparse matrices</td></tr>
<tr align=left valign=top><td><a href=#unit_spdgevd class=toc>spdgevd</a></td><td>Generalized symmetric eigensolver</td></tr>
<tr align=left valign=top><td><a href=#unit_svd class=toc>svd</a></td><td>Singular value decomposition</td></tr>
<tr align=left valign=top><td><a href=#unit_trfac class=toc>trfac</a></td><td>LU and Cholesky decompositions (dense and sparse)</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_ablas></a><h4 class=pageheader>8.7.1. ablas Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_cmatrixcopy class=toc>cmatrixcopy</a> |
<a href=#sub_cmatrixgemm class=toc>cmatrixgemm</a> |
<a href=#sub_cmatrixherk class=toc>cmatrixherk</a> |
<a href=#sub_cmatrixlefttrsm class=toc>cmatrixlefttrsm</a> |
<a href=#sub_cmatrixmv class=toc>cmatrixmv</a> |
<a href=#sub_cmatrixrank1 class=toc>cmatrixrank1</a> |
<a href=#sub_cmatrixrighttrsm class=toc>cmatrixrighttrsm</a> |
<a href=#sub_cmatrixsyrk class=toc>cmatrixsyrk</a> |
<a href=#sub_cmatrixtranspose class=toc>cmatrixtranspose</a> |
<a href=#sub_rmatrixcopy class=toc>rmatrixcopy</a> |
<a href=#sub_rmatrixenforcesymmetricity class=toc>rmatrixenforcesymmetricity</a> |
<a href=#sub_rmatrixgemm class=toc>rmatrixgemm</a> |
<a href=#sub_rmatrixgemv class=toc>rmatrixgemv</a> |
<a href=#sub_rmatrixgencopy class=toc>rmatrixgencopy</a> |
<a href=#sub_rmatrixger class=toc>rmatrixger</a> |
<a href=#sub_rmatrixlefttrsm class=toc>rmatrixlefttrsm</a> |
<a href=#sub_rmatrixmv class=toc>rmatrixmv</a> |
<a href=#sub_rmatrixrank1 class=toc>rmatrixrank1</a> |
<a href=#sub_rmatrixrighttrsm class=toc>rmatrixrighttrsm</a> |
<a href=#sub_rmatrixsymv class=toc>rmatrixsymv</a> |
<a href=#sub_rmatrixsyrk class=toc>rmatrixsyrk</a> |
<a href=#sub_rmatrixsyvmv class=toc>rmatrixsyvmv</a> |
<a href=#sub_rmatrixtranspose class=toc>rmatrixtranspose</a> |
<a href=#sub_rmatrixtrsv class=toc>rmatrixtrsv</a> |
<a href=#sub_rvectorcopy class=toc>rvectorcopy</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_ablas_d_gemm class=toc>ablas_d_gemm</a></td><td width=15>&nbsp;</td><td>Matrix multiplication (single-threaded)</td></tr>
<tr align=left valign=top><td><a href=#example_ablas_d_syrk class=toc>ablas_d_syrk</a></td><td width=15>&nbsp;</td><td>Symmetric rank-K update (single-threaded)</td></tr>
</table>
</div>
<a name=sub_cmatrixcopy></a><h6 class=pageheader>cmatrixcopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Copy

Inputs:
    M   -   number of rows
    N   -   number of columns
    A   -   source matrix, MxN submatrix is copied and transposed
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    B   -   destination matrix, must be large enough to store result
    IB  -   submatrix offset (row index)
    JB  -   submatrix offset (column index)
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixcopy(ae_int_t m, ae_int_t n, complex_2d_array a, ae_int_t ia, ae_int_t ja, complex_2d_array &amp;b, ae_int_t ib, ae_int_t jb);
</pre>
<a name=sub_cmatrixgemm></a><h6 class=pageheader>cmatrixgemm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates C = alpha*op1(A)*op2(B) +beta*C where:
* C is MxN general matrix
* op1(A) is MxK matrix
* op2(B) is KxN matrix
* &quot;op&quot; may be identity transformation, transposition, conjugate transposition

Additional info:
* cache-oblivious algorithm is used.
* multiplication result replaces C. If Beta=0, C elements are not used in
  calculations (not multiplied by zero - just not referenced)
* if Alpha=0, A is not used (not multiplied by zero - just not referenced)
* if both Beta and Alpha are zero, C is filled by zeros.

IMPORTANT:

This function does NOT preallocate output matrix C, it MUST be preallocated
by caller prior to calling this function. In case C does not have  enough
space to store result, exception will be generated.

Inputs:
    M       -   matrix size, M &gt; 0
    N       -   matrix size, N &gt; 0
    K       -   matrix size, K &gt; 0
    Alpha   -   coefficient
    A       -   matrix
    IA      -   submatrix offset
    JA      -   submatrix offset
    OpTypeA -   transformation type:
                * 0 - no transformation
                * 1 - transposition
                * 2 - conjugate transposition
    B       -   matrix
    IB      -   submatrix offset
    JB      -   submatrix offset
    OpTypeB -   transformation type:
                * 0 - no transformation
                * 1 - transposition
                * 2 - conjugate transposition
    Beta    -   coefficient
    C       -   matrix (PREALLOCATED, large enough to store result)
    IC      -   submatrix offset
    JC      -   submatrix offset
ALGLIB Routine: Copyright 2009-2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixgemm(ae_int_t m, ae_int_t n, ae_int_t k, complex alpha, complex_2d_array a, ae_int_t ia, ae_int_t ja, ae_int_t optypea, complex_2d_array b, ae_int_t ib, ae_int_t jb, ae_int_t optypeb, complex beta, complex_2d_array c, ae_int_t ic, ae_int_t jc);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ablas_d_gemm class=nav>ablas_d_gemm</a> ]</p>
<a name=sub_cmatrixherk></a><h6 class=pageheader>cmatrixherk Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates  C=alpha*A*A^H+beta*C  or  C=alpha*A^H*A+beta*C
where:
* C is NxN Hermitian matrix given by its upper/lower triangle
* A is NxK matrix when A*A^H is calculated, KxN matrix otherwise

Additional info:
* multiplication result replaces C. If Beta=0, C elements are not used in
  calculations (not multiplied by zero - just not referenced)
* if Alpha=0, A is not used (not multiplied by zero - just not referenced)
* if both Beta and Alpha are zero, C is filled by zeros.

Inputs:
    N       -   matrix size, N &ge; 0
    K       -   matrix size, K &ge; 0
    Alpha   -   coefficient
    A       -   matrix
    IA      -   submatrix offset (row index)
    JA      -   submatrix offset (column index)
    OpTypeA -   multiplication type:
                * 0 - A*A^H is calculated
                * 2 - A^H*A is calculated
    Beta    -   coefficient
    C       -   preallocated input/output matrix
    IC      -   submatrix offset (row index)
    JC      -   submatrix offset (column index)
    IsUpper -   whether upper or lower triangle of C is updated;
                this function updates only one half of C, leaving
                other half unchanged (not referenced at all).
ALGLIB Routine: Copyright 16.12.2009-22.01.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixherk(ae_int_t n, ae_int_t k, <b>double</b> alpha, complex_2d_array a, ae_int_t ia, ae_int_t ja, ae_int_t optypea, <b>double</b> beta, complex_2d_array c, ae_int_t ic, ae_int_t jc, <b>bool</b> isupper);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ablas_d_syrk class=nav>ablas_d_syrk</a> ]</p>
<a name=sub_cmatrixlefttrsm></a><h6 class=pageheader>cmatrixlefttrsm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates op(A^-1)*X where:
* X is MxN general matrix
* A is MxM upper/lower triangular/unitriangular matrix
* &quot;op&quot; may be identity transformation, transposition, conjugate transposition
Multiplication result replaces X.

Inputs:
    N   -   matrix size, N &ge; 0
    M   -   matrix size, N &ge; 0
    A       -   matrix, actial matrix is stored in A[I1:I1+M-1,J1:J1+M-1]
    I1      -   submatrix offset
    J1      -   submatrix offset
    IsUpper -   whether matrix is upper triangular
    IsUnit  -   whether matrix is unitriangular
    OpType  -   transformation type:
                * 0 - no transformation
                * 1 - transposition
                * 2 - conjugate transposition
    X   -   matrix, actial matrix is stored in X[I2:I2+M-1,J2:J2+N-1]
    I2  -   submatrix offset
    J2  -   submatrix offset
ALGLIB Routine: Copyright 15.12.2009-22.01.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixlefttrsm(ae_int_t m, ae_int_t n, complex_2d_array a, ae_int_t i1, ae_int_t j1, <b>bool</b> isupper, <b>bool</b> isunit, ae_int_t optype, complex_2d_array x, ae_int_t i2, ae_int_t j2);
</pre>
<a name=sub_cmatrixmv></a><h6 class=pageheader>cmatrixmv Function</h6>
<hr width=600 align=left>
<pre class=narration>
Matrix-vector product: y := op(A)*x

Inputs:
    M   -   number of rows of op(A)
            M &ge; 0
    N   -   number of columns of op(A)
            N &ge; 0
    A   -   target matrix
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    OpA -   operation type:
            * OpA=0     &rArr;  op(A) = A
            * OpA=1     &rArr;  op(A) = A^T
            * OpA=2     &rArr;  op(A) = A^H
    X   -   input vector
    IX  -   subvector offset
    IY  -   subvector offset
    Y   -   preallocated matrix, must be large enough to store result

Outputs:
    Y   -   vector which stores result

if M=0, then subroutine does nothing.
if N=0, Y is filled by zeros.
ALGLIB Routine: Copyright 28.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixmv(ae_int_t m, ae_int_t n, complex_2d_array a, ae_int_t ia, ae_int_t ja, ae_int_t opa, complex_1d_array x, ae_int_t ix, complex_1d_array &amp;y, ae_int_t iy);
</pre>
<a name=sub_cmatrixrank1></a><h6 class=pageheader>cmatrixrank1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Rank-1 correction: A := A + u*v'

Inputs:
    M   -   number of rows
    N   -   number of columns
    A   -   target matrix, MxN submatrix is updated
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    U   -   vector #1
    IU  -   subvector offset
    V   -   vector #2
    IV  -   subvector offset
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixrank1(ae_int_t m, ae_int_t n, complex_2d_array &amp;a, ae_int_t ia, ae_int_t ja, complex_1d_array &amp;u, ae_int_t iu, complex_1d_array &amp;v, ae_int_t iv);
</pre>
<a name=sub_cmatrixrighttrsm></a><h6 class=pageheader>cmatrixrighttrsm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates X*op(A^-1) where:
* X is MxN general matrix
* A is NxN upper/lower triangular/unitriangular matrix
* &quot;op&quot; may be identity transformation, transposition, conjugate transposition
Multiplication result replaces X.

Inputs:
    N   -   matrix size, N &ge; 0
    M   -   matrix size, N &ge; 0
    A       -   matrix, actial matrix is stored in A[I1:I1+N-1,J1:J1+N-1]
    I1      -   submatrix offset
    J1      -   submatrix offset
    IsUpper -   whether matrix is upper triangular
    IsUnit  -   whether matrix is unitriangular
    OpType  -   transformation type:
                * 0 - no transformation
                * 1 - transposition
                * 2 - conjugate transposition
    X   -   matrix, actial matrix is stored in X[I2:I2+M-1,J2:J2+N-1]
    I2  -   submatrix offset
    J2  -   submatrix offset
ALGLIB Routine: Copyright 20.01.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixrighttrsm(ae_int_t m, ae_int_t n, complex_2d_array a, ae_int_t i1, ae_int_t j1, <b>bool</b> isupper, <b>bool</b> isunit, ae_int_t optype, complex_2d_array x, ae_int_t i2, ae_int_t j2);
</pre>
<a name=sub_cmatrixsyrk></a><h6 class=pageheader>cmatrixsyrk Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine is an older version of CMatrixHERK(), one with wrong  name
(it is HErmitian update, not SYmmetric). It  is  left  here  for  backward
compatibility.
ALGLIB Routine: Copyright 16.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixsyrk(ae_int_t n, ae_int_t k, <b>double</b> alpha, complex_2d_array a, ae_int_t ia, ae_int_t ja, ae_int_t optypea, <b>double</b> beta, complex_2d_array c, ae_int_t ic, ae_int_t jc, <b>bool</b> isupper);
</pre>
<a name=sub_cmatrixtranspose></a><h6 class=pageheader>cmatrixtranspose Function</h6>
<hr width=600 align=left>
<pre class=narration>
Cache-oblivous complex &quot;copy-and-transpose&quot;

Inputs:
    M   -   number of rows
    N   -   number of columns
    A   -   source matrix, MxN submatrix is copied and transposed
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    B   -   destination matrix, must be large enough to store result
    IB  -   submatrix offset (row index)
    JB  -   submatrix offset (column index)
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixtranspose(ae_int_t m, ae_int_t n, complex_2d_array a, ae_int_t ia, ae_int_t ja, complex_2d_array &amp;b, ae_int_t ib, ae_int_t jb);
</pre>
<a name=sub_rmatrixcopy></a><h6 class=pageheader>rmatrixcopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Copy

Inputs:
    M   -   number of rows
    N   -   number of columns
    A   -   source matrix, MxN submatrix is copied and transposed
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    B   -   destination matrix, must be large enough to store result
    IB  -   submatrix offset (row index)
    JB  -   submatrix offset (column index)
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixcopy(ae_int_t m, ae_int_t n, real_2d_array a, ae_int_t ia, ae_int_t ja, real_2d_array &amp;b, ae_int_t ib, ae_int_t jb);
</pre>
<a name=sub_rmatrixenforcesymmetricity></a><h6 class=pageheader>rmatrixenforcesymmetricity Function</h6>
<hr width=600 align=left>
<pre class=narration>
This code enforces symmetricy of the matrix by copying Upper part to lower
one (or vice versa).

Inputs:
    A   -   matrix
    N   -   number of rows/columns
    IsUpper - whether we want to copy upper triangle to lower one (True)
            or vice versa (False).
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixenforcesymmetricity(real_2d_array a, ae_int_t n, <b>bool</b> isupper);
</pre>
<a name=sub_rmatrixgemm></a><h6 class=pageheader>rmatrixgemm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates C = alpha*op1(A)*op2(B) +beta*C where:
* C is MxN general matrix
* op1(A) is MxK matrix
* op2(B) is KxN matrix
* &quot;op&quot; may be identity transformation, transposition

Additional info:
* cache-oblivious algorithm is used.
* multiplication result replaces C. If Beta=0, C elements are not used in
  calculations (not multiplied by zero - just not referenced)
* if Alpha=0, A is not used (not multiplied by zero - just not referenced)
* if both Beta and Alpha are zero, C is filled by zeros.

IMPORTANT:

This function does NOT preallocate output matrix C, it MUST be preallocated
by caller prior to calling this function. In case C does not have  enough
space to store result, exception will be generated.

Inputs:
    M       -   matrix size, M &gt; 0
    N       -   matrix size, N &gt; 0
    K       -   matrix size, K &gt; 0
    Alpha   -   coefficient
    A       -   matrix
    IA      -   submatrix offset
    JA      -   submatrix offset
    OpTypeA -   transformation type:
                * 0 - no transformation
                * 1 - transposition
    B       -   matrix
    IB      -   submatrix offset
    JB      -   submatrix offset
    OpTypeB -   transformation type:
                * 0 - no transformation
                * 1 - transposition
    Beta    -   coefficient
    C       -   PREALLOCATED output matrix, large enough to store result
    IC      -   submatrix offset
    JC      -   submatrix offset
ALGLIB Routine: Copyright 2009-2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixgemm(ae_int_t m, ae_int_t n, ae_int_t k, <b>double</b> alpha, real_2d_array a, ae_int_t ia, ae_int_t ja, ae_int_t optypea, real_2d_array b, ae_int_t ib, ae_int_t jb, ae_int_t optypeb, <b>double</b> beta, real_2d_array c, ae_int_t ic, ae_int_t jc);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ablas_d_gemm class=nav>ablas_d_gemm</a> ]</p>
<a name=sub_rmatrixgemv></a><h6 class=pageheader>rmatrixgemv Function</h6>
<hr width=600 align=left>
<pre class=narration>
Scaled matrix-vector addition: y += alpha a x.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixgemv(ae_int_t m, ae_int_t n, <b>double</b> alpha, real_2d_array a, ae_int_t ia, ae_int_t ja, ae_int_t opa, real_1d_array x, ae_int_t ix, <b>double</b> beta, real_1d_array y, ae_int_t iy);
</pre>
<a name=sub_rmatrixgencopy></a><h6 class=pageheader>rmatrixgencopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Performs generalized copy: B := Beta*B + Alpha*A.

If Beta=0, then previous contents of B is simply ignored. If Alpha=0, then
A is ignored and not referenced. If both Alpha and Beta  are  zero,  B  is
filled by zeros.

Inputs:
    M   -   number of rows
    N   -   number of columns
    Alpha-  coefficient
    A   -   source matrix, MxN submatrix is copied and transposed
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    Beta-   coefficient
    B   -   destination matrix, must be large enough to store result
    IB  -   submatrix offset (row index)
    JB  -   submatrix offset (column index)
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixgencopy(ae_int_t m, ae_int_t n, <b>double</b> alpha, real_2d_array a, ae_int_t ia, ae_int_t ja, <b>double</b> beta, real_2d_array b, ae_int_t ib, ae_int_t jb);
</pre>
<a name=sub_rmatrixger></a><h6 class=pageheader>rmatrixger Function</h6>
<hr width=600 align=left>
<pre class=narration>
Rank-1 correction: A := A + alpha*u*v'

NOTE: this  function  expects  A  to  be  large enough to store result. No
      automatic preallocation happens for  smaller  arrays.  No  integrity
      checks is performed for sizes of A, u, v.

Inputs:
    M   -   number of rows
    N   -   number of columns
    A   -   target matrix, MxN submatrix is updated
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    Alpha-  coefficient
    U   -   vector #1
    IU  -   subvector offset
    V   -   vector #2
    IV  -   subvector offset
ALGLIB Routine: Copyright 16.10.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixger(ae_int_t m, ae_int_t n, real_2d_array a, ae_int_t ia, ae_int_t ja, <b>double</b> alpha, real_1d_array u, ae_int_t iu, real_1d_array v, ae_int_t iv);
</pre>
<a name=sub_rmatrixlefttrsm></a><h6 class=pageheader>rmatrixlefttrsm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates op(A^-1)*X where:
* X is MxN general matrix
* A is MxM upper/lower triangular/unitriangular matrix
* &quot;op&quot; may be identity transformation, transposition
Multiplication result replaces X.

Inputs:
    N   -   matrix size, N &ge; 0
    M   -   matrix size, N &ge; 0
    A       -   matrix, actial matrix is stored in A[I1:I1+M-1,J1:J1+M-1]
    I1      -   submatrix offset
    J1      -   submatrix offset
    IsUpper -   whether matrix is upper triangular
    IsUnit  -   whether matrix is unitriangular
    OpType  -   transformation type:
                * 0 - no transformation
                * 1 - transposition
    X   -   matrix, actial matrix is stored in X[I2:I2+M-1,J2:J2+N-1]
    I2  -   submatrix offset
    J2  -   submatrix offset
ALGLIB Routine: Copyright 15.12.2009-22.01.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixlefttrsm(ae_int_t m, ae_int_t n, real_2d_array a, ae_int_t i1, ae_int_t j1, <b>bool</b> isupper, <b>bool</b> isunit, ae_int_t optype, real_2d_array x, ae_int_t i2, ae_int_t j2);
</pre>
<a name=sub_rmatrixmv></a><h6 class=pageheader>rmatrixmv Function</h6>
<hr width=600 align=left>
<pre class=narration>
IMPORTANT: this function is deprecated since ALGLIB 3.13. Use RMatrixGEMV()
           which is more generic version of this function.

Matrix-vector product: y := op(A)*x

Inputs:
    M   -   number of rows of op(A)
    N   -   number of columns of op(A)
    A   -   target matrix
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    OpA -   operation type:
            * OpA=0     &rArr;  op(A) = A
            * OpA=1     &rArr;  op(A) = A^T
    X   -   input vector
    IX  -   subvector offset
    IY  -   subvector offset
    Y   -   preallocated matrix, must be large enough to store result

Outputs:
    Y   -   vector which stores result

if M=0, then subroutine does nothing.
if N=0, Y is filled by zeros.
ALGLIB Routine: Copyright 28.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixmv(ae_int_t m, ae_int_t n, real_2d_array a, ae_int_t ia, ae_int_t ja, ae_int_t opa, real_1d_array x, ae_int_t ix, real_1d_array &amp;y, ae_int_t iy);
</pre>
<a name=sub_rmatrixrank1></a><h6 class=pageheader>rmatrixrank1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
IMPORTANT: this function is deprecated since ALGLIB 3.13. Use RMatrixGER()
           which is more generic version of this function.

Rank-1 correction: A := A + u*v'

Inputs:
    M   -   number of rows
    N   -   number of columns
    A   -   target matrix, MxN submatrix is updated
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    U   -   vector #1
    IU  -   subvector offset
    V   -   vector #2
    IV  -   subvector offset
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixrank1(ae_int_t m, ae_int_t n, real_2d_array &amp;a, ae_int_t ia, ae_int_t ja, real_1d_array &amp;u, ae_int_t iu, real_1d_array &amp;v, ae_int_t iv);
</pre>
<a name=sub_rmatrixrighttrsm></a><h6 class=pageheader>rmatrixrighttrsm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates X*op(A^-1) where:
* X is MxN general matrix
* A is NxN upper/lower triangular/unitriangular matrix
* &quot;op&quot; may be identity transformation, transposition
Multiplication result replaces X.

Inputs:
    N   -   matrix size, N &ge; 0
    M   -   matrix size, N &ge; 0
    A       -   matrix, actial matrix is stored in A[I1:I1+N-1,J1:J1+N-1]
    I1      -   submatrix offset
    J1      -   submatrix offset
    IsUpper -   whether matrix is upper triangular
    IsUnit  -   whether matrix is unitriangular
    OpType  -   transformation type:
                * 0 - no transformation
                * 1 - transposition
    X   -   matrix, actial matrix is stored in X[I2:I2+M-1,J2:J2+N-1]
    I2  -   submatrix offset
    J2  -   submatrix offset
ALGLIB Routine: Copyright 15.12.2009-22.01.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixrighttrsm(ae_int_t m, ae_int_t n, real_2d_array a, ae_int_t i1, ae_int_t j1, <b>bool</b> isupper, <b>bool</b> isunit, ae_int_t optype, real_2d_array x, ae_int_t i2, ae_int_t j2);
</pre>
<a name=sub_rmatrixsymv></a><h6 class=pageheader>rmatrixsymv Function</h6>
<hr width=600 align=left>
<pre class=narration>
Scaled vector-matrix-vector addition: y = alpha a x + beta y.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixsymv(ae_int_t n, <b>double</b> alpha, real_2d_array a, ae_int_t ia, ae_int_t ja, <b>bool</b> isupper, real_1d_array x, ae_int_t ix, <b>double</b> beta, real_1d_array y, ae_int_t iy);
</pre>
<a name=sub_rmatrixsyrk></a><h6 class=pageheader>rmatrixsyrk Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine calculates  C=alpha*A*A^T+beta*C  or  C=alpha*A^T*A+beta*C
where:
* C is NxN symmetric matrix given by its upper/lower triangle
* A is NxK matrix when A*A^T is calculated, KxN matrix otherwise

Additional info:
* multiplication result replaces C. If Beta=0, C elements are not used in
  calculations (not multiplied by zero - just not referenced)
* if Alpha=0, A is not used (not multiplied by zero - just not referenced)
* if both Beta and Alpha are zero, C is filled by zeros.

Inputs:
    N       -   matrix size, N &ge; 0
    K       -   matrix size, K &ge; 0
    Alpha   -   coefficient
    A       -   matrix
    IA      -   submatrix offset (row index)
    JA      -   submatrix offset (column index)
    OpTypeA -   multiplication type:
                * 0 - A*A^T is calculated
                * 2 - A^T*A is calculated
    Beta    -   coefficient
    C       -   preallocated input/output matrix
    IC      -   submatrix offset (row index)
    JC      -   submatrix offset (column index)
    IsUpper -   whether C is upper triangular or lower triangular
ALGLIB Routine: Copyright 16.12.2009-22.01.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixsyrk(ae_int_t n, ae_int_t k, <b>double</b> alpha, real_2d_array a, ae_int_t ia, ae_int_t ja, ae_int_t optypea, <b>double</b> beta, real_2d_array c, ae_int_t ic, ae_int_t jc, <b>bool</b> isupper);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_ablas_d_syrk class=nav>ablas_d_syrk</a> ]</p>
<a name=sub_rmatrixsyvmv></a><h6 class=pageheader>rmatrixsyvmv Function</h6>
<hr width=600 align=left>
<pre class=narration>
Vector-matrix-vector multiplication: x^T a x (with tmp = a x).
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rmatrixsyvmv(ae_int_t n, real_2d_array a, ae_int_t ia, ae_int_t ja, <b>bool</b> isupper, real_1d_array x, ae_int_t ix, real_1d_array tmp);
</pre>
<a name=sub_rmatrixtranspose></a><h6 class=pageheader>rmatrixtranspose Function</h6>
<hr width=600 align=left>
<pre class=narration>
Cache-oblivous real &quot;copy-and-transpose&quot;

Inputs:
    M   -   number of rows
    N   -   number of columns
    A   -   source matrix, MxN submatrix is copied and transposed
    IA  -   submatrix offset (row index)
    JA  -   submatrix offset (column index)
    B   -   destination matrix, must be large enough to store result
    IB  -   submatrix offset (row index)
    JB  -   submatrix offset (column index)
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixtranspose(ae_int_t m, ae_int_t n, real_2d_array a, ae_int_t ia, ae_int_t ja, real_2d_array &amp;b, ae_int_t ib, ae_int_t jb);
</pre>
<a name=sub_rmatrixtrsv></a><h6 class=pageheader>rmatrixtrsv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine solves linear system op(A)*x=b where:
* A is NxN upper/lower triangular/unitriangular matrix
* X and B are Nx1 vectors
* &quot;op&quot; may be identity transformation, transposition, conjugate transposition

Solution replaces X.

IMPORTANT: * no overflow/underflow/denegeracy tests is performed.
           * no integrity checks for operand sizes, out-of-bounds accesses
             and so on is performed

Inputs:
    N   -   matrix size, N &ge; 0
    A       -   matrix, actial matrix is stored in A[IA:IA+N-1,JA:JA+N-1]
    IA      -   submatrix offset
    JA      -   submatrix offset
    IsUpper -   whether matrix is upper triangular
    IsUnit  -   whether matrix is unitriangular
    OpType  -   transformation type:
                * 0 - no transformation
                * 1 - transposition
    X       -   right part, actual vector is stored in X[IX:IX+N-1]
    IX      -   offset

Outputs:
    X       -   solution replaces elements X[IX:IX+N-1]
ALGLIB Routine: Copyright (c) 2017 by Sergey Bochkanov - converted to ALGLIB, remastered from LAPACK's DTRSV.
Copyright (c) 2016 Reference BLAS level1 routine (LAPACK version 3.7.0)
     Reference BLAS is a software package provided by Univ. of Tennessee,
     Univ. of California Berkeley, Univ. of Colorado Denver and NAG Ltd.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixtrsv(ae_int_t n, real_2d_array a, ae_int_t ia, ae_int_t ja, <b>bool</b> isupper, <b>bool</b> isunit, ae_int_t optype, real_1d_array x, ae_int_t ix);
</pre>
<a name=sub_rvectorcopy></a><h6 class=pageheader>rvectorcopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Copy

Inputs:
    N   -   subvector size
    A   -   source vector, N elements are copied
    IA  -   source offset (first element index)
    B   -   destination vector, must be large enough to store result
    IB  -   destination offset (first element index)
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rvectorcopy(ae_int_t n, real_1d_array a, ae_int_t ia, real_1d_array b, ae_int_t ib);
</pre>
<a name=example_ablas_d_gemm></a><h6 class=pageheader>ablas_d_gemm Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_2d_array a = <font color=blue><b>&quot;[[2,1],[1,3]]&quot;</b></font>;
   real_2d_array b = <font color=blue><b>&quot;[[2,1],[0,1]]&quot;</b></font>;
   real_2d_array c = <font color=blue><b>&quot;[[0,0],[0,0]]&quot;</b></font>;
<font color=navy>// rmatrixgemm() function allows us to calculate matrix product C:=A*B or</font>
<font color=navy>// to perform more general operation, C:=alpha*op1(A)*op2(B)+beta*C,</font>
<font color=navy>// where A, B, C are rectangular matrices, op(X) can be X or X^T,</font>
<font color=navy>// alpha and beta are scalars.</font>
<font color=navy>//</font>
<font color=navy>// This function:</font>
<font color=navy>// * can apply transposition and/or multiplication by scalar to operands</font>
<font color=navy>// * can use arbitrary part of matrices A/B (given by submatrix offset)</font>
<font color=navy>// * can store result into arbitrary part of C</font>
<font color=navy>// * <b>for</b> performance reasons requires C to be preallocated</font>
<font color=navy>//</font>
<font color=navy>// Parameters of this function are:</font>
<font color=navy>// * M, N, K            -   sizes of op1(A) (which is MxK), op2(B) (which</font>
<font color=navy>//                          is KxN) and C (which is MxN)</font>
<font color=navy>// * Alpha              -   coefficient before A*B</font>
<font color=navy>// * A, IA, JA          -   matrix A and offset of the submatrix</font>
<font color=navy>// * OpTypeA            -   transformation type:</font>
<font color=navy>//                          0 - no transformation</font>
<font color=navy>//                          1 - transposition</font>
<font color=navy>// * B, IB, JB          -   matrix B and offset of the submatrix</font>
<font color=navy>// * OpTypeB            -   transformation type:</font>
<font color=navy>//                          0 - no transformation</font>
<font color=navy>//                          1 - transposition</font>
<font color=navy>// * Beta               -   coefficient before C</font>
<font color=navy>// * C, IC, JC          -   preallocated matrix C and offset of the submatrix</font>
<font color=navy>//</font>
<font color=navy>// Below we perform simple product C:=A*B (alpha=1, beta=0)</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: this function works with preallocated C, which must be large</font>
<font color=navy>//            enough to store multiplication result.</font>
   ae_int_t m = 2;
   ae_int_t n = 2;
   ae_int_t k = 2;
   <b>double</b> alpha = 1.0;
   ae_int_t ia = 0;
   ae_int_t ja = 0;
   ae_int_t optypea = 0;
   ae_int_t ib = 0;
   ae_int_t jb = 0;
   ae_int_t optypeb = 0;
   <b>double</b> beta = 0.0;
   ae_int_t ic = 0;
   ae_int_t jc = 0;
   rmatrixgemm(m, n, k, alpha, a, ia, ja, optypea, b, ib, jb, optypeb, beta, c, ic, jc);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(3).c_str()); <font color=navy>// EXPECTED: [[4,3],[2,4]]</font>
<font color=navy>//</font>
<font color=navy>// Now we try to apply some simple transformation to operands: C:=A*B^T</font>
   optypeb = 1;
   rmatrixgemm(m, n, k, alpha, a, ia, ja, optypea, b, ib, jb, optypeb, beta, c, ic, jc);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(3).c_str()); <font color=navy>// EXPECTED: [[5,1],[5,3]]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_ablas_d_syrk></a><h6 class=pageheader>ablas_d_syrk Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// rmatrixsyrk() function allows us to calculate symmetric rank-K update</font>
<font color=navy>// C := beta*C + alpha*A'*A, where C is square N*N matrix, A is square K*N</font>
<font color=navy>// matrix, alpha and beta are scalars. It is also possible to update by</font>
<font color=navy>// adding A*A' instead of A'*A.</font>
<font color=navy>//</font>
<font color=navy>// Parameters of this function are:</font>
<font color=navy>// * N, K       -   matrix size</font>
<font color=navy>// * Alpha      -   coefficient before A</font>
<font color=navy>// * A, IA, JA  -   matrix and submatrix offsets</font>
<font color=navy>// * OpTypeA    -   multiplication type:</font>
<font color=navy>//                  * 0 - A*A^T is calculated</font>
<font color=navy>//                  * 2 - A^T*A is calculated</font>
<font color=navy>// * Beta       -   coefficient before C</font>
<font color=navy>// * C, IC, JC  -   preallocated input/output matrix and submatrix offsets</font>
<font color=navy>// * IsUpper    -   whether upper or lower triangle of C is updated;</font>
<font color=navy>//                  this function updates only one half of C, leaving</font>
<font color=navy>//                  other half unchanged (not referenced at all).</font>
<font color=navy>//</font>
<font color=navy>// Below we will show how to calculate simple product C:=A'*A</font>
<font color=navy>//</font>
<font color=navy>// NOTE: beta=0 and we <b>do</b> not use previous value of C, but still it</font>
<font color=navy>//       MUST be preallocated.</font>
   ae_int_t n = 2;
   ae_int_t k = 1;
   <b>double</b> alpha = 1.0;
   ae_int_t ia = 0;
   ae_int_t ja = 0;
   ae_int_t optypea = 2;
   <b>double</b> beta = 0.0;
   ae_int_t ic = 0;
   ae_int_t jc = 0;
   <b>bool</b> isupper = true;
   real_2d_array a = <font color=blue><b>&quot;[[1,2]]&quot;</b></font>;

<font color=navy>// preallocate space to store result</font>
   real_2d_array c = <font color=blue><b>&quot;[[0,0],[0,0]]&quot;</b></font>;

<font color=navy>// calculate product, store result into upper part of c</font>
   rmatrixsyrk(n, k, alpha, a, ia, ja, optypea, beta, c, ic, jc, isupper);

<font color=navy>// output result.</font>
<font color=navy>// IMPORTANT: lower triangle of C was NOT updated!</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(3).c_str()); <font color=navy>// EXPECTED: [[1,2],[0,4]]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_bdsvd></a><h4 class=pageheader>8.7.2. bdsvd Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_rmatrixbdsvd class=toc>rmatrixbdsvd</a>
]</font>
</div>
<a name=sub_rmatrixbdsvd></a><h6 class=pageheader>rmatrixbdsvd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Singular value decomposition of a bidiagonal matrix (extended algorithm)

The algorithm performs the singular value decomposition  of  a  bidiagonal
matrix B (upper or lower) representing it as B = Q*S*P^T, where Q and  P -
orthogonal matrices, S - diagonal matrix with non-negative elements on the
main diagonal, in descending order.

The  algorithm  finds  singular  values.  In  addition,  the algorithm can
calculate  matrices  Q  and P (more precisely, not the matrices, but their
product  with  given  matrices U and VT - U*Q and (P^T)*VT)).  Of  course,
matrices U and VT can be of any type, including identity. Furthermore, the
algorithm can calculate Q'*C (this product is calculated more  effectively
than U*Q,  because  this calculation operates with rows instead  of matrix
columns).

The feature of the algorithm is its ability to find  all  singular  values
including those which are arbitrarily close to 0  with  relative  accuracy
close to  machine precision. If the parameter IsFractionalAccuracyRequired
is set to True, all singular values will have high relative accuracy close
to machine precision. If the parameter is set to False, only  the  biggest
singular value will have relative accuracy  close  to  machine  precision.
The absolute error of other singular values is equal to the absolute error
of the biggest singular value.

Inputs:
    D       -   main diagonal of matrix B.
                Array whose index ranges within [0..N-1].
    E       -   superdiagonal (or subdiagonal) of matrix B.
                Array whose index ranges within [0..N-2].
    N       -   size of matrix B.
    IsUpper -   True, if the matrix is upper bidiagonal.
    IsFractionalAccuracyRequired -
                THIS PARAMETER IS IGNORED SINCE ALGLIB 3.5.0
                SINGULAR VALUES ARE ALWAYS SEARCHED WITH HIGH ACCURACY.
    U       -   matrix to be multiplied by Q.
                Array whose indexes range within [0..NRU-1, 0..N-1].
                The matrix can be bigger, in that case only the  submatrix
                [0..NRU-1, 0..N-1] will be multiplied by Q.
    NRU     -   number of rows in matrix U.
    C       -   matrix to be multiplied by Q'.
                Array whose indexes range within [0..N-1, 0..NCC-1].
                The matrix can be bigger, in that case only the  submatrix
                [0..N-1, 0..NCC-1] will be multiplied by Q'.
    NCC     -   number of columns in matrix C.
    VT      -   matrix to be multiplied by P^T.
                Array whose indexes range within [0..N-1, 0..NCVT-1].
                The matrix can be bigger, in that case only the  submatrix
                [0..N-1, 0..NCVT-1] will be multiplied by P^T.
    NCVT    -   number of columns in matrix VT.

Outputs:
    D       -   singular values of matrix B in descending order.
    U       -   if NRU &gt; 0, contains matrix U*Q.
    VT      -   if NCVT &gt; 0, contains matrix (P^T)*VT.
    C       -   if NCC &gt; 0, contains matrix Q'*C.

Result:
    True, if the algorithm has converged.
    False, if the algorithm hasn't converged (rare case).

NOTE: multiplication U*Q is performed by means of transposition to internal
      buffer, multiplication and backward transposition. It helps to avoid
      costly columnwise operations and speed-up algorithm.

Additional information:
    The type of convergence is controlled by the internal  parameter  TOL.
    If the parameter is greater than 0, the singular values will have
    relative accuracy TOL. If TOL &lt; 0, the singular values will have
    absolute accuracy ABS(TOL)*norm(B).
    By default, |TOL| falls within the range of 10*Epsilon and 100*Epsilon,
    where Epsilon is the machine precision. It is not  recommended  to  use
    TOL less than 10*Epsilon since this will  considerably  slow  down  the
    algorithm and may not lead to error decreasing.

History:
    * 31 March, 2007.
        changed MAXITR from 6 to 12.

  -- LAPACK routine (version 3.0) --
     Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,
     Courant Institute, Argonne National Lab, and Rice University
     October 31, 1999.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> rmatrixbdsvd(real_1d_array &amp;d, real_1d_array e, ae_int_t n, <b>bool</b> isupper, <b>bool</b> isfractionalaccuracyrequired, real_2d_array &amp;u, ae_int_t nru, real_2d_array &amp;c, ae_int_t ncc, real_2d_array &amp;vt, ae_int_t ncvt);
</pre>
<a name=unit_evd></a><h4 class=pageheader>8.7.3. evd Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_eigsubspacereport class=toc>eigsubspacereport</a> |
<a href=#struct_eigsubspacestate class=toc>eigsubspacestate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_eigsubspacecreate class=toc>eigsubspacecreate</a> |
<a href=#sub_eigsubspacecreatebuf class=toc>eigsubspacecreatebuf</a> |
<a href=#sub_eigsubspaceooccontinue class=toc>eigsubspaceooccontinue</a> |
<a href=#sub_eigsubspaceoocgetrequestdata class=toc>eigsubspaceoocgetrequestdata</a> |
<a href=#sub_eigsubspaceoocgetrequestinfo class=toc>eigsubspaceoocgetrequestinfo</a> |
<a href=#sub_eigsubspaceoocsendresult class=toc>eigsubspaceoocsendresult</a> |
<a href=#sub_eigsubspaceoocstart class=toc>eigsubspaceoocstart</a> |
<a href=#sub_eigsubspaceoocstop class=toc>eigsubspaceoocstop</a> |
<a href=#sub_eigsubspacesetcond class=toc>eigsubspacesetcond</a> |
<a href=#sub_eigsubspacesetwarmstart class=toc>eigsubspacesetwarmstart</a> |
<a href=#sub_eigsubspacesolvedenses class=toc>eigsubspacesolvedenses</a> |
<a href=#sub_eigsubspacesolvesparses class=toc>eigsubspacesolvesparses</a> |
<a href=#sub_hmatrixevd class=toc>hmatrixevd</a> |
<a href=#sub_hmatrixevdi class=toc>hmatrixevdi</a> |
<a href=#sub_hmatrixevdr class=toc>hmatrixevdr</a> |
<a href=#sub_rmatrixevd class=toc>rmatrixevd</a> |
<a href=#sub_smatrixevd class=toc>smatrixevd</a> |
<a href=#sub_smatrixevdi class=toc>smatrixevdi</a> |
<a href=#sub_smatrixevdr class=toc>smatrixevdr</a> |
<a href=#sub_smatrixtdevd class=toc>smatrixtdevd</a> |
<a href=#sub_smatrixtdevdi class=toc>smatrixtdevdi</a> |
<a href=#sub_smatrixtdevdr class=toc>smatrixtdevdr</a>
]</font>
</div>
<a name=struct_eigsubspacereport></a><h6 class=pageheader>eigsubspacereport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores state of the subspace iteration algorithm.
You should use ALGLIB functions to work with this object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> eigsubspacereport {
   ae_int_t iterationscount;
};
</pre>
<a name=struct_eigsubspacestate></a><h6 class=pageheader>eigsubspacestate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores state of the subspace iteration algorithm.
You should use ALGLIB functions to work with this object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> eigsubspacestate {
};
</pre>
<a name=sub_eigsubspacecreate></a><h6 class=pageheader>eigsubspacecreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function initializes subspace iteration solver. This solver  is  used
to solve symmetric real eigenproblems where just a few (top K) eigenvalues
and corresponding eigenvectors is required.

This solver can be significantly faster than  complete  EVD  decomposition
in the following case:
* when only just a small fraction  of  top  eigenpairs  of dense matrix is
  required. When K approaches N, this solver is slower than complete dense
  EVD
* when problem matrix is sparse (and/or is not known explicitly, i.e. only
  matrix-matrix product can be performed)

USAGE (explicit dense/sparse matrix):
1. User initializes algorithm state with eigsubspacecreate() call
2. [optional] User tunes solver parameters by calling eigsubspacesetcond()
   or other functions
3. User  calls  eigsubspacesolvedense() or eigsubspacesolvesparse() methods,
   which take algorithm state and 2D array or alglib.sparsematrix object.

USAGE (out-of-core mode):
1. User initializes algorithm state with eigsubspacecreate() call
2. [optional] User tunes solver parameters by calling eigsubspacesetcond()
   or other functions
3. User activates out-of-core mode of  the  solver  and  repeatedly  calls
   communication functions in a loop like below:
   &gt; alglib.eigsubspaceoocstart(state)
   &gt; while alglib.eigsubspaceooccontinue(state) do
   &gt;     alglib.eigsubspaceoocgetrequestinfo(state, out RequestType, out M)
   &gt;     alglib.eigsubspaceoocgetrequestdata(state, out X)
   &gt;     [calculate  Y=A*X, with X=R^NxM]
   &gt;     alglib.eigsubspaceoocsendresult(state, in Y)
   &gt; alglib.eigsubspaceoocstop(state, out W, out Z, out Report)

Inputs:
    N       -   problem dimensionality, N &gt; 0
    K       -   number of top eigenvector to calculate, 0 &lt; K &le; N.

Outputs:
    State   -   structure which stores algorithm state

NOTE: if you solve many similar EVD problems you may  find  it  useful  to
      reuse previous subspace as warm-start point for new EVD problem.  It
      can be done with eigsubspacesetwarmstart() function.
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspacecreate(ae_int_t n, ae_int_t k, eigsubspacestate &amp;state);
</pre>
<a name=sub_eigsubspacecreatebuf></a><h6 class=pageheader>eigsubspacecreatebuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Buffered version of constructor which aims to reuse  previously  allocated
memory as much as possible.
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspacecreatebuf(ae_int_t n, ae_int_t k, eigsubspacestate state);
</pre>
<a name=sub_eigsubspaceooccontinue></a><h6 class=pageheader>eigsubspaceooccontinue Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs subspace iteration  in  the  out-of-core  mode.  It
should be used in conjunction with other out-of-core-related functions  of
this subspackage in a loop like below:

&gt; alglib.eigsubspaceoocstart(state)
&gt; while alglib.eigsubspaceooccontinue(state) do
&gt;     alglib.eigsubspaceoocgetrequestinfo(state, out RequestType, out M)
&gt;     alglib.eigsubspaceoocgetrequestdata(state, out X)
&gt;     [calculate  Y=A*X, with X=R^NxM]
&gt;     alglib.eigsubspaceoocsendresult(state, in Y)
&gt; alglib.eigsubspaceoocstop(state, out W, out Z, out Report)
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> eigsubspaceooccontinue(eigsubspacestate state);
</pre>
<a name=sub_eigsubspaceoocgetrequestdata></a><h6 class=pageheader>eigsubspaceoocgetrequestdata Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to retrieve information  about  out-of-core  request
sent by solver to user code: matrix X (array[N,RequestSize) which have  to
be multiplied by out-of-core matrix A in a product A*X.

This function returns just request data; in order to get size of  the data
prior to processing requestm, use eigsubspaceoocgetrequestinfo().

It should be used in conjunction with other out-of-core-related  functions
of this subspackage in a loop like below:

&gt; alglib.eigsubspaceoocstart(state)
&gt; while alglib.eigsubspaceooccontinue(state) do
&gt;     alglib.eigsubspaceoocgetrequestinfo(state, out RequestType, out M)
&gt;     alglib.eigsubspaceoocgetrequestdata(state, out X)
&gt;     [calculate  Y=A*X, with X=R^NxM]
&gt;     alglib.eigsubspaceoocsendresult(state, in Y)
&gt; alglib.eigsubspaceoocstop(state, out W, out Z, out Report)

Inputs:
    State           -   solver running in out-of-core mode
    X               -   possibly  preallocated   storage;  reallocated  if
                        needed, left unchanged, if large enough  to  store
                        request data.

Outputs:
    X               -   array[N,RequestSize] or larger, leading  rectangle
                        is filled with dense matrix X.
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspaceoocgetrequestdata(eigsubspacestate state, real_2d_array &amp;x);
</pre>
<a name=sub_eigsubspaceoocgetrequestinfo></a><h6 class=pageheader>eigsubspaceoocgetrequestinfo Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to retrieve information  about  out-of-core  request
sent by solver to user code: request type (current version  of  the solver
sends only requests for matrix-matrix products) and request size (size  of
the matrices being multiplied).

This function returns just request metrics; in order  to  get contents  of
the matrices being multiplied, use eigsubspaceoocgetrequestdata().

It should be used in conjunction with other out-of-core-related  functions
of this subspackage in a loop like below:

&gt; alglib.eigsubspaceoocstart(state)
&gt; while alglib.eigsubspaceooccontinue(state) do
&gt;     alglib.eigsubspaceoocgetrequestinfo(state, out RequestType, out M)
&gt;     alglib.eigsubspaceoocgetrequestdata(state, out X)
&gt;     [calculate  Y=A*X, with X=R^NxM]
&gt;     alglib.eigsubspaceoocsendresult(state, in Y)
&gt; alglib.eigsubspaceoocstop(state, out W, out Z, out Report)

Inputs:
    State           -   solver running in out-of-core mode

Outputs:
    RequestType     -   type of the request to process:
                        * 0 - for matrix-matrix product A*X, with A  being
                          NxN matrix whose eigenvalues/vectors are needed,
                          and X being NxREQUESTSIZE one which is  returned
                          by the eigsubspaceoocgetrequestdata().
    RequestSize     -   size of the X matrix (number of columns),  usually
                        it is several times larger than number of  vectors
                        K requested by user.
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspaceoocgetrequestinfo(eigsubspacestate state, ae_int_t &amp;requesttype, ae_int_t &amp;requestsize);
</pre>
<a name=sub_eigsubspaceoocsendresult></a><h6 class=pageheader>eigsubspaceoocsendresult Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to send user reply to out-of-core  request  sent  by
solver. Usually it is product A*X for returned by solver matrix X.

It should be used in conjunction with other out-of-core-related  functions
of this subspackage in a loop like below:

&gt; alglib.eigsubspaceoocstart(state)
&gt; while alglib.eigsubspaceooccontinue(state) do
&gt;     alglib.eigsubspaceoocgetrequestinfo(state, out RequestType, out M)
&gt;     alglib.eigsubspaceoocgetrequestdata(state, out X)
&gt;     [calculate  Y=A*X, with X=R^NxM]
&gt;     alglib.eigsubspaceoocsendresult(state, in Y)
&gt; alglib.eigsubspaceoocstop(state, out W, out Z, out Report)

Inputs:
    State           -   solver running in out-of-core mode
    AX              -   array[N,RequestSize] or larger, leading  rectangle
                        is filled with product A*X.
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspaceoocsendresult(eigsubspacestate state, real_2d_array ax);
</pre>
<a name=sub_eigsubspaceoocstart></a><h6 class=pageheader>eigsubspaceoocstart Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  initiates  out-of-core  mode  of  subspace eigensolver. It
should be used in conjunction with other out-of-core-related functions  of
this subspackage in a loop like below:

&gt; alglib.eigsubspaceoocstart(state)
&gt; while alglib.eigsubspaceooccontinue(state) do
&gt;     alglib.eigsubspaceoocgetrequestinfo(state, out RequestType, out M)
&gt;     alglib.eigsubspaceoocgetrequestdata(state, out X)
&gt;     [calculate  Y=A*X, with X=R^NxM]
&gt;     alglib.eigsubspaceoocsendresult(state, in Y)
&gt; alglib.eigsubspaceoocstop(state, out W, out Z, out Report)

Inputs:
    State       -   solver object
    MType       -   matrix type:
                    * 0 for real  symmetric  matrix  (solver  assumes that
                      matrix  being   processed  is  symmetric;  symmetric
                      direct eigensolver is used for  smaller  subproblems
                      arising during solution of larger &quot;full&quot; task)
                    Future versions of ALGLIB may  introduce  support  for
                    other  matrix   types;   for   now,   only   symmetric
                    eigenproblems are supported.
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspaceoocstart(eigsubspacestate state, ae_int_t mtype);
</pre>
<a name=sub_eigsubspaceoocstop></a><h6 class=pageheader>eigsubspaceoocstop Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  finalizes out-of-core  mode  of  subspace eigensolver.  It
should be used in conjunction with other out-of-core-related functions  of
this subspackage in a loop like below:

&gt; alglib.eigsubspaceoocstart(state)
&gt; while alglib.eigsubspaceooccontinue(state) do
&gt;     alglib.eigsubspaceoocgetrequestinfo(state, out RequestType, out M)
&gt;     alglib.eigsubspaceoocgetrequestdata(state, out X)
&gt;     [calculate  Y=A*X, with X=R^NxM]
&gt;     alglib.eigsubspaceoocsendresult(state, in Y)
&gt; alglib.eigsubspaceoocstop(state, out W, out Z, out Report)

Inputs:
    State       -   solver state

Outputs:
    W           -   array[K], depending on solver settings:
                    * top  K  eigenvalues ordered  by  descending   -   if
                      eigenvectors are returned in Z
                    * zeros - if invariant subspace is returned in Z
    Z           -   array[N,K], depending on solver settings either:
                    * matrix of eigenvectors found
                    * orthogonal basis of K-dimensional invariant subspace
    Rep         -   report with additional parameters
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspaceoocstop(eigsubspacestate state, real_1d_array &amp;w, real_2d_array &amp;z, eigsubspacereport &amp;rep);
</pre>
<a name=sub_eigsubspacesetcond></a><h6 class=pageheader>eigsubspacesetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping critera for the solver:
* error in eigenvector/value allowed by solver
* maximum number of iterations to perform

Inputs:
    State       -   solver structure
    Eps         -   eps &ge; 0,  with non-zero value used to tell solver  that
                    it can  stop  after  all  eigenvalues  converged  with
                    error  roughly  proportional  to  eps*MAX(LAMBDA_MAX),
                    where LAMBDA_MAX is a maximum eigenvalue.
                    Zero  value  means  that  no  check  for  precision is
                    performed.
    MaxIts      -   maxits &ge; 0,  with non-zero value used  to  tell  solver
                    that it can stop after maxits  steps  (no  matter  how
                    precise current estimate is)

NOTE: passing  eps=0  and  maxits=0  results  in  automatic  selection  of
      moderate eps as stopping criteria (1.0E-6 in current implementation,
      but it may change without notice).

NOTE: very small values of eps are possible (say, 1.0E-12),  although  the
      larger problem you solve (N and/or K), the  harder  it  is  to  find
      precise eigenvectors because rounding errors tend to accumulate.

NOTE: passing non-zero eps results in  some performance  penalty,  roughly
      equal to 2N*(2K)^2 FLOPs per iteration. These additional computations
      are required in order to estimate current error in  eigenvalues  via
      Rayleigh-Ritz process.
      Most of this additional time is  spent  in  construction  of  ~2Kx2K
      symmetric  subproblem  whose  eigenvalues  are  checked  with  exact
      eigensolver.
      This additional time is negligible if you search for eigenvalues  of
      the large dense matrix, but may become noticeable on  highly  sparse
      EVD problems, where cost of matrix-matrix product is low.
      If you set eps to exactly zero,  Rayleigh-Ritz  phase  is completely
      turned off.
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspacesetcond(eigsubspacestate state, <b>double</b> eps, ae_int_t maxits);
</pre>
<a name=sub_eigsubspacesetwarmstart></a><h6 class=pageheader>eigsubspacesetwarmstart Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets warm-start mode of the solver: next call to the  solver
will reuse previous subspace as warm-start  point.  It  can  significantly
speed-up convergence when you solve many similar eigenproblems.

Inputs:
    State       -   solver structure
    UseWarmStart-   either True or False
ALGLIB: Copyright 12.11.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspacesetwarmstart(eigsubspacestate state, <b>bool</b> usewarmstart);
</pre>
<a name=sub_eigsubspacesolvedenses></a><h6 class=pageheader>eigsubspacesolvedenses Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function runs eigensolver for dense NxN symmetric matrix A, given by
upper or lower triangle.

This function can not process nonsymmetric matrices.

Inputs:
    State       -   solver state
    A           -   array[N,N], symmetric NxN matrix given by one  of  its
                    triangles
    IsUpper     -   whether upper or lower triangle of  A  is  given  (the
                    other one is not referenced at all).

Outputs:
    W           -   array[K], top  K  eigenvalues ordered  by   descending
                    of their absolute values
    Z           -   array[N,K], matrix of eigenvectors found
    Rep         -   report with additional parameters

NOTE: internally this function allocates a copy of NxN dense A. You should
      take it into account when working with very large matrices occupying
      almost all RAM.
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspacesolvedenses(eigsubspacestate state, real_2d_array a, <b>bool</b> isupper, real_1d_array &amp;w, real_2d_array &amp;z, eigsubspacereport &amp;rep);
</pre>
<a name=sub_eigsubspacesolvesparses></a><h6 class=pageheader>eigsubspacesolvesparses Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function runs eigensolver for dense NxN symmetric matrix A, given by
upper or lower triangle.

This function can not process nonsymmetric matrices.

Inputs:
    State       -   solver state
    A           -   NxN symmetric matrix given by one of its triangles
    IsUpper     -   whether upper or lower triangle of  A  is  given  (the
                    other one is not referenced at all).

Outputs:
    W           -   array[K], top  K  eigenvalues ordered  by   descending
                    of their absolute values
    Z           -   array[N,K], matrix of eigenvectors found
    Rep         -   report with additional parameters
ALGLIB: Copyright 16.01.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> eigsubspacesolvesparses(eigsubspacestate state, sparsematrix a, <b>bool</b> isupper, real_1d_array &amp;w, real_2d_array &amp;z, eigsubspacereport &amp;rep);
</pre>
<a name=sub_hmatrixevd></a><h6 class=pageheader>hmatrixevd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Finding the eigenvalues and eigenvectors of a Hermitian matrix

The algorithm finds eigen pairs of a Hermitian matrix by  reducing  it  to
real tridiagonal form and using the QL/QR algorithm.

Inputs:
    A       -   Hermitian matrix which is given  by  its  upper  or  lower
                triangular part.
                Array whose indexes range within [0..N-1, 0..N-1].
    N       -   size of matrix A.
    IsUpper -   storage format.
    ZNeeded -   flag controlling whether the eigenvectors  are  needed  or
                not. If ZNeeded is equal to:
                 * 0, the eigenvectors are not returned;
                 * 1, the eigenvectors are returned.

Outputs:
    D       -   eigenvalues in ascending order.
                Array whose index ranges within [0..N-1].
    Z       -   if ZNeeded is equal to:
                 * 0, Z hasn't changed;
                 * 1, Z contains the eigenvectors.
                Array whose indexes range within [0..N-1, 0..N-1].
                The eigenvectors are stored in the matrix columns.

Result:
    True, if the algorithm has converged.
    False, if the algorithm hasn't converged (rare case).

Note:
    eigenvectors of Hermitian matrix are defined up to  multiplication  by
    a complex number L, such that |L|=1.
ALGLIB: Copyright 2005, 2007 March 23 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> hmatrixevd(complex_2d_array a, ae_int_t n, ae_int_t zneeded, <b>bool</b> isupper, real_1d_array &amp;d, complex_2d_array &amp;z);
</pre>
<a name=sub_hmatrixevdi></a><h6 class=pageheader>hmatrixevdi Function</h6>
<hr width=600 align=left>
<pre class=narration>
Subroutine for finding the eigenvalues and  eigenvectors  of  a  Hermitian
matrix with given indexes by using bisection and inverse iteration methods

Inputs:
    A       -   Hermitian matrix which is given  by  its  upper  or  lower
                triangular part.
                Array whose indexes range within [0..N-1, 0..N-1].
    N       -   size of matrix A.
    ZNeeded -   flag controlling whether the eigenvectors  are  needed  or
                not. If ZNeeded is equal to:
                 * 0, the eigenvectors are not returned;
                 * 1, the eigenvectors are returned.
    IsUpperA -  storage format of matrix A.
    I1, I2 -    index interval for searching (from I1 to I2).
                0 &le; I1 &le; I2 &le; N-1.

Outputs:
    W       -   array of the eigenvalues found.
                Array whose index ranges within [0..I2-I1].
    Z       -   if ZNeeded is equal to:
                 * 0, Z hasn't changed;
                 * 1, Z contains eigenvectors.
                Array whose indexes range within [0..N-1, 0..I2-I1].
                In  that  case,  the eigenvectors are stored in the matrix
                columns.

Result:
    True, if successful. W contains the eigenvalues, Z contains the
    eigenvectors (if needed).

    False, if the bisection method subroutine  wasn't  able  to  find  the
    eigenvalues  in  the  given  interval  or  if  the  inverse  iteration
    subroutine wasn't able to find  all  the  corresponding  eigenvectors.
    In that case, the eigenvalues and eigenvectors are not returned.

Note:
    eigen vectors of Hermitian matrix are defined up to multiplication  by
    a complex number L, such as |L|=1.
ALGLIB: Copyright 07.01.2006, 24.03.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> hmatrixevdi(complex_2d_array a, ae_int_t n, ae_int_t zneeded, <b>bool</b> isupper, ae_int_t i1, ae_int_t i2, real_1d_array &amp;w, complex_2d_array &amp;z);
</pre>
<a name=sub_hmatrixevdr></a><h6 class=pageheader>hmatrixevdr Function</h6>
<hr width=600 align=left>
<pre class=narration>
Subroutine for finding the eigenvalues (and eigenvectors) of  a  Hermitian
matrix  in  a  given half-interval (A, B] by using a bisection and inverse
iteration

Inputs:
    A       -   Hermitian matrix which is given  by  its  upper  or  lower
                triangular  part.  Array  whose   indexes   range   within
                [0..N-1, 0..N-1].
    N       -   size of matrix A.
    ZNeeded -   flag controlling whether the eigenvectors  are  needed  or
                not. If ZNeeded is equal to:
                 * 0, the eigenvectors are not returned;
                 * 1, the eigenvectors are returned.
    IsUpperA -  storage format of matrix A.
    B1, B2 -    half-interval (B1, B2] to search eigenvalues in.

Outputs:
    M       -   number of eigenvalues found in a given half-interval, M &ge; 0
    W       -   array of the eigenvalues found.
                Array whose index ranges within [0..M-1].
    Z       -   if ZNeeded is equal to:
                 * 0, Z hasn't changed;
                 * 1, Z contains eigenvectors.
                Array whose indexes range within [0..N-1, 0..M-1].
                The eigenvectors are stored in the matrix columns.

Result:
    True, if successful. M contains the number of eigenvalues in the given
    half-interval (could be equal to 0), W contains the eigenvalues,
    Z contains the eigenvectors (if needed).

    False, if the bisection method subroutine  wasn't  able  to  find  the
    eigenvalues  in  the  given  interval  or  if  the  inverse  iteration
    subroutine  wasn't  able  to  find all the corresponding eigenvectors.
    In that case, the eigenvalues and eigenvectors are not returned, M  is
    equal to 0.

Note:
    eigen vectors of Hermitian matrix are defined up to multiplication  by
    a complex number L, such as |L|=1.
ALGLIB: Copyright 07.01.2006, 24.03.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> hmatrixevdr(complex_2d_array a, ae_int_t n, ae_int_t zneeded, <b>bool</b> isupper, <b>double</b> b1, <b>double</b> b2, ae_int_t &amp;m, real_1d_array &amp;w, complex_2d_array &amp;z);
</pre>
<a name=sub_rmatrixevd></a><h6 class=pageheader>rmatrixevd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Finding eigenvalues and eigenvectors of a general (unsymmetric) matrix

The algorithm finds eigenvalues and eigenvectors of a general matrix by
using the QR algorithm with multiple shifts. The algorithm can find
eigenvalues and both left and right eigenvectors.

The right eigenvector is a vector x such that A*x = w*x, and the left
eigenvector is a vector y such that y'*A = w*y' (here y' implies a complex
conjugate transposition of vector y).

Inputs:
    A       -   matrix. Array whose indexes range within [0..N-1, 0..N-1].
    N       -   size of matrix A.
    VNeeded -   flag controlling whether eigenvectors are needed or not.
                If VNeeded is equal to:
                 * 0, eigenvectors are not returned;
                 * 1, right eigenvectors are returned;
                 * 2, left eigenvectors are returned;
                 * 3, both left and right eigenvectors are returned.

Outputs:
    WR      -   real parts of eigenvalues.
                Array whose index ranges within [0..N-1].
    WR      -   imaginary parts of eigenvalues.
                Array whose index ranges within [0..N-1].
    VL, VR  -   arrays of left and right eigenvectors (if they are needed).
                If WI[i]=0, the respective eigenvalue is a real number,
                and it corresponds to the column number I of matrices VL/VR.
                If WI[i] &gt; 0, we have a pair of complex conjugate numbers with
                positive and negative imaginary parts:
                    the first eigenvalue WR[i] + sqrt(-1)*WI[i];
                    the second eigenvalue WR[i+1] + sqrt(-1)*WI[i+1];
                    WI[i] &gt; 0
                    WI[i+1] = -WI[i] &lt; 0
                In that case, the eigenvector  corresponding to the first
                eigenvalue is located in i and i+1 columns of matrices
                VL/VR (the column number i contains the real part, and the
                column number i+1 contains the imaginary part), and the vector
                corresponding to the second eigenvalue is a complex conjugate to
                the first vector.
                Arrays whose indexes range within [0..N-1, 0..N-1].

Result:
    True, if the algorithm has converged.
    False, if the algorithm has not converged.

Note 1:
    Some users may ask the following question: what if WI[N-1] &gt; 0?
    WI[N] must contain an eigenvalue which is complex conjugate to the
    N-th eigenvalue, but the array has only size N?
    The answer is as follows: such a situation cannot occur because the
    algorithm finds a pairs of eigenvalues, therefore, if WI[i] &gt; 0, I is
    strictly less than N-1.

Note 2:
    The algorithm performance depends on the value of the internal parameter
    NS of the InternalSchurDecomposition subroutine which defines the number
    of shifts in the QR algorithm (similarly to the block width in block-matrix
    algorithms of linear algebra). If you require maximum performance
    on your machine, it is recommended to adjust this parameter manually.

See also the InternalTREVC subroutine.

The algorithm is based on the LAPACK 3.0 library.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> rmatrixevd(real_2d_array a, ae_int_t n, ae_int_t vneeded, real_1d_array &amp;wr, real_1d_array &amp;wi, real_2d_array &amp;vl, real_2d_array &amp;vr);
</pre>
<a name=sub_smatrixevd></a><h6 class=pageheader>smatrixevd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Finding the eigenvalues and eigenvectors of a symmetric matrix

The algorithm finds eigen pairs of a symmetric matrix by reducing it to
tridiagonal form and using the QL/QR algorithm.

Inputs:
    A       -   symmetric matrix which is given by its upper or lower
                triangular part.
                Array whose indexes range within [0..N-1, 0..N-1].
    N       -   size of matrix A.
    ZNeeded -   flag controlling whether the eigenvectors are needed or not.
                If ZNeeded is equal to:
                 * 0, the eigenvectors are not returned;
                 * 1, the eigenvectors are returned.
    IsUpper -   storage format.

Outputs:
    D       -   eigenvalues in ascending order.
                Array whose index ranges within [0..N-1].
    Z       -   if ZNeeded is equal to:
                 * 0, Z hasn't changed;
                 * 1, Z contains the eigenvectors.
                Array whose indexes range within [0..N-1, 0..N-1].
                The eigenvectors are stored in the matrix columns.

Result:
    True, if the algorithm has converged.
    False, if the algorithm hasn't converged (rare case).
ALGLIB: Copyright 2005-2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> smatrixevd(real_2d_array a, ae_int_t n, ae_int_t zneeded, <b>bool</b> isupper, real_1d_array &amp;d, real_2d_array &amp;z);
</pre>
<a name=sub_smatrixevdi></a><h6 class=pageheader>smatrixevdi Function</h6>
<hr width=600 align=left>
<pre class=narration>
Subroutine for finding the eigenvalues and  eigenvectors  of  a  symmetric
matrix with given indexes by using bisection and inverse iteration methods.

Inputs:
    A       -   symmetric matrix which is given by its upper or lower
                triangular part. Array whose indexes range within [0..N-1, 0..N-1].
    N       -   size of matrix A.
    ZNeeded -   flag controlling whether the eigenvectors are needed or not.
                If ZNeeded is equal to:
                 * 0, the eigenvectors are not returned;
                 * 1, the eigenvectors are returned.
    IsUpperA -  storage format of matrix A.
    I1, I2 -    index interval for searching (from I1 to I2).
                0 &le; I1 &le; I2 &le; N-1.

Outputs:
    W       -   array of the eigenvalues found.
                Array whose index ranges within [0..I2-I1].
    Z       -   if ZNeeded is equal to:
                 * 0, Z hasn't changed;
                 * 1, Z contains eigenvectors.
                Array whose indexes range within [0..N-1, 0..I2-I1].
                In that case, the eigenvectors are stored in the matrix columns.

Result:
    True, if successful. W contains the eigenvalues, Z contains the
    eigenvectors (if needed).

    False, if the bisection method subroutine wasn't able to find the
    eigenvalues in the given interval or if the inverse iteration subroutine
    wasn't able to find all the corresponding eigenvectors.
    In that case, the eigenvalues and eigenvectors are not returned.
ALGLIB: Copyright 07.01.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> smatrixevdi(real_2d_array a, ae_int_t n, ae_int_t zneeded, <b>bool</b> isupper, ae_int_t i1, ae_int_t i2, real_1d_array &amp;w, real_2d_array &amp;z);
</pre>
<a name=sub_smatrixevdr></a><h6 class=pageheader>smatrixevdr Function</h6>
<hr width=600 align=left>
<pre class=narration>
Subroutine for finding the eigenvalues (and eigenvectors) of  a  symmetric
matrix  in  a  given half open interval (A, B] by using  a  bisection  and
inverse iteration

Inputs:
    A       -   symmetric matrix which is given by its upper or lower
                triangular part. Array [0..N-1, 0..N-1].
    N       -   size of matrix A.
    ZNeeded -   flag controlling whether the eigenvectors are needed or not.
                If ZNeeded is equal to:
                 * 0, the eigenvectors are not returned;
                 * 1, the eigenvectors are returned.
    IsUpperA -  storage format of matrix A.
    B1, B2 -    half open interval (B1, B2] to search eigenvalues in.

Outputs:
    M       -   number of eigenvalues found in a given half-interval (M &ge; 0).
    W       -   array of the eigenvalues found.
                Array whose index ranges within [0..M-1].
    Z       -   if ZNeeded is equal to:
                 * 0, Z hasn't changed;
                 * 1, Z contains eigenvectors.
                Array whose indexes range within [0..N-1, 0..M-1].
                The eigenvectors are stored in the matrix columns.

Result:
    True, if successful. M contains the number of eigenvalues in the given
    half-interval (could be equal to 0), W contains the eigenvalues,
    Z contains the eigenvectors (if needed).

    False, if the bisection method subroutine wasn't able to find the
    eigenvalues in the given interval or if the inverse iteration subroutine
    wasn't able to find all the corresponding eigenvectors.
    In that case, the eigenvalues and eigenvectors are not returned,
    M is equal to 0.
ALGLIB: Copyright 07.01.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> smatrixevdr(real_2d_array a, ae_int_t n, ae_int_t zneeded, <b>bool</b> isupper, <b>double</b> b1, <b>double</b> b2, ae_int_t &amp;m, real_1d_array &amp;w, real_2d_array &amp;z);
</pre>
<a name=sub_smatrixtdevd></a><h6 class=pageheader>smatrixtdevd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Finding the eigenvalues and eigenvectors of a tridiagonal symmetric matrix

The algorithm finds the eigen pairs of a tridiagonal symmetric matrix by
using an QL/QR algorithm with implicit shifts.

Inputs:
    D       -   the main diagonal of a tridiagonal matrix.
                Array whose index ranges within [0..N-1].
    E       -   the secondary diagonal of a tridiagonal matrix.
                Array whose index ranges within [0..N-2].
    N       -   size of matrix A.
    ZNeeded -   flag controlling whether the eigenvectors are needed or not.
                If ZNeeded is equal to:
                 * 0, the eigenvectors are not needed;
                 * 1, the eigenvectors of a tridiagonal matrix
                   are multiplied by the square matrix Z. It is used if the
                   tridiagonal matrix is obtained by the similarity
                   transformation of a symmetric matrix;
                 * 2, the eigenvectors of a tridiagonal matrix replace the
                   square matrix Z;
                 * 3, matrix Z contains the first row of the eigenvectors
                   matrix.
    Z       -   if ZNeeded=1, Z contains the square matrix by which the
                eigenvectors are multiplied.
                Array whose indexes range within [0..N-1, 0..N-1].

Outputs:
    D       -   eigenvalues in ascending order.
                Array whose index ranges within [0..N-1].
    Z       -   if ZNeeded is equal to:
                 * 0, Z hasn't changed;
                 * 1, Z contains the product of a given matrix (from the left)
                   and the eigenvectors matrix (from the right);
                 * 2, Z contains the eigenvectors.
                 * 3, Z contains the first row of the eigenvectors matrix.
                If ZNeeded &lt; 3, Z is the array whose indexes range within [0..N-1, 0..N-1].
                In that case, the eigenvectors are stored in the matrix columns.
                If ZNeeded=3, Z is the array whose indexes range within [0..0, 0..N-1].

Result:
    True, if the algorithm has converged.
    False, if the algorithm hasn't converged.

  -- LAPACK routine (version 3.0) --
     Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,
     Courant Institute, Argonne National Lab, and Rice University
     September 30, 1994
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> smatrixtdevd(real_1d_array &amp;d, real_1d_array e, ae_int_t n, ae_int_t zneeded, real_2d_array &amp;z);
</pre>
<a name=sub_smatrixtdevdi></a><h6 class=pageheader>smatrixtdevdi Function</h6>
<hr width=600 align=left>
<pre class=narration>
Subroutine for finding tridiagonal matrix eigenvalues/vectors with given
indexes (in ascending order) by using the bisection and inverse iteraion.

Inputs:
    D       -   the main diagonal of a tridiagonal matrix.
                Array whose index ranges within [0..N-1].
    E       -   the secondary diagonal of a tridiagonal matrix.
                Array whose index ranges within [0..N-2].
    N       -   size of matrix. N &ge; 0.
    ZNeeded -   flag controlling whether the eigenvectors are needed or not.
                If ZNeeded is equal to:
                 * 0, the eigenvectors are not needed;
                 * 1, the eigenvectors of a tridiagonal matrix are multiplied
                   by the square matrix Z. It is used if the
                   tridiagonal matrix is obtained by the similarity transformation
                   of a symmetric matrix.
                 * 2, the eigenvectors of a tridiagonal matrix replace
                   matrix Z.
    I1, I2  -   index interval for searching (from I1 to I2).
                0 &le; I1 &le; I2 &le; N-1.
    Z       -   if ZNeeded is equal to:
                 * 0, Z isn't used and remains unchanged;
                 * 1, Z contains the square matrix (array whose indexes range within [0..N-1, 0..N-1])
                   which reduces the given symmetric matrix to  tridiagonal form;
                 * 2, Z isn't used (but changed on the exit).

Outputs:
    D       -   array of the eigenvalues found.
                Array whose index ranges within [0..I2-I1].
    Z       -   if ZNeeded is equal to:
                 * 0, doesn't contain any information;
                 * 1, contains the product of a given NxN matrix Z (from the left) and
                   Nx(I2-I1) matrix of the eigenvectors found (from the right).
                   Array whose indexes range within [0..N-1, 0..I2-I1].
                 * 2, contains the matrix of the eigenvalues found.
                   Array whose indexes range within [0..N-1, 0..I2-I1].

Result:
    True, if successful. In that case, D contains the eigenvalues,
    Z contains the eigenvectors (if needed).
    It should be noted that the subroutine changes the size of arrays D and Z.

    False, if the bisection method subroutine wasn't able to find the eigenvalues
    in the given interval or if the inverse iteration subroutine wasn't able
    to find all the corresponding eigenvectors. In that case, the eigenvalues
    and eigenvectors are not returned.
ALGLIB: Copyright 25.12.2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> smatrixtdevdi(real_1d_array &amp;d, real_1d_array e, ae_int_t n, ae_int_t zneeded, ae_int_t i1, ae_int_t i2, real_2d_array &amp;z);
</pre>
<a name=sub_smatrixtdevdr></a><h6 class=pageheader>smatrixtdevdr Function</h6>
<hr width=600 align=left>
<pre class=narration>
Subroutine for finding the tridiagonal matrix eigenvalues/vectors in a
given half-interval (A, B] by using bisection and inverse iteration.

Inputs:
    D       -   the main diagonal of a tridiagonal matrix.
                Array whose index ranges within [0..N-1].
    E       -   the secondary diagonal of a tridiagonal matrix.
                Array whose index ranges within [0..N-2].
    N       -   size of matrix, N &ge; 0.
    ZNeeded -   flag controlling whether the eigenvectors are needed or not.
                If ZNeeded is equal to:
                 * 0, the eigenvectors are not needed;
                 * 1, the eigenvectors of a tridiagonal matrix are multiplied
                   by the square matrix Z. It is used if the tridiagonal
                   matrix is obtained by the similarity transformation
                   of a symmetric matrix.
                 * 2, the eigenvectors of a tridiagonal matrix replace matrix Z.
    A, B    -   half-interval (A, B] to search eigenvalues in.
    Z       -   if ZNeeded is equal to:
                 * 0, Z isn't used and remains unchanged;
                 * 1, Z contains the square matrix (array whose indexes range
                   within [0..N-1, 0..N-1]) which reduces the given symmetric
                   matrix to tridiagonal form;
                 * 2, Z isn't used (but changed on the exit).

Outputs:
    D       -   array of the eigenvalues found.
                Array whose index ranges within [0..M-1].
    M       -   number of eigenvalues found in the given half-interval (M &ge; 0).
    Z       -   if ZNeeded is equal to:
                 * 0, doesn't contain any information;
                 * 1, contains the product of a given NxN matrix Z (from the
                   left) and NxM matrix of the eigenvectors found (from the
                   right). Array whose indexes range within [0..N-1, 0..M-1].
                 * 2, contains the matrix of the eigenvectors found.
                   Array whose indexes range within [0..N-1, 0..M-1].

Result:

    True, if successful. In that case, M contains the number of eigenvalues
    in the given half-interval (could be equal to 0), D contains the eigenvalues,
    Z contains the eigenvectors (if needed).
    It should be noted that the subroutine changes the size of arrays D and Z.

    False, if the bisection method subroutine wasn't able to find the
    eigenvalues in the given interval or if the inverse iteration subroutine
    wasn't able to find all the corresponding eigenvectors. In that case,
    the eigenvalues and eigenvectors are not returned, M is equal to 0.
ALGLIB: Copyright 31.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> smatrixtdevdr(real_1d_array &amp;d, real_1d_array e, ae_int_t n, ae_int_t zneeded, <b>double</b> a, <b>double</b> b, ae_int_t &amp;m, real_2d_array &amp;z);
</pre>
<a name=unit_inverseupdate></a><h4 class=pageheader>8.7.4. inverseupdate Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_rmatrixinvupdatecolumn class=toc>rmatrixinvupdatecolumn</a> |
<a href=#sub_rmatrixinvupdaterow class=toc>rmatrixinvupdaterow</a> |
<a href=#sub_rmatrixinvupdatesimple class=toc>rmatrixinvupdatesimple</a> |
<a href=#sub_rmatrixinvupdateuv class=toc>rmatrixinvupdateuv</a>
]</font>
</div>
<a name=sub_rmatrixinvupdatecolumn></a><h6 class=pageheader>rmatrixinvupdatecolumn Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse matrix update by the Sherman-Morrison formula

The algorithm updates matrix A^-1 when adding a vector to a column
of matrix A.

Inputs:
    InvA        -   inverse of matrix A.
                    Array whose indexes range within [0..N-1, 0..N-1].
    N           -   size of matrix A.
    UpdColumn   -   the column of A whose vector U was added.
                    0 &le; UpdColumn &le; N-1
    U           -   the vector to be added to a column.
                    Array whose index ranges within [0..N-1].

Outputs:
    InvA        -   inverse of modified matrix A.
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixinvupdatecolumn(real_2d_array &amp;inva, ae_int_t n, ae_int_t updcolumn, real_1d_array u);
</pre>
<a name=sub_rmatrixinvupdaterow></a><h6 class=pageheader>rmatrixinvupdaterow Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse matrix update by the Sherman-Morrison formula

The algorithm updates matrix A^-1 when adding a vector to a row
of matrix A.

Inputs:
    InvA    -   inverse of matrix A.
                Array whose indexes range within [0..N-1, 0..N-1].
    N       -   size of matrix A.
    UpdRow  -   the row of A whose vector V was added.
                0 &le; Row &le; N-1
    V       -   the vector to be added to a row.
                Array whose index ranges within [0..N-1].

Outputs:
    InvA    -   inverse of modified matrix A.
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixinvupdaterow(real_2d_array &amp;inva, ae_int_t n, ae_int_t updrow, real_1d_array v);
</pre>
<a name=sub_rmatrixinvupdatesimple></a><h6 class=pageheader>rmatrixinvupdatesimple Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse matrix update by the Sherman-Morrison formula

The algorithm updates matrix A^-1 when adding a number to an element
of matrix A.

Inputs:
    InvA    -   inverse of matrix A.
                Array whose indexes range within [0..N-1, 0..N-1].
    N       -   size of matrix A.
    UpdRow  -   row where the element to be updated is stored.
    UpdColumn - column where the element to be updated is stored.
    UpdVal  -   a number to be added to the element.

Outputs:
    InvA    -   inverse of modified matrix A.
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixinvupdatesimple(real_2d_array &amp;inva, ae_int_t n, ae_int_t updrow, ae_int_t updcolumn, <b>double</b> updval);
</pre>
<a name=sub_rmatrixinvupdateuv></a><h6 class=pageheader>rmatrixinvupdateuv Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse matrix update by the Sherman-Morrison formula

The algorithm computes the inverse of matrix A+u*v' by using the given matrix
A^-1 and the vectors u and v.

Inputs:
    InvA    -   inverse of matrix A.
                Array whose indexes range within [0..N-1, 0..N-1].
    N       -   size of matrix A.
    U       -   the vector modifying the matrix.
                Array whose index ranges within [0..N-1].
    V       -   the vector modifying the matrix.
                Array whose index ranges within [0..N-1].

Outputs:
    InvA - inverse of matrix A + u*v'.
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixinvupdateuv(real_2d_array &amp;inva, ae_int_t n, real_1d_array u, real_1d_array v);
</pre>
<a name=unit_matdet></a><h4 class=pageheader>8.7.5. matdet Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_cmatrixdet class=toc>cmatrixdet</a> |
<a href=#sub_cmatrixludet class=toc>cmatrixludet</a> |
<a href=#sub_rmatrixdet class=toc>rmatrixdet</a> |
<a href=#sub_rmatrixludet class=toc>rmatrixludet</a> |
<a href=#sub_spdmatrixcholeskydet class=toc>spdmatrixcholeskydet</a> |
<a href=#sub_spdmatrixdet class=toc>spdmatrixdet</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_matdet_d_1 class=toc>matdet_d_1</a></td><td width=15>&nbsp;</td><td>Determinant calculation, real matrix, short form</td></tr>
<tr align=left valign=top><td><a href=#example_matdet_d_2 class=toc>matdet_d_2</a></td><td width=15>&nbsp;</td><td>Determinant calculation, real matrix, full form</td></tr>
<tr align=left valign=top><td><a href=#example_matdet_d_3 class=toc>matdet_d_3</a></td><td width=15>&nbsp;</td><td>Determinant calculation, complex matrix, short form</td></tr>
<tr align=left valign=top><td><a href=#example_matdet_d_4 class=toc>matdet_d_4</a></td><td width=15>&nbsp;</td><td>Determinant calculation, complex matrix, full form</td></tr>
<tr align=left valign=top><td><a href=#example_matdet_d_5 class=toc>matdet_d_5</a></td><td width=15>&nbsp;</td><td>Determinant calculation, complex matrix with zero imaginary part, short form</td></tr>
</table>
</div>
<a name=sub_cmatrixdet></a><h6 class=pageheader>cmatrixdet Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the determinant of a general matrix

Inputs:
    A       -   matrix, array[0..N-1, 0..N-1]
    N       -   (optional) size of matrix A:
                * if given, only principal NxN submatrix is processed and
                  overwritten. other elements are unchanged.
                * if not given, automatically determined from matrix size
                  (A must be square matrix)

Result: determinant of matrix A.
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
complex cmatrixdet(complex_2d_array a, ae_int_t n);
complex cmatrixdet(complex_2d_array a);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_matdet_d_3 class=nav>matdet_d_3</a> | <a href=#example_matdet_d_4 class=nav>matdet_d_4</a> | <a href=#example_matdet_d_5 class=nav>matdet_d_5</a> ]</p>
<a name=sub_cmatrixludet></a><h6 class=pageheader>cmatrixludet Function</h6>
<hr width=600 align=left>
<pre class=narration>
Determinant calculation of the matrix given by its LU decomposition.

Inputs:
    A       -   LU decomposition of the matrix (output of
                RMatrixLU subroutine).
    Pivots  -   table of permutations which were made during
                the LU decomposition.
                Output of RMatrixLU subroutine.
    N       -   (optional) size of matrix A:
                * if given, only principal NxN submatrix is processed and
                  overwritten. other elements are unchanged.
                * if not given, automatically determined from matrix size
                  (A must be square matrix)

Result: matrix determinant.
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
complex cmatrixludet(complex_2d_array a, integer_1d_array pivots, ae_int_t n);
complex cmatrixludet(complex_2d_array a, integer_1d_array pivots);
</pre>
<a name=sub_rmatrixdet></a><h6 class=pageheader>rmatrixdet Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the determinant of a general matrix

Inputs:
    A       -   matrix, array[0..N-1, 0..N-1]
    N       -   (optional) size of matrix A:
                * if given, only principal NxN submatrix is processed and
                  overwritten. other elements are unchanged.
                * if not given, automatically determined from matrix size
                  (A must be square matrix)

Result: determinant of matrix A.
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rmatrixdet(real_2d_array a, ae_int_t n);
<b>double</b> rmatrixdet(real_2d_array a);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_matdet_d_1 class=nav>matdet_d_1</a> | <a href=#example_matdet_d_2 class=nav>matdet_d_2</a> ]</p>
<a name=sub_rmatrixludet></a><h6 class=pageheader>rmatrixludet Function</h6>
<hr width=600 align=left>
<pre class=narration>
Determinant calculation of the matrix given by its LU decomposition.

Inputs:
    A       -   LU decomposition of the matrix (output of
                RMatrixLU subroutine).
    Pivots  -   table of permutations which were made during
                the LU decomposition.
                Output of RMatrixLU subroutine.
    N       -   (optional) size of matrix A:
                * if given, only principal NxN submatrix is processed and
                  overwritten. other elements are unchanged.
                * if not given, automatically determined from matrix size
                  (A must be square matrix)

Result: matrix determinant.
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rmatrixludet(real_2d_array a, integer_1d_array pivots, ae_int_t n);
<b>double</b> rmatrixludet(real_2d_array a, integer_1d_array pivots);
</pre>
<a name=sub_spdmatrixcholeskydet></a><h6 class=pageheader>spdmatrixcholeskydet Function</h6>
<hr width=600 align=left>
<pre class=narration>
Determinant calculation of the matrix given by the Cholesky decomposition.

Inputs:
    A       -   Cholesky decomposition,
                output of SMatrixCholesky subroutine.
    N       -   (optional) size of matrix A:
                * if given, only principal NxN submatrix is processed and
                  overwritten. other elements are unchanged.
                * if not given, automatically determined from matrix size
                  (A must be square matrix)

As the determinant is equal to the product of squares of diagonal elements,
it's not necessary to specify which triangle - lower or upper - the matrix
is stored in.

Result:
    matrix determinant.
ALGLIB: Copyright 2005-2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spdmatrixcholeskydet(real_2d_array a, ae_int_t n);
<b>double</b> spdmatrixcholeskydet(real_2d_array a);
</pre>
<a name=sub_spdmatrixdet></a><h6 class=pageheader>spdmatrixdet Function</h6>
<hr width=600 align=left>
<pre class=narration>
Determinant calculation of the symmetric positive definite matrix.

Inputs:
    A       -   matrix. Array with elements [0..N-1, 0..N-1].
    N       -   (optional) size of matrix A:
                * if given, only principal NxN submatrix is processed and
                  overwritten. other elements are unchanged.
                * if not given, automatically determined from matrix size
                  (A must be square matrix)
    IsUpper -   (optional) storage type:
                * if True, symmetric matrix  A  is  given  by  its  upper
                  triangle, and the lower triangle isn't used/changed  by
                  function
                * if False, symmetric matrix  A  is  given  by  its lower
                  triangle, and the upper triangle isn't used/changed  by
                  function
                * if not given, both lower and upper  triangles  must  be
                  filled.

Result:
    determinant of matrix A.
    If matrix A is not positive definite, exception is thrown.
ALGLIB: Copyright 2005-2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spdmatrixdet(real_2d_array a, ae_int_t n, <b>bool</b> isupper);
<b>double</b> spdmatrixdet(real_2d_array a);
</pre>
<a name=example_matdet_d_1></a><h6 class=pageheader>matdet_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_2d_array b = <font color=blue><b>&quot;[[1,2],[2,1]]&quot;</b></font>;
   <b>double</b> a;
   a = rmatrixdet(b);
   printf(<font color=blue><b>&quot;%.3f\n&quot;</b></font>, <b>double</b>(a)); <font color=navy>// EXPECTED: -3</font>
   <b>return</b> 0;
}
</pre>
<a name=example_matdet_d_2></a><h6 class=pageheader>matdet_d_2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_2d_array b = <font color=blue><b>&quot;[[5,4],[4,5]]&quot;</b></font>;
   <b>double</b> a;
   a = rmatrixdet(b, 2);
   printf(<font color=blue><b>&quot;%.3f\n&quot;</b></font>, <b>double</b>(a)); <font color=navy>// EXPECTED: 9</font>
   <b>return</b> 0;
}
</pre>
<a name=example_matdet_d_3></a><h6 class=pageheader>matdet_d_3 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   complex_2d_array b = <font color=blue><b>&quot;[[1+1i,2],[2,1-1i]]&quot;</b></font>;
   complex a;
   a = cmatrixdet(b);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a.tostring(3).c_str()); <font color=navy>// EXPECTED: -2</font>
   <b>return</b> 0;
}
</pre>
<a name=example_matdet_d_4></a><h6 class=pageheader>matdet_d_4 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   complex a;
   complex_2d_array b = <font color=blue><b>&quot;[[5i,4],[4i,5]]&quot;</b></font>;
   a = cmatrixdet(b, 2);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a.tostring(3).c_str()); <font color=navy>// EXPECTED: 9i</font>
   <b>return</b> 0;
}
</pre>
<a name=example_matdet_d_5></a><h6 class=pageheader>matdet_d_5 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   complex a;
   complex_2d_array b = <font color=blue><b>&quot;[[9,1],[2,1]]&quot;</b></font>;
   a = cmatrixdet(b);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a.tostring(3).c_str()); <font color=navy>// EXPECTED: 7</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_matgen></a><h4 class=pageheader>8.7.6. matgen Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_cmatrixrndcond class=toc>cmatrixrndcond</a> |
<a href=#sub_cmatrixrndorthogonal class=toc>cmatrixrndorthogonal</a> |
<a href=#sub_cmatrixrndorthogonalfromtheleft class=toc>cmatrixrndorthogonalfromtheleft</a> |
<a href=#sub_cmatrixrndorthogonalfromtheright class=toc>cmatrixrndorthogonalfromtheright</a> |
<a href=#sub_hmatrixrndcond class=toc>hmatrixrndcond</a> |
<a href=#sub_hmatrixrndmultiply class=toc>hmatrixrndmultiply</a> |
<a href=#sub_hpdmatrixrndcond class=toc>hpdmatrixrndcond</a> |
<a href=#sub_rmatrixrndcond class=toc>rmatrixrndcond</a> |
<a href=#sub_rmatrixrndorthogonal class=toc>rmatrixrndorthogonal</a> |
<a href=#sub_rmatrixrndorthogonalfromtheleft class=toc>rmatrixrndorthogonalfromtheleft</a> |
<a href=#sub_rmatrixrndorthogonalfromtheright class=toc>rmatrixrndorthogonalfromtheright</a> |
<a href=#sub_smatrixrndcond class=toc>smatrixrndcond</a> |
<a href=#sub_smatrixrndmultiply class=toc>smatrixrndmultiply</a> |
<a href=#sub_spdmatrixrndcond class=toc>spdmatrixrndcond</a>
]</font>
</div>
<a name=sub_cmatrixrndcond></a><h6 class=pageheader>cmatrixrndcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generation of random NxN complex matrix with given condition number C and
norm2(A)=1

Inputs:
    N   -   matrix size
    C   -   condition number (in 2-norm)

Outputs:
    A   -   random matrix with norm2(A)=1 and cond(A)=C
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixrndcond(ae_int_t n, <b>double</b> c, complex_2d_array &amp;a);
</pre>
<a name=sub_cmatrixrndorthogonal></a><h6 class=pageheader>cmatrixrndorthogonal Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generation of a random Haar distributed orthogonal complex matrix

Inputs:
    N   -   matrix size, N &ge; 1

Outputs:
    A   -   orthogonal NxN matrix, array[0..N-1,0..N-1]

NOTE: this function uses algorithm  described  in  Stewart, G. W.  (1980),
      &quot;The Efficient Generation of  Random  Orthogonal  Matrices  with  an
      Application to Condition Estimators&quot;.

      Speaking short, to generate an (N+1)x(N+1) orthogonal matrix, it:
      * takes an NxN one
      * takes uniformly distributed unit vector of dimension N+1.
      * constructs a Householder reflection from the vector, then applies
        it to the smaller matrix (embedded in the larger size with a 1 at
        the bottom right corner).
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixrndorthogonal(ae_int_t n, complex_2d_array &amp;a);
</pre>
<a name=sub_cmatrixrndorthogonalfromtheleft></a><h6 class=pageheader>cmatrixrndorthogonalfromtheleft Function</h6>
<hr width=600 align=left>
<pre class=narration>
Multiplication of MxN complex matrix by MxM random Haar distributed
complex orthogonal matrix

Inputs:
    A   -   matrix, array[0..M-1, 0..N-1]
    M, N-   matrix size

Outputs:
    A   -   Q*A, where Q is random MxM orthogonal matrix
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixrndorthogonalfromtheleft(complex_2d_array &amp;a, ae_int_t m, ae_int_t n);
</pre>
<a name=sub_cmatrixrndorthogonalfromtheright></a><h6 class=pageheader>cmatrixrndorthogonalfromtheright Function</h6>
<hr width=600 align=left>
<pre class=narration>
Multiplication of MxN complex matrix by NxN random Haar distributed
complex orthogonal matrix

Inputs:
    A   -   matrix, array[0..M-1, 0..N-1]
    M, N-   matrix size

Outputs:
    A   -   A*Q, where Q is random NxN orthogonal matrix
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixrndorthogonalfromtheright(complex_2d_array &amp;a, ae_int_t m, ae_int_t n);
</pre>
<a name=sub_hmatrixrndcond></a><h6 class=pageheader>hmatrixrndcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generation of random NxN Hermitian matrix with given condition number  and
norm2(A)=1

Inputs:
    N   -   matrix size
    C   -   condition number (in 2-norm)

Outputs:
    A   -   random matrix with norm2(A)=1 and cond(A)=C
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hmatrixrndcond(ae_int_t n, <b>double</b> c, complex_2d_array &amp;a);
</pre>
<a name=sub_hmatrixrndmultiply></a><h6 class=pageheader>hmatrixrndmultiply Function</h6>
<hr width=600 align=left>
<pre class=narration>
Hermitian multiplication of NxN matrix by random Haar distributed
complex orthogonal matrix

Inputs:
    A   -   matrix, array[0..N-1, 0..N-1]
    N   -   matrix size

Outputs:
    A   -   Q^H*A*Q, where Q is random NxN orthogonal matrix
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hmatrixrndmultiply(complex_2d_array &amp;a, ae_int_t n);
</pre>
<a name=sub_hpdmatrixrndcond></a><h6 class=pageheader>hpdmatrixrndcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generation of random NxN Hermitian positive definite matrix with given
condition number and norm2(A)=1

Inputs:
    N   -   matrix size
    C   -   condition number (in 2-norm)

Outputs:
    A   -   random HPD matrix with norm2(A)=1 and cond(A)=C
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixrndcond(ae_int_t n, <b>double</b> c, complex_2d_array &amp;a);
</pre>
<a name=sub_rmatrixrndcond></a><h6 class=pageheader>rmatrixrndcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generation of random NxN matrix with given condition number and norm2(A)=1

Inputs:
    N   -   matrix size
    C   -   condition number (in 2-norm)

Outputs:
    A   -   random matrix with norm2(A)=1 and cond(A)=C
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixrndcond(ae_int_t n, <b>double</b> c, real_2d_array &amp;a);
</pre>
<a name=sub_rmatrixrndorthogonal></a><h6 class=pageheader>rmatrixrndorthogonal Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generation of a random uniformly distributed (Haar) orthogonal matrix

Inputs:
    N   -   matrix size, N &ge; 1

Outputs:
    A   -   orthogonal NxN matrix, array[0..N-1,0..N-1]

NOTE: this function uses algorithm  described  in  Stewart, G. W.  (1980),
      &quot;The Efficient Generation of  Random  Orthogonal  Matrices  with  an
      Application to Condition Estimators&quot;.

      Speaking short, to generate an (N+1)x(N+1) orthogonal matrix, it:
      * takes an NxN one
      * takes uniformly distributed unit vector of dimension N+1.
      * constructs a Householder reflection from the vector, then applies
        it to the smaller matrix (embedded in the larger size with a 1 at
        the bottom right corner).
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixrndorthogonal(ae_int_t n, real_2d_array &amp;a);
</pre>
<a name=sub_rmatrixrndorthogonalfromtheleft></a><h6 class=pageheader>rmatrixrndorthogonalfromtheleft Function</h6>
<hr width=600 align=left>
<pre class=narration>
Multiplication of MxN matrix by MxM random Haar distributed orthogonal matrix

Inputs:
    A   -   matrix, array[0..M-1, 0..N-1]
    M, N-   matrix size

Outputs:
    A   -   Q*A, where Q is random MxM orthogonal matrix
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixrndorthogonalfromtheleft(real_2d_array &amp;a, ae_int_t m, ae_int_t n);
</pre>
<a name=sub_rmatrixrndorthogonalfromtheright></a><h6 class=pageheader>rmatrixrndorthogonalfromtheright Function</h6>
<hr width=600 align=left>
<pre class=narration>
Multiplication of MxN matrix by NxN random Haar distributed orthogonal matrix

Inputs:
    A   -   matrix, array[0..M-1, 0..N-1]
    M, N-   matrix size

Outputs:
    A   -   A*Q, where Q is random NxN orthogonal matrix
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixrndorthogonalfromtheright(real_2d_array &amp;a, ae_int_t m, ae_int_t n);
</pre>
<a name=sub_smatrixrndcond></a><h6 class=pageheader>smatrixrndcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generation of random NxN symmetric matrix with given condition number  and
norm2(A)=1

Inputs:
    N   -   matrix size
    C   -   condition number (in 2-norm)

Outputs:
    A   -   random matrix with norm2(A)=1 and cond(A)=C
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> smatrixrndcond(ae_int_t n, <b>double</b> c, real_2d_array &amp;a);
</pre>
<a name=sub_smatrixrndmultiply></a><h6 class=pageheader>smatrixrndmultiply Function</h6>
<hr width=600 align=left>
<pre class=narration>
Symmetric multiplication of NxN matrix by random Haar distributed
orthogonal  matrix

Inputs:
    A   -   matrix, array[0..N-1, 0..N-1]
    N   -   matrix size

Outputs:
    A   -   Q'*A*Q, where Q is random NxN orthogonal matrix
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> smatrixrndmultiply(real_2d_array &amp;a, ae_int_t n);
</pre>
<a name=sub_spdmatrixrndcond></a><h6 class=pageheader>spdmatrixrndcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Generation of random NxN symmetric positive definite matrix with given
condition number and norm2(A)=1

Inputs:
    N   -   matrix size
    C   -   condition number (in 2-norm)

Outputs:
    A   -   random SPD matrix with norm2(A)=1 and cond(A)=C
ALGLIB Routine: Copyright 04.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixrndcond(ae_int_t n, <b>double</b> c, real_2d_array &amp;a);
</pre>
<a name=unit_matinv></a><h4 class=pageheader>8.7.7. matinv Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_matinvreport class=toc>matinvreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_cmatrixinverse class=toc>cmatrixinverse</a> |
<a href=#sub_cmatrixluinverse class=toc>cmatrixluinverse</a> |
<a href=#sub_cmatrixtrinverse class=toc>cmatrixtrinverse</a> |
<a href=#sub_hpdmatrixcholeskyinverse class=toc>hpdmatrixcholeskyinverse</a> |
<a href=#sub_hpdmatrixinverse class=toc>hpdmatrixinverse</a> |
<a href=#sub_rmatrixinverse class=toc>rmatrixinverse</a> |
<a href=#sub_rmatrixluinverse class=toc>rmatrixluinverse</a> |
<a href=#sub_rmatrixtrinverse class=toc>rmatrixtrinverse</a> |
<a href=#sub_spdmatrixcholeskyinverse class=toc>spdmatrixcholeskyinverse</a> |
<a href=#sub_spdmatrixinverse class=toc>spdmatrixinverse</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_matinv_d_c1 class=toc>matinv_d_c1</a></td><td width=15>&nbsp;</td><td>Complex matrix inverse</td></tr>
<tr align=left valign=top><td><a href=#example_matinv_d_hpd1 class=toc>matinv_d_hpd1</a></td><td width=15>&nbsp;</td><td>HPD matrix inverse</td></tr>
<tr align=left valign=top><td><a href=#example_matinv_d_r1 class=toc>matinv_d_r1</a></td><td width=15>&nbsp;</td><td>Real matrix inverse</td></tr>
<tr align=left valign=top><td><a href=#example_matinv_d_spd1 class=toc>matinv_d_spd1</a></td><td width=15>&nbsp;</td><td>SPD matrix inverse</td></tr>
</table>
</div>
<a name=struct_matinvreport></a><h6 class=pageheader>matinvreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Matrix inverse report:
* R1    reciprocal of condition number in 1-norm
* RInf  reciprocal of condition number in inf-norm
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> matinvreport {
   <b>double</b> r1;
   <b>double</b> rinf;
};
</pre>
<a name=sub_cmatrixinverse></a><h6 class=pageheader>cmatrixinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inversion of a general matrix.

Inputs:
    A       -   matrix
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)

Outputs:
    Info    -   return code, same as in RMatrixLUInverse
    Rep     -   solver report, same as in RMatrixLUInverse
    A       -   inverse of matrix A, same as in RMatrixLUInverse
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixinverse(complex_2d_array &amp;a, ae_int_t n, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> cmatrixinverse(complex_2d_array &amp;a, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_matinv_d_c1 class=nav>matinv_d_c1</a> ]</p>
<a name=sub_cmatrixluinverse></a><h6 class=pageheader>cmatrixluinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inversion of a matrix given by its LU decomposition.

Inputs:
    A       -   LU decomposition of the matrix
                (output of CMatrixLU subroutine).
    Pivots  -   table of permutations
                (the output of CMatrixLU subroutine).
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)

Outputs:
    Info    -   return code, same as in RMatrixLUInverse
    Rep     -   solver report, same as in RMatrixLUInverse
    A       -   inverse of matrix A, same as in RMatrixLUInverse
ALGLIB Routine: Copyright 05.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixluinverse(complex_2d_array &amp;a, integer_1d_array pivots, ae_int_t n, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> cmatrixluinverse(complex_2d_array &amp;a, integer_1d_array pivots, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<a name=sub_cmatrixtrinverse></a><h6 class=pageheader>cmatrixtrinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Triangular matrix inverse (complex)

The subroutine inverts the following types of matrices:
    * upper triangular
    * upper triangular with unit diagonal
    * lower triangular
    * lower triangular with unit diagonal

In case of an upper (lower) triangular matrix,  the  inverse  matrix  will
also be upper (lower) triangular, and after the end of the algorithm,  the
inverse matrix replaces the source matrix. The elements  below (above) the
main diagonal are not changed by the algorithm.

If  the matrix  has a unit diagonal, the inverse matrix also  has  a  unit
diagonal, and the diagonal elements are not passed to the algorithm.

Inputs:
    A       -   matrix, array[0..N-1, 0..N-1].
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)
    IsUpper -   True, if the matrix is upper triangular.
    IsUnit  -   diagonal type (optional):
                * if True, matrix has unit diagonal (a[i,i] are NOT used)
                * if False, matrix diagonal is arbitrary
                * if not given, False is assumed

Outputs:
    Info    -   same as for RMatrixLUInverse
    Rep     -   same as for RMatrixLUInverse
    A       -   same as for RMatrixLUInverse.
ALGLIB: Copyright 05.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixtrinverse(complex_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper, <b>bool</b> isunit, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> cmatrixtrinverse(complex_2d_array &amp;a, <b>bool</b> isupper, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<a name=sub_hpdmatrixcholeskyinverse></a><h6 class=pageheader>hpdmatrixcholeskyinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inversion of a Hermitian positive definite matrix which is given
by Cholesky decomposition.

Inputs:
    A       -   Cholesky decomposition of the matrix to be inverted:
                A=U'*U or A = L*L'.
                Output of  HPDMatrixCholesky subroutine.
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)
    IsUpper -   storage type (optional):
                * if True, symmetric  matrix  A  is  given  by  its  upper
                  triangle, and the lower triangle isn't  used/changed  by
                  function
                * if False,  symmetric matrix  A  is  given  by  its lower
                  triangle, and the  upper triangle isn't used/changed  by
                  function
                * if not given, lower half is used.

Outputs:
    Info    -   return code, same as in RMatrixLUInverse
    Rep     -   solver report, same as in RMatrixLUInverse
    A       -   inverse of matrix A, same as in RMatrixLUInverse
ALGLIB Routine: Copyright 10.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixcholeskyinverse(complex_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> hpdmatrixcholeskyinverse(complex_2d_array &amp;a, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<a name=sub_hpdmatrixinverse></a><h6 class=pageheader>hpdmatrixinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inversion of a Hermitian positive definite matrix.

Given an upper or lower triangle of a Hermitian positive definite matrix,
the algorithm generates matrix A^-1 and saves the upper or lower triangle
depending on the input.

Inputs:
    A       -   matrix to be inverted (upper or lower triangle).
                Array with elements [0..N-1,0..N-1].
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)
    IsUpper -   storage type (optional):
                * if True, symmetric  matrix  A  is  given  by  its  upper
                  triangle, and the lower triangle isn't  used/changed  by
                  function
                * if False,  symmetric matrix  A  is  given  by  its lower
                  triangle, and the  upper triangle isn't used/changed  by
                  function
                * if not given,  both lower and upper  triangles  must  be
                  filled.

Outputs:
    Info    -   return code, same as in RMatrixLUInverse
    Rep     -   solver report, same as in RMatrixLUInverse
    A       -   inverse of matrix A, same as in RMatrixLUInverse
ALGLIB Routine: Copyright 10.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixinverse(complex_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> hpdmatrixinverse(complex_2d_array &amp;a, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_matinv_d_hpd1 class=nav>matinv_d_hpd1</a> ]</p>
<a name=sub_rmatrixinverse></a><h6 class=pageheader>rmatrixinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inversion of a general matrix.

Inputs:
    A       -   matrix.
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)

Outputs:
    Info    -   return code, same as in RMatrixLUInverse
    Rep     -   solver report, same as in RMatrixLUInverse
    A       -   inverse of matrix A, same as in RMatrixLUInverse

Result:
    True, if the matrix is not singular.
    False, if the matrix is singular.
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixinverse(real_2d_array &amp;a, ae_int_t n, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> rmatrixinverse(real_2d_array &amp;a, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_matinv_d_r1 class=nav>matinv_d_r1</a> ]</p>
<a name=sub_rmatrixluinverse></a><h6 class=pageheader>rmatrixluinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inversion of a matrix given by its LU decomposition.

Inputs:
    A       -   LU decomposition of the matrix
                (output of RMatrixLU subroutine).
    Pivots  -   table of permutations
                (the output of RMatrixLU subroutine).
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)

Outputs:
    Info    -   return code:
                * -3    A is singular, or VERY close to singular.
                        it is filled by zeros in such cases.
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   solver report, see below for more info
    A       -   inverse of matrix A.
                Array whose indexes range within [0..N-1, 0..N-1].

SOLVER REPORT

Subroutine sets following fields of the Rep structure:
* R1        reciprocal of condition number: 1/cond(A), 1-norm.
* RInf      reciprocal of condition number: 1/cond(A), inf-norm.
ALGLIB Routine: Copyright 05.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixluinverse(real_2d_array &amp;a, integer_1d_array pivots, ae_int_t n, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> rmatrixluinverse(real_2d_array &amp;a, integer_1d_array pivots, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<a name=sub_rmatrixtrinverse></a><h6 class=pageheader>rmatrixtrinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Triangular matrix inverse (real)

The subroutine inverts the following types of matrices:
    * upper triangular
    * upper triangular with unit diagonal
    * lower triangular
    * lower triangular with unit diagonal

In case of an upper (lower) triangular matrix,  the  inverse  matrix  will
also be upper (lower) triangular, and after the end of the algorithm,  the
inverse matrix replaces the source matrix. The elements  below (above) the
main diagonal are not changed by the algorithm.

If  the matrix  has a unit diagonal, the inverse matrix also  has  a  unit
diagonal, and the diagonal elements are not passed to the algorithm.

Inputs:
    A       -   matrix, array[0..N-1, 0..N-1].
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)
    IsUpper -   True, if the matrix is upper triangular.
    IsUnit  -   diagonal type (optional):
                * if True, matrix has unit diagonal (a[i,i] are NOT used)
                * if False, matrix diagonal is arbitrary
                * if not given, False is assumed

Outputs:
    Info    -   same as for RMatrixLUInverse
    Rep     -   same as for RMatrixLUInverse
    A       -   same as for RMatrixLUInverse.
ALGLIB: Copyright 05.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixtrinverse(real_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper, <b>bool</b> isunit, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> rmatrixtrinverse(real_2d_array &amp;a, <b>bool</b> isupper, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<a name=sub_spdmatrixcholeskyinverse></a><h6 class=pageheader>spdmatrixcholeskyinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inversion of a symmetric positive definite matrix which is given
by Cholesky decomposition.

Inputs:
    A       -   Cholesky decomposition of the matrix to be inverted:
                A=U'*U or A = L*L'.
                Output of  SPDMatrixCholesky subroutine.
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)
    IsUpper -   storage type (optional):
                * if True, symmetric  matrix  A  is  given  by  its  upper
                  triangle, and the lower triangle isn't  used/changed  by
                  function
                * if False,  symmetric matrix  A  is  given  by  its lower
                  triangle, and the  upper triangle isn't used/changed  by
                  function
                * if not given, lower half is used.

Outputs:
    Info    -   return code, same as in RMatrixLUInverse
    Rep     -   solver report, same as in RMatrixLUInverse
    A       -   inverse of matrix A, same as in RMatrixLUInverse
ALGLIB Routine: Copyright 10.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixcholeskyinverse(real_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> spdmatrixcholeskyinverse(real_2d_array &amp;a, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<a name=sub_spdmatrixinverse></a><h6 class=pageheader>spdmatrixinverse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inversion of a symmetric positive definite matrix.

Given an upper or lower triangle of a symmetric positive definite matrix,
the algorithm generates matrix A^-1 and saves the upper or lower triangle
depending on the input.

Inputs:
    A       -   matrix to be inverted (upper or lower triangle).
                Array with elements [0..N-1,0..N-1].
    N       -   size of matrix A (optional) :
                * if given, only principal NxN submatrix is processed  and
                  overwritten. other elements are unchanged.
                * if not given,  size  is  automatically  determined  from
                  matrix size (A must be square matrix)
    IsUpper -   storage type (optional):
                * if True, symmetric  matrix  A  is  given  by  its  upper
                  triangle, and the lower triangle isn't  used/changed  by
                  function
                * if False,  symmetric matrix  A  is  given  by  its lower
                  triangle, and the  upper triangle isn't used/changed  by
                  function
                * if not given,  both lower and upper  triangles  must  be
                  filled.

Outputs:
    Info    -   return code, same as in RMatrixLUInverse
    Rep     -   solver report, same as in RMatrixLUInverse
    A       -   inverse of matrix A, same as in RMatrixLUInverse
ALGLIB Routine: Copyright 10.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixinverse(real_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper, ae_int_t &amp;info, matinvreport &amp;rep);
<b>void</b> spdmatrixinverse(real_2d_array &amp;a, ae_int_t &amp;info, matinvreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_matinv_d_spd1 class=nav>matinv_d_spd1</a> ]</p>
<a name=example_matinv_d_c1></a><h6 class=pageheader>matinv_d_c1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   complex_2d_array a = <font color=blue><b>&quot;[[1i,-1],[1i,1]]&quot;</b></font>;
   ae_int_t info;
   matinvreport rep;
   cmatrixinverse(a, info, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a.tostring(4).c_str()); <font color=navy>// EXPECTED: [[-0.5i,-0.5i],[-0.5,0.5]]</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(rep.r1)); <font color=navy>// EXPECTED: 0.5</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(rep.rinf)); <font color=navy>// EXPECTED: 0.5</font>
   <b>return</b> 0;
}
</pre>
<a name=example_matinv_d_hpd1></a><h6 class=pageheader>matinv_d_hpd1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   complex_2d_array a = <font color=blue><b>&quot;[[2,1],[1,2]]&quot;</b></font>;
   ae_int_t info;
   matinvreport rep;
   hpdmatrixinverse(a, info, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a.tostring(4).c_str()); <font color=navy>// EXPECTED: [[0.666666,-0.333333],[-0.333333,0.666666]]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_matinv_d_r1></a><h6 class=pageheader>matinv_d_r1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_2d_array a = <font color=blue><b>&quot;[[1,-1],[1,1]]&quot;</b></font>;
   ae_int_t info;
   matinvreport rep;
   rmatrixinverse(a, info, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a.tostring(4).c_str()); <font color=navy>// EXPECTED: [[0.5,0.5],[-0.5,0.5]]</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(rep.r1)); <font color=navy>// EXPECTED: 0.5</font>
   printf(<font color=blue><b>&quot;%.4f\n&quot;</b></font>, <b>double</b>(rep.rinf)); <font color=navy>// EXPECTED: 0.5</font>
   <b>return</b> 0;
}
</pre>
<a name=example_matinv_d_spd1></a><h6 class=pageheader>matinv_d_spd1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_2d_array a = <font color=blue><b>&quot;[[2,1],[1,2]]&quot;</b></font>;
   ae_int_t info;
   matinvreport rep;
   spdmatrixinverse(a, info, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(info)); <font color=navy>// EXPECTED: 1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, a.tostring(4).c_str()); <font color=navy>// EXPECTED: [[0.666666,-0.333333],[-0.333333,0.666666]]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_normestimator></a><h4 class=pageheader>8.7.8. normestimator Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_normestimatorstate class=toc>normestimatorstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_normestimatorcreate class=toc>normestimatorcreate</a> |
<a href=#sub_normestimatorestimatesparse class=toc>normestimatorestimatesparse</a> |
<a href=#sub_normestimatorresults class=toc>normestimatorresults</a> |
<a href=#sub_normestimatorsetseed class=toc>normestimatorsetseed</a>
]</font>
</div>
<a name=struct_normestimatorstate></a><h6 class=pageheader>normestimatorstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores state of the iterative norm estimation algorithm.
You should use ALGLIB functions to work with this object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> normestimatorstate {
};
</pre>
<a name=sub_normestimatorcreate></a><h6 class=pageheader>normestimatorcreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This procedure initializes matrix norm estimator.

USAGE:
1. User initializes algorithm state with NormEstimatorCreate() call
2. User calls NormEstimatorEstimateSparse() (or NormEstimatorIteration())
3. User calls NormEstimatorResults() to get solution.

Inputs:
    M       -   number of rows in the matrix being estimated, M &gt; 0
    N       -   number of columns in the matrix being estimated, N &gt; 0
    NStart  -   number of random starting vectors
                recommended value - at least 5.
    NIts    -   number of iterations to do with best starting vector
                recommended value - at least 5.

Outputs:
    State   -   structure which stores algorithm state

NOTE: this algorithm is effectively deterministic, i.e. it always  returns
same result when repeatedly called for the same matrix. In fact, algorithm
uses randomized starting vectors, but internal  random  numbers  generator
always generates same sequence of the random values (it is a  feature, not
bug).

Algorithm can be made non-deterministic with NormEstimatorSetSeed(0) call.
ALGLIB: Copyright 06.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> normestimatorcreate(ae_int_t m, ae_int_t n, ae_int_t nstart, ae_int_t nits, normestimatorstate &amp;state);
</pre>
<a name=sub_normestimatorestimatesparse></a><h6 class=pageheader>normestimatorestimatesparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function estimates norm of the sparse M*N matrix A.

Inputs:
    State       -   norm estimator state, must be initialized with a  call
                    to NormEstimatorCreate()
    A           -   sparse M*N matrix, must be converted to CRS format
                    prior to calling this function.

After this function  is  over  you can call NormEstimatorResults() to get
estimate of the norm(A).
ALGLIB: Copyright 06.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> normestimatorestimatesparse(normestimatorstate state, sparsematrix a);
</pre>
<a name=sub_normestimatorresults></a><h6 class=pageheader>normestimatorresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Matrix norm estimation results

Inputs:
    State   -   algorithm state

Outputs:
    Nrm     -   estimate of the matrix norm, Nrm &ge; 0
ALGLIB: Copyright 06.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> normestimatorresults(normestimatorstate state, <b>double</b> &amp;nrm);
</pre>
<a name=sub_normestimatorsetseed></a><h6 class=pageheader>normestimatorsetseed Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function changes seed value used by algorithm. In some cases we  need
deterministic processing, i.e. subsequent calls must return equal results,
in other cases we need non-deterministic algorithm which returns different
results for the same matrix on every pass.

Setting zero seed will lead to non-deterministic algorithm, while non-zero
value will make our algorithm deterministic.

Inputs:
    State       -   norm estimator state, must be initialized with a  call
                    to NormEstimatorCreate()
    SeedVal     -   seed value, &ge; 0. Zero value = non-deterministic algo.
ALGLIB: Copyright 06.12.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> normestimatorsetseed(normestimatorstate state, ae_int_t seedval);
</pre>
<a name=unit_ortfac></a><h4 class=pageheader>8.7.9. ortfac Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_cmatrixlq class=toc>cmatrixlq</a> |
<a href=#sub_cmatrixlqunpackl class=toc>cmatrixlqunpackl</a> |
<a href=#sub_cmatrixlqunpackq class=toc>cmatrixlqunpackq</a> |
<a href=#sub_cmatrixqr class=toc>cmatrixqr</a> |
<a href=#sub_cmatrixqrunpackq class=toc>cmatrixqrunpackq</a> |
<a href=#sub_cmatrixqrunpackr class=toc>cmatrixqrunpackr</a> |
<a href=#sub_hmatrixtd class=toc>hmatrixtd</a> |
<a href=#sub_hmatrixtdunpackq class=toc>hmatrixtdunpackq</a> |
<a href=#sub_rmatrixbd class=toc>rmatrixbd</a> |
<a href=#sub_rmatrixbdmultiplybyp class=toc>rmatrixbdmultiplybyp</a> |
<a href=#sub_rmatrixbdmultiplybyq class=toc>rmatrixbdmultiplybyq</a> |
<a href=#sub_rmatrixbdunpackdiagonals class=toc>rmatrixbdunpackdiagonals</a> |
<a href=#sub_rmatrixbdunpackpt class=toc>rmatrixbdunpackpt</a> |
<a href=#sub_rmatrixbdunpackq class=toc>rmatrixbdunpackq</a> |
<a href=#sub_rmatrixhessenberg class=toc>rmatrixhessenberg</a> |
<a href=#sub_rmatrixhessenbergunpackh class=toc>rmatrixhessenbergunpackh</a> |
<a href=#sub_rmatrixhessenbergunpackq class=toc>rmatrixhessenbergunpackq</a> |
<a href=#sub_rmatrixlq class=toc>rmatrixlq</a> |
<a href=#sub_rmatrixlqunpackl class=toc>rmatrixlqunpackl</a> |
<a href=#sub_rmatrixlqunpackq class=toc>rmatrixlqunpackq</a> |
<a href=#sub_rmatrixqr class=toc>rmatrixqr</a> |
<a href=#sub_rmatrixqrunpackq class=toc>rmatrixqrunpackq</a> |
<a href=#sub_rmatrixqrunpackr class=toc>rmatrixqrunpackr</a> |
<a href=#sub_smatrixtd class=toc>smatrixtd</a> |
<a href=#sub_smatrixtdunpackq class=toc>smatrixtdunpackq</a>
]</font>
</div>
<a name=sub_cmatrixlq></a><h6 class=pageheader>cmatrixlq Function</h6>
<hr width=600 align=left>
<pre class=narration>
LQ decomposition of a rectangular complex matrix of size MxN

Inputs:
    A   -   matrix A whose indexes range within [0..M-1, 0..N-1]
    M   -   number of rows in matrix A.
    N   -   number of columns in matrix A.

Outputs:
    A   -   matrices Q and L in compact form
    Tau -   array of scalar factors which are used to form matrix Q. Array
            whose indexes range within [0.. Min(M,N)-1]

Matrix A is represented as A = LQ, where Q is an orthogonal matrix of size
MxM, L - lower triangular (or lower trapezoid) matrix of size MxN.

  -- LAPACK routine (version 3.0) --
     Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,
     Courant Institute, Argonne National Lab, and Rice University
     September 30, 1994
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixlq(complex_2d_array &amp;a, ae_int_t m, ae_int_t n, complex_1d_array &amp;tau);
</pre>
<a name=sub_cmatrixlqunpackl></a><h6 class=pageheader>cmatrixlqunpackl Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking of matrix L from the LQ decomposition of a matrix A

Inputs:
    A       -   matrices Q and L in compact form.
                Output of CMatrixLQ subroutine.
    M       -   number of rows in given matrix A. M &ge; 0.
    N       -   number of columns in given matrix A. N &ge; 0.

Outputs:
    L       -   matrix L, array[0..M-1, 0..N-1].
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixlqunpackl(complex_2d_array a, ae_int_t m, ae_int_t n, complex_2d_array &amp;l);
</pre>
<a name=sub_cmatrixlqunpackq></a><h6 class=pageheader>cmatrixlqunpackq Function</h6>
<hr width=600 align=left>
<pre class=narration>
Partial unpacking of matrix Q from LQ decomposition of a complex matrix A.

Inputs:
    A           -   matrices Q and R in compact form.
                    Output of CMatrixLQ subroutine .
    M           -   number of rows in matrix A. M &ge; 0.
    N           -   number of columns in matrix A. N &ge; 0.
    Tau         -   scalar factors which are used to form Q.
                    Output of CMatrixLQ subroutine .
    QRows       -   required number of rows in matrix Q. N &ge; QColumns &ge; 0.

Outputs:
    Q           -   first QRows rows of matrix Q.
                    Array whose index ranges within [0..QRows-1, 0..N-1].
                    If QRows=0, array isn't changed.
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixlqunpackq(complex_2d_array a, ae_int_t m, ae_int_t n, complex_1d_array tau, ae_int_t qrows, complex_2d_array &amp;q);
</pre>
<a name=sub_cmatrixqr></a><h6 class=pageheader>cmatrixqr Function</h6>
<hr width=600 align=left>
<pre class=narration>
QR decomposition of a rectangular complex matrix of size MxN

Inputs:
    A   -   matrix A whose indexes range within [0..M-1, 0..N-1]
    M   -   number of rows in matrix A.
    N   -   number of columns in matrix A.

Outputs:
    A   -   matrices Q and R in compact form
    Tau -   array of scalar factors which are used to form matrix Q. Array
            whose indexes range within [0.. Min(M,N)-1]

Matrix A is represented as A = QR, where Q is an orthogonal matrix of size
MxM, R - upper triangular (or upper trapezoid) matrix of size MxN.

  -- LAPACK routine (version 3.0) --
     Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,
     Courant Institute, Argonne National Lab, and Rice University
     September 30, 1994
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixqr(complex_2d_array &amp;a, ae_int_t m, ae_int_t n, complex_1d_array &amp;tau);
</pre>
<a name=sub_cmatrixqrunpackq></a><h6 class=pageheader>cmatrixqrunpackq Function</h6>
<hr width=600 align=left>
<pre class=narration>
Partial unpacking of matrix Q from QR decomposition of a complex matrix A.

Inputs:
    A           -   matrices Q and R in compact form.
                    Output of CMatrixQR subroutine .
    M           -   number of rows in matrix A. M &ge; 0.
    N           -   number of columns in matrix A. N &ge; 0.
    Tau         -   scalar factors which are used to form Q.
                    Output of CMatrixQR subroutine .
    QColumns    -   required number of columns in matrix Q. M &ge; QColumns &ge; 0.

Outputs:
    Q           -   first QColumns columns of matrix Q.
                    Array whose index ranges within [0..M-1, 0..QColumns-1].
                    If QColumns=0, array isn't changed.
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixqrunpackq(complex_2d_array a, ae_int_t m, ae_int_t n, complex_1d_array tau, ae_int_t qcolumns, complex_2d_array &amp;q);
</pre>
<a name=sub_cmatrixqrunpackr></a><h6 class=pageheader>cmatrixqrunpackr Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking of matrix R from the QR decomposition of a matrix A

Inputs:
    A       -   matrices Q and R in compact form.
                Output of CMatrixQR subroutine.
    M       -   number of rows in given matrix A. M &ge; 0.
    N       -   number of columns in given matrix A. N &ge; 0.

Outputs:
    R       -   matrix R, array[0..M-1, 0..N-1].
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixqrunpackr(complex_2d_array a, ae_int_t m, ae_int_t n, complex_2d_array &amp;r);
</pre>
<a name=sub_hmatrixtd></a><h6 class=pageheader>hmatrixtd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Reduction of a Hermitian matrix which is given  by  its  higher  or  lower
triangular part to a real  tridiagonal  matrix  using  unitary  similarity
transformation: Q'*A*Q = T.

Inputs:
    A       -   matrix to be transformed
                array with elements [0..N-1, 0..N-1].
    N       -   size of matrix A.
    IsUpper -   storage format. If IsUpper = True, then matrix A is  given
                by its upper triangle, and the lower triangle is not  used
                and not modified by the algorithm, and vice versa
                if IsUpper = False.

Outputs:
    A       -   matrices T and Q in  compact form (see lower)
    Tau     -   array of factors which are forming matrices H(i)
                array with elements [0..N-2].
    D       -   main diagonal of real symmetric matrix T.
                array with elements [0..N-1].
    E       -   secondary diagonal of real symmetric matrix T.
                array with elements [0..N-2].

  If IsUpper=True, the matrix Q is represented as a product of elementary
  reflectors

     Q = H(n-2) . . . H(2) H(0).

  Each H(i) has the form

     H(i) = I - tau * v * v'

  where tau is a complex scalar, and v is a complex vector with
  v(i+1:n-1) = 0, v(i) = 1, v(0:i-1) is stored on exit in
  A(0:i-1,i+1), and tau in TAU(i).

  If IsUpper=False, the matrix Q is represented as a product of elementary
  reflectors

     Q = H(0) H(2) . . . H(n-2).

  Each H(i) has the form

     H(i) = I - tau * v * v'

  where tau is a complex scalar, and v is a complex vector with
  v(0:i) = 0, v(i+1) = 1, v(i+2:n-1) is stored on exit in A(i+2:n-1,i),
  and tau in TAU(i).

  The contents of A on exit are illustrated by the following examples
  with n = 5:

  if UPLO = 'U':                       if UPLO = 'L':

    (  d   e   v1  v2  v3 )              (  d                  )
    (      d   e   v2  v3 )              (  e   d              )
    (          d   e   v3 )              (  v0  e   d          )
    (              d   e  )              (  v0  v1  e   d      )
    (                  d  )              (  v0  v1  v2  e   d  )

where d and e denote diagonal and off-diagonal elements of T, and vi
denotes an element of the vector defining H(i).

  -- LAPACK routine (version 3.0) --
     Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,
     Courant Institute, Argonne National Lab, and Rice University
     October 31, 1992
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hmatrixtd(complex_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper, complex_1d_array &amp;tau, real_1d_array &amp;d, real_1d_array &amp;e);
</pre>
<a name=sub_hmatrixtdunpackq></a><h6 class=pageheader>hmatrixtdunpackq Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking matrix Q which reduces a Hermitian matrix to a real  tridiagonal
form.

Inputs:
    A       -   the result of a HMatrixTD subroutine
    N       -   size of matrix A.
    IsUpper -   storage format (a parameter of HMatrixTD subroutine)
    Tau     -   the result of a HMatrixTD subroutine

Outputs:
    Q       -   transformation matrix.
                array with elements [0..N-1, 0..N-1].
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hmatrixtdunpackq(complex_2d_array a, ae_int_t n, <b>bool</b> isupper, complex_1d_array tau, complex_2d_array &amp;q);
</pre>
<a name=sub_rmatrixbd></a><h6 class=pageheader>rmatrixbd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Reduction of a rectangular matrix to  bidiagonal form

The algorithm reduces the rectangular matrix A to  bidiagonal form by
orthogonal transformations P and Q: A = Q*B*(P^T).

Inputs:
    A       -   source matrix. array[0..M-1, 0..N-1]
    M       -   number of rows in matrix A.
    N       -   number of columns in matrix A.

Outputs:
    A       -   matrices Q, B, P in compact form (see below).
    TauQ    -   scalar factors which are used to form matrix Q.
    TauP    -   scalar factors which are used to form matrix P.

The main diagonal and one of the  secondary  diagonals  of  matrix  A  are
replaced with bidiagonal  matrix  B.  Other  elements  contain  elementary
reflections which form MxM matrix Q and NxN matrix P, respectively.

If M &ge; N, B is the upper  bidiagonal  MxN  matrix  and  is  stored  in  the
corresponding  elements  of  matrix  A.  Matrix  Q  is  represented  as  a
product   of   elementary   reflections   Q = H(0)*H(1)*...*H(n-1),  where
H(i) = 1-tau*v*v'. Here tau is a scalar which is stored  in  TauQ[i],  and
vector v has the following  structure:  v(0:i-1)=0, v(i)=1, v(i+1:m-1)  is
stored   in   elements   A(i+1:m-1,i).   Matrix   P  is  as  follows:  P =
G(0)*G(1)*...*G(n-2), where G(i) = 1 - tau*u*u'. Tau is stored in TauP[i],
u(0:i)=0, u(i+1)=1, u(i+2:n-1) is stored in elements A(i,i+2:n-1).

If M &lt; N, B is the  lower  bidiagonal  MxN  matrix  and  is  stored  in  the
corresponding   elements  of  matrix  A.  Q = H(0)*H(1)*...*H(m-2),  where
H(i) = 1 - tau*v*v', tau is stored in TauQ, v(0:i)=0, v(i+1)=1, v(i+2:m-1)
is    stored    in   elements   A(i+2:m-1,i).    P = G(0)*G(1)*...*G(m-1),
G(i) = 1-tau*u*u', tau is stored in  TauP,  u(0:i-1)=0, u(i)=1, u(i+1:n-1)
is stored in A(i,i+1:n-1).

EXAMPLE:

m=6, n=5 (m &gt; n):               m=5, n=6 (m &lt; n):

(  d   e   u1  u1  u1 )         (  d   u1  u1  u1  u1  u1 )
(  v1  d   e   u2  u2 )         (  e   d   u2  u2  u2  u2 )
(  v1  v2  d   e   u3 )         (  v1  e   d   u3  u3  u3 )
(  v1  v2  v3  d   e  )         (  v1  v2  e   d   u4  u4 )
(  v1  v2  v3  v4  d  )         (  v1  v2  v3  e   d   u5 )
(  v1  v2  v3  v4  v5 )

Here vi and ui are vectors which form H(i) and G(i), and d and e -
are the diagonal and off-diagonal elements of matrix B.

  -- LAPACK routine (version 3.0) --
     Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,
     Courant Institute, Argonne National Lab, and Rice University
     September 30, 1994.
     Sergey Bochkanov, ALGLIB project, translation from FORTRAN to
     pseudocode, 2007-2010.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixbd(real_2d_array &amp;a, ae_int_t m, ae_int_t n, real_1d_array &amp;tauq, real_1d_array &amp;taup);
</pre>
<a name=sub_rmatrixbdmultiplybyp></a><h6 class=pageheader>rmatrixbdmultiplybyp Function</h6>
<hr width=600 align=left>
<pre class=narration>
Multiplication by matrix P which reduces matrix A to  bidiagonal form.

The algorithm allows pre- or post-multiply by P or P'.

Inputs:
    QP          -   matrices Q and P in compact form.
                    Output of RMatrixBD subroutine.
    M           -   number of rows in matrix A.
    N           -   number of columns in matrix A.
    TAUP        -   scalar factors which are used to form P.
                    Output of RMatrixBD subroutine.
    Z           -   multiplied matrix.
                    Array whose indexes range within [0..ZRows-1,0..ZColumns-1].
    ZRows       -   number of rows in matrix Z. If FromTheRight=False,
                    ZRows=N, otherwise ZRows can be arbitrary.
    ZColumns    -   number of columns in matrix Z. If FromTheRight=True,
                    ZColumns=N, otherwise ZColumns can be arbitrary.
    FromTheRight -  pre- or post-multiply.
    DoTranspose -   multiply by P or P'.

Outputs:
    Z - product of Z and P.
                Array whose indexes range within [0..ZRows-1,0..ZColumns-1].
                If ZRows=0 or ZColumns=0, the array is not modified.
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixbdmultiplybyp(real_2d_array qp, ae_int_t m, ae_int_t n, real_1d_array taup, real_2d_array &amp;z, ae_int_t zrows, ae_int_t zcolumns, <b>bool</b> fromtheright, <b>bool</b> dotranspose);
</pre>
<a name=sub_rmatrixbdmultiplybyq></a><h6 class=pageheader>rmatrixbdmultiplybyq Function</h6>
<hr width=600 align=left>
<pre class=narration>
Multiplication by matrix Q which reduces matrix A to  bidiagonal form.

The algorithm allows pre- or post-multiply by Q or Q'.

Inputs:
    QP          -   matrices Q and P in compact form.
                    Output of ToBidiagonal subroutine.
    M           -   number of rows in matrix A.
    N           -   number of columns in matrix A.
    TAUQ        -   scalar factors which are used to form Q.
                    Output of ToBidiagonal subroutine.
    Z           -   multiplied matrix.
                    array[0..ZRows-1,0..ZColumns-1]
    ZRows       -   number of rows in matrix Z. If FromTheRight=False,
                    ZRows=M, otherwise ZRows can be arbitrary.
    ZColumns    -   number of columns in matrix Z. If FromTheRight=True,
                    ZColumns=M, otherwise ZColumns can be arbitrary.
    FromTheRight -  pre- or post-multiply.
    DoTranspose -   multiply by Q or Q'.

Outputs:
    Z           -   product of Z and Q.
                    Array[0..ZRows-1,0..ZColumns-1]
                    If ZRows=0 or ZColumns=0, the array is not modified.
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixbdmultiplybyq(real_2d_array qp, ae_int_t m, ae_int_t n, real_1d_array tauq, real_2d_array &amp;z, ae_int_t zrows, ae_int_t zcolumns, <b>bool</b> fromtheright, <b>bool</b> dotranspose);
</pre>
<a name=sub_rmatrixbdunpackdiagonals></a><h6 class=pageheader>rmatrixbdunpackdiagonals Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking of the main and secondary diagonals of bidiagonal decomposition
of matrix A.

Inputs:
    B   -   output of RMatrixBD subroutine.
    M   -   number of rows in matrix B.
    N   -   number of columns in matrix B.

Outputs:
    IsUpper -   True, if the matrix is upper bidiagonal.
                otherwise IsUpper is False.
    D       -   the main diagonal.
                Array whose index ranges within [0..Min(M,N)-1].
    E       -   the secondary diagonal (upper or lower, depending on
                the value of IsUpper).
                Array index ranges within [0..Min(M,N)-1], the last
                element is not used.
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixbdunpackdiagonals(real_2d_array b, ae_int_t m, ae_int_t n, <b>bool</b> &amp;isupper, real_1d_array &amp;d, real_1d_array &amp;e);
</pre>
<a name=sub_rmatrixbdunpackpt></a><h6 class=pageheader>rmatrixbdunpackpt Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking matrix P which reduces matrix A to bidiagonal form.
The subroutine returns transposed matrix P.

Inputs:
    QP      -   matrices Q and P in compact form.
                Output of ToBidiagonal subroutine.
    M       -   number of rows in matrix A.
    N       -   number of columns in matrix A.
    TAUP    -   scalar factors which are used to form P.
                Output of ToBidiagonal subroutine.
    PTRows  -   required number of rows of matrix P^T. N &ge; PTRows &ge; 0.

Outputs:
    PT      -   first PTRows columns of matrix P^T
                Array[0..PTRows-1, 0..N-1]
                If PTRows=0, the array is not modified.
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixbdunpackpt(real_2d_array qp, ae_int_t m, ae_int_t n, real_1d_array taup, ae_int_t ptrows, real_2d_array &amp;pt);
</pre>
<a name=sub_rmatrixbdunpackq></a><h6 class=pageheader>rmatrixbdunpackq Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking matrix Q which reduces a matrix to bidiagonal form.

Inputs:
    QP          -   matrices Q and P in compact form.
                    Output of ToBidiagonal subroutine.
    M           -   number of rows in matrix A.
    N           -   number of columns in matrix A.
    TAUQ        -   scalar factors which are used to form Q.
                    Output of ToBidiagonal subroutine.
    QColumns    -   required number of columns in matrix Q.
                    M &ge; QColumns &ge; 0.

Outputs:
    Q           -   first QColumns columns of matrix Q.
                    Array[0..M-1, 0..QColumns-1]
                    If QColumns=0, the array is not modified.
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixbdunpackq(real_2d_array qp, ae_int_t m, ae_int_t n, real_1d_array tauq, ae_int_t qcolumns, real_2d_array &amp;q);
</pre>
<a name=sub_rmatrixhessenberg></a><h6 class=pageheader>rmatrixhessenberg Function</h6>
<hr width=600 align=left>
<pre class=narration>
Reduction of a square matrix to  upper Hessenberg form: Q'*A*Q = H,
where Q is an orthogonal matrix, H - Hessenberg matrix.

Inputs:
    A       -   matrix A with elements [0..N-1, 0..N-1]
    N       -   size of matrix A.

Outputs:
    A       -   matrices Q and P in  compact form (see below).
    Tau     -   array of scalar factors which are used to form matrix Q.
                Array whose index ranges within [0..N-2]

Matrix H is located on the main diagonal, on the lower secondary  diagonal
and above the main diagonal of matrix A. The elements which are used to
form matrix Q are situated in array Tau and below the lower secondary
diagonal of matrix A as follows:

Matrix Q is represented as a product of elementary reflections

Q = H(0)*H(2)*...*H(n-2),

where each H(i) is given by

H(i) = 1 - tau * v * (v^T)

where tau is a scalar stored in Tau[I]; v - is a real vector,
so that v(0:i) = 0, v(i+1) = 1, v(i+2:n-1) stored in A(i+2:n-1,i).

  -- LAPACK routine (version 3.0) --
     Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,
     Courant Institute, Argonne National Lab, and Rice University
     October 31, 1992
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixhessenberg(real_2d_array &amp;a, ae_int_t n, real_1d_array &amp;tau);
</pre>
<a name=sub_rmatrixhessenbergunpackh></a><h6 class=pageheader>rmatrixhessenbergunpackh Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking matrix H (the result of matrix A reduction to upper Hessenberg form)

Inputs:
    A   -   output of RMatrixHessenberg subroutine.
    N   -   size of matrix A.

Outputs:
    H   -   matrix H. Array whose indexes range within [0..N-1, 0..N-1].
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixhessenbergunpackh(real_2d_array a, ae_int_t n, real_2d_array &amp;h);
</pre>
<a name=sub_rmatrixhessenbergunpackq></a><h6 class=pageheader>rmatrixhessenbergunpackq Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking matrix Q which reduces matrix A to upper Hessenberg form

Inputs:
    A   -   output of RMatrixHessenberg subroutine.
    N   -   size of matrix A.
    Tau -   scalar factors which are used to form Q.
            Output of RMatrixHessenberg subroutine.

Outputs:
    Q   -   matrix Q.
            Array whose indexes range within [0..N-1, 0..N-1].
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixhessenbergunpackq(real_2d_array a, ae_int_t n, real_1d_array tau, real_2d_array &amp;q);
</pre>
<a name=sub_rmatrixlq></a><h6 class=pageheader>rmatrixlq Function</h6>
<hr width=600 align=left>
<pre class=narration>
LQ decomposition of a rectangular matrix of size MxN

Inputs:
    A   -   matrix A whose indexes range within [0..M-1, 0..N-1].
    M   -   number of rows in matrix A.
    N   -   number of columns in matrix A.

Outputs:
    A   -   matrices L and Q in compact form (see below)
    Tau -   array of scalar factors which are used to form
            matrix Q. Array whose index ranges within [0..Min(M,N)-1].

Matrix A is represented as A = LQ, where Q is an orthogonal matrix of size
MxM, L - lower triangular (or lower trapezoid) matrix of size M x N.

The elements of matrix L are located on and below  the  main  diagonal  of
matrix A. The elements which are located in Tau array and above  the  main
diagonal of matrix A are used to form matrix Q as follows:

Matrix Q is represented as a product of elementary reflections

Q = H(k-1)*H(k-2)*...*H(1)*H(0),

where k = min(m,n), and each H(i) is of the form

H(i) = 1 - tau * v * (v^T)

where tau is a scalar stored in Tau[I]; v - real vector, so that v(0:i-1)=0,
v(i) = 1, v(i+1:n-1) stored in A(i,i+1:n-1).
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixlq(real_2d_array &amp;a, ae_int_t m, ae_int_t n, real_1d_array &amp;tau);
</pre>
<a name=sub_rmatrixlqunpackl></a><h6 class=pageheader>rmatrixlqunpackl Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking of matrix L from the LQ decomposition of a matrix A

Inputs:
    A       -   matrices Q and L in compact form.
                Output of RMatrixLQ subroutine.
    M       -   number of rows in given matrix A. M &ge; 0.
    N       -   number of columns in given matrix A. N &ge; 0.

Outputs:
    L       -   matrix L, array[0..M-1, 0..N-1].
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixlqunpackl(real_2d_array a, ae_int_t m, ae_int_t n, real_2d_array &amp;l);
</pre>
<a name=sub_rmatrixlqunpackq></a><h6 class=pageheader>rmatrixlqunpackq Function</h6>
<hr width=600 align=left>
<pre class=narration>
Partial unpacking of matrix Q from the LQ decomposition of a matrix A

Inputs:
    A       -   matrices L and Q in compact form.
                Output of RMatrixLQ subroutine.
    M       -   number of rows in given matrix A. M &ge; 0.
    N       -   number of columns in given matrix A. N &ge; 0.
    Tau     -   scalar factors which are used to form Q.
                Output of the RMatrixLQ subroutine.
    QRows   -   required number of rows in matrix Q. N &ge; QRows &ge; 0.

Outputs:
    Q       -   first QRows rows of matrix Q. Array whose indexes range
                within [0..QRows-1, 0..N-1]. If QRows=0, the array remains
                unchanged.
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixlqunpackq(real_2d_array a, ae_int_t m, ae_int_t n, real_1d_array tau, ae_int_t qrows, real_2d_array &amp;q);
</pre>
<a name=sub_rmatrixqr></a><h6 class=pageheader>rmatrixqr Function</h6>
<hr width=600 align=left>
<pre class=narration>
QR decomposition of a rectangular matrix of size MxN

Inputs:
    A   -   matrix A whose indexes range within [0..M-1, 0..N-1].
    M   -   number of rows in matrix A.
    N   -   number of columns in matrix A.

Outputs:
    A   -   matrices Q and R in compact form (see below).
    Tau -   array of scalar factors which are used to form
            matrix Q. Array whose index ranges within [0.. Min(M-1,N-1)].

Matrix A is represented as A = QR, where Q is an orthogonal matrix of size
MxM, R - upper triangular (or upper trapezoid) matrix of size M x N.

The elements of matrix R are located on and above the main diagonal of
matrix A. The elements which are located in Tau array and below the main
diagonal of matrix A are used to form matrix Q as follows:

Matrix Q is represented as a product of elementary reflections

Q = H(0)*H(2)*...*H(k-1),

where k = min(m,n), and each H(i) is in the form

H(i) = 1 - tau * v * (v^T)

where tau is a scalar stored in Tau[I]; v - real vector,
so that v(0:i-1) = 0, v(i) = 1, v(i+1:m-1) stored in A(i+1:m-1,i).
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixqr(real_2d_array &amp;a, ae_int_t m, ae_int_t n, real_1d_array &amp;tau);
</pre>
<a name=sub_rmatrixqrunpackq></a><h6 class=pageheader>rmatrixqrunpackq Function</h6>
<hr width=600 align=left>
<pre class=narration>
Partial unpacking of matrix Q from the QR decomposition of a matrix A

Inputs:
    A       -   matrices Q and R in compact form.
                Output of RMatrixQR subroutine.
    M       -   number of rows in given matrix A. M &ge; 0.
    N       -   number of columns in given matrix A. N &ge; 0.
    Tau     -   scalar factors which are used to form Q.
                Output of the RMatrixQR subroutine.
    QColumns -  required number of columns of matrix Q. M &ge; QColumns &ge; 0.

Outputs:
    Q       -   first QColumns columns of matrix Q.
                Array whose indexes range within [0..M-1, 0..QColumns-1].
                If QColumns=0, the array remains unchanged.
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixqrunpackq(real_2d_array a, ae_int_t m, ae_int_t n, real_1d_array tau, ae_int_t qcolumns, real_2d_array &amp;q);
</pre>
<a name=sub_rmatrixqrunpackr></a><h6 class=pageheader>rmatrixqrunpackr Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking of matrix R from the QR decomposition of a matrix A

Inputs:
    A       -   matrices Q and R in compact form.
                Output of RMatrixQR subroutine.
    M       -   number of rows in given matrix A. M &ge; 0.
    N       -   number of columns in given matrix A. N &ge; 0.

Outputs:
    R       -   matrix R, array[0..M-1, 0..N-1].
ALGLIB Routine: Copyright 17.02.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixqrunpackr(real_2d_array a, ae_int_t m, ae_int_t n, real_2d_array &amp;r);
</pre>
<a name=sub_smatrixtd></a><h6 class=pageheader>smatrixtd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Reduction of a symmetric matrix which is given by its higher or lower
triangular part to a tridiagonal matrix using orthogonal similarity
transformation: Q'*A*Q=T.

Inputs:
    A       -   matrix to be transformed
                array with elements [0..N-1, 0..N-1].
    N       -   size of matrix A.
    IsUpper -   storage format. If IsUpper = True, then matrix A is given
                by its upper triangle, and the lower triangle is not used
                and not modified by the algorithm, and vice versa
                if IsUpper = False.

Outputs:
    A       -   matrices T and Q in  compact form (see lower)
    Tau     -   array of factors which are forming matrices H(i)
                array with elements [0..N-2].
    D       -   main diagonal of symmetric matrix T.
                array with elements [0..N-1].
    E       -   secondary diagonal of symmetric matrix T.
                array with elements [0..N-2].

  If IsUpper=True, the matrix Q is represented as a product of elementary
  reflectors

     Q = H(n-2) . . . H(2) H(0).

  Each H(i) has the form

     H(i) = I - tau * v * v'

  where tau is a real scalar, and v is a real vector with
  v(i+1:n-1) = 0, v(i) = 1, v(0:i-1) is stored on exit in
  A(0:i-1,i+1), and tau in TAU(i).

  If IsUpper=False, the matrix Q is represented as a product of elementary
  reflectors

     Q = H(0) H(2) . . . H(n-2).

  Each H(i) has the form

     H(i) = I - tau * v * v'

  where tau is a real scalar, and v is a real vector with
  v(0:i) = 0, v(i+1) = 1, v(i+2:n-1) is stored on exit in A(i+2:n-1,i),
  and tau in TAU(i).

  The contents of A on exit are illustrated by the following examples
  with n = 5:

  if UPLO = 'U':                       if UPLO = 'L':

    (  d   e   v1  v2  v3 )              (  d                  )
    (      d   e   v2  v3 )              (  e   d              )
    (          d   e   v3 )              (  v0  e   d          )
    (              d   e  )              (  v0  v1  e   d      )
    (                  d  )              (  v0  v1  v2  e   d  )

  where d and e denote diagonal and off-diagonal elements of T, and vi
  denotes an element of the vector defining H(i).

  -- LAPACK routine (version 3.0) --
     Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,
     Courant Institute, Argonne National Lab, and Rice University
     October 31, 1992
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> smatrixtd(real_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper, real_1d_array &amp;tau, real_1d_array &amp;d, real_1d_array &amp;e);
</pre>
<a name=sub_smatrixtdunpackq></a><h6 class=pageheader>smatrixtdunpackq Function</h6>
<hr width=600 align=left>
<pre class=narration>
Unpacking matrix Q which reduces symmetric matrix to a tridiagonal
form.

Inputs:
    A       -   the result of a SMatrixTD subroutine
    N       -   size of matrix A.
    IsUpper -   storage format (a parameter of SMatrixTD subroutine)
    Tau     -   the result of a SMatrixTD subroutine

Outputs:
    Q       -   transformation matrix.
                array with elements [0..N-1, 0..N-1].
ALGLIB: Copyright 2005-2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> smatrixtdunpackq(real_2d_array a, ae_int_t n, <b>bool</b> isupper, real_1d_array tau, real_2d_array &amp;q);
</pre>
<a name=unit_rcond></a><h4 class=pageheader>8.7.10. rcond Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_cmatrixlurcond1 class=toc>cmatrixlurcond1</a> |
<a href=#sub_cmatrixlurcondinf class=toc>cmatrixlurcondinf</a> |
<a href=#sub_cmatrixrcond1 class=toc>cmatrixrcond1</a> |
<a href=#sub_cmatrixrcondinf class=toc>cmatrixrcondinf</a> |
<a href=#sub_cmatrixtrrcond1 class=toc>cmatrixtrrcond1</a> |
<a href=#sub_cmatrixtrrcondinf class=toc>cmatrixtrrcondinf</a> |
<a href=#sub_hpdmatrixcholeskyrcond class=toc>hpdmatrixcholeskyrcond</a> |
<a href=#sub_hpdmatrixrcond class=toc>hpdmatrixrcond</a> |
<a href=#sub_rmatrixlurcond1 class=toc>rmatrixlurcond1</a> |
<a href=#sub_rmatrixlurcondinf class=toc>rmatrixlurcondinf</a> |
<a href=#sub_rmatrixrcond1 class=toc>rmatrixrcond1</a> |
<a href=#sub_rmatrixrcondinf class=toc>rmatrixrcondinf</a> |
<a href=#sub_rmatrixtrrcond1 class=toc>rmatrixtrrcond1</a> |
<a href=#sub_rmatrixtrrcondinf class=toc>rmatrixtrrcondinf</a> |
<a href=#sub_spdmatrixcholeskyrcond class=toc>spdmatrixcholeskyrcond</a> |
<a href=#sub_spdmatrixrcond class=toc>spdmatrixrcond</a>
]</font>
</div>
<a name=sub_cmatrixlurcond1></a><h6 class=pageheader>cmatrixlurcond1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Estimate of the condition number of a matrix given by its LU decomposition (1-norm)

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    LUA         -   LU decomposition of a matrix in compact form. Output of
                    the CMatrixLU subroutine.
    N           -   size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> cmatrixlurcond1(complex_2d_array lua, ae_int_t n);
</pre>
<a name=sub_cmatrixlurcondinf></a><h6 class=pageheader>cmatrixlurcondinf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Estimate of the condition number of a matrix given by its LU decomposition
(infinity norm).

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    LUA     -   LU decomposition of a matrix in compact form. Output of
                the CMatrixLU subroutine.
    N       -   size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> cmatrixlurcondinf(complex_2d_array lua, ae_int_t n);
</pre>
<a name=sub_cmatrixrcond1></a><h6 class=pageheader>cmatrixrcond1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Estimate of a matrix condition number (1-norm)

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    A   -   matrix. Array whose indexes range within [0..N-1, 0..N-1].
    N   -   size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> cmatrixrcond1(complex_2d_array a, ae_int_t n);
</pre>
<a name=sub_cmatrixrcondinf></a><h6 class=pageheader>cmatrixrcondinf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Estimate of a matrix condition number (infinity-norm).

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    A   -   matrix. Array whose indexes range within [0..N-1, 0..N-1].
    N   -   size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> cmatrixrcondinf(complex_2d_array a, ae_int_t n);
</pre>
<a name=sub_cmatrixtrrcond1></a><h6 class=pageheader>cmatrixtrrcond1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Triangular matrix: estimate of a condition number (1-norm)

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    A       -   matrix. Array[0..N-1, 0..N-1].
    N       -   size of A.
    IsUpper -   True, if the matrix is upper triangular.
    IsUnit  -   True, if the matrix has a unit diagonal.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> cmatrixtrrcond1(complex_2d_array a, ae_int_t n, <b>bool</b> isupper, <b>bool</b> isunit);
</pre>
<a name=sub_cmatrixtrrcondinf></a><h6 class=pageheader>cmatrixtrrcondinf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Triangular matrix: estimate of a matrix condition number (infinity-norm).

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    A   -   matrix. Array whose indexes range within [0..N-1, 0..N-1].
    N   -   size of matrix A.
    IsUpper -   True, if the matrix is upper triangular.
    IsUnit  -   True, if the matrix has a unit diagonal.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> cmatrixtrrcondinf(complex_2d_array a, ae_int_t n, <b>bool</b> isupper, <b>bool</b> isunit);
</pre>
<a name=sub_hpdmatrixcholeskyrcond></a><h6 class=pageheader>hpdmatrixcholeskyrcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Condition number estimate of a Hermitian positive definite matrix given by
Cholesky decomposition.

The algorithm calculates a lower bound of the condition number. In this
case, the algorithm does not return a lower bound of the condition number,
but an inverse number (to avoid an overflow in case of a singular matrix).

It should be noted that 1-norm and inf-norm condition numbers of symmetric
matrices are equal, so the algorithm doesn't take into account the
differences between these types of norms.

Inputs:
    CD  - Cholesky decomposition of matrix A,
          output of SMatrixCholesky subroutine.
    N   - size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hpdmatrixcholeskyrcond(complex_2d_array a, ae_int_t n, <b>bool</b> isupper);
</pre>
<a name=sub_hpdmatrixrcond></a><h6 class=pageheader>hpdmatrixrcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Condition number estimate of a Hermitian positive definite matrix.

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

It should be noted that 1-norm and inf-norm of condition numbers of symmetric
matrices are equal, so the algorithm doesn't take into account the
differences between these types of norms.

Inputs:
    A       -   Hermitian positive definite matrix which is given by its
                upper or lower triangle depending on the value of
                IsUpper. Array with elements [0..N-1, 0..N-1].
    N       -   size of matrix A.
    IsUpper -   storage format.

Result:
    1/LowerBound(cond(A)), if matrix A is positive definite,
   -1, if matrix A is not positive definite, and its condition number
    could not be found by this algorithm.

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hpdmatrixrcond(complex_2d_array a, ae_int_t n, <b>bool</b> isupper);
</pre>
<a name=sub_rmatrixlurcond1></a><h6 class=pageheader>rmatrixlurcond1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Estimate of the condition number of a matrix given by its LU decomposition (1-norm)

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    LUA         -   LU decomposition of a matrix in compact form. Output of
                    the RMatrixLU subroutine.
    N           -   size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rmatrixlurcond1(real_2d_array lua, ae_int_t n);
</pre>
<a name=sub_rmatrixlurcondinf></a><h6 class=pageheader>rmatrixlurcondinf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Estimate of the condition number of a matrix given by its LU decomposition
(infinity norm).

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    LUA     -   LU decomposition of a matrix in compact form. Output of
                the RMatrixLU subroutine.
    N       -   size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rmatrixlurcondinf(real_2d_array lua, ae_int_t n);
</pre>
<a name=sub_rmatrixrcond1></a><h6 class=pageheader>rmatrixrcond1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Estimate of a matrix condition number (1-norm)

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    A   -   matrix. Array whose indexes range within [0..N-1, 0..N-1].
    N   -   size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rmatrixrcond1(real_2d_array a, ae_int_t n);
</pre>
<a name=sub_rmatrixrcondinf></a><h6 class=pageheader>rmatrixrcondinf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Estimate of a matrix condition number (infinity-norm).

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    A   -   matrix. Array whose indexes range within [0..N-1, 0..N-1].
    N   -   size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rmatrixrcondinf(real_2d_array a, ae_int_t n);
</pre>
<a name=sub_rmatrixtrrcond1></a><h6 class=pageheader>rmatrixtrrcond1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Triangular matrix: estimate of a condition number (1-norm)

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    A       -   matrix. Array[0..N-1, 0..N-1].
    N       -   size of A.
    IsUpper -   True, if the matrix is upper triangular.
    IsUnit  -   True, if the matrix has a unit diagonal.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rmatrixtrrcond1(real_2d_array a, ae_int_t n, <b>bool</b> isupper, <b>bool</b> isunit);
</pre>
<a name=sub_rmatrixtrrcondinf></a><h6 class=pageheader>rmatrixtrrcondinf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Triangular matrix: estimate of a matrix condition number (infinity-norm).

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

Inputs:
    A   -   matrix. Array whose indexes range within [0..N-1, 0..N-1].
    N   -   size of matrix A.
    IsUpper -   True, if the matrix is upper triangular.
    IsUnit  -   True, if the matrix has a unit diagonal.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> rmatrixtrrcondinf(real_2d_array a, ae_int_t n, <b>bool</b> isupper, <b>bool</b> isunit);
</pre>
<a name=sub_spdmatrixcholeskyrcond></a><h6 class=pageheader>spdmatrixcholeskyrcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Condition number estimate of a symmetric positive definite matrix given by
Cholesky decomposition.

The algorithm calculates a lower bound of the condition number. In this
case, the algorithm does not return a lower bound of the condition number,
but an inverse number (to avoid an overflow in case of a singular matrix).

It should be noted that 1-norm and inf-norm condition numbers of symmetric
matrices are equal, so the algorithm doesn't take into account the
differences between these types of norms.

Inputs:
    CD  - Cholesky decomposition of matrix A,
          output of SMatrixCholesky subroutine.
    N   - size of matrix A.

Result: 1/LowerBound(cond(A))

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spdmatrixcholeskyrcond(real_2d_array a, ae_int_t n, <b>bool</b> isupper);
</pre>
<a name=sub_spdmatrixrcond></a><h6 class=pageheader>spdmatrixrcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Condition number estimate of a symmetric positive definite matrix.

The algorithm calculates a lower bound of the condition number. In this case,
the algorithm does not return a lower bound of the condition number, but an
inverse number (to avoid an overflow in case of a singular matrix).

It should be noted that 1-norm and inf-norm of condition numbers of symmetric
matrices are equal, so the algorithm doesn't take into account the
differences between these types of norms.

Inputs:
    A       -   symmetric positive definite matrix which is given by its
                upper or lower triangle depending on the value of
                IsUpper. Array with elements [0..N-1, 0..N-1].
    N       -   size of matrix A.
    IsUpper -   storage format.

Result:
    1/LowerBound(cond(A)), if matrix A is positive definite,
   -1, if matrix A is not positive definite, and its condition number
    could not be found by this algorithm.

NOTE:
    if k(A) is very large, then matrix is  assumed  degenerate,  k(A)=INF,
    0.0 is returned in such cases.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spdmatrixrcond(real_2d_array a, ae_int_t n, <b>bool</b> isupper);
</pre>
<a name=unit_schur></a><h4 class=pageheader>8.7.11. schur Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_rmatrixschur class=toc>rmatrixschur</a>
]</font>
</div>
<a name=sub_rmatrixschur></a><h6 class=pageheader>rmatrixschur Function</h6>
<hr width=600 align=left>
<pre class=narration>
Subroutine performing the Schur decomposition of a general matrix by using
the QR algorithm with multiple shifts.

The source matrix A is represented as S'*A*S = T, where S is an orthogonal
matrix (Schur vectors), T - upper quasi-triangular matrix (with blocks of
sizes 1x1 and 2x2 on the main diagonal).

Inputs:
    A   -   matrix to be decomposed.
            Array whose indexes range within [0..N-1, 0..N-1].
    N   -   size of A, N &ge; 0.

Outputs:
    A   -   contains matrix T.
            Array whose indexes range within [0..N-1, 0..N-1].
    S   -   contains Schur vectors.
            Array whose indexes range within [0..N-1, 0..N-1].

Note 1:
    The block structure of matrix T can be easily recognized: since all
    the elements below the blocks are zeros, the elements a[i+1,i] which
    are equal to 0 show the block border.

Note 2:
    The algorithm performance depends on the value of the internal parameter
    NS of the InternalSchurDecomposition subroutine which defines the number
    of shifts in the QR algorithm (similarly to the block width in block-matrix
    algorithms in linear algebra). If you require maximum performance on
    your machine, it is recommended to adjust this parameter manually.

Result:
    True,
        if the algorithm has converged and parameters A and S contain the result.
    False,
        if the algorithm has not converged.

Algorithm implemented on the basis of the DHSEQR subroutine (LAPACK 3.0 library).
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> rmatrixschur(real_2d_array &amp;a, ae_int_t n, real_2d_array &amp;s);
</pre>
<a name=unit_sparse></a><h4 class=pageheader>8.7.12. sparse Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_sparsebuffers class=toc>sparsebuffers</a> |
<a href=#struct_sparsematrix class=toc>sparsematrix</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_sparseadd class=toc>sparseadd</a> |
<a href=#sub_sparseconvertto class=toc>sparseconvertto</a> |
<a href=#sub_sparseconverttocrs class=toc>sparseconverttocrs</a> |
<a href=#sub_sparseconverttohash class=toc>sparseconverttohash</a> |
<a href=#sub_sparseconverttosks class=toc>sparseconverttosks</a> |
<a href=#sub_sparsecopy class=toc>sparsecopy</a> |
<a href=#sub_sparsecopybuf class=toc>sparsecopybuf</a> |
<a href=#sub_sparsecopytobuf class=toc>sparsecopytobuf</a> |
<a href=#sub_sparsecopytocrs class=toc>sparsecopytocrs</a> |
<a href=#sub_sparsecopytocrsbuf class=toc>sparsecopytocrsbuf</a> |
<a href=#sub_sparsecopytohash class=toc>sparsecopytohash</a> |
<a href=#sub_sparsecopytohashbuf class=toc>sparsecopytohashbuf</a> |
<a href=#sub_sparsecopytosks class=toc>sparsecopytosks</a> |
<a href=#sub_sparsecopytosksbuf class=toc>sparsecopytosksbuf</a> |
<a href=#sub_sparsecopytransposecrs class=toc>sparsecopytransposecrs</a> |
<a href=#sub_sparsecopytransposecrsbuf class=toc>sparsecopytransposecrsbuf</a> |
<a href=#sub_sparsecreate class=toc>sparsecreate</a> |
<a href=#sub_sparsecreatebuf class=toc>sparsecreatebuf</a> |
<a href=#sub_sparsecreatecrs class=toc>sparsecreatecrs</a> |
<a href=#sub_sparsecreatecrsbuf class=toc>sparsecreatecrsbuf</a> |
<a href=#sub_sparsecreatesks class=toc>sparsecreatesks</a> |
<a href=#sub_sparsecreatesksband class=toc>sparsecreatesksband</a> |
<a href=#sub_sparsecreatesksbandbuf class=toc>sparsecreatesksbandbuf</a> |
<a href=#sub_sparsecreatesksbuf class=toc>sparsecreatesksbuf</a> |
<a href=#sub_sparseenumerate class=toc>sparseenumerate</a> |
<a href=#sub_sparseexists class=toc>sparseexists</a> |
<a href=#sub_sparsefree class=toc>sparsefree</a> |
<a href=#sub_sparsegemv class=toc>sparsegemv</a> |
<a href=#sub_sparseget class=toc>sparseget</a> |
<a href=#sub_sparsegetcompressedrow class=toc>sparsegetcompressedrow</a> |
<a href=#sub_sparsegetdiagonal class=toc>sparsegetdiagonal</a> |
<a href=#sub_sparsegetlowercount class=toc>sparsegetlowercount</a> |
<a href=#sub_sparsegetmatrixtype class=toc>sparsegetmatrixtype</a> |
<a href=#sub_sparsegetncols class=toc>sparsegetncols</a> |
<a href=#sub_sparsegetnrows class=toc>sparsegetnrows</a> |
<a href=#sub_sparsegetrow class=toc>sparsegetrow</a> |
<a href=#sub_sparsegetuppercount class=toc>sparsegetuppercount</a> |
<a href=#sub_sparseiscrs class=toc>sparseiscrs</a> |
<a href=#sub_sparseishash class=toc>sparseishash</a> |
<a href=#sub_sparseissks class=toc>sparseissks</a> |
<a href=#sub_sparsemm class=toc>sparsemm</a> |
<a href=#sub_sparsemm2 class=toc>sparsemm2</a> |
<a href=#sub_sparsemtm class=toc>sparsemtm</a> |
<a href=#sub_sparsemtv class=toc>sparsemtv</a> |
<a href=#sub_sparsemv class=toc>sparsemv</a> |
<a href=#sub_sparsemv2 class=toc>sparsemv2</a> |
<a href=#sub_sparseresizematrix class=toc>sparseresizematrix</a> |
<a href=#sub_sparserewriteexisting class=toc>sparserewriteexisting</a> |
<a href=#sub_sparseset class=toc>sparseset</a> |
<a href=#sub_sparsesmm class=toc>sparsesmm</a> |
<a href=#sub_sparsesmv class=toc>sparsesmv</a> |
<a href=#sub_sparseswap class=toc>sparseswap</a> |
<a href=#sub_sparsesymmpermtbl class=toc>sparsesymmpermtbl</a> |
<a href=#sub_sparsesymmpermtblbuf class=toc>sparsesymmpermtblbuf</a> |
<a href=#sub_sparsetransposecrs class=toc>sparsetransposecrs</a> |
<a href=#sub_sparsetransposesks class=toc>sparsetransposesks</a> |
<a href=#sub_sparsetrmv class=toc>sparsetrmv</a> |
<a href=#sub_sparsetrsv class=toc>sparsetrsv</a> |
<a href=#sub_sparsevsmv class=toc>sparsevsmv</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_sparse_d_1 class=toc>sparse_d_1</a></td><td width=15>&nbsp;</td><td>Basic operations with sparse matrices</td></tr>
<tr align=left valign=top><td><a href=#example_sparse_d_crs class=toc>sparse_d_crs</a></td><td width=15>&nbsp;</td><td>Advanced topic: creation in the CRS format.</td></tr>
</table>
</div>
<a name=struct_sparsebuffers></a><h6 class=pageheader>sparsebuffers Class</h6>
<hr width=600 align=left>
<pre class=narration>
Temporary buffers for sparse matrix operations.

You should pass an instance of this structure to factorization  functions.
It allows to reuse memory during repeated sparse  factorizations.  You  do
not have to call some initialization function - simply passing an instance
to factorization function is enough.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> sparsebuffers {
};
</pre>
<a name=struct_sparsematrix></a><h6 class=pageheader>sparsematrix Class</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse matrix structure.
You should use ALGLIB functions to work with sparse matrix. Never  try  to
access its fields directly!

NOTES ON THE SPARSE STORAGE FORMATS

Sparse matrices can be stored using several formats:
* Hash-Table representation
* Compressed Row Storage (CRS)
* Skyline matrix storage (SKS)

Each of the formats has benefits and drawbacks:
* Hash-table is good for dynamic operations (insertion of new elements),
  but does not support linear algebra operations
* CRS is good for operations like matrix-vector or matrix-matrix products,
  but its initialization is less convenient - you have to tell row   sizes
  at the initialization, and you have to fill  matrix  only  row  by  row,
  from left to right.
* SKS is a special format which is used to store triangular  factors  from
  Cholesky factorization. It does not support  dynamic  modification,  and
  support for linear algebra operations is very limited.

Tables below outline information about these two formats:

    OPERATIONS WITH MATRIX      HASH        CRS         SKS
    creation                    +           +           +
    SparseGet                   +           +           +
    SparseExists                +           +           +
    SparseRewriteExisting       +           +           +
    SparseSet                   +           +           +
    SparseAdd                   +
    SparseGetRow                            +           +
    SparseGetCompressedRow                  +           +
    sparse-dense linear algebra             +           +
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> sparsematrix {
};
</pre>
<a name=sub_sparseadd></a><h6 class=pageheader>sparseadd Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function adds value to S[i,j] - element of the sparse matrix. Matrix
must be in a Hash-Table mode.

In case S[i,j] already exists in the table, V i added to  its  value.  In
case  S[i,j]  is  non-existent,  it  is  inserted  in  the  table.  Table
automatically grows when necessary.

Inputs:
    S           -   sparse M*N matrix in Hash-Table representation.
                    Exception will be thrown for CRS matrix.
    I           -   row index of the element to modify, 0 &le; I &lt; M
    J           -   column index of the element to modify, 0 &le; J &lt; N
    V           -   value to add, must be finite number

Outputs:
    S           -   modified matrix

NOTE 1:  when  S[i,j]  is exactly zero after modification, it is  deleted
from the table.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparseadd(sparsematrix s, ae_int_t i, ae_int_t j, <b>double</b> v);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_sparse_d_1 class=nav>sparse_d_1</a> ]</p>
<a name=sub_sparseconvertto></a><h6 class=pageheader>sparseconvertto Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  performs  in-place  conversion  to  desired sparse storage
format.

Inputs:
    S0      -   sparse matrix in any format.
    Fmt     -   desired storage format  of  the  output,  as  returned  by
                SparseGetMatrixType() function:
                * 0 for hash-based storage
                * 1 for CRS
                * 2 for SKS

Outputs:
    S0          -   sparse matrix in requested format.

NOTE: in-place conversion wastes a lot of memory which is  used  to  store
      temporaries.  If  you  perform  a  lot  of  repeated conversions, we
      recommend to use out-of-place buffered  conversion  functions,  like
      SparseCopyToBuf(), which can reuse already allocated memory.
ALGLIB Project: Copyright 16.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparseconvertto(sparsematrix s0, ae_int_t fmt);
</pre>
<a name=sub_sparseconverttocrs></a><h6 class=pageheader>sparseconverttocrs Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function converts matrix to CRS format.

Some  algorithms  (linear  algebra ones, for example) require matrices in
CRS format. This function allows to perform in-place conversion.

Inputs:
    S           -   sparse M*N matrix in any format

Outputs:
    S           -   matrix in CRS format

NOTE: this   function  has  no  effect  when  called with matrix which is
      already in CRS mode.

NOTE: this function allocates temporary memory to store a   copy  of  the
      matrix. If you perform a lot of repeated conversions, we  recommend
      you  to  use  SparseCopyToCRSBuf()  function,   which   can   reuse
      previously allocated memory.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparseconverttocrs(sparsematrix s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_sparse_d_1 class=nav>sparse_d_1</a> ]</p>
<a name=sub_sparseconverttohash></a><h6 class=pageheader>sparseconverttohash Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs in-place conversion to Hash table storage.

Inputs:
    S           -   sparse matrix in CRS format.

Outputs:
    S           -   sparse matrix in Hash table format.

NOTE: this  function  has   no  effect  when  called with matrix which  is
      already in Hash table mode.

NOTE: in-place conversion involves allocation of temporary arrays. If  you
      perform a lot of repeated in- place  conversions,  it  may  lead  to
      memory fragmentation. Consider using out-of-place SparseCopyToHashBuf()
      function in this case.
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparseconverttohash(sparsematrix s);
</pre>
<a name=sub_sparseconverttosks></a><h6 class=pageheader>sparseconverttosks Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs in-place conversion to SKS format.

Inputs:
    S           -   sparse matrix in any format.

Outputs:
    S           -   sparse matrix in SKS format.

NOTE: this  function  has   no  effect  when  called with matrix which  is
      already in SKS mode.

NOTE: in-place conversion involves allocation of temporary arrays. If  you
      perform a lot of repeated in- place  conversions,  it  may  lead  to
      memory fragmentation. Consider using out-of-place SparseCopyToSKSBuf()
      function in this case.
ALGLIB Project: Copyright 15.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparseconverttosks(sparsematrix s);
</pre>
<a name=sub_sparsecopy></a><h6 class=pageheader>sparsecopy Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function copies S0 to S1.
This function completely deallocates memory owned by S1 before creating a
copy of S0. If you want to reuse memory, use SparseCopyBuf.

NOTE:  this  function  does  not verify its arguments, it just copies all
fields of the structure.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopy(sparsematrix s0, sparsematrix &amp;s1);
</pre>
<a name=sub_sparsecopybuf></a><h6 class=pageheader>sparsecopybuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function copies S0 to S1.
Memory already allocated in S1 is reused as much as possible.

NOTE:  this  function  does  not verify its arguments, it just copies all
fields of the structure.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopybuf(sparsematrix s0, sparsematrix s1);
</pre>
<a name=sub_sparsecopytobuf></a><h6 class=pageheader>sparsecopytobuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  performs out-of-place conversion to desired sparse storage
format. S0 is copied to S1 and converted on-the-fly. Memory  allocated  in
S1 is reused to maximum extent possible.

Inputs:
    S0      -   sparse matrix in any format.
    Fmt     -   desired storage format  of  the  output,  as  returned  by
                SparseGetMatrixType() function:
                * 0 for hash-based storage
                * 1 for CRS
                * 2 for SKS

Outputs:
    S1          -   sparse matrix in requested format.
ALGLIB Project: Copyright 16.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopytobuf(sparsematrix s0, ae_int_t fmt, sparsematrix s1);
</pre>
<a name=sub_sparsecopytocrs></a><h6 class=pageheader>sparsecopytocrs Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  performs  out-of-place  conversion  to  CRS format.  S0 is
copied to S1 and converted on-the-fly.

Inputs:
    S0          -   sparse matrix in any format.

Outputs:
    S1          -   sparse matrix in CRS format.

NOTE: if S0 is stored as CRS, it is just copied without conversion.

NOTE: this function de-allocates memory occupied by S1 before starting CRS
      conversion. If you perform a lot of repeated CRS conversions, it may
      lead to memory fragmentation. In this case we recommend you  to  use
      SparseCopyToCRSBuf() function which re-uses memory in S1 as much  as
      possible.
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopytocrs(sparsematrix s0, sparsematrix &amp;s1);
</pre>
<a name=sub_sparsecopytocrsbuf></a><h6 class=pageheader>sparsecopytocrsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  performs  out-of-place  conversion  to  CRS format.  S0 is
copied to S1 and converted on-the-fly. Memory allocated in S1 is reused to
maximum extent possible.

Inputs:
    S0          -   sparse matrix in any format.
    S1          -   matrix which may contain some pre-allocated memory, or
                    can be just uninitialized structure.

Outputs:
    S1          -   sparse matrix in CRS format.

NOTE: if S0 is stored as CRS, it is just copied without conversion.
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopytocrsbuf(sparsematrix s0, sparsematrix s1);
</pre>
<a name=sub_sparsecopytohash></a><h6 class=pageheader>sparsecopytohash Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  performs  out-of-place  conversion  to  Hash table storage
format. S0 is copied to S1 and converted on-the-fly.

Inputs:
    S0          -   sparse matrix in any format.

Outputs:
    S1          -   sparse matrix in Hash table format.

NOTE: if S0 is stored as Hash-table, it is just copied without conversion.

NOTE: this function de-allocates memory  occupied  by  S1 before  starting
      conversion. If you perform a  lot  of  repeated  conversions, it may
      lead to memory fragmentation. In this case we recommend you  to  use
      SparseCopyToHashBuf() function which re-uses memory in S1 as much as
      possible.
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopytohash(sparsematrix s0, sparsematrix &amp;s1);
</pre>
<a name=sub_sparsecopytohashbuf></a><h6 class=pageheader>sparsecopytohashbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  performs  out-of-place  conversion  to  Hash table storage
format. S0 is copied to S1 and converted on-the-fly. Memory  allocated  in
S1 is reused to maximum extent possible.

Inputs:
    S0          -   sparse matrix in any format.

Outputs:
    S1          -   sparse matrix in Hash table format.

NOTE: if S0 is stored as Hash-table, it is just copied without conversion.
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopytohashbuf(sparsematrix s0, sparsematrix s1);
</pre>
<a name=sub_sparsecopytosks></a><h6 class=pageheader>sparsecopytosks Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  performs  out-of-place  conversion  to SKS storage format.
S0 is copied to S1 and converted on-the-fly.

Inputs:
    S0          -   sparse matrix in any format.

Outputs:
    S1          -   sparse matrix in SKS format.

NOTE: if S0 is stored as SKS, it is just copied without conversion.

NOTE: this function de-allocates memory  occupied  by  S1 before  starting
      conversion. If you perform a  lot  of  repeated  conversions, it may
      lead to memory fragmentation. In this case we recommend you  to  use
      SparseCopyToSKSBuf() function which re-uses memory in S1 as much  as
      possible.
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopytosks(sparsematrix s0, sparsematrix &amp;s1);
</pre>
<a name=sub_sparsecopytosksbuf></a><h6 class=pageheader>sparsecopytosksbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  performs  out-of-place  conversion  to SKS format.  S0  is
copied to S1 and converted on-the-fly. Memory  allocated  in S1 is  reused
to maximum extent possible.

Inputs:
    S0          -   sparse matrix in any format.

Outputs:
    S1          -   sparse matrix in SKS format.

NOTE: if S0 is stored as SKS, it is just copied without conversion.
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopytosksbuf(sparsematrix s0, sparsematrix s1);
</pre>
<a name=sub_sparsecopytransposecrs></a><h6 class=pageheader>sparsecopytransposecrs Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs copying with transposition of CRS matrix.

Inputs:
    S0      -   sparse matrix in CRS format.

Outputs:
    S1      -   sparse matrix, transposed
ALGLIB Project: Copyright 23.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopytransposecrs(sparsematrix s0, sparsematrix &amp;s1);
</pre>
<a name=sub_sparsecopytransposecrsbuf></a><h6 class=pageheader>sparsecopytransposecrsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs copying with transposition of CRS matrix  (buffered
version which reuses memory already allocated by  the  target as  much  as
possible).

Inputs:
    S0      -   sparse matrix in CRS format.

Outputs:
    S1      -   sparse matrix, transposed; previously allocated memory  is
                reused if possible.
ALGLIB Project: Copyright 23.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecopytransposecrsbuf(sparsematrix s0, sparsematrix s1);
</pre>
<a name=sub_sparsecreate></a><h6 class=pageheader>sparsecreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates sparse matrix in a Hash-Table format.

This function creates Hast-Table matrix, which can be  converted  to  CRS
format after its initialization is over. Typical  usage  scenario  for  a
sparse matrix is:
1. creation in a Hash-Table format
2. insertion of the matrix elements
3. conversion to the CRS representation
4. matrix is passed to some linear algebra algorithm

Some  information  about  different matrix formats can be found below, in
the &quot;NOTES&quot; section.

Inputs:
    M           -   number of rows in a matrix, M &ge; 1
    N           -   number of columns in a matrix, N &ge; 1
    K           -   K &ge; 0, expected number of non-zero elements in a matrix.
                    K can be inexact approximation, can be less than actual
                    number  of  elements  (table will grow when needed) or
                    even zero).
                    It is important to understand that although hash-table
                    may grow automatically, it is better to  provide  good
                    estimate of data size.

Outputs:
    S           -   sparse M*N matrix in Hash-Table representation.
                    All elements of the matrix are zero.

NOTE 1

Hash-tables use memory inefficiently, and they have to keep  some  amount
of the &quot;spare memory&quot; in order to have good performance. Hash  table  for
matrix with K non-zero elements will  need  C*K*(8+2*sizeof(int))  bytes,
where C is a small constant, about 1.5-2 in magnitude.

CRS storage, from the other side, is  more  memory-efficient,  and  needs
just K*(8+sizeof(int))+M*sizeof(int) bytes, where M is a number  of  rows
in a matrix.

When you convert from the Hash-Table to CRS  representation, all unneeded
memory will be freed.

NOTE 2

Comments of SparseMatrix structure outline  information  about  different
sparse storage formats. We recommend you to read them before starting  to
use ALGLIB sparse matrices.

NOTE 3

This function completely  overwrites S with new sparse matrix. Previously
allocated storage is NOT reused. If you  want  to reuse already allocated
memory, call SparseCreateBuf function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecreate(ae_int_t m, ae_int_t n, ae_int_t k, sparsematrix &amp;s);
<b>void</b> sparsecreate(ae_int_t m, ae_int_t n, sparsematrix &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_sparse_d_1 class=nav>sparse_d_1</a> ]</p>
<a name=sub_sparsecreatebuf></a><h6 class=pageheader>sparsecreatebuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This version of SparseCreate function creates sparse matrix in Hash-Table
format, reusing previously allocated storage as much  as  possible.  Read
comments for SparseCreate() for more information.

Inputs:
    M           -   number of rows in a matrix, M &ge; 1
    N           -   number of columns in a matrix, N &ge; 1
    K           -   K &ge; 0, expected number of non-zero elements in a matrix.
                    K can be inexact approximation, can be less than actual
                    number  of  elements  (table will grow when needed) or
                    even zero).
                    It is important to understand that although hash-table
                    may grow automatically, it is better to  provide  good
                    estimate of data size.
    S           -   SparseMatrix structure which MAY contain some  already
                    allocated storage.

Outputs:
    S           -   sparse M*N matrix in Hash-Table representation.
                    All elements of the matrix are zero.
                    Previously allocated storage is reused, if  its  size
                    is compatible with expected number of non-zeros K.
ALGLIB Project: Copyright 14.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecreatebuf(ae_int_t m, ae_int_t n, ae_int_t k, sparsematrix s);
<b>void</b> sparsecreatebuf(ae_int_t m, ae_int_t n, sparsematrix s);
</pre>
<a name=sub_sparsecreatecrs></a><h6 class=pageheader>sparsecreatecrs Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates sparse matrix in a CRS format (expert function for
situations when you are running out of memory).

This function creates CRS matrix. Typical usage scenario for a CRS matrix
is:
1. creation (you have to tell number of non-zero elements at each row  at
   this moment)
2. insertion of the matrix elements (row by row, from left to right)
3. matrix is passed to some linear algebra algorithm

This function is a memory-efficient alternative to SparseCreate(), but it
is more complex because it requires you to know in advance how large your
matrix is. Some  information about  different matrix formats can be found
in comments on SparseMatrix structure.  We recommend  you  to  read  them
before starting to use ALGLIB sparse matrices..

Inputs:
    M           -   number of rows in a matrix, M &ge; 1
    N           -   number of columns in a matrix, N &ge; 1
    NER         -   number of elements at each row, array[M], NER[I] &ge; 0

Outputs:
    S           -   sparse M*N matrix in CRS representation.
                    You have to fill ALL non-zero elements by calling
                    SparseSet() BEFORE you try to use this matrix.

NOTE: this function completely  overwrites  S  with  new  sparse  matrix.
      Previously allocated storage is NOT reused. If you  want  to  reuse
      already allocated memory, call SparseCreateCRSBuf function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecreatecrs(ae_int_t m, ae_int_t n, integer_1d_array ner, sparsematrix &amp;s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_sparse_d_crs class=nav>sparse_d_crs</a> ]</p>
<a name=sub_sparsecreatecrsbuf></a><h6 class=pageheader>sparsecreatecrsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates sparse matrix in a CRS format (expert function  for
situations when you are running out  of  memory).  This  version  of  CRS
matrix creation function may reuse memory already allocated in S.

This function creates CRS matrix. Typical usage scenario for a CRS matrix
is:
1. creation (you have to tell number of non-zero elements at each row  at
   this moment)
2. insertion of the matrix elements (row by row, from left to right)
3. matrix is passed to some linear algebra algorithm

This function is a memory-efficient alternative to SparseCreate(), but it
is more complex because it requires you to know in advance how large your
matrix is. Some  information about  different matrix formats can be found
in comments on SparseMatrix structure.  We recommend  you  to  read  them
before starting to use ALGLIB sparse matrices..

Inputs:
    M           -   number of rows in a matrix, M &ge; 1
    N           -   number of columns in a matrix, N &ge; 1
    NER         -   number of elements at each row, array[M], NER[I] &ge; 0
    S           -   sparse matrix structure with possibly preallocated
                    memory.

Outputs:
    S           -   sparse M*N matrix in CRS representation.
                    You have to fill ALL non-zero elements by calling
                    SparseSet() BEFORE you try to use this matrix.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecreatecrsbuf(ae_int_t m, ae_int_t n, integer_1d_array ner, sparsematrix s);
</pre>
<a name=sub_sparsecreatesks></a><h6 class=pageheader>sparsecreatesks Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates sparse matrix in  a  SKS  format  (skyline  storage
format). In most cases you do not need this function - CRS format  better
suits most use cases.

Inputs:
    M, N        -   number of rows(M) and columns (N) in a matrix:
                    * M=N (as for now, ALGLIB supports only square SKS)
                    * N &ge; 1
                    * M &ge; 1
    D           -   &quot;bottom&quot; bandwidths, array[M], D[I] &ge; 0.
                    I-th element stores number of non-zeros at I-th  row,
                    below the diagonal (diagonal itself is not  included)
    U           -   &quot;top&quot; bandwidths, array[N], U[I] &ge; 0.
                    I-th element stores number of non-zeros  at I-th row,
                    above the diagonal (diagonal itself  is not included)

Outputs:
    S           -   sparse M*N matrix in SKS representation.
                    All elements are filled by zeros.
                    You may use sparseset() to change their values.

NOTE: this function completely  overwrites  S  with  new  sparse  matrix.
      Previously allocated storage is NOT reused. If you  want  to  reuse
      already allocated memory, call SparseCreateSKSBuf function.
ALGLIB Project: Copyright 13.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecreatesks(ae_int_t m, ae_int_t n, integer_1d_array d, integer_1d_array u, sparsematrix &amp;s);
</pre>
<a name=sub_sparsecreatesksband></a><h6 class=pageheader>sparsecreatesksband Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function creates sparse matrix in  a  SKS  format  (skyline  storage
format). Unlike more general  sparsecreatesks(),  this  function  creates
sparse matrix with constant bandwidth.

You may want to use this function instead of sparsecreatesks() when  your
matrix has  constant  or  nearly-constant  bandwidth,  and  you  want  to
simplify source code.

Inputs:
    M, N        -   number of rows(M) and columns (N) in a matrix:
                    * M=N (as for now, ALGLIB supports only square SKS)
                    * N &ge; 1
                    * M &ge; 1
    BW          -   matrix bandwidth, BW &ge; 0

Outputs:
    S           -   sparse M*N matrix in SKS representation.
                    All elements are filled by zeros.
                    You may use sparseset() to  change  their values.

NOTE: this function completely  overwrites  S  with  new  sparse  matrix.
      Previously allocated storage is NOT reused. If you  want  to  reuse
      already allocated memory, call sparsecreatesksbandbuf function.
ALGLIB Project: Copyright 25.12.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecreatesksband(ae_int_t m, ae_int_t n, ae_int_t bw, sparsematrix &amp;s);
</pre>
<a name=sub_sparsecreatesksbandbuf></a><h6 class=pageheader>sparsecreatesksbandbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This is &quot;buffered&quot; version  of  sparsecreatesksband() which reuses memory
previously allocated in S (of course, memory is reallocated if needed).

You may want to use this function instead  of  sparsecreatesksbuf()  when
your matrix has  constant or nearly-constant  bandwidth,  and you want to
simplify source code.

Inputs:
    M, N        -   number of rows(M) and columns (N) in a matrix:
                    * M=N (as for now, ALGLIB supports only square SKS)
                    * N &ge; 1
                    * M &ge; 1
    BW          -   bandwidth, BW &ge; 0

Outputs:
    S           -   sparse M*N matrix in SKS representation.
                    All elements are filled by zeros.
                    You may use sparseset() to change their values.
ALGLIB Project: Copyright 13.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecreatesksbandbuf(ae_int_t m, ae_int_t n, ae_int_t bw, sparsematrix s);
</pre>
<a name=sub_sparsecreatesksbuf></a><h6 class=pageheader>sparsecreatesksbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This is &quot;buffered&quot;  version  of  SparseCreateSKS()  which  reuses  memory
previously allocated in S (of course, memory is reallocated if needed).

This function creates sparse matrix in  a  SKS  format  (skyline  storage
format). In most cases you do not need this function - CRS format  better
suits most use cases.

Inputs:
    M, N        -   number of rows(M) and columns (N) in a matrix:
                    * M=N (as for now, ALGLIB supports only square SKS)
                    * N &ge; 1
                    * M &ge; 1
    D           -   &quot;bottom&quot; bandwidths, array[M], 0 &le; D[I] &le; I.
                    I-th element stores number of non-zeros at I-th row,
                    below the diagonal (diagonal itself is not included)
    U           -   &quot;top&quot; bandwidths, array[N], 0 &le; U[I] &le; I.
                    I-th element stores number of non-zeros at I-th row,
                    above the diagonal (diagonal itself is not included)

Outputs:
    S           -   sparse M*N matrix in SKS representation.
                    All elements are filled by zeros.
                    You may use sparseset() to change their values.
ALGLIB Project: Copyright 13.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecreatesksbuf(ae_int_t m, ae_int_t n, integer_1d_array d, integer_1d_array u, sparsematrix s);
</pre>
<a name=sub_sparseenumerate></a><h6 class=pageheader>sparseenumerate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  is  used  to enumerate all elements of the sparse matrix.
Before  first  call  user  initializes  T0 and T1 counters by zero. These
counters are used to remember current position in a  matrix;  after  each
call they are updated by the function.

Subsequent calls to this function return non-zero elements of the  sparse
matrix, one by one. If you enumerate CRS matrix, matrix is traversed from
left to right, from top to bottom. In case you enumerate matrix stored as
Hash table, elements are returned in random order.

EXAMPLE
    &gt; T0=0
    &gt; T1=0
    &gt; while SparseEnumerate(S,T0,T1,I,J,V) do
    &gt;     ....do something with I,J,V

Inputs:
    S           -   sparse M*N matrix in Hash-Table or CRS representation.
    T0          -   internal counter
    T1          -   internal counter

Outputs:
    T0          -   new value of the internal counter
    T1          -   new value of the internal counter
    I           -   row index of non-zero element, 0 &le; I &lt; M.
    J           -   column index of non-zero element, 0 &le; J &lt; N
    V           -   value of the T-th element

Result:
    True in case of success (next non-zero element was retrieved)
    False in case all non-zero elements were enumerated

NOTE: you may call SparseRewriteExisting() during enumeration, but it  is
      THE  ONLY  matrix  modification  function  you  can  call!!!  Other
      matrix modification functions should not be called during enumeration!
ALGLIB Project: Copyright 14.03.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparseenumerate(sparsematrix s, ae_int_t &amp;t0, ae_int_t &amp;t1, ae_int_t &amp;i, ae_int_t &amp;j, <b>double</b> &amp;v);
</pre>
<a name=sub_sparseexists></a><h6 class=pageheader>sparseexists Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function checks whether S[i,j] is present in the sparse  matrix.  It
returns True even for elements  that  are  numerically  zero  (but  still
have place allocated for them).

The matrix  can be in any mode (Hash-Table, CRS, SKS), but this  function
is less efficient for CRS matrices. Hash-Table and SKS matrices can  find
element in O(1) time, while  CRS  matrices need O(log(RS)) time, where RS
is an number of non-zero elements in a row.

Inputs:
    S           -   sparse M*N matrix
    I           -   row index of the element to modify, 0 &le; I &lt; M
    J           -   column index of the element to modify, 0 &le; J &lt; N

Result:
    whether S[I,J] is present in the data structure or not
ALGLIB Project: Copyright 14.10.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparseexists(sparsematrix s, ae_int_t i, ae_int_t j);
</pre>
<a name=sub_sparsefree></a><h6 class=pageheader>sparsefree Function</h6>
<hr width=600 align=left>
<pre class=narration>
The function frees all memory occupied by  sparse  matrix.  Sparse  matrix
structure becomes unusable after this call.

Outputs:
    S   -   sparse matrix to delete
ALGLIB Project: Copyright 24.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsefree(sparsematrix &amp;s);
</pre>
<a name=sub_sparsegemv></a><h6 class=pageheader>sparsegemv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates generalized sparse matrix-vector product

    y := alpha*op(S)*x + beta*y

Matrix S must be stored in CRS or SKS format (exception  will  be  thrown
otherwise). op(S) can be either S or S^T.

NOTE: this  function  expects  Y  to  be  large enough to store result. No
      automatic preallocation happens for smaller arrays.

Inputs:
    S           -   sparse matrix in CRS or SKS format.
    Alpha       -   source coefficient
    OpS         -   operation type:
                    * OpS=0     &rArr;  op(S) = S
                    * OpS=1     &rArr;  op(S) = S^T
    X           -   input vector, must have at least Cols(op(S))+IX elements
    IX          -   subvector offset
    Beta        -   destination coefficient
    Y           -   preallocated output array, must have at least Rows(op(S))+IY elements
    IY          -   subvector offset

Outputs:
    Y           -   elements [IY...IY+Rows(op(S))-1] are replaced by result,
                    other elements are not modified

HANDLING OF SPECIAL CASES:
* below M=Rows(op(S)) and N=Cols(op(S)). Although current  ALGLIB  version
  does not allow you to  create  zero-sized  sparse  matrices,  internally
  ALGLIB  can  deal  with  such matrices. So, comments for M or N equal to
  zero are for internal use only.
* if M=0, then subroutine does nothing. It does not even touch arrays.
* if N=0 or Alpha=0.0, then:
  * if Beta=0, then Y is filled by zeros. S and X are  not  referenced  at
    all. Initial values of Y are ignored (we do not  multiply  Y by  zero,
    we just rewrite it by zeros)
  * if Beta &ne; 0, then Y is replaced by Beta*Y
* if M &gt; 0, N &gt; 0, Alpha &ne; 0, but  Beta=0, then  Y is replaced by alpha*op(S)*x
  initial state of Y  is ignored (rewritten without initial multiplication
  by zeros).

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 10.12.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsegemv(sparsematrix s, <b>double</b> alpha, ae_int_t ops, real_1d_array x, ae_int_t ix, <b>double</b> beta, real_1d_array y, ae_int_t iy);
</pre>
<a name=sub_sparseget></a><h6 class=pageheader>sparseget Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns S[i,j] - element of the sparse matrix.  Matrix  can
be in any mode (Hash-Table, CRS, SKS), but this function is less efficient
for CRS matrices. Hash-Table and SKS matrices can find  element  in  O(1)
time, while  CRS  matrices need O(log(RS)) time, where RS is an number of
non-zero elements in a row.

Inputs:
    S           -   sparse M*N matrix
    I           -   row index of the element to modify, 0 &le; I &lt; M
    J           -   column index of the element to modify, 0 &le; J &lt; N

Result:
    value of S[I,J] or zero (in case no element with such index is found)
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> sparseget(sparsematrix s, ae_int_t i, ae_int_t j);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_sparse_d_1 class=nav>sparse_d_1</a> | <a href=#example_sparse_d_crs class=nav>sparse_d_crs</a> ]</p>
<a name=sub_sparsegetcompressedrow></a><h6 class=pageheader>sparsegetcompressedrow Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns I-th row of the sparse matrix IN COMPRESSED FORMAT -
only non-zero elements are returned (with their indexes). Matrix  must  be
stored in CRS or SKS format.

Inputs:
    S           -   sparse M*N matrix in CRS format
    I           -   row index, 0 &le; I &lt; M
    ColIdx      -   output buffer for column indexes, can be preallocated.
                    In case buffer size is too small to store I-th row, it
                    is automatically reallocated.
    Vals        -   output buffer for values, can be preallocated. In case
                    buffer size is too small to  store  I-th  row,  it  is
                    automatically reallocated.

Outputs:
    ColIdx      -   column   indexes   of  non-zero  elements,  sorted  by
                    ascending. Symbolically non-zero elements are  counted
                    (i.e. if you allocated place for element, but  it  has
                    zero numerical value - it is counted).
    Vals        -   values. Vals[K] stores value of  matrix  element  with
                    indexes (I,ColIdx[K]). Symbolically non-zero  elements
                    are counted (i.e. if you allocated place for  element,
                    but it has zero numerical value - it is counted).
    NZCnt       -   number of symbolically non-zero elements per row.

NOTE: when  incorrect  I  (outside  of  [0,M-1]) or  matrix (non  CRS/SKS)
      is passed, this function throws exception.

NOTE: this function may allocate additional, unnecessary place for  ColIdx
      and Vals arrays. It is dictated by  performance  reasons  -  on  SKS
      matrices it is faster  to  allocate  space  at  the  beginning  with
      some &quot;extra&quot;-space, than performing two passes over matrix  -  first
      time to calculate exact space required for data, second  time  -  to
      store data itself.
ALGLIB Project: Copyright 10.12.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsegetcompressedrow(sparsematrix s, ae_int_t i, integer_1d_array &amp;colidx, real_1d_array &amp;vals, ae_int_t &amp;nzcnt);
</pre>
<a name=sub_sparsegetdiagonal></a><h6 class=pageheader>sparsegetdiagonal Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns I-th diagonal element of the sparse matrix.

Matrix can be in any mode (Hash-Table or CRS storage), but this  function
is most efficient for CRS matrices - it requires less than 50 CPU  cycles
to extract diagonal element. For Hash-Table matrices we still  have  O(1)
query time, but function is many times slower.

Inputs:
    S           -   sparse M*N matrix in Hash-Table representation.
                    Exception will be thrown for CRS matrix.
    I           -   index of the element to modify, 0 &le; I &lt; min(M,N)

Result:
    value of S[I,I] or zero (in case no element with such index is found)
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> sparsegetdiagonal(sparsematrix s, ae_int_t i);
</pre>
<a name=sub_sparsegetlowercount></a><h6 class=pageheader>sparsegetlowercount Function</h6>
<hr width=600 align=left>
<pre class=narration>
The function returns number of strictly lower triangular non-zero elements
in  the  matrix.  It  counts  SYMBOLICALLY non-zero elements, i.e. entries
in the sparse matrix data structure. If some element  has  zero  numerical
value, it is still counted.

This function has different cost for different types of matrices:
* for hash-based matrices it involves complete pass over entire hash-table
  with O(NNZ) cost, where NNZ is number of non-zero elements
* for CRS and SKS matrix types cost of counting is O(N) (N - matrix size).

Result: number of non-zero elements strictly below main diagonal
ALGLIB Project: Copyright 12.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t sparsegetlowercount(sparsematrix s);
</pre>
<a name=sub_sparsegetmatrixtype></a><h6 class=pageheader>sparsegetmatrixtype Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns type of the matrix storage format.

Inputs:
    S           -   sparse matrix.

Result:
    sparse storage format used by matrix:
        0   -   Hash-table
        1   -   CRS (compressed row storage)
        2   -   SKS (skyline)

NOTE: future  versions  of  ALGLIB  may  include additional sparse storage
      formats.
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t sparsegetmatrixtype(sparsematrix s);
</pre>
<a name=sub_sparsegetncols></a><h6 class=pageheader>sparsegetncols Function</h6>
<hr width=600 align=left>
<pre class=narration>
The function returns number of columns of a sparse matrix.

Result: number of columns of a sparse matrix.
ALGLIB Project: Copyright 23.08.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t sparsegetncols(sparsematrix s);
</pre>
<a name=sub_sparsegetnrows></a><h6 class=pageheader>sparsegetnrows Function</h6>
<hr width=600 align=left>
<pre class=narration>
The function returns number of rows of a sparse matrix.

Result: number of rows of a sparse matrix.
ALGLIB Project: Copyright 23.08.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t sparsegetnrows(sparsematrix s);
</pre>
<a name=sub_sparsegetrow></a><h6 class=pageheader>sparsegetrow Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function returns I-th row of the sparse matrix. Matrix must be stored
in CRS or SKS format.

Inputs:
    S           -   sparse M*N matrix in CRS format
    I           -   row index, 0 &le; I &lt; M
    IRow        -   output buffer, can be  preallocated.  In  case  buffer
                    size  is  too  small  to  store  I-th   row,   it   is
                    automatically reallocated.

Outputs:
    IRow        -   array[M], I-th row.

NOTE: this function has O(N) running time, where N is a  column  count. It
      allocates and fills N-element  array,  even  although  most  of  its
      elemets are zero.

NOTE: If you have O(non-zeros-per-row) time and memory  requirements,  use
      SparseGetCompressedRow() function. It  returns  data  in  compressed
      format.

NOTE: when  incorrect  I  (outside  of  [0,M-1]) or  matrix (non  CRS/SKS)
      is passed, this function throws exception.
ALGLIB Project: Copyright 10.12.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsegetrow(sparsematrix s, ae_int_t i, real_1d_array &amp;irow);
</pre>
<a name=sub_sparsegetuppercount></a><h6 class=pageheader>sparsegetuppercount Function</h6>
<hr width=600 align=left>
<pre class=narration>
The function returns number of strictly upper triangular non-zero elements
in  the  matrix.  It  counts  SYMBOLICALLY non-zero elements, i.e. entries
in the sparse matrix data structure. If some element  has  zero  numerical
value, it is still counted.

This function has different cost for different types of matrices:
* for hash-based matrices it involves complete pass over entire hash-table
  with O(NNZ) cost, where NNZ is number of non-zero elements
* for CRS and SKS matrix types cost of counting is O(N) (N - matrix size).

Result: number of non-zero elements strictly above main diagonal
ALGLIB Project: Copyright 12.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t sparsegetuppercount(sparsematrix s);
</pre>
<a name=sub_sparseiscrs></a><h6 class=pageheader>sparseiscrs Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function checks matrix storage format and returns True when matrix is
stored using CRS representation.

Inputs:
    S   -   sparse matrix.

Result:
    True if matrix type is CRS
    False if matrix type is not CRS
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparseiscrs(sparsematrix s);
</pre>
<a name=sub_sparseishash></a><h6 class=pageheader>sparseishash Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function checks matrix storage format and returns True when matrix is
stored using Hash table representation.

Inputs:
    S   -   sparse matrix.

Result:
    True if matrix type is Hash table
    False if matrix type is not Hash table
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparseishash(sparsematrix s);
</pre>
<a name=sub_sparseissks></a><h6 class=pageheader>sparseissks Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function checks matrix storage format and returns True when matrix is
stored using SKS representation.

Inputs:
    S   -   sparse matrix.

Result:
    True if matrix type is SKS
    False if matrix type is not SKS
ALGLIB Project: Copyright 20.07.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparseissks(sparsematrix s);
</pre>
<a name=sub_sparsemm></a><h6 class=pageheader>sparsemm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates matrix-matrix product  S*A.  Matrix  S  must  be
stored in CRS or SKS format (exception will be thrown otherwise).

Inputs:
    S           -   sparse M*N matrix in CRS or SKS format.
    A           -   array[N][K], input dense matrix. For  performance reasons
                    we make only quick checks - we check that array size
                    is at least N, but we do not check for NAN's or INF's.
    K           -   number of columns of matrix (A).
    B           -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.

Outputs:
    B           -   array[M][K], S*A

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsemm(sparsematrix s, real_2d_array a, ae_int_t k, real_2d_array &amp;b);
</pre>
<a name=sub_sparsemm2></a><h6 class=pageheader>sparsemm2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function simultaneously calculates two matrix-matrix products:
    S*A and S^T*A.
S  must  be  square (non-rectangular) matrix stored in CRS or  SKS  format
(exception will be thrown otherwise).

Inputs:
    S           -   sparse N*N matrix in CRS or SKS format.
    A           -   array[N][K], input dense matrix. For performance reasons
                    we make only quick checks - we check that array size  is
                    at least N, but we do not check for NAN's or INF's.
    K           -   number of columns of matrix (A).
    B0          -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.
    B1          -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.

Outputs:
    B0          -   array[N][K], S*A
    B1          -   array[N][K], S^T*A

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsemm2(sparsematrix s, real_2d_array a, ae_int_t k, real_2d_array &amp;b0, real_2d_array &amp;b1);
</pre>
<a name=sub_sparsemtm></a><h6 class=pageheader>sparsemtm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates matrix-matrix product  S^T*A. Matrix S  must  be
stored in CRS or SKS format (exception will be thrown otherwise).

Inputs:
    S           -   sparse M*N matrix in CRS or SKS format.
    A           -   array[M][K], input dense matrix. For performance reasons
                    we make only quick checks - we check that array size  is
                    at least M, but we do not check for NAN's or INF's.
    K           -   number of columns of matrix (A).
    B           -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.

Outputs:
    B           -   array[N][K], S^T*A

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsemtm(sparsematrix s, real_2d_array a, ae_int_t k, real_2d_array &amp;b);
</pre>
<a name=sub_sparsemtv></a><h6 class=pageheader>sparsemtv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates matrix-vector product  S^T*x. Matrix S  must  be
stored in CRS or SKS format (exception will be thrown otherwise).

Inputs:
    S           -   sparse M*N matrix in CRS or SKS format.
    X           -   array[M], input vector. For  performance  reasons  we
                    make only quick checks - we check that array size  is
                    at least M, but we do not check for NAN's or INF's.
    Y           -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.

Outputs:
    Y           -   array[N], S^T*x

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsemtv(sparsematrix s, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_sparsemv></a><h6 class=pageheader>sparsemv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates matrix-vector product  S*x.  Matrix  S  must  be
stored in CRS or SKS format (exception will be thrown otherwise).

Inputs:
    S           -   sparse M*N matrix in CRS or SKS format.
    X           -   array[N], input vector. For  performance  reasons  we
                    make only quick checks - we check that array size  is
                    at least N, but we do not check for NAN's or INF's.
    Y           -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.

Outputs:
    Y           -   array[M], S*x

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsemv(sparsematrix s, real_1d_array x, real_1d_array &amp;y);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_sparse_d_1 class=nav>sparse_d_1</a> | <a href=#example_sparse_d_crs class=nav>sparse_d_crs</a> ]</p>
<a name=sub_sparsemv2></a><h6 class=pageheader>sparsemv2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function simultaneously calculates two matrix-vector products:
    S*x and S^T*x.
S must be square (non-rectangular) matrix stored in  CRS  or  SKS  format
(exception will be thrown otherwise).

Inputs:
    S           -   sparse N*N matrix in CRS or SKS format.
    X           -   array[N], input vector. For  performance  reasons  we
                    make only quick checks - we check that array size  is
                    at least N, but we do not check for NAN's or INF's.
    Y0          -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.
    Y1          -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.

Outputs:
    Y0          -   array[N], S*x
    Y1          -   array[N], S^T*x

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsemv2(sparsematrix s, real_1d_array x, real_1d_array &amp;y0, real_1d_array &amp;y1);
</pre>
<a name=sub_sparseresizematrix></a><h6 class=pageheader>sparseresizematrix Function</h6>
<hr width=600 align=left>
<pre class=narration>
This procedure resizes Hash-Table matrix. It can be called when you  have
deleted too many elements from the matrix, and you want to  free unneeded
memory.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparseresizematrix(sparsematrix s);
</pre>
<a name=sub_sparserewriteexisting></a><h6 class=pageheader>sparserewriteexisting Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function rewrites existing (non-zero) element. It  returns  True   if
element  exists  or  False,  when  it  is  called for non-existing  (zero)
element.

This function works with any kind of the matrix.

The purpose of this function is to provide convenient thread-safe  way  to
modify  sparse  matrix.  Such  modification  (already  existing element is
rewritten) is guaranteed to be thread-safe without any synchronization, as
long as different threads modify different elements.

Inputs:
    S           -   sparse M*N matrix in any kind of representation
                    (Hash, SKS, CRS).
    I           -   row index of non-zero element to modify, 0 &le; I &lt; M
    J           -   column index of non-zero element to modify, 0 &le; J &lt; N
    V           -   value to rewrite, must be finite number

Outputs:
    S           -   modified matrix

Result:
    True in case when element exists
    False in case when element doesn't exist or it is zero
ALGLIB Project: Copyright 14.03.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparserewriteexisting(sparsematrix s, ae_int_t i, ae_int_t j, <b>double</b> v);
</pre>
<a name=sub_sparseset></a><h6 class=pageheader>sparseset Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function modifies S[i,j] - element of the sparse matrix.

For Hash-based storage format:
* this function can be called at any moment - during matrix initialization
  or later
* new value can be zero or non-zero.  In case new value of S[i,j] is zero,
  this element is deleted from the table.
* this  function  has  no  effect when called with zero V for non-existent
  element.

For CRS-bases storage format:
* this function can be called ONLY DURING MATRIX INITIALIZATION
* zero values are stored in the matrix similarly to non-zero ones
* elements must be initialized in correct order -  from top row to bottom,
  within row - from left to right.

For SKS storage:
* this function can be called at any moment - during matrix initialization
  or later
* zero values are stored in the matrix similarly to non-zero ones
* this function CAN NOT be called for non-existent (outside  of  the  band
  specified during SKS matrix creation) elements. Say, if you created  SKS
  matrix  with  bandwidth=2  and  tried to call sparseset(s,0,10,VAL),  an
  exception will be generated.

Inputs:
    S           -   sparse M*N matrix in Hash-Table, SKS or CRS format.
    I           -   row index of the element to modify, 0 &le; I &lt; M
    J           -   column index of the element to modify, 0 &le; J &lt; N
    V           -   value to set, must be finite number, can be zero

Outputs:
    S           -   modified matrix
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparseset(sparsematrix s, ae_int_t i, ae_int_t j, <b>double</b> v);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_sparse_d_1 class=nav>sparse_d_1</a> | <a href=#example_sparse_d_crs class=nav>sparse_d_crs</a> ]</p>
<a name=sub_sparsesmm></a><h6 class=pageheader>sparsesmm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates matrix-matrix product  S*A, when S  is  symmetric
matrix. Matrix S must be stored in CRS or SKS format  (exception  will  be
thrown otherwise).

Inputs:
    S           -   sparse M*M matrix in CRS or SKS format.
    IsUpper     -   whether upper or lower triangle of S is given:
                    * if upper triangle is given,  only   S[i,j] for j &ge; i
                      are used, and lower triangle is ignored (it can  be
                      empty - these elements are not referenced at all).
                    * if lower triangle is given,  only   S[i,j] for j &le; i
                      are used, and upper triangle is ignored.
    A           -   array[M][K], input dense matrix. For performance reasons
                    we make only quick checks - we check that array size is
                    at least M, but we do not check for NAN's or INF's.
    K           -   number of columns of matrix (A).
    B           -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.

Outputs:
    B           -   array[M][K], S*A

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsesmm(sparsematrix s, <b>bool</b> isupper, real_2d_array a, ae_int_t k, real_2d_array &amp;b);
</pre>
<a name=sub_sparsesmv></a><h6 class=pageheader>sparsesmv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates matrix-vector product  S*x, when S is  symmetric
matrix. Matrix S  must be stored in CRS or SKS format  (exception will be
thrown otherwise).

Inputs:
    S           -   sparse M*M matrix in CRS or SKS format.
    IsUpper     -   whether upper or lower triangle of S is given:
                    * if upper triangle is given,  only   S[i,j] for j &ge; i
                      are used, and lower triangle is ignored (it can  be
                      empty - these elements are not referenced at all).
                    * if lower triangle is given,  only   S[i,j] for j &le; i
                      are used, and upper triangle is ignored.
    X           -   array[N], input vector. For  performance  reasons  we
                    make only quick checks - we check that array size  is
                    at least N, but we do not check for NAN's or INF's.
    Y           -   output buffer, possibly preallocated. In case  buffer
                    size is too small to store  result,  this  buffer  is
                    automatically resized.

Outputs:
    Y           -   array[M], S*x

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 14.10.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsesmv(sparsematrix s, <b>bool</b> isupper, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_sparseswap></a><h6 class=pageheader>sparseswap Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function efficiently swaps contents of S0 and S1.
ALGLIB Project: Copyright 16.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparseswap(sparsematrix s0, sparsematrix s1);
</pre>
<a name=sub_sparsesymmpermtbl></a><h6 class=pageheader>sparsesymmpermtbl Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function applies permutation given by permutation table P (as opposed
to product form of permutation) to sparse symmetric  matrix  A,  given  by
either upper or lower triangle: B := P*A*P'.

This function allocates completely new instance of B. Use buffered version
SparseSymmPermTblBuf() if you want to reuse already allocated structure.

Inputs:
    A           -   sparse square matrix in CRS format.
    IsUpper     -   whether upper or lower triangle of A is used:
                    * if upper triangle is given,  only   A[i,j] for  j &ge; i
                      are used, and lower triangle is  ignored (it can  be
                      empty - these elements are not referenced at all).
                    * if lower triangle is given,  only   A[i,j] for  j &le; i
                      are used, and upper triangle is ignored.
    P           -   array[N] which stores permutation table;  P[I]=J means
                    that I-th row/column of matrix  A  is  moved  to  J-th
                    position. For performance reasons we do NOT check that
                    P[] is  a   correct   permutation  (that there  is  no
                    repetitions, just that all its elements  are  in [0,N)
                    range.

Outputs:
    B           -   permuted matrix.  Permutation  is  applied  to A  from
                    the both sides, only upper or lower triangle (depending
                    on IsUpper) is stored.

NOTE: this function throws exception when called for non-CRS  matrix.  You
      must convert your matrix with SparseConvertToCRS() before using this
      function.
ALGLIB Project: Copyright 05.10.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsesymmpermtbl(sparsematrix a, <b>bool</b> isupper, integer_1d_array p, sparsematrix &amp;b);
</pre>
<a name=sub_sparsesymmpermtblbuf></a><h6 class=pageheader>sparsesymmpermtblbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is a buffered version  of  SparseSymmPermTbl()  that  reuses
previously allocated storage in B as much as possible.

This function applies permutation given by permutation table P (as opposed
to product form of permutation) to sparse symmetric  matrix  A,  given  by
either upper or lower triangle: B := P*A*P'.

Inputs:
    A           -   sparse square matrix in CRS format.
    IsUpper     -   whether upper or lower triangle of A is used:
                    * if upper triangle is given,  only   A[i,j] for  j &ge; i
                      are used, and lower triangle is  ignored (it can  be
                      empty - these elements are not referenced at all).
                    * if lower triangle is given,  only   A[i,j] for  j &le; i
                      are used, and upper triangle is ignored.
    P           -   array[N] which stores permutation table;  P[I]=J means
                    that I-th row/column of matrix  A  is  moved  to  J-th
                    position. For performance reasons we do NOT check that
                    P[] is  a   correct   permutation  (that there  is  no
                    repetitions, just that all its elements  are  in [0,N)
                    range.
    B           -   sparse matrix object that will hold output.
                    Previously allocated memory will be reused as much  as
                    possible.

Outputs:
    B           -   permuted matrix.  Permutation  is  applied  to A  from
                    the both sides, only upper or lower triangle (depending
                    on IsUpper) is stored.

NOTE: this function throws exception when called for non-CRS  matrix.  You
      must convert your matrix with SparseConvertToCRS() before using this
      function.
ALGLIB Project: Copyright 05.10.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsesymmpermtblbuf(sparsematrix a, <b>bool</b> isupper, integer_1d_array p, sparsematrix b);
</pre>
<a name=sub_sparsetransposecrs></a><h6 class=pageheader>sparsetransposecrs Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs transpose of CRS matrix.

Inputs:
    S       -   sparse matrix in CRS format.

Outputs:
    S           -   sparse matrix, transposed.

NOTE: internal  temporary  copy  is  allocated   for   the   purposes   of
      transposition. It is deallocated after transposition.
ALGLIB Project: Copyright 30.01.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsetransposecrs(sparsematrix s);
</pre>
<a name=sub_sparsetransposesks></a><h6 class=pageheader>sparsetransposesks Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function performs efficient in-place  transpose  of  SKS  matrix.  No
additional memory is allocated during transposition.

This function supports only skyline storage format (SKS).

Inputs:
    S       -   sparse matrix in SKS format.

Outputs:
    S           -   sparse matrix, transposed.
ALGLIB Project: Copyright 16.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsetransposesks(sparsematrix s);
</pre>
<a name=sub_sparsetrmv></a><h6 class=pageheader>sparsetrmv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates matrix-vector product op(S)*x, when x is  vector,
S is symmetric triangular matrix, op(S) is transposition or no  operation.
Matrix S must be stored in CRS or SKS format  (exception  will  be  thrown
otherwise).

Inputs:
    S           -   sparse square matrix in CRS or SKS format.
    IsUpper     -   whether upper or lower triangle of S is used:
                    * if upper triangle is given,  only   S[i,j] for  j &ge; i
                      are used, and lower triangle is  ignored (it can  be
                      empty - these elements are not referenced at all).
                    * if lower triangle is given,  only   S[i,j] for  j &le; i
                      are used, and upper triangle is ignored.
    IsUnit      -   unit or non-unit diagonal:
                    * if True, diagonal elements of triangular matrix  are
                      considered equal to 1.0. Actual elements  stored  in
                      S are not referenced at all.
                    * if False, diagonal stored in S is used
    OpType      -   operation type:
                    * if 0, S*x is calculated
                    * if 1, (S^T)*x is calculated (transposition)
    X           -   array[N] which stores input  vector.  For  performance
                    reasons we make only quick  checks  -  we  check  that
                    array  size  is  at  least  N, but we do not check for
                    NAN's or INF's.
    Y           -   possibly  preallocated  input   buffer.  Automatically
                    resized if its size is too small.

Outputs:
    Y           -   array[N], op(S)*x

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 20.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsetrmv(sparsematrix s, <b>bool</b> isupper, <b>bool</b> isunit, ae_int_t optype, real_1d_array x, real_1d_array &amp;y);
</pre>
<a name=sub_sparsetrsv></a><h6 class=pageheader>sparsetrsv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function solves linear system op(S)*y=x  where  x  is  vector,  S  is
symmetric  triangular  matrix,  op(S)  is  transposition  or no operation.
Matrix S must be stored in CRS or SKS format  (exception  will  be  thrown
otherwise).

Inputs:
    S           -   sparse square matrix in CRS or SKS format.
    IsUpper     -   whether upper or lower triangle of S is used:
                    * if upper triangle is given,  only   S[i,j] for  j &ge; i
                      are used, and lower triangle is  ignored (it can  be
                      empty - these elements are not referenced at all).
                    * if lower triangle is given,  only   S[i,j] for  j &le; i
                      are used, and upper triangle is ignored.
    IsUnit      -   unit or non-unit diagonal:
                    * if True, diagonal elements of triangular matrix  are
                      considered equal to 1.0. Actual elements  stored  in
                      S are not referenced at all.
                    * if False, diagonal stored in S is used. It  is  your
                      responsibility  to  make  sure  that   diagonal   is
                      non-zero.
    OpType      -   operation type:
                    * if 0, S*x is calculated
                    * if 1, (S^T)*x is calculated (transposition)
    X           -   array[N] which stores input  vector.  For  performance
                    reasons we make only quick  checks  -  we  check  that
                    array  size  is  at  least  N, but we do not check for
                    NAN's or INF's.

Outputs:
    X           -   array[N], inv(op(S))*x

NOTE: this function throws exception when called for  non-CRS/SKS  matrix.
      You must convert your matrix  with  SparseConvertToCRS/SKS()  before
      using this function.

NOTE: no assertion or tests are done during algorithm  operation.   It  is
      your responsibility to provide invertible matrix to algorithm.
ALGLIB Project: Copyright 20.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsetrsv(sparsematrix s, <b>bool</b> isupper, <b>bool</b> isunit, ae_int_t optype, real_1d_array x);
</pre>
<a name=sub_sparsevsmv></a><h6 class=pageheader>sparsevsmv Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function calculates vector-matrix-vector product x'*S*x, where  S is
symmetric matrix. Matrix S must be stored in CRS or SKS format (exception
will be thrown otherwise).

Inputs:
    S           -   sparse M*M matrix in CRS or SKS format.
    IsUpper     -   whether upper or lower triangle of S is given:
                    * if upper triangle is given,  only   S[i,j] for j &ge; i
                      are used, and lower triangle is ignored (it can  be
                      empty - these elements are not referenced at all).
                    * if lower triangle is given,  only   S[i,j] for j &le; i
                      are used, and upper triangle is ignored.
    X           -   array[N], input vector. For  performance  reasons  we
                    make only quick checks - we check that array size  is
                    at least N, but we do not check for NAN's or INF's.

Result:
    x'*S*x

NOTE: this function throws exception when called for non-CRS/SKS  matrix.
You must convert your matrix with SparseConvertToCRS/SKS()  before  using
this function.
ALGLIB Project: Copyright 27.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> sparsevsmv(sparsematrix s, <b>bool</b> isupper, real_1d_array x);
</pre>
<a name=example_sparse_d_1></a><h6 class=pageheader>sparse_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates creation/initialization of the sparse matrix</font>
<font color=navy>// and matrix-vector multiplication.</font>
<font color=navy>//</font>
<font color=navy>// First, we have to create matrix and initialize it. Matrix is initially created</font>
<font color=navy>// in the Hash-Table format, which allows convenient initialization. We can modify</font>
<font color=navy>// Hash-Table matrix with sparseset() and sparseadd() functions.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: Unlike CRS format, Hash-Table representation allows you to initialize</font>
<font color=navy>// elements in the arbitrary order. You may see that we initialize a[0][0] first,</font>
<font color=navy>// then move to the second row, and then move back to the first row.</font>
   sparsematrix s;
   sparsecreate(2, 2, s);
   sparseset(s, 0, 0, 2.0);
   sparseset(s, 1, 1, 1.0);
   sparseset(s, 0, 1, 1.0);
   sparseadd(s, 1, 1, 4.0);
<font color=navy>// Now S is equal to</font>
<font color=navy>//   [ 2 1 ]</font>
<font color=navy>//   [   5 ]</font>
<font color=navy>// Lets check it by reading matrix contents with sparseget().</font>
<font color=navy>// You may see that with sparseget() you may read both non-zero</font>
<font color=navy>// and zero elements.</font>
   <b>double</b> v;
   v = sparseget(s, 0, 0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.0000</font>
   v = sparseget(s, 0, 1);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.0000</font>
   v = sparseget(s, 1, 0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.0000</font>
   v = sparseget(s, 1, 1);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 5.0000</font>
<font color=navy>//</font>
<font color=navy>// After successful creation we can use our matrix <b>for</b> linear operations.</font>
<font color=navy>//</font>
<font color=navy>// However, there is one more thing we MUST <b>do</b> before using S in linear</font>
<font color=navy>// operations: we have to convert it from HashTable representation (used <b>for</b></font>
<font color=navy>// initialization and dynamic operations) to CRS format with sparseconverttocrs()</font>
<font color=navy>// call. If you omit this call, ALGLIB will generate exception on the first</font>
<font color=navy>// attempt to use S in linear operations. </font>
   sparseconverttocrs(s);
<font color=navy>// Now S is in the CRS format and we are ready to <b>do</b> linear operations.</font>
<font color=navy>// Lets calculate A*x <b>for</b> some x.</font>
   real_1d_array x = <font color=blue><b>&quot;[1,-1]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[]&quot;</b></font>;
   sparsemv(s, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(2).c_str()); <font color=navy>// EXPECTED: [1.000,-5.000]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_sparse_d_crs></a><h6 class=pageheader>sparse_d_crs Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;LinAlg.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates creation/initialization of the sparse matrix in the</font>
<font color=navy>// CRS format.</font>
<font color=navy>//</font>
<font color=navy>// Hash-Table format used by default is very convenient (it allows easy</font>
<font color=navy>// insertion of elements, automatic memory reallocation), but has</font>
<font color=navy>// significant memory and performance overhead. Insertion of one element </font>
<font color=navy>// costs hundreds of CPU cycles, and memory consumption is several times</font>
<font color=navy>// higher than that of CRS.</font>
<font color=navy>//</font>
<font color=navy>// When you work with really large matrices and when you can tell in </font>
<font color=navy>// advance how many elements EXACTLY you need, it can be beneficial to </font>
<font color=navy>// create matrix in the CRS format from the very beginning.</font>
<font color=navy>//</font>
<font color=navy>// If you want to create matrix in the CRS format, you should:</font>
<font color=navy>// * use sparsecreatecrs() function</font>
<font color=navy>// * know row sizes in advance (number of non-zero entries in the each row)</font>
<font color=navy>// * initialize matrix with sparseset() - another function, sparseadd(), is not allowed</font>
<font color=navy>// * initialize elements from left to right, from top to bottom, each</font>
<font color=navy>//   element is initialized only once.</font>
   sparsematrix s;
   integer_1d_array row_sizes = <font color=blue><b>&quot;[2,2,2,1]&quot;</b></font>;
   sparsecreatecrs(4, 4, row_sizes, s);
   sparseset(s, 0, 0, 2.0);
   sparseset(s, 0, 1, 1.0);
   sparseset(s, 1, 1, 4.0);
   sparseset(s, 1, 2, 2.0);
   sparseset(s, 2, 2, 3.0);
   sparseset(s, 2, 3, 1.0);
   sparseset(s, 3, 3, 9.0);
<font color=navy>// Now S is equal to</font>
<font color=navy>//   [ 2 1     ]</font>
<font color=navy>//   [   4 2   ]</font>
<font color=navy>//   [     3 1 ]</font>
<font color=navy>//   [       9 ]</font>
<font color=navy>//</font>
<font color=navy>// We should point that we have initialized S elements from left to right,</font>
<font color=navy>// from top to bottom. CRS representation does NOT allow you to <b>do</b> so in</font>
<font color=navy>// the different order. Try to change order of the sparseset() calls above,</font>
<font color=navy>// and you will see that your program generates exception.</font>
<font color=navy>//</font>
<font color=navy>// We can check it by reading matrix contents with sparseget().</font>
<font color=navy>// However, you should remember that sparseget() is inefficient on</font>
<font color=navy>// CRS matrices (it may have to pass through all elements of the row </font>
<font color=navy>// until it finds element you need).</font>
   <b>double</b> v;
   v = sparseget(s, 0, 0);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 2.0000</font>
   v = sparseget(s, 2, 3);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.0000</font>
<font color=navy>// you may see that you can read zero elements (which are not stored) with sparseget()</font>
   v = sparseget(s, 3, 2);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.0000</font>
<font color=navy>//</font>
<font color=navy>// After successful creation we can use our matrix <b>for</b> linear operations.</font>
<font color=navy>// Lets calculate A*x <b>for</b> some x.</font>
   real_1d_array x = <font color=blue><b>&quot;[1,-1,1,-1]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[]&quot;</b></font>;
   sparsemv(s, x, y);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, y.tostring(2).c_str()); <font color=navy>// EXPECTED: [1.000,-2.000,2.000,-9]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_spdgevd></a><h4 class=pageheader>8.7.13. spdgevd Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_smatrixgevd class=toc>smatrixgevd</a> |
<a href=#sub_smatrixgevdreduce class=toc>smatrixgevdreduce</a>
]</font>
</div>
<a name=sub_smatrixgevd></a><h6 class=pageheader>smatrixgevd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Algorithm for solving the following generalized symmetric positive-definite
eigenproblem:
    A*x = lambda*B*x (1) or
    A*B*x = lambda*x (2) or
    B*A*x = lambda*x (3).
where A is a symmetric matrix, B - symmetric positive-definite matrix.
The problem is solved by reducing it to an ordinary  symmetric  eigenvalue
problem.

Inputs:
    A           -   symmetric matrix which is given by its upper or lower
                    triangular part.
                    Array whose indexes range within [0..N-1, 0..N-1].
    N           -   size of matrices A and B.
    IsUpperA    -   storage format of matrix A.
    B           -   symmetric positive-definite matrix which is given by
                    its upper or lower triangular part.
                    Array whose indexes range within [0..N-1, 0..N-1].
    IsUpperB    -   storage format of matrix B.
    ZNeeded     -   if ZNeeded is equal to:
                     * 0, the eigenvectors are not returned;
                     * 1, the eigenvectors are returned.
    ProblemType -   if ProblemType is equal to:
                     * 1, the following problem is solved: A*x = lambda*B*x;
                     * 2, the following problem is solved: A*B*x = lambda*x;
                     * 3, the following problem is solved: B*A*x = lambda*x.

Outputs:
    D           -   eigenvalues in ascending order.
                    Array whose index ranges within [0..N-1].
    Z           -   if ZNeeded is equal to:
                     * 0, Z hasn't changed;
                     * 1, Z contains eigenvectors.
                    Array whose indexes range within [0..N-1, 0..N-1].
                    The eigenvectors are stored in matrix columns. It should
                    be noted that the eigenvectors in such problems do not
                    form an orthogonal system.

Result:
    True, if the problem was solved successfully.
    False, if the error occurred during the Cholesky decomposition of matrix
    B (the matrix isn't positive-definite) or during the work of the iterative
    algorithm for solving the symmetric eigenproblem.

See also the GeneralizedSymmetricDefiniteEVDReduce subroutine.
ALGLIB: Copyright 01.28.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> smatrixgevd(real_2d_array a, ae_int_t n, <b>bool</b> isuppera, real_2d_array b, <b>bool</b> isupperb, ae_int_t zneeded, ae_int_t problemtype, real_1d_array &amp;d, real_2d_array &amp;z);
</pre>
<a name=sub_smatrixgevdreduce></a><h6 class=pageheader>smatrixgevdreduce Function</h6>
<hr width=600 align=left>
<pre class=narration>
Algorithm for reduction of the following generalized symmetric positive-
definite eigenvalue problem:
    A*x = lambda*B*x (1) or
    A*B*x = lambda*x (2) or
    B*A*x = lambda*x (3)
to the symmetric eigenvalues problem C*y = lambda*y (eigenvalues of this and
the given problems are the same, and the eigenvectors of the given problem
could be obtained by multiplying the obtained eigenvectors by the
transformation matrix x = R*y).

Here A is a symmetric matrix, B - symmetric positive-definite matrix.

Inputs:
    A           -   symmetric matrix which is given by its upper or lower
                    triangular part.
                    Array whose indexes range within [0..N-1, 0..N-1].
    N           -   size of matrices A and B.
    IsUpperA    -   storage format of matrix A.
    B           -   symmetric positive-definite matrix which is given by
                    its upper or lower triangular part.
                    Array whose indexes range within [0..N-1, 0..N-1].
    IsUpperB    -   storage format of matrix B.
    ProblemType -   if ProblemType is equal to:
                     * 1, the following problem is solved: A*x = lambda*B*x;
                     * 2, the following problem is solved: A*B*x = lambda*x;
                     * 3, the following problem is solved: B*A*x = lambda*x.

Outputs:
    A           -   symmetric matrix which is given by its upper or lower
                    triangle depending on IsUpperA. Contains matrix C.
                    Array whose indexes range within [0..N-1, 0..N-1].
    R           -   upper triangular or low triangular transformation matrix
                    which is used to obtain the eigenvectors of a given problem
                    as the product of eigenvectors of C (from the right) and
                    matrix R (from the left). If the matrix is upper
                    triangular, the elements below the main diagonal
                    are equal to 0 (and vice versa). Thus, we can perform
                    the multiplication without taking into account the
                    internal structure (which is an easier though less
                    effective way).
                    Array whose indexes range within [0..N-1, 0..N-1].
    IsUpperR    -   type of matrix R (upper or lower triangular).

Result:
    True, if the problem was reduced successfully.
    False, if the error occurred during the Cholesky decomposition of
        matrix B (the matrix is not positive-definite).
ALGLIB: Copyright 01.28.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> smatrixgevdreduce(real_2d_array &amp;a, ae_int_t n, <b>bool</b> isuppera, real_2d_array b, <b>bool</b> isupperb, ae_int_t problemtype, real_2d_array &amp;r, <b>bool</b> &amp;isupperr);
</pre>
<a name=unit_svd></a><h4 class=pageheader>8.7.14. svd Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_rmatrixsvd class=toc>rmatrixsvd</a>
]</font>
</div>
<a name=sub_rmatrixsvd></a><h6 class=pageheader>rmatrixsvd Function</h6>
<hr width=600 align=left>
<pre class=narration>
Singular value decomposition of a rectangular matrix.

The algorithm calculates the singular value decomposition of a matrix of
size MxN: A = U * S * V^T

The algorithm finds the singular values and, optionally, matrices U and V^T.
The algorithm can find both first min(M,N) columns of matrix U and rows of
matrix V^T (singular vectors), and matrices U and V^T wholly (of sizes MxM
and NxN respectively).

Take into account that the subroutine does not return matrix V but V^T.

Inputs:
    A           -   matrix to be decomposed.
                    Array whose indexes range within [0..M-1, 0..N-1].
    M           -   number of rows in matrix A.
    N           -   number of columns in matrix A.
    UNeeded     -   0, 1 or 2. See the description of the parameter U.
    VTNeeded    -   0, 1 or 2. See the description of the parameter VT.
    AdditionalMemory -
                    If the parameter:
                     * equals 0, the algorithm doesn't use additional
                       memory (lower requirements, lower performance).
                     * equals 1, the algorithm uses additional
                       memory of size min(M,N)*min(M,N) of real numbers.
                       It often speeds up the algorithm.
                     * equals 2, the algorithm uses additional
                       memory of size M*min(M,N) of real numbers.
                       It allows to get a maximum performance.
                    The recommended value of the parameter is 2.

Outputs:
    W           -   contains singular values in descending order.
    U           -   if UNeeded=0, U isn't changed, the left singular vectors
                    are not calculated.
                    if Uneeded=1, U contains left singular vectors (first
                    min(M,N) columns of matrix U). Array whose indexes range
                    within [0..M-1, 0..Min(M,N)-1].
                    if UNeeded=2, U contains matrix U wholly. Array whose
                    indexes range within [0..M-1, 0..M-1].
    VT          -   if VTNeeded=0, VT isn't changed, the right singular vectors
                    are not calculated.
                    if VTNeeded=1, VT contains right singular vectors (first
                    min(M,N) rows of matrix V^T). Array whose indexes range
                    within [0..min(M,N)-1, 0..N-1].
                    if VTNeeded=2, VT contains matrix V^T wholly. Array whose
                    indexes range within [0..N-1, 0..N-1].
ALGLIB: Copyright 2005 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> rmatrixsvd(real_2d_array a, ae_int_t m, ae_int_t n, ae_int_t uneeded, ae_int_t vtneeded, ae_int_t additionalmemory, real_1d_array &amp;w, real_2d_array &amp;u, real_2d_array &amp;vt);
</pre>
<a name=unit_trfac></a><h4 class=pageheader>8.7.15. trfac Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_sparsedecompositionanalysis class=toc>sparsedecompositionanalysis</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_cmatrixlu class=toc>cmatrixlu</a> |
<a href=#sub_hpdmatrixcholesky class=toc>hpdmatrixcholesky</a> |
<a href=#sub_rmatrixlu class=toc>rmatrixlu</a> |
<a href=#sub_sparsecholesky class=toc>sparsecholesky</a> |
<a href=#sub_sparsecholeskyanalyze class=toc>sparsecholeskyanalyze</a> |
<a href=#sub_sparsecholeskyfactorize class=toc>sparsecholeskyfactorize</a> |
<a href=#sub_sparsecholeskyp class=toc>sparsecholeskyp</a> |
<a href=#sub_sparsecholeskyreload class=toc>sparsecholeskyreload</a> |
<a href=#sub_sparsecholeskyskyline class=toc>sparsecholeskyskyline</a> |
<a href=#sub_sparselu class=toc>sparselu</a> |
<a href=#sub_spdmatrixcholesky class=toc>spdmatrixcholesky</a> |
<a href=#sub_spdmatrixcholeskyupdateadd1 class=toc>spdmatrixcholeskyupdateadd1</a> |
<a href=#sub_spdmatrixcholeskyupdateadd1buf class=toc>spdmatrixcholeskyupdateadd1buf</a> |
<a href=#sub_spdmatrixcholeskyupdatefix class=toc>spdmatrixcholeskyupdatefix</a> |
<a href=#sub_spdmatrixcholeskyupdatefixbuf class=toc>spdmatrixcholeskyupdatefixbuf</a>
]</font>
</div>
<a name=struct_sparsedecompositionanalysis></a><h6 class=pageheader>sparsedecompositionanalysis Class</h6>
<hr width=600 align=left>
<pre class=narration>
An analysis of the sparse matrix decomposition, performed prior to  actual
numerical factorization. You should not directly  access  fields  of  this
object - use appropriate ALGLIB functions to work with this object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> sparsedecompositionanalysis {
};
</pre>
<a name=sub_cmatrixlu></a><h6 class=pageheader>cmatrixlu Function</h6>
<hr width=600 align=left>
<pre class=narration>
LU decomposition of a general complex matrix with row pivoting

A is represented as A = P*L*U, where:
* L is lower unitriangular matrix
* U is upper triangular matrix
* P = P0*P1*...*PK, K=min(M,N)-1,
  Pi - permutation matrix for I and Pivots[I]

Inputs:
    A       -   array[0..M-1, 0..N-1].
    M       -   number of rows in matrix A.
    N       -   number of columns in matrix A.

Outputs:
    A       -   matrices L and U in compact form:
                * L is stored under main diagonal
                * U is stored on and above main diagonal
    Pivots  -   permutation matrix in compact form.
                array[0..Min(M-1,N-1)].
ALGLIB Routine: Copyright 10.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixlu(complex_2d_array &amp;a, ae_int_t m, ae_int_t n, integer_1d_array &amp;pivots);
</pre>
<a name=sub_hpdmatrixcholesky></a><h6 class=pageheader>hpdmatrixcholesky Function</h6>
<hr width=600 align=left>
<pre class=narration>
Cache-oblivious Cholesky decomposition

The algorithm computes Cholesky decomposition  of  a  Hermitian  positive-
definite matrix. The result of an algorithm is a representation  of  A  as
A=U'*U  or A=L*L' (here X' denotes conj(X^T)).

Inputs:
    A       -   upper or lower triangle of a factorized matrix.
                array with elements [0..N-1, 0..N-1].
    N       -   size of matrix A.
    IsUpper -   if IsUpper=True, then A contains an upper triangle of
                a symmetric matrix, otherwise A contains a lower one.

Outputs:
    A       -   the result of factorization. If IsUpper=True, then
                the upper triangle contains matrix U, so that A = U'*U,
                and the elements below the main diagonal are not modified.
                Similarly, if IsUpper = False.

Result:
    If  the  matrix  is  positive-definite,  the  function  returns  True.
    Otherwise, the function returns False. Contents of A is not determined
    in such case.
ALGLIB Routine: Copyright 15.12.2009-22.01.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> hpdmatrixcholesky(complex_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper);
</pre>
<a name=sub_rmatrixlu></a><h6 class=pageheader>rmatrixlu Function</h6>
<hr width=600 align=left>
<pre class=narration>
LU decomposition of a general real matrix with row pivoting

A is represented as A = P*L*U, where:
* L is lower unitriangular matrix
* U is upper triangular matrix
* P = P0*P1*...*PK, K=min(M,N)-1,
  Pi - permutation matrix for I and Pivots[I]

Inputs:
    A       -   array[0..M-1, 0..N-1].
    M       -   number of rows in matrix A.
    N       -   number of columns in matrix A.

Outputs:
    A       -   matrices L and U in compact form:
                * L is stored under main diagonal
                * U is stored on and above main diagonal
    Pivots  -   permutation matrix in compact form.
                array[0..Min(M-1,N-1)].
ALGLIB Routine: Copyright 10.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixlu(real_2d_array &amp;a, ae_int_t m, ae_int_t n, integer_1d_array &amp;pivots);
</pre>
<a name=sub_sparsecholesky></a><h6 class=pageheader>sparsecholesky Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse Cholesky decomposition for a matrix  stored  in  any sparse storage,
without rows/cols permutation.

This function is the most convenient (less parameters to specify), although
less efficient, version of sparse Cholesky.

Internally it:
* calls SparseCholeskyAnalyze()  function  to  perform  symbolic  analysis
  phase with no permutation being configured.
* calls SparseCholeskyFactorize() function to perform numerical  phase  of
  the factorization

Following alternatives may result in better performance:
* using SparseCholeskyP(), which selects best  pivoting  available,  which
  almost always results in improved sparsity and cache locality
* using  SparseCholeskyAnalyze() and SparseCholeskyFactorize()   functions
  directly,  which  may  improve  performance of repetitive factorizations
  with same sparsity patterns.

The latter also allows one to perform  LDLT  factorization  of  indefinite
matrix (one with strictly diagonal D, which is known  to  be  stable  only
in few special cases, like quasi-definite matrices).

Inputs:
    A       -   a square NxN sparse matrix, stored in any storage format.
    IsUpper -   if IsUpper=True, then factorization is performed on  upper
                triangle.  Another triangle is ignored on  input,  dropped
                on output. Similarly, if IsUpper=False, the lower triangle
                is processed.

Outputs:
    A       -   the result of factorization, stored in CRS format:
                * if IsUpper=True, then the upper triangle contains matrix
                  U such  that  A = U^T*U and the lower triangle is empty.
                * similarly, if IsUpper=False, then lower triangular L  is
                  returned and we have A = L*(L^T).
                Note that THIS function does not  perform  permutation  of
                the rows to reduce fill-in.

Result:
    If  the  matrix  is  positive-definite,  the  function  returns  True.
    Otherwise, the function returns False.  Contents  of  A  is  undefined
    in such case.

NOTE: for  performance  reasons  this  function  does NOT check that input
      matrix  includes  only  finite  values. It is your responsibility to
      make sure that there are no infinite or NAN values in the matrix.
ALGLIB Routine: Copyright 16.09.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparsecholesky(sparsematrix a, <b>bool</b> isupper);
</pre>
<a name=sub_sparsecholeskyanalyze></a><h6 class=pageheader>sparsecholeskyanalyze Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse Cholesky/LDLT decomposition: symbolic analysis phase.

This function is a part of the 'expert' sparse Cholesky API:
* SparseCholeskyAnalyze(), that performs symbolic analysis phase and loads
  matrix to be factorized into internal storage
* SparseCholeskySetModType(), that allows to  use  modified  Cholesky/LDLT
  with lower bounds on pivot magnitudes and additional overflow safeguards
* SparseCholeskyFactorize(),  that performs  numeric  factorization  using
  precomputed symbolic analysis and internally stored matrix - and outputs
  result
* SparseCholeskyReload(), that reloads one more matrix with same  sparsity
  pattern into internal storage so  one  may  reuse  previously  allocated
  temporaries and previously performed symbolic analysis

This specific function performs preliminary analysis of the  Cholesky/LDLT
factorization. It allows to choose  different  permutation  types  and  to
choose between classic Cholesky and  indefinite  LDLT  factorization  (the
latter is computed with strictly diagonal D, i.e.  without  Bunch-Kauffman
pivoting).

NOTE: L*D*LT family of factorization may be used to  factorize  indefinite
      matrices. However, numerical stability is guaranteed ONLY for a class
      of quasi-definite matrices.

NOTE: all internal processing is performed with lower triangular  matrices
      stored  in  CRS  format.  Any  other  storage  formats  and/or upper
      triangular storage means  that  one  format  conversion  and/or  one
      transposition will be performed  internally  for  the  analysis  and
      factorization phases. Thus, highest  performance  is  achieved  when
      input is a lower triangular CRS matrix.

Inputs:
    A           -   sparse square matrix in any sparse storage format.
    IsUpper     -   whether upper or lower  triangle  is  decomposed  (the
                    other one is ignored).
    FactType    -   factorization type:
                    * 0 for traditional Cholesky of SPD matrix
                    * 1 for LDLT decomposition with strictly  diagonal  D,
                        which may have non-positive entries.
    PermType    -   permutation type:
                    *-1 for absence of permutation
                    * 0 for best fill-in reducing permutation available
                    * 1 for supernodal ordering (improves locality and
                      performance, does NOT change fill-in factor)
                    * 2 for AMD (approximate minimum degree) ordering

Outputs:
    Analysis    -   contains:
                    * symbolic analysis of the matrix structure which will
                      be used later to guide numerical factorization.
                    * specific numeric values loaded into internal  memory
                      waiting for the factorization to be performed

This function fails if and only if the matrix A is symbolically degenerate
i.e. has diagonal element which is exactly zero. In  such  case  False  is
returned, contents of Analysis object is undefined.
ALGLIB Routine: Copyright 20.09.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparsecholeskyanalyze(sparsematrix a, <b>bool</b> isupper, ae_int_t facttype, ae_int_t permtype, sparsedecompositionanalysis &amp;analysis);
</pre>
<a name=sub_sparsecholeskyfactorize></a><h6 class=pageheader>sparsecholeskyfactorize Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse Cholesky decomposition: numerical analysis phase.

This function is a part of the 'expert' sparse Cholesky API:
* SparseCholeskyAnalyze(), that performs symbolic analysis phase and loads
  matrix to be factorized into internal storage
* SparseCholeskySetModType(), that allows to  use  modified  Cholesky/LDLT
  with lower bounds on pivot magnitudes and additional overflow safeguards
* SparseCholeskyFactorize(),  that performs  numeric  factorization  using
  precomputed symbolic analysis and internally stored matrix - and outputs
  result
* SparseCholeskyReload(), that reloads one more matrix with same  sparsity
  pattern into internal storage so  one  may  reuse  previously  allocated
  temporaries and previously performed symbolic analysis

Depending on settings specified during SparseCholeskyAnalyze() call it may
produce classic Cholesky or L*D*LT  decomposition  (with strictly diagonal
D), without permutation or with performance-enhancing permutation P.

NOTE: all internal processing is performed with lower triangular  matrices
      stored  in  CRS  format.  Any  other  storage  formats  and/or upper
      triangular storage means  that  one  format  conversion  and/or  one
      transposition will be performed  internally  for  the  analysis  and
      factorization phases. Thus, highest  performance  is  achieved  when
      input is a lower triangular CRS matrix, and lower triangular  output
      is requested.

NOTE: L*D*LT family of factorization may be used to  factorize  indefinite
      matrices. However, numerical stability is guaranteed ONLY for a class
      of quasi-definite matrices.

Inputs:
    Analysis    -   prior analysis with internally stored matrix which will
                    be factorized
    NeedUpper   -   whether upper triangular or lower triangular output is
                    needed

Outputs:
    A           -   Cholesky decomposition of A stored in lower triangular
                    CRS format, i.e. A=L*L' (or upper triangular CRS, with
                    A=U'*U, depending on NeedUpper parameter).
    D           -   array[N], diagonal factor. If no diagonal  factor  was
                    required during analysis  phase,  still  returned  but
                    filled with 1's
    P           -   array[N], pivots. Permutation matrix P is a product of
                    P(0)*P(1)*...*P(N-1), where P(i) is a  permutation  of
                    row/col I and P[I] (with P[I] &ge; I).
                    If no permutation was requested during analysis phase,
                    still returned but filled with identity permutation.

The function returns True  when  factorization  resulted  in nondegenerate
matrix. False is returned when factorization fails (Cholesky factorization
of indefinite matrix) or LDLT factorization has exactly zero  elements  at
the diagonal. In the latter case contents of A, D and P is undefined.

The analysis object is not changed during  the  factorization.  Subsequent
calls to SparseCholeskyFactorize() will result in same factorization being
performed one more time.
ALGLIB Routine: Copyright 20.09.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparsecholeskyfactorize(sparsedecompositionanalysis analysis, <b>bool</b> needupper, sparsematrix &amp;a, real_1d_array &amp;d, integer_1d_array &amp;p);
</pre>
<a name=sub_sparsecholeskyp></a><h6 class=pageheader>sparsecholeskyp Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse Cholesky decomposition for a matrix  stored  in  any sparse storage
format, with performance-enhancing permutation of rows/cols.

Present version is configured  to  perform  supernodal  permutation  which
sparsity reducing ordering.

This function is a wrapper around generic sparse  decomposition  functions
that internally:
* calls SparseCholeskyAnalyze()  function  to  perform  symbolic  analysis
  phase with best available permutation being configured.
* calls SparseCholeskyFactorize() function to perform numerical  phase  of
  the factorization.

NOTE: using  SparseCholeskyAnalyze() and SparseCholeskyFactorize() directly
      may improve  performance  of  repetitive  factorizations  with  same
      sparsity patterns. It also allows one to perform  LDLT factorization
      of  indefinite  matrix  -  a factorization with strictly diagonal D,
      which  is  known to be stable only in few special cases, like quasi-
      definite matrices.

Inputs:
    A       -   a square NxN sparse matrix, stored in any storage format.
    IsUpper -   if IsUpper=True, then factorization is performed on  upper
                triangle.  Another triangle is ignored on  input,  dropped
                on output. Similarly, if IsUpper=False, the lower triangle
                is processed.

Outputs:
    A       -   the result of factorization, stored in CRS format:
                * if IsUpper=True, then the upper triangle contains matrix
                  U such  that  A = U^T*U and the lower triangle is empty.
                * similarly, if IsUpper=False, then lower triangular L  is
                  returned and we have A = L*(L^T).
    P       -   a row/column permutation, a product of P0*P1*...*Pk, k=N-1,
                with Pi being permutation of rows/cols I and P[I]

Result:
    If  the  matrix  is  positive-definite,  the  function  returns  True.
    Otherwise, the function returns False.  Contents  of  A  is  undefined
    in such case.

NOTE: for  performance  reasons  this  function  does NOT check that input
      matrix  includes  only  finite  values. It is your responsibility to
      make sure that there are no infinite or NAN values in the matrix.
ALGLIB Routine: Copyright 16.09.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparsecholeskyp(sparsematrix a, <b>bool</b> isupper, integer_1d_array &amp;p);
</pre>
<a name=sub_sparsecholeskyreload></a><h6 class=pageheader>sparsecholeskyreload Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse  Cholesky  decomposition:  update  internally  stored  matrix  with
another one with exactly same sparsity pattern.

This function is a part of the 'expert' sparse Cholesky API:
* SparseCholeskyAnalyze(), that performs symbolic analysis phase and loads
  matrix to be factorized into internal storage
* SparseCholeskySetModType(), that allows to  use  modified  Cholesky/LDLT
  with lower bounds on pivot magnitudes and additional overflow safeguards
* SparseCholeskyFactorize(),  that performs  numeric  factorization  using
  precomputed symbolic analysis and internally stored matrix - and outputs
  result
* SparseCholeskyReload(), that reloads one more matrix with same  sparsity
  pattern into internal storage so  one  may  reuse  previously  allocated
  temporaries and previously performed symbolic analysis

This specific function replaces internally stored  numerical  values  with
ones from another sparse matrix (but having exactly same sparsity  pattern
as one that was used for initial SparseCholeskyAnalyze() call).

NOTE: all internal processing is performed with lower triangular  matrices
      stored  in  CRS  format.  Any  other  storage  formats  and/or upper
      triangular storage means  that  one  format  conversion  and/or  one
      transposition will be performed  internally  for  the  analysis  and
      factorization phases. Thus, highest  performance  is  achieved  when
      input is a lower triangular CRS matrix.

Inputs:
    Analysis    -   analysis object
    A           -   sparse square matrix in any sparse storage format.  It
                    MUST have exactly same sparsity pattern as that of the
                    matrix that was passed to SparseCholeskyAnalyze().
                    Any difference (missing elements or additional elements)
                    may result in unpredictable and undefined  behavior  -
                    an algorithm may fail due to memory access violation.
    IsUpper     -   whether upper or lower  triangle  is  decomposed  (the
                    other one is ignored).

Outputs:
    Analysis    -   contains:
                    * symbolic analysis of the matrix structure which will
                      be used later to guide numerical factorization.
                    * specific numeric values loaded into internal  memory
                      waiting for the factorization to be performed
ALGLIB Routine: Copyright 20.09.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsecholeskyreload(sparsedecompositionanalysis analysis, sparsematrix a, <b>bool</b> isupper);
</pre>
<a name=sub_sparsecholeskyskyline></a><h6 class=pageheader>sparsecholeskyskyline Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse Cholesky decomposition for skyline matrix using in-place algorithm
without allocating additional storage.

The algorithm computes Cholesky decomposition  of  a  symmetric  positive-
definite sparse matrix. The result of an algorithm is a representation  of
A as A=U^T*U or A=L*L^T

This function allows to perform very efficient decomposition of low-profile
matrices (average bandwidth is ~5-10 elements). For larger matrices it  is
recommended to use supernodal Cholesky decomposition: SparseCholeskyP() or
SparseCholeskyAnalyze()/SparseCholeskyFactorize().

Inputs:
    A       -   sparse matrix in skyline storage (SKS) format.
    N       -   size of matrix A (can be smaller than actual size of A)
    IsUpper -   if IsUpper=True, then factorization is performed on  upper
                triangle. Another triangle is ignored (it may contant some
                data, but it is not changed).

Outputs:
    A       -   the result of factorization, stored in SKS. If IsUpper=True,
                then the upper  triangle  contains  matrix  U,  such  that
                A = U^T*U. Lower triangle is not changed.
                Similarly, if IsUpper = False. In this case L is returned,
                and we have A = L*(L^T).
                Note that THIS function does not  perform  permutation  of
                rows to reduce bandwidth.

Result:
    If  the  matrix  is  positive-definite,  the  function  returns  True.
    Otherwise, the function returns False. Contents of A is not determined
    in such case.

NOTE: for  performance  reasons  this  function  does NOT check that input
      matrix  includes  only  finite  values. It is your responsibility to
      make sure that there are no infinite or NAN values in the matrix.
ALGLIB Routine: Copyright 16.01.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparsecholeskyskyline(sparsematrix a, ae_int_t n, <b>bool</b> isupper);
</pre>
<a name=sub_sparselu></a><h6 class=pageheader>sparselu Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse LU decomposition with column pivoting for sparsity and row pivoting
for stability. Input must be square sparse matrix stored in CRS format.

The algorithm  computes  LU  decomposition  of  a  general  square  matrix
(rectangular ones are not supported). The result  of  an  algorithm  is  a
representation of A as A = P*L*U*Q, where:
* L is lower unitriangular matrix
* U is upper triangular matrix
* P = P0*P1*...*PK, K=N-1, Pi - permutation matrix for I and P[I]
* Q = QK*...*Q1*Q0, K=N-1, Qi - permutation matrix for I and Q[I]

This function pivots columns for higher sparsity, and then pivots rows for
stability (larger element at the diagonal).

Inputs:
    A       -   sparse NxN matrix in CRS format. An exception is generated
                if matrix is non-CRS or non-square.
    PivotType-  pivoting strategy:
                * 0 for best pivoting available (2 in current version)
                * 1 for row-only pivoting (NOT RECOMMENDED)
                * 2 for complete pivoting which produces most sparse outputs

Outputs:
    A       -   the result of factorization, matrices L and U stored in
                compact form using CRS sparse storage format:
                * lower unitriangular L is stored strictly under main diagonal
                * upper triangilar U is stored ON and ABOVE main diagonal
    P       -   row permutation matrix in compact form, array[N]
    Q       -   col permutation matrix in compact form, array[N]

This function always succeeds, i.e. it ALWAYS returns valid factorization,
but for your convenience it also returns  boolean  value  which  helps  to
detect symbolically degenerate matrices:
* function returns TRUE, if the matrix was factorized AND symbolically
  non-degenerate
* function returns FALSE, if the matrix was factorized but U has strictly
  zero elements at the diagonal (the factorization is returned anyway).
ALGLIB Routine: Copyright 03.09.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> sparselu(sparsematrix a, ae_int_t pivottype, integer_1d_array &amp;p, integer_1d_array &amp;q);
</pre>
<a name=sub_spdmatrixcholesky></a><h6 class=pageheader>spdmatrixcholesky Function</h6>
<hr width=600 align=left>
<pre class=narration>
Cache-oblivious Cholesky decomposition

The algorithm computes Cholesky decomposition  of  a  symmetric  positive-
definite matrix. The result of an algorithm is a representation  of  A  as
A=U^T*U  or A=L*L^T

Inputs:
    A       -   upper or lower triangle of a factorized matrix.
                array with elements [0..N-1, 0..N-1].
    N       -   size of matrix A.
    IsUpper -   if IsUpper=True, then A contains an upper triangle of
                a symmetric matrix, otherwise A contains a lower one.

Outputs:
    A       -   the result of factorization. If IsUpper=True, then
                the upper triangle contains matrix U, so that A = U^T*U,
                and the elements below the main diagonal are not modified.
                Similarly, if IsUpper = False.

Result:
    If  the  matrix  is  positive-definite,  the  function  returns  True.
    Otherwise, the function returns False. Contents of A is not determined
    in such case.
ALGLIB Routine: Copyright 15.12.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>bool</b> spdmatrixcholesky(real_2d_array &amp;a, ae_int_t n, <b>bool</b> isupper);
</pre>
<a name=sub_spdmatrixcholeskyupdateadd1></a><h6 class=pageheader>spdmatrixcholeskyupdateadd1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Update of Cholesky decomposition: rank-1 update to original A.  &quot;Buffered&quot;
version which uses preallocated buffer which is saved  between  subsequent
function calls.

This function uses internally allocated buffer which is not saved  between
subsequent  calls.  So,  if  you  perform  a lot  of  subsequent  updates,
we  recommend   you   to   use   &quot;buffered&quot;   version   of  this function:
SPDMatrixCholeskyUpdateAdd1Buf().

Inputs:
    A       -   upper or lower Cholesky factor.
                array with elements [0..N-1, 0..N-1].
                Exception is thrown if array size is too small.
    N       -   size of matrix A, N &gt; 0
    IsUpper -   if IsUpper=True, then A contains  upper  Cholesky  factor;
                otherwise A contains a lower one.
    U       -   array[N], rank-1 update to A: A_mod = A + u*u'
                Exception is thrown if array size is too small.
    BufR    -   possibly preallocated  buffer;  automatically  resized  if
                needed. It is recommended to  reuse  this  buffer  if  you
                perform a lot of subsequent decompositions.

Outputs:
    A       -   updated factorization.  If  IsUpper=True,  then  the  upper
                triangle contains matrix U, and the elements below the main
                diagonal are not modified. Similarly, if IsUpper = False.

NOTE: this function always succeeds, so it does not return completion code

NOTE: this function checks sizes of input arrays, but it does  NOT  checks
      for presence of infinities or NAN's.
ALGLIB: Copyright 03.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixcholeskyupdateadd1(real_2d_array a, ae_int_t n, <b>bool</b> isupper, real_1d_array u);
</pre>
<a name=sub_spdmatrixcholeskyupdateadd1buf></a><h6 class=pageheader>spdmatrixcholeskyupdateadd1buf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Update of Cholesky decomposition: rank-1 update to original A.  &quot;Buffered&quot;
version which uses preallocated buffer which is saved  between  subsequent
function calls.

See comments for SPDMatrixCholeskyUpdateAdd1() for more information.

Inputs:
    A       -   upper or lower Cholesky factor.
                array with elements [0..N-1, 0..N-1].
                Exception is thrown if array size is too small.
    N       -   size of matrix A, N &gt; 0
    IsUpper -   if IsUpper=True, then A contains  upper  Cholesky  factor;
                otherwise A contains a lower one.
    U       -   array[N], rank-1 update to A: A_mod = A + u*u'
                Exception is thrown if array size is too small.
    BufR    -   possibly preallocated  buffer;  automatically  resized  if
                needed. It is recommended to  reuse  this  buffer  if  you
                perform a lot of subsequent decompositions.

Outputs:
    A       -   updated factorization.  If  IsUpper=True,  then  the  upper
                triangle contains matrix U, and the elements below the main
                diagonal are not modified. Similarly, if IsUpper = False.
ALGLIB: Copyright 03.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixcholeskyupdateadd1buf(real_2d_array a, ae_int_t n, <b>bool</b> isupper, real_1d_array u, real_1d_array &amp;bufr);
</pre>
<a name=sub_spdmatrixcholeskyupdatefix></a><h6 class=pageheader>spdmatrixcholeskyupdatefix Function</h6>
<hr width=600 align=left>
<pre class=narration>
Update of Cholesky decomposition: &quot;fixing&quot; some variables.

This function uses internally allocated buffer which is not saved  between
subsequent  calls.  So,  if  you  perform  a lot  of  subsequent  updates,
we  recommend   you   to   use   &quot;buffered&quot;   version   of  this function:
SPDMatrixCholeskyUpdateFixBuf().

&quot;FIXING&quot; EXPLAINED:

    Suppose we have N*N positive definite matrix A. &quot;Fixing&quot; some variable
    means filling corresponding row/column of  A  by  zeros,  and  setting
    diagonal element to 1.

    For example, if we fix 2nd variable in 4*4 matrix A, it becomes Af:

        ( A00  A01  A02  A03 )      ( Af00  0   Af02 Af03 )
        ( A10  A11  A12  A13 )      (  0    1    0    0   )
        ( A20  A21  A22  A23 )  &rArr;  ( Af20  0   Af22 Af23 )
        ( A30  A31  A32  A33 )      ( Af30  0   Af32 Af33 )

    If we have Cholesky decomposition of A, it must be recalculated  after
    variables were  fixed.  However,  it  is  possible  to  use  efficient
    algorithm, which needs O(K*N^2)  time  to  &quot;fix&quot;  K  variables,  given
    Cholesky decomposition of original, &quot;unfixed&quot; A.

Inputs:
    A       -   upper or lower Cholesky factor.
                array with elements [0..N-1, 0..N-1].
                Exception is thrown if array size is too small.
    N       -   size of matrix A, N &gt; 0
    IsUpper -   if IsUpper=True, then A contains  upper  Cholesky  factor;
                otherwise A contains a lower one.
    Fix     -   array[N], I-th element is True if I-th  variable  must  be
                fixed. Exception is thrown if array size is too small.
    BufR    -   possibly preallocated  buffer;  automatically  resized  if
                needed. It is recommended to  reuse  this  buffer  if  you
                perform a lot of subsequent decompositions.

Outputs:
    A       -   updated factorization.  If  IsUpper=True,  then  the  upper
                triangle contains matrix U, and the elements below the main
                diagonal are not modified. Similarly, if IsUpper = False.

NOTE: this function always succeeds, so it does not return completion code

NOTE: this function checks sizes of input arrays, but it does  NOT  checks
      for presence of infinities or NAN's.

NOTE: this  function  is  efficient  only  for  moderate amount of updated
      variables - say, 0.1*N or 0.3*N. For larger amount of  variables  it
      will  still  work,  but  you  may  get   better   performance   with
      straightforward Cholesky.
ALGLIB: Copyright 03.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixcholeskyupdatefix(real_2d_array a, ae_int_t n, <b>bool</b> isupper, boolean_1d_array fix);
</pre>
<a name=sub_spdmatrixcholeskyupdatefixbuf></a><h6 class=pageheader>spdmatrixcholeskyupdatefixbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Update of Cholesky  decomposition:  &quot;fixing&quot;  some  variables.  &quot;Buffered&quot;
version which uses preallocated buffer which is saved  between  subsequent
function calls.

See comments for SPDMatrixCholeskyUpdateFix() for more information.

Inputs:
    A       -   upper or lower Cholesky factor.
                array with elements [0..N-1, 0..N-1].
                Exception is thrown if array size is too small.
    N       -   size of matrix A, N &gt; 0
    IsUpper -   if IsUpper=True, then A contains  upper  Cholesky  factor;
                otherwise A contains a lower one.
    Fix     -   array[N], I-th element is True if I-th  variable  must  be
                fixed. Exception is thrown if array size is too small.
    BufR    -   possibly preallocated  buffer;  automatically  resized  if
                needed. It is recommended to  reuse  this  buffer  if  you
                perform a lot of subsequent decompositions.

Outputs:
    A       -   updated factorization.  If  IsUpper=True,  then  the  upper
                triangle contains matrix U, and the elements below the main
                diagonal are not modified. Similarly, if IsUpper = False.
ALGLIB: Copyright 03.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixcholeskyupdatefixbuf(real_2d_array a, ae_int_t n, <b>bool</b> isupper, boolean_1d_array fix, real_1d_array &amp;bufr);
</pre>
</p>
<p>
<a name=pck_Optimization class=sheader></a><h3>8.8. Optimization Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_minbc class=toc>minbc</a></td><td>Box constrained optimizer with fast activation of multiple constraints per step</td></tr>
<tr align=left valign=top><td><a href=#unit_minbleic class=toc>minbleic</a></td><td>Bound constrained optimizer with additional linear equality/inequality constraints</td></tr>
<tr align=left valign=top><td><a href=#unit_mincg class=toc>mincg</a></td><td>Conjugate gradient optimizer</td></tr>
<tr align=left valign=top><td><a href=#unit_mincomp class=toc>mincomp</a></td><td>Backward compatibility functions</td></tr>
<tr align=left valign=top><td><a href=#unit_minlbfgs class=toc>minlbfgs</a></td><td>Limited memory BFGS optimizer</td></tr>
<tr align=left valign=top><td><a href=#unit_minlm class=toc>minlm</a></td><td>Improved Levenberg-Marquardt optimizer</td></tr>
<tr align=left valign=top><td><a href=#unit_minlp class=toc>minlp</a></td><td>Linear programming suite</td></tr>
<tr align=left valign=top><td><a href=#unit_minnlc class=toc>minnlc</a></td><td>Nonlinearly constrained optimizer</td></tr>
<tr align=left valign=top><td><a href=#unit_minns class=toc>minns</a></td><td>Nonsmooth constrained optimizer</td></tr>
<tr align=left valign=top><td><a href=#unit_minqp class=toc>minqp</a></td><td>Quadratic programming with bound and linear equality/inequality constraints</td></tr>
<tr align=left valign=top><td><a href=#unit_optguardapi class=toc>optguardapi</a></td><td>OptGuard integrity checking for nonlinear models</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_minbc></a><h4 class=pageheader>8.8.1. minbc Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_minbcreport class=toc>minbcreport</a> |
<a href=#struct_minbcstate class=toc>minbcstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_minbccreate class=toc>minbccreate</a> |
<a href=#sub_minbccreatef class=toc>minbccreatef</a> |
<a href=#sub_minbcoptguardgradient class=toc>minbcoptguardgradient</a> |
<a href=#sub_minbcoptguardnonc1test0results class=toc>minbcoptguardnonc1test0results</a> |
<a href=#sub_minbcoptguardnonc1test1results class=toc>minbcoptguardnonc1test1results</a> |
<a href=#sub_minbcoptguardresults class=toc>minbcoptguardresults</a> |
<a href=#sub_minbcoptguardsmoothness class=toc>minbcoptguardsmoothness</a> |
<a href=#sub_minbcoptimize class=toc>minbcoptimize</a> |
<a href=#sub_minbcrequesttermination class=toc>minbcrequesttermination</a> |
<a href=#sub_minbcrestartfrom class=toc>minbcrestartfrom</a> |
<a href=#sub_minbcresults class=toc>minbcresults</a> |
<a href=#sub_minbcresultsbuf class=toc>minbcresultsbuf</a> |
<a href=#sub_minbcsetbc class=toc>minbcsetbc</a> |
<a href=#sub_minbcsetcond class=toc>minbcsetcond</a> |
<a href=#sub_minbcsetprecdefault class=toc>minbcsetprecdefault</a> |
<a href=#sub_minbcsetprecdiag class=toc>minbcsetprecdiag</a> |
<a href=#sub_minbcsetprecscale class=toc>minbcsetprecscale</a> |
<a href=#sub_minbcsetscale class=toc>minbcsetscale</a> |
<a href=#sub_minbcsetstpmax class=toc>minbcsetstpmax</a> |
<a href=#sub_minbcsetxrep class=toc>minbcsetxrep</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_minbc_d_1 class=toc>minbc_d_1</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization with box constraints</td></tr>
<tr align=left valign=top><td><a href=#example_minbc_numdiff class=toc>minbc_numdiff</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization with bound constraints and numerical differentiation</td></tr>
</table>
</div>
<a name=struct_minbcreport></a><h6 class=pageheader>minbcreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure stores optimization report:
* iterationscount           number of iterations
* nfev                      number of gradient evaluations
* terminationtype           termination type (see below)

TERMINATION CODES

terminationtype field contains completion code, which can be:
  -8    internal integrity control detected  infinite  or  NAN  values  in
        function/gradient. Abnormal termination signalled.
  -3    inconsistent constraints.
   1    relative function improvement is no more than EpsF.
   2    relative step is no more than EpsX.
   4    gradient norm is no more than EpsG
   5    MaxIts steps was taken
   7    stopping conditions are too stringent,
        further improvement is impossible,
        X contains best point found so far.
   8    terminated by user who called minbcrequesttermination(). X contains
        point which was &quot;current accepted&quot; when  termination  request  was
        submitted.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minbcreport {
   ae_int_t iterationscount;
   ae_int_t nfev;
   ae_int_t varidx;
   ae_int_t terminationtype;
};
</pre>
<a name=struct_minbcstate></a><h6 class=pageheader>minbcstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores nonlinear optimizer state.
You should use functions provided by MinBC subpackage to work with this
object
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minbcstate {
   bool needf;
   bool needfg;
   bool xupdated;
   double f;
   real_1d_array g;
   real_1d_array x;
};
</pre>
<a name=sub_minbccreate></a><h6 class=pageheader>minbccreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
BOX CONSTRAINED OPTIMIZATION
WITH FAST ACTIVATION OF MULTIPLE BOX CONSTRAINTS
The  subroutine  minimizes  function   F(x) of N arguments subject  to box
constraints (with some of box constraints actually being equality ones).

This optimizer uses algorithm similar to that of MinBLEIC (optimizer  with
general linear constraints), but presence of box-only  constraints  allows
us to use faster constraint activation strategies. On large-scale problems,
with multiple constraints active at the solution, this  optimizer  can  be
several times faster than BLEIC.

REQUIREMENTS:
* user must provide function value and gradient
* starting point X0 must be feasible or
  not too far away from the feasible set
* grad(f) must be Lipschitz continuous on a level set:
  L = { x : f(x) &le; f(x0) }
* function must be defined everywhere on the feasible set F

USAGE:

Constrained optimization if far more complex than the unconstrained one.
Here we give very brief outline of the BC optimizer. We strongly recommend
you to read examples in the ALGLIB Reference Manual and to read ALGLIB User Guide
on optimization, which is available at http://www.alglib.net/optimization/

1. User initializes algorithm state with MinBCCreate() call

2. USer adds box constraints by calling MinBCSetBC() function.

3. User sets stopping conditions with MinBCSetCond().

4. User calls MinBCOptimize() function which takes algorithm  state and
   pointer (delegate, etc.) to callback function which calculates F/G.

5. User calls MinBCResults() to get solution

6. Optionally user may call MinBCRestartFrom() to solve another problem
   with same N but another starting point.
   MinBCRestartFrom() allows to reuse already initialized structure.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size ofX
    X       -   starting point, array[N]:
                * it is better to set X to a feasible point
                * but X can be infeasible, in which case algorithm will try
                  to find feasible point first, using X as initial
                  approximation.

Outputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbccreate(ae_int_t n, real_1d_array x, minbcstate &amp;state);
<b>void</b> minbccreate(real_1d_array x, minbcstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbc_d_1 class=nav>minbc_d_1</a> ]</p>
<a name=sub_minbccreatef></a><h6 class=pageheader>minbccreatef Function</h6>
<hr width=600 align=left>
<pre class=narration>
The subroutine is finite difference variant of MinBCCreate().  It  uses
finite differences in order to differentiate target function.

Description below contains information which is specific to  this function
only. We recommend to read comments on MinBCCreate() in  order  to  get
more information about creation of BC optimizer.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    X       -   starting point, array[0..N-1].
    DiffStep-   differentiation step, &gt; 0

Outputs:
    State   -   structure which stores algorithm state

NOTES:
1. algorithm uses 4-point central formula for differentiation.
2. differentiation step along I-th axis is equal to DiffStep*S[I] where
   S[] is scaling vector which can be set by MinBCSetScale() call.
3. we recommend you to use moderate values of  differentiation  step.  Too
   large step will result in too large truncation  errors, while too small
   step will result in too large numerical  errors.  1.0E-6  can  be  good
   value to start with.
4. Numerical  differentiation  is   very   inefficient  -   one   gradient
   calculation needs 4*N function evaluations. This function will work for
   any N - either small (1...10), moderate (10...100) or  large  (100...).
   However, performance penalty will be too severe for any N's except  for
   small ones.
   We should also say that code which relies on numerical  differentiation
   is  less  robust and precise. CG needs exact gradient values. Imprecise
   gradient may slow  down  convergence, especially  on  highly  nonlinear
   problems.
   Thus  we  recommend to use this function for fast prototyping on small-
   dimensional problems only, and to implement analytical gradient as soon
   as possible.
ALGLIB: Copyright 16.05.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbccreatef(ae_int_t n, real_1d_array x, <b>double</b> diffstep, minbcstate &amp;state);
<b>void</b> minbccreatef(real_1d_array x, <b>double</b> diffstep, minbcstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbc_numdiff class=nav>minbc_numdiff</a> ]</p>
<a name=sub_minbcoptguardgradient></a><h6 class=pageheader>minbcoptguardgradient Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates verification  of  the  user-supplied
analytic gradient.

Upon  activation  of  this  option  OptGuard  integrity  checker  performs
numerical differentiation of your target function  at  the  initial  point
(note: future versions may also perform check  at  the  final  point)  and
compares numerical gradient with analytic one provided by you.

If difference is too large, an error flag is set and optimization  session
continues. After optimization session is over, you can retrieve the report
which  stores  both  gradients  and  specific  components  highlighted  as
suspicious by the OptGuard.

The primary OptGuard report can be retrieved with minbcoptguardresults().

IMPORTANT: gradient check is a high-overhead option which  will  cost  you
           about 3*N additional function evaluations. In many cases it may
           cost as much as the rest of the optimization session.

           YOU SHOULD NOT USE IT IN THE PRODUCTION CODE UNLESS YOU WANT TO
           CHECK DERIVATIVES PROVIDED BY SOME THIRD PARTY.

NOTE: unlike previous incarnation of the gradient checking code,  OptGuard
      does NOT interrupt optimization even if it discovers bad gradient.

Inputs:
    State       -   structure used to store algorithm state
    TestStep    -   verification step used for numerical differentiation:
                    * TestStep=0 turns verification off
                    * TestStep &gt; 0 activates verification
                    You should carefully choose TestStep. Value  which  is
                    too large (so large that  function  behavior  is  non-
                    cubic at this scale) will lead  to  false  alarms. Too
                    short step will result in rounding  errors  dominating
                    numerical derivative.

                    You may use different step for different parameters by
                    means of setting scale with minbcsetscale().

==== EXPLANATION ====

In order to verify gradient algorithm performs following steps:
  * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    where X[i] is i-th component of the initial point and S[i] is a  scale
    of i-th parameter
  * F(X) is evaluated at these trial points
  * we perform one more evaluation in the middle point of the interval
  * we  build  cubic  model using function values and derivatives at trial
    points and we compare its prediction with actual value in  the  middle
    point
ALGLIB: Copyright 15.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcoptguardgradient(minbcstate state, <b>double</b> teststep);
</pre>
<a name=sub_minbcoptguardnonc1test0results></a><h6 class=pageheader>minbcoptguardnonc1test0results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #0

Nonsmoothness (non-C1) test #0 studies  function  values  (not  gradient!)
obtained during line searches and monitors  behavior  of  the  directional
derivative estimate.

This test is less powerful than test #1, but it does  not  depend  on  the
gradient values and thus it is more robust against artifacts introduced by
numerical differentiation.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], f[] - arrays of length CNT which store step lengths and  function
  values at these points; f[i] is evaluated in x0+stp[i]*d.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #0 &quot;strong&quot; report
    LngRep  -   C1 test #0 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcoptguardnonc1test0results(minbcstate state, optguardnonc1test0report &amp;strrep, optguardnonc1test0report &amp;lngrep);
</pre>
<a name=sub_minbcoptguardnonc1test1results></a><h6 class=pageheader>minbcoptguardnonc1test1results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #1

Nonsmoothness (non-C1)  test  #1  studies  individual  components  of  the
gradient computed during line search.

When precise analytic gradient is provided this test is more powerful than
test #0  which  works  with  function  values  and  ignores  user-provided
gradient.  However,  test  #0  becomes  more   powerful   when   numerical
differentiation is employed (in such cases test #1 detects  higher  levels
of numerical noise and becomes too conservative).

This test also tells specific components of the gradient which violate  C1
continuity, which makes it more informative than #0, which just tells that
continuity is violated.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* vidx - is an index of the variable in [0,N) with nonsmooth derivative
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], g[] - arrays of length CNT which store step lengths and  gradient
  values at these points; g[i] is evaluated in  x0+stp[i]*d  and  contains
  vidx-th component of the gradient.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #1 &quot;strong&quot; report
    LngRep  -   C1 test #1 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcoptguardnonc1test1results(minbcstate state, optguardnonc1test1report &amp;strrep, optguardnonc1test1report &amp;lngrep);
</pre>
<a name=sub_minbcoptguardresults></a><h6 class=pageheader>minbcoptguardresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Results of OptGuard integrity check, should be called  after  optimization
session is over.

==== PRIMARY REPORT ====

OptGuard performs several checks which are intended to catch common errors
in the implementation of nonlinear function/gradient:
* incorrect analytic gradient
* discontinuous (non-C0) target functions (constraints)
* nonsmooth     (non-C1) target functions (constraints)

Each of these checks is activated with appropriate function:
* minbcoptguardgradient() for gradient verification
* minbcoptguardsmoothness() for C0/C1 checks

Following flags are set when these errors are suspected:
* rep.badgradsuspected, and additionally:
  * rep.badgradvidx for specific variable (gradient element) suspected
  * rep.badgradxbase, a point where gradient is tested
  * rep.badgraduser, user-provided gradient  (stored  as  2D  matrix  with
    single row in order to make  report  structure  compatible  with  more
    complex optimizers like MinNLC or MinLM)
  * rep.badgradnum,   reference    gradient    obtained    via   numerical
    differentiation (stored as  2D matrix with single row in order to make
    report structure compatible with more complex optimizers  like  MinNLC
    or MinLM)
* rep.nonc0suspected
* rep.nonc1suspected

==== ADDITIONAL REPORTS/LOGS ====

Several different tests are performed to catch C0/C1 errors, you can  find
out specific test signaled error by looking to:
* rep.nonc0test0positive, for non-C0 test #0
* rep.nonc1test0positive, for non-C1 test #0
* rep.nonc1test1positive, for non-C1 test #1

Additional information (including line search logs)  can  be  obtained  by
means of:
* minbcoptguardnonc1test0results()
* minbcoptguardnonc1test1results()
which return detailed error reports, specific points where discontinuities
were found, and so on.

Inputs:
    State   -   algorithm state

Outputs:
    Rep     -   generic OptGuard report;  more  detailed  reports  can  be
                retrieved with other functions.

NOTE: false negatives (nonsmooth problems are not identified as  nonsmooth
      ones) are possible although unlikely.

      The reason  is  that  you  need  to  make several evaluations around
      nonsmoothness  in  order  to  accumulate  enough  information  about
      function curvature. Say, if you start right from the nonsmooth point,
      optimizer simply won't get enough data to understand what  is  going
      wrong before it terminates due to abrupt changes in the  derivative.
      It is also  possible  that  &quot;unlucky&quot;  step  will  move  us  to  the
      termination too quickly.

      Our current approach is to have less than 0.1%  false  negatives  in
      our test examples  (measured  with  multiple  restarts  from  random
      points), and to have exactly 0% false positives.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcoptguardresults(minbcstate state, optguardreport &amp;rep);
</pre>
<a name=sub_minbcoptguardsmoothness></a><h6 class=pageheader>minbcoptguardsmoothness Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates nonsmoothness monitoring  option  of
the  OptGuard  integrity  checker. Smoothness  monitor  silently  observes
solution process and tries to detect ill-posed problems, i.e. ones with:
a) discontinuous target function (non-C0)
b) nonsmooth     target function (non-C1)

Smoothness monitoring does NOT interrupt optimization  even if it suspects
that your problem is nonsmooth. It just sets corresponding  flags  in  the
OptGuard report which can be retrieved after optimization is over.

Smoothness monitoring is a moderate overhead option which often adds  less
than 1% to the optimizer running time. Thus, you can use it even for large
scale problems.

NOTE: OptGuard does  NOT  guarantee  that  it  will  always  detect  C0/C1
      continuity violations.

      First, minor errors are hard to  catch - say, a 0.0001 difference in
      the model values at two sides of the gap may be due to discontinuity
      of the model - or simply because the model has changed.

      Second, C1-violations  are  especially  difficult  to  detect  in  a
      noninvasive way. The optimizer usually  performs  very  short  steps
      near the nonsmoothness, and differentiation  usually   introduces  a
      lot of numerical noise.  It  is  hard  to  tell  whether  some  tiny
      discontinuity in the slope is due to real nonsmoothness or just  due
      to numerical noise alone.

      Our top priority was to avoid false positives, so in some rare cases
      minor errors may went unnoticed (however, in most cases they can  be
      spotted with restart from different initial point).

Inputs:
    State   -   algorithm state
    Level   -   monitoring level:
                * 0 - monitoring is disabled
                * 1 - noninvasive low-overhead monitoring; function values
                      and/or gradients are recorded, but OptGuard does not
                      try to perform additional evaluations  in  order  to
                      get more information about suspicious locations.

==== EXPLANATION ====

One major source of headache during optimization  is  the  possibility  of
the coding errors in the target function/constraints (or their gradients).
Such  errors   most   often   manifest   themselves  as  discontinuity  or
nonsmoothness of the target/constraints.

Another frequent situation is when you try to optimize something involving
lots of min() and max() operations, i.e. nonsmooth target. Although not  a
coding error, it is nonsmoothness anyway - and smooth  optimizers  usually
stop right after encountering nonsmoothness, well before reaching solution.

OptGuard integrity checker helps you to catch such situations: it monitors
function values/gradients being passed  to  the  optimizer  and  tries  to
errors. Upon discovering suspicious pair of points it  raises  appropriate
flag (and allows you to continue optimization). When optimization is done,
you can study OptGuard result.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcoptguardsmoothness(minbcstate state, ae_int_t level);
<b>void</b> minbcoptguardsmoothness(minbcstate state);
</pre>
<a name=sub_minbcoptimize></a><h6 class=pageheader>minbcoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear optimizer

These functions accept following parameters:
    state   -   algorithm state
    func    -   callback which calculates function (or merit function)
                value func at given point x
    grad    -   callback which calculates function (or merit function)
                value func and gradient grad at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL

NOTES:

1. This function has two different implementations: one which  uses  exact
   (analytical) user-supplied gradient,  and one which uses function value
   only  and  numerically  differentiates  function  in  order  to  obtain
   gradient.

   Depending  on  the  specific  function  used to create optimizer object
   (either  MinBCCreate() for analytical gradient or  MinBCCreateF()
   for numerical differentiation) you should choose appropriate variant of
   MinBCOptimize() - one  which  accepts  function  AND gradient or one
   which accepts function ONLY.

   Be careful to choose variant of MinBCOptimize() which corresponds to
   your optimization scheme! Table below lists different  combinations  of
   callback (function/gradient) passed to MinBCOptimize()  and specific
   function used to create optimizer.

                     |         USER PASSED TO MinBCOptimize()
   CREATED WITH      |  function only   |  function and gradient
   ------------------------------------------------------------
   MinBCCreateF()    |     works               FAILS
   MinBCCreate()     |     FAILS               works

   Here &quot;FAIL&quot; denotes inappropriate combinations  of  optimizer  creation
   function  and  MinBCOptimize()  version.   Attemps   to   use   such
   combination (for  example,  to  create optimizer with MinBCCreateF()
   and  to  pass  gradient  information  to  MinCGOptimize()) will lead to
   exception being thrown. Either  you  did  not pass gradient when it WAS
   needed or you passed gradient when it was NOT needed.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcoptimize(minbcstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> minbcoptimize(minbcstate &amp;state, <b>void</b> (*grad)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbc_d_1 class=nav>minbc_d_1</a> | <a href=#example_minbc_numdiff class=nav>minbc_numdiff</a> ]</p>
<a name=sub_minbcrequesttermination></a><h6 class=pageheader>minbcrequesttermination Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine submits request for termination of running  optimizer.  It
should be called from user-supplied callback when user decides that it  is
time to &quot;smoothly&quot; terminate optimization process.  As  result,  optimizer
stops at point which was &quot;current accepted&quot; when termination  request  was
submitted and returns error code 8 (successful termination).

Inputs:
    State   -   optimizer structure

NOTE: after  request  for  termination  optimizer  may   perform   several
      additional calls to user-supplied callbacks. It does  NOT  guarantee
      to stop immediately - it just guarantees that these additional calls
      will be discarded later.

NOTE: calling this function on optimizer which is NOT running will have no
      effect.

NOTE: multiple calls to this function are possible. First call is counted,
      subsequent calls are silently ignored.
ALGLIB: Copyright 08.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcrequesttermination(minbcstate state);
</pre>
<a name=sub_minbcrestartfrom></a><h6 class=pageheader>minbcrestartfrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine restarts algorithm from new point.
All optimization parameters (including constraints) are left unchanged.

This  function  allows  to  solve multiple  optimization  problems  (which
must have  same number of dimensions) without object reallocation penalty.

Inputs:
    State   -   structure previously allocated with MinBCCreate call.
    X       -   new starting point.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcrestartfrom(minbcstate state, real_1d_array x);
</pre>
<a name=sub_minbcresults></a><h6 class=pageheader>minbcresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
BC results

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[0..N-1], solution
    Rep     -   optimization report. You should check Rep.TerminationType
                in  order  to  distinguish  successful  termination  from
                unsuccessful one:
                * -8    internal integrity control  detected  infinite or
                        NAN   values   in   function/gradient.   Abnormal
                        termination signalled.
                * -3   inconsistent constraints.
                *  1   relative function improvement is no more than EpsF.
                *  2   scaled step is no more than EpsX.
                *  4   scaled gradient norm is no more than EpsG.
                *  5   MaxIts steps was taken
                *  8   terminated by user who called minbcrequesttermination().
                       X contains point which was &quot;current accepted&quot;  when
                       termination request was submitted.
                More information about fields of this  structure  can  be
                found in the comments on MinBCReport datatype.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcresults(minbcstate state, real_1d_array &amp;x, minbcreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbc_d_1 class=nav>minbc_d_1</a> | <a href=#example_minbc_numdiff class=nav>minbc_numdiff</a> ]</p>
<a name=sub_minbcresultsbuf></a><h6 class=pageheader>minbcresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
BC results

Buffered implementation of MinBCResults() which uses pre-allocated buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcresultsbuf(minbcstate state, real_1d_array &amp;x, minbcreport &amp;rep);
</pre>
<a name=sub_minbcsetbc></a><h6 class=pageheader>minbcsetbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets boundary constraints for BC optimizer.

Boundary constraints are inactive by default (after initial creation).
They are preserved after algorithm restart with MinBCRestartFrom().

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very small number or -INF.
    BndU    -   upper bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very large number or +INF.

NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
variable will be &quot;frozen&quot; at X[i]=BndL[i]=BndU[i].

NOTE 2: this solver has following useful properties:
* bound constraints are always satisfied exactly
* function is evaluated only INSIDE area specified by  bound  constraints,
  even  when  numerical  differentiation is used (algorithm adjusts  nodes
  according to boundary constraints)
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcsetbc(minbcstate state, real_1d_array bndl, real_1d_array bndu);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbc_d_1 class=nav>minbc_d_1</a> | <a href=#example_minbc_numdiff class=nav>minbc_numdiff</a> ]</p>
<a name=sub_minbcsetcond></a><h6 class=pageheader>minbcsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping conditions for the optimizer.

Inputs:
    State   -   structure which stores algorithm state
    EpsG    - &ge; 0
                The  subroutine  finishes  its  work   if   the  condition
                |v| &lt; EpsG is satisfied, where:
                * |.| means Euclidian norm
                * v - scaled gradient vector, v[i]=g[i]*s[i]
                * g - gradient
                * s - scaling coefficients set by MinBCSetScale()
    EpsF    - &ge; 0
                The  subroutine  finishes  its work if on k+1-th iteration
                the  condition  |F(k+1)-F(k)| &le; EpsF*max{|F(k)|,|F(k+1)|,1}
                is satisfied.
    EpsX    - &ge; 0
                The subroutine finishes its work if  on  k+1-th  iteration
                the condition |v| &le; EpsX is fulfilled, where:
                * |.| means Euclidian norm
                * v - scaled step vector, v[i]=dx[i]/s[i]
                * dx - step vector, dx=X(k+1)-X(k)
                * s - scaling coefficients set by MinBCSetScale()
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations is unlimited.

Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
to automatic stopping criterion selection.

NOTE: when SetCond() called with non-zero MaxIts, BC solver may perform
      slightly more than MaxIts iterations. I.e., MaxIts  sets  non-strict
      limit on iterations count.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcsetcond(minbcstate state, <b>double</b> epsg, <b>double</b> epsf, <b>double</b> epsx, ae_int_t maxits);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbc_d_1 class=nav>minbc_d_1</a> | <a href=#example_minbc_numdiff class=nav>minbc_numdiff</a> ]</p>
<a name=sub_minbcsetprecdefault></a><h6 class=pageheader>minbcsetprecdefault Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification of the preconditioner: preconditioning is turned off.

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcsetprecdefault(minbcstate state);
</pre>
<a name=sub_minbcsetprecdiag></a><h6 class=pageheader>minbcsetprecdiag Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification  of  the  preconditioner:  diagonal of approximate Hessian is
used.

Inputs:
    State   -   structure which stores algorithm state
    D       -   diagonal of the approximate Hessian, array[0..N-1],
                (if larger, only leading N elements are used).

NOTE 1: D[i] should be positive. Exception will be thrown otherwise.

NOTE 2: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcsetprecdiag(minbcstate state, real_1d_array d);
</pre>
<a name=sub_minbcsetprecscale></a><h6 class=pageheader>minbcsetprecscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification of the preconditioner: scale-based diagonal preconditioning.

This preconditioning mode can be useful when you  don't  have  approximate
diagonal of Hessian, but you know that your  variables  are  badly  scaled
(for  example,  one  variable is in [1,10], and another in [1000,100000]),
and most part of the ill-conditioning comes from different scales of vars.

In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
can greatly improve convergence.

IMPRTANT: you should set scale of your variables  with  MinBCSetScale()
call  (before  or after MinBCSetPrecScale() call). Without knowledge of
the scale of your variables scale-based preconditioner will be  just  unit
matrix.

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcsetprecscale(minbcstate state);
</pre>
<a name=sub_minbcsetscale></a><h6 class=pageheader>minbcsetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients for BC optimizer.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison with tolerances).  Scale of
the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the function

Scaling is also used by finite difference variant of the optimizer  - step
along I-th axis is equal to DiffStep*S[I].

In  most  optimizers  (and  in  the  BC  too)  scaling is NOT a form of
preconditioning. It just  affects  stopping  conditions.  You  should  set
preconditioner  by  separate  call  to  one  of  the  MinBCSetPrec...()
functions.

There is a special  preconditioning  mode, however,  which  uses   scaling
coefficients to form diagonal preconditioning matrix. You  can  turn  this
mode on, if you want.   But  you should understand that scaling is not the
same thing as preconditioning - these are two different, although  related
forms of tuning solver.

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcsetscale(minbcstate state, real_1d_array s);
</pre>
<a name=sub_minbcsetstpmax></a><h6 class=pageheader>minbcsetstpmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets maximum step length

Inputs:
    State   -   structure which stores algorithm state
    StpMax  -   maximum step length, &ge; 0. Set StpMax to 0.0,  if you don't
                want to limit step length.

Use this subroutine when you optimize target function which contains exp()
or  other  fast  growing  functions,  and optimization algorithm makes too
large  steps  which  lead   to overflow. This function allows us to reject
steps  that  are  too  large  (and  therefore  expose  us  to the possible
overflow) without actually calculating function value at the x+stp*d.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcsetstpmax(minbcstate state, <b>double</b> stpmax);
</pre>
<a name=sub_minbcsetxrep></a><h6 class=pageheader>minbcsetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to MinBCOptimize().
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbcsetxrep(minbcstate state, <b>bool</b> needxrep);
</pre>
<a name=example_minbc_d_1></a><h6 class=pageheader>minbc_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_grad(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
<font color=navy>// and its derivatives df/d0 and df/dx1</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
   grad[0] = 400*pow(x[0]+3,3);
   grad[1] = 4*pow(x[1]-3,3);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>//</font>
<font color=navy>// subject to box constraints</font>
<font color=navy>//</font>
<font color=navy>//     -1 &le; x &le; +1, -1 &le; y &le; +1</font>
<font color=navy>//</font>
<font color=navy>// using MinBC optimizer with:</font>
<font color=navy>// * initial point x=[0,0]</font>
<font color=navy>// * unit scale being set <b>for</b> all variables (see minbcsetscale <b>for</b> more info)</font>
<font color=navy>// * stopping criteria set to <font color=blue><b>&quot;terminate after short enough step&quot;</b></font></font>
<font color=navy>// * OptGuard integrity check being used to check problem statement</font>
<font color=navy>//   <b>for</b> some common errors like nonsmoothness or bad analytic gradient</font>
<font color=navy>//</font>
<font color=navy>// First, we create optimizer object and tune its properties:</font>
<font color=navy>// * set box constraints</font>
<font color=navy>// * set variable scales</font>
<font color=navy>// * set stopping criteria</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[-1,-1]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[+1,+1]&quot;</b></font>;
   minbcstate state;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   minbccreate(x, state);
   minbcsetbc(state, bndl, bndu);
   minbcsetscale(state, s);
   minbcsetcond(state, epsg, epsf, epsx, maxits);
<font color=navy>// Then we activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard monitor helps to catch common coding and problem statement</font>
<font color=navy>// issues, like:</font>
<font color=navy>// * discontinuity of the target function (C0 continuity violation)</font>
<font color=navy>// * nonsmoothness of the target function (C1 continuity violation)</font>
<font color=navy>// * erroneous analytic gradient, i.e. one inconsistent with actual</font>
<font color=navy>//   change in the target/constraints</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential <b>for</b> early prototyping stages because such</font>
<font color=navy>// problems often result in premature termination of the optimizer</font>
<font color=navy>// which is really hard to distinguish from the correct termination.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: GRADIENT VERIFICATION IS PERFORMED BY MEANS OF NUMERICAL</font>
<font color=navy>//            DIFFERENTIATION. DO NOT USE IT IN PRODUCTION CODE!!!!!!!</font>
<font color=navy>//</font>
<font color=navy>//            Other OptGuard checks add moderate overhead, but anyway</font>
<font color=navy>//            it is better to turn them off when they are not needed.</font>
   minbcoptguardsmoothness(state);
   minbcoptguardgradient(state, 0.001);
<font color=navy>// Optimize and evaluate results</font>
   minbcreport rep;
   minbcoptimize(state, function1_grad);
   minbcresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-1,1]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// NOTE: want to test OptGuard? Try breaking the gradient - say, add</font>
<font color=navy>//       1.0 to some of its components.</font>
   optguardreport ogrep;
   minbcoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minbc_numdiff></a><h6 class=pageheader>minbc_numdiff Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_func(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>//</font>
<font color=navy>// subject to box constraints</font>
<font color=navy>//</font>
<font color=navy>//    -1 &le; x &le; +1, -1 &le; y &le; +1</font>
<font color=navy>//</font>
<font color=navy>// using MinBC optimizer with:</font>
<font color=navy>// * numerical differentiation being used</font>
<font color=navy>// * initial point x=[0,0]</font>
<font color=navy>// * unit scale being set <b>for</b> all variables (see minbcsetscale <b>for</b> more info)</font>
<font color=navy>// * stopping criteria set to <font color=blue><b>&quot;terminate after short enough step&quot;</b></font></font>
<font color=navy>// * OptGuard integrity check being used to check problem statement</font>
<font color=navy>//   <b>for</b> some common errors like nonsmoothness or bad analytic gradient</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[-1,-1]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[+1,+1]&quot;</b></font>;
   minbcstate state;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   <b>double</b> diffstep = 1.0e-6;
<font color=navy>// Now we are ready to actually optimize something:</font>
<font color=navy>// * first we create optimizer</font>
<font color=navy>// * we add boundary constraints</font>
<font color=navy>// * we tune stopping conditions</font>
<font color=navy>// * and, finally, optimize and obtain results...</font>
   minbccreatef(x, diffstep, state);
   minbcsetbc(state, bndl, bndu);
   minbcsetscale(state, s);
   minbcsetcond(state, epsg, epsf, epsx, maxits);
<font color=navy>// Then we activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// Numerical differentiation always produces <font color=blue><b>&quot;correct&quot;</b></font> gradient</font>
<font color=navy>// (with some truncation error, but unbiased). Thus, we just have</font>
<font color=navy>// to check smoothness properties of the target: C0 and C1 continuity.</font>
<font color=navy>//</font>
<font color=navy>// Sometimes user accidentally tries to solve nonsmooth problems</font>
<font color=navy>// with smooth optimizer. OptGuard helps to detect such situations</font>
<font color=navy>// early, at the prototyping stage.</font>
   minbcoptguardsmoothness(state);
<font color=navy>// Optimize and evaluate results</font>
   minbcreport rep;
   minbcoptimize(state, function1_func);
   minbcresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-1,1]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// Want to challenge OptGuard? Try to make your problem</font>
<font color=navy>// nonsmooth by replacing 100*(x+3)^4 by 100*|x+3| and</font>
<font color=navy>// re-run optimizer.</font>
   optguardreport ogrep;
   minbcoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_minbleic></a><h4 class=pageheader>8.8.2. minbleic Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_minbleicreport class=toc>minbleicreport</a> |
<a href=#struct_minbleicstate class=toc>minbleicstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_minbleiccreate class=toc>minbleiccreate</a> |
<a href=#sub_minbleiccreatef class=toc>minbleiccreatef</a> |
<a href=#sub_minbleicoptguardgradient class=toc>minbleicoptguardgradient</a> |
<a href=#sub_minbleicoptguardnonc1test0results class=toc>minbleicoptguardnonc1test0results</a> |
<a href=#sub_minbleicoptguardnonc1test1results class=toc>minbleicoptguardnonc1test1results</a> |
<a href=#sub_minbleicoptguardresults class=toc>minbleicoptguardresults</a> |
<a href=#sub_minbleicoptguardsmoothness class=toc>minbleicoptguardsmoothness</a> |
<a href=#sub_minbleicoptimize class=toc>minbleicoptimize</a> |
<a href=#sub_minbleicrequesttermination class=toc>minbleicrequesttermination</a> |
<a href=#sub_minbleicrestartfrom class=toc>minbleicrestartfrom</a> |
<a href=#sub_minbleicresults class=toc>minbleicresults</a> |
<a href=#sub_minbleicresultsbuf class=toc>minbleicresultsbuf</a> |
<a href=#sub_minbleicsetbc class=toc>minbleicsetbc</a> |
<a href=#sub_minbleicsetcond class=toc>minbleicsetcond</a> |
<a href=#sub_minbleicsetlc class=toc>minbleicsetlc</a> |
<a href=#sub_minbleicsetprecdefault class=toc>minbleicsetprecdefault</a> |
<a href=#sub_minbleicsetprecdiag class=toc>minbleicsetprecdiag</a> |
<a href=#sub_minbleicsetprecscale class=toc>minbleicsetprecscale</a> |
<a href=#sub_minbleicsetscale class=toc>minbleicsetscale</a> |
<a href=#sub_minbleicsetstpmax class=toc>minbleicsetstpmax</a> |
<a href=#sub_minbleicsetxrep class=toc>minbleicsetxrep</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_minbleic_d_1 class=toc>minbleic_d_1</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization with bound constraints</td></tr>
<tr align=left valign=top><td><a href=#example_minbleic_d_2 class=toc>minbleic_d_2</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization with linear inequality constraints</td></tr>
<tr align=left valign=top><td><a href=#example_minbleic_numdiff class=toc>minbleic_numdiff</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization with bound constraints and numerical differentiation</td></tr>
</table>
</div>
<a name=struct_minbleicreport></a><h6 class=pageheader>minbleicreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure stores optimization report:
* IterationsCount           number of iterations
* NFEV                      number of gradient evaluations
* TerminationType           termination type (see below)

TERMINATION CODES

TerminationType field contains completion code, which can be:
  -8    internal integrity control detected  infinite  or  NAN  values  in
        function/gradient. Abnormal termination signalled.
  -3    inconsistent constraints. Feasible point is
        either nonexistent or too hard to find. Try to
        restart optimizer with better initial approximation
   1    relative function improvement is no more than EpsF.
   2    relative step is no more than EpsX.
   4    gradient norm is no more than EpsG
   5    MaxIts steps was taken
   7    stopping conditions are too stringent,
        further improvement is impossible,
        X contains best point found so far.
   8    terminated by user who called minbleicrequesttermination(). X contains
        point which was &quot;current accepted&quot; when  termination  request  was
        submitted.

ADDITIONAL FIELDS

There are additional fields which can be used for debugging:
* DebugEqErr                error in the equality constraints (2-norm)
* DebugFS                   f, calculated at projection of initial point
                            to the feasible set
* DebugFF                   f, calculated at the final point
* DebugDX                   |X_start-X_final|
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minbleicreport {
   ae_int_t iterationscount;
   ae_int_t nfev;
   ae_int_t varidx;
   ae_int_t terminationtype;
   <b>double</b> debugeqerr;
   <b>double</b> debugfs;
   <b>double</b> debugff;
   <b>double</b> debugdx;
   ae_int_t debugfeasqpits;
   ae_int_t debugfeasgpaits;
   ae_int_t inneriterationscount;
   ae_int_t outeriterationscount;
};
</pre>
<a name=struct_minbleicstate></a><h6 class=pageheader>minbleicstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores nonlinear optimizer state.
You should use functions provided by MinBLEIC subpackage to work with this
object
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minbleicstate {
   bool needf;
   bool needfg;
   bool xupdated;
   double f;
   real_1d_array g;
   real_1d_array x;
};
</pre>
<a name=sub_minbleiccreate></a><h6 class=pageheader>minbleiccreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
BOUND CONSTRAINED OPTIMIZATION
WITH ADDITIONAL LINEAR EQUALITY AND INEQUALITY CONSTRAINTS
The  subroutine  minimizes  function   F(x)  of N arguments subject to any
combination of:
* bound constraints
* linear inequality constraints
* linear equality constraints

REQUIREMENTS:
* user must provide function value and gradient
* starting point X0 must be feasible or
  not too far away from the feasible set
* grad(f) must be Lipschitz continuous on a level set:
  L = { x : f(x) &le; f(x0) }
* function must be defined everywhere on the feasible set F

USAGE:

Constrained optimization if far more complex than the unconstrained one.
Here we give very brief outline of the BLEIC optimizer. We strongly recommend
you to read examples in the ALGLIB Reference Manual and to read ALGLIB User Guide
on optimization, which is available at http://www.alglib.net/optimization/

1. User initializes algorithm state with MinBLEICCreate() call

2. USer adds boundary and/or linear constraints by calling
   MinBLEICSetBC() and MinBLEICSetLC() functions.

3. User sets stopping conditions with MinBLEICSetCond().

4. User calls MinBLEICOptimize() function which takes algorithm  state and
   pointer (delegate, etc.) to callback function which calculates F/G.

5. User calls MinBLEICResults() to get solution

6. Optionally user may call MinBLEICRestartFrom() to solve another problem
   with same N but another starting point.
   MinBLEICRestartFrom() allows to reuse already initialized structure.

NOTE: if you have box-only constraints (no  general  linear  constraints),
      then MinBC optimizer can be better option. It uses  special,  faster
      constraint activation method, which performs better on problems with
      multiple constraints active at the solution.

      On small-scale problems performance of MinBC is similar to  that  of
      MinBLEIC, but on large-scale ones (hundreds and thousands of  active
      constraints) it can be several times faster than MinBLEIC.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size ofX
    X       -   starting point, array[N]:
                * it is better to set X to a feasible point
                * but X can be infeasible, in which case algorithm will try
                  to find feasible point first, using X as initial
                  approximation.

Outputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleiccreate(ae_int_t n, real_1d_array x, minbleicstate &amp;state);
<b>void</b> minbleiccreate(real_1d_array x, minbleicstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbleic_d_1 class=nav>minbleic_d_1</a> | <a href=#example_minbleic_d_2 class=nav>minbleic_d_2</a> ]</p>
<a name=sub_minbleiccreatef></a><h6 class=pageheader>minbleiccreatef Function</h6>
<hr width=600 align=left>
<pre class=narration>
The subroutine is finite difference variant of MinBLEICCreate().  It  uses
finite differences in order to differentiate target function.

Description below contains information which is specific to  this function
only. We recommend to read comments on MinBLEICCreate() in  order  to  get
more information about creation of BLEIC optimizer.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    X       -   starting point, array[0..N-1].
    DiffStep-   differentiation step, &gt; 0

Outputs:
    State   -   structure which stores algorithm state

NOTES:
1. algorithm uses 4-point central formula for differentiation.
2. differentiation step along I-th axis is equal to DiffStep*S[I] where
   S[] is scaling vector which can be set by MinBLEICSetScale() call.
3. we recommend you to use moderate values of  differentiation  step.  Too
   large step will result in too large truncation  errors, while too small
   step will result in too large numerical  errors.  1.0E-6  can  be  good
   value to start with.
4. Numerical  differentiation  is   very   inefficient  -   one   gradient
   calculation needs 4*N function evaluations. This function will work for
   any N - either small (1...10), moderate (10...100) or  large  (100...).
   However, performance penalty will be too severe for any N's except  for
   small ones.
   We should also say that code which relies on numerical  differentiation
   is  less  robust and precise. CG needs exact gradient values. Imprecise
   gradient may slow  down  convergence, especially  on  highly  nonlinear
   problems.
   Thus  we  recommend to use this function for fast prototyping on small-
   dimensional problems only, and to implement analytical gradient as soon
   as possible.
ALGLIB: Copyright 16.05.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleiccreatef(ae_int_t n, real_1d_array x, <b>double</b> diffstep, minbleicstate &amp;state);
<b>void</b> minbleiccreatef(real_1d_array x, <b>double</b> diffstep, minbleicstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbleic_numdiff class=nav>minbleic_numdiff</a> ]</p>
<a name=sub_minbleicoptguardgradient></a><h6 class=pageheader>minbleicoptguardgradient Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates verification  of  the  user-supplied
analytic gradient.

Upon  activation  of  this  option  OptGuard  integrity  checker  performs
numerical differentiation of your target function  at  the  initial  point
(note: future versions may also perform check  at  the  final  point)  and
compares numerical gradient with analytic one provided by you.

If difference is too large, an error flag is set and optimization  session
continues. After optimization session is over, you can retrieve the report
which  stores  both  gradients  and  specific  components  highlighted  as
suspicious by the OptGuard.

The primary OptGuard report can be retrieved with minbleicoptguardresults().

IMPORTANT: gradient check is a high-overhead option which  will  cost  you
           about 3*N additional function evaluations. In many cases it may
           cost as much as the rest of the optimization session.

           YOU SHOULD NOT USE IT IN THE PRODUCTION CODE UNLESS YOU WANT TO
           CHECK DERIVATIVES PROVIDED BY SOME THIRD PARTY.

NOTE: unlike previous incarnation of the gradient checking code,  OptGuard
      does NOT interrupt optimization even if it discovers bad gradient.

Inputs:
    State       -   structure used to store algorithm state
    TestStep    -   verification step used for numerical differentiation:
                    * TestStep=0 turns verification off
                    * TestStep &gt; 0 activates verification
                    You should carefully choose TestStep. Value  which  is
                    too large (so large that  function  behavior  is  non-
                    cubic at this scale) will lead  to  false  alarms. Too
                    short step will result in rounding  errors  dominating
                    numerical derivative.

                    You may use different step for different parameters by
                    means of setting scale with minbleicsetscale().

==== EXPLANATION ====

In order to verify gradient algorithm performs following steps:
  * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    where X[i] is i-th component of the initial point and S[i] is a  scale
    of i-th parameter
  * F(X) is evaluated at these trial points
  * we perform one more evaluation in the middle point of the interval
  * we  build  cubic  model using function values and derivatives at trial
    points and we compare its prediction with actual value in  the  middle
    point
ALGLIB: Copyright 15.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicoptguardgradient(minbleicstate state, <b>double</b> teststep);
</pre>
<a name=sub_minbleicoptguardnonc1test0results></a><h6 class=pageheader>minbleicoptguardnonc1test0results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #0

Nonsmoothness (non-C1) test #0 studies  function  values  (not  gradient!)
obtained during line searches and monitors  behavior  of  the  directional
derivative estimate.

This test is less powerful than test #1, but it does  not  depend  on  the
gradient values and thus it is more robust against artifacts introduced by
numerical differentiation.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], f[] - arrays of length CNT which store step lengths and  function
  values at these points; f[i] is evaluated in x0+stp[i]*d.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #0 &quot;strong&quot; report
    LngRep  -   C1 test #0 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicoptguardnonc1test0results(minbleicstate state, optguardnonc1test0report &amp;strrep, optguardnonc1test0report &amp;lngrep);
</pre>
<a name=sub_minbleicoptguardnonc1test1results></a><h6 class=pageheader>minbleicoptguardnonc1test1results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #1

Nonsmoothness (non-C1)  test  #1  studies  individual  components  of  the
gradient computed during line search.

When precise analytic gradient is provided this test is more powerful than
test #0  which  works  with  function  values  and  ignores  user-provided
gradient.  However,  test  #0  becomes  more   powerful   when   numerical
differentiation is employed (in such cases test #1 detects  higher  levels
of numerical noise and becomes too conservative).

This test also tells specific components of the gradient which violate  C1
continuity, which makes it more informative than #0, which just tells that
continuity is violated.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* vidx - is an index of the variable in [0,N) with nonsmooth derivative
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], g[] - arrays of length CNT which store step lengths and  gradient
  values at these points; g[i] is evaluated in  x0+stp[i]*d  and  contains
  vidx-th component of the gradient.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #1 &quot;strong&quot; report
    LngRep  -   C1 test #1 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicoptguardnonc1test1results(minbleicstate state, optguardnonc1test1report &amp;strrep, optguardnonc1test1report &amp;lngrep);
</pre>
<a name=sub_minbleicoptguardresults></a><h6 class=pageheader>minbleicoptguardresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Results of OptGuard integrity check, should be called  after  optimization
session is over.

==== PRIMARY REPORT ====

OptGuard performs several checks which are intended to catch common errors
in the implementation of nonlinear function/gradient:
* incorrect analytic gradient
* discontinuous (non-C0) target functions (constraints)
* nonsmooth     (non-C1) target functions (constraints)

Each of these checks is activated with appropriate function:
* minbleicoptguardgradient() for gradient verification
* minbleicoptguardsmoothness() for C0/C1 checks

Following flags are set when these errors are suspected:
* rep.badgradsuspected, and additionally:
  * rep.badgradvidx for specific variable (gradient element) suspected
  * rep.badgradxbase, a point where gradient is tested
  * rep.badgraduser, user-provided gradient  (stored  as  2D  matrix  with
    single row in order to make  report  structure  compatible  with  more
    complex optimizers like MinNLC or MinLM)
  * rep.badgradnum,   reference    gradient    obtained    via   numerical
    differentiation (stored as  2D matrix with single row in order to make
    report structure compatible with more complex optimizers  like  MinNLC
    or MinLM)
* rep.nonc0suspected
* rep.nonc1suspected

==== ADDITIONAL REPORTS/LOGS ====

Several different tests are performed to catch C0/C1 errors, you can  find
out specific test signaled error by looking to:
* rep.nonc0test0positive, for non-C0 test #0
* rep.nonc1test0positive, for non-C1 test #0
* rep.nonc1test1positive, for non-C1 test #1

Additional information (including line search logs)  can  be  obtained  by
means of:
* minbleicoptguardnonc1test0results()
* minbleicoptguardnonc1test1results()
which return detailed error reports, specific points where discontinuities
were found, and so on.

Inputs:
    State   -   algorithm state

Outputs:
    Rep     -   generic OptGuard report;  more  detailed  reports  can  be
                retrieved with other functions.

NOTE: false negatives (nonsmooth problems are not identified as  nonsmooth
      ones) are possible although unlikely.

      The reason  is  that  you  need  to  make several evaluations around
      nonsmoothness  in  order  to  accumulate  enough  information  about
      function curvature. Say, if you start right from the nonsmooth point,
      optimizer simply won't get enough data to understand what  is  going
      wrong before it terminates due to abrupt changes in the  derivative.
      It is also  possible  that  &quot;unlucky&quot;  step  will  move  us  to  the
      termination too quickly.

      Our current approach is to have less than 0.1%  false  negatives  in
      our test examples  (measured  with  multiple  restarts  from  random
      points), and to have exactly 0% false positives.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicoptguardresults(minbleicstate state, optguardreport &amp;rep);
</pre>
<a name=sub_minbleicoptguardsmoothness></a><h6 class=pageheader>minbleicoptguardsmoothness Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates nonsmoothness monitoring  option  of
the  OptGuard  integrity  checker. Smoothness  monitor  silently  observes
solution process and tries to detect ill-posed problems, i.e. ones with:
a) discontinuous target function (non-C0)
b) nonsmooth     target function (non-C1)

Smoothness monitoring does NOT interrupt optimization  even if it suspects
that your problem is nonsmooth. It just sets corresponding  flags  in  the
OptGuard report which can be retrieved after optimization is over.

Smoothness monitoring is a moderate overhead option which often adds  less
than 1% to the optimizer running time. Thus, you can use it even for large
scale problems.

NOTE: OptGuard does  NOT  guarantee  that  it  will  always  detect  C0/C1
      continuity violations.

      First, minor errors are hard to  catch - say, a 0.0001 difference in
      the model values at two sides of the gap may be due to discontinuity
      of the model - or simply because the model has changed.

      Second, C1-violations  are  especially  difficult  to  detect  in  a
      noninvasive way. The optimizer usually  performs  very  short  steps
      near the nonsmoothness, and differentiation  usually   introduces  a
      lot of numerical noise.  It  is  hard  to  tell  whether  some  tiny
      discontinuity in the slope is due to real nonsmoothness or just  due
      to numerical noise alone.

      Our top priority was to avoid false positives, so in some rare cases
      minor errors may went unnoticed (however, in most cases they can  be
      spotted with restart from different initial point).

Inputs:
    State   -   algorithm state
    Level   -   monitoring level:
                * 0 - monitoring is disabled
                * 1 - noninvasive low-overhead monitoring; function values
                      and/or gradients are recorded, but OptGuard does not
                      try to perform additional evaluations  in  order  to
                      get more information about suspicious locations.

==== EXPLANATION ====

One major source of headache during optimization  is  the  possibility  of
the coding errors in the target function/constraints (or their gradients).
Such  errors   most   often   manifest   themselves  as  discontinuity  or
nonsmoothness of the target/constraints.

Another frequent situation is when you try to optimize something involving
lots of min() and max() operations, i.e. nonsmooth target. Although not  a
coding error, it is nonsmoothness anyway - and smooth  optimizers  usually
stop right after encountering nonsmoothness, well before reaching solution.

OptGuard integrity checker helps you to catch such situations: it monitors
function values/gradients being passed  to  the  optimizer  and  tries  to
errors. Upon discovering suspicious pair of points it  raises  appropriate
flag (and allows you to continue optimization). When optimization is done,
you can study OptGuard result.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicoptguardsmoothness(minbleicstate state, ae_int_t level);
<b>void</b> minbleicoptguardsmoothness(minbleicstate state);
</pre>
<a name=sub_minbleicoptimize></a><h6 class=pageheader>minbleicoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear optimizer

These functions accept following parameters:
    state   -   algorithm state
    func    -   callback which calculates function (or merit function)
                value func at given point x
    grad    -   callback which calculates function (or merit function)
                value func and gradient grad at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL

NOTES:

1. This function has two different implementations: one which  uses  exact
   (analytical) user-supplied gradient,  and one which uses function value
   only  and  numerically  differentiates  function  in  order  to  obtain
   gradient.

   Depending  on  the  specific  function  used to create optimizer object
   (either  MinBLEICCreate() for analytical gradient or  MinBLEICCreateF()
   for numerical differentiation) you should choose appropriate variant of
   MinBLEICOptimize() - one  which  accepts  function  AND gradient or one
   which accepts function ONLY.

   Be careful to choose variant of MinBLEICOptimize() which corresponds to
   your optimization scheme! Table below lists different  combinations  of
   callback (function/gradient) passed to MinBLEICOptimize()  and specific
   function used to create optimizer.

                     |         USER PASSED TO MinBLEICOptimize()
   CREATED WITH      |  function only   |  function and gradient
   ------------------------------------------------------------
   MinBLEICCreateF() |     work                FAIL
   MinBLEICCreate()  |     FAIL                work

   Here &quot;FAIL&quot; denotes inappropriate combinations  of  optimizer  creation
   function  and  MinBLEICOptimize()  version.   Attemps   to   use   such
   combination (for  example,  to  create optimizer with MinBLEICCreateF()
   and  to  pass  gradient information to MinBLEICOptimize()) will lead to
   exception being thrown. Either  you  did  not pass gradient when it WAS
   needed or you passed gradient when it was NOT needed.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicoptimize(minbleicstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> minbleicoptimize(minbleicstate &amp;state, <b>void</b> (*grad)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbleic_d_1 class=nav>minbleic_d_1</a> | <a href=#example_minbleic_d_2 class=nav>minbleic_d_2</a> | <a href=#example_minbleic_numdiff class=nav>minbleic_numdiff</a> ]</p>
<a name=sub_minbleicrequesttermination></a><h6 class=pageheader>minbleicrequesttermination Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine submits request for termination of running  optimizer.  It
should be called from user-supplied callback when user decides that it  is
time to &quot;smoothly&quot; terminate optimization process.  As  result,  optimizer
stops at point which was &quot;current accepted&quot; when termination  request  was
submitted and returns error code 8 (successful termination).

Inputs:
    State   -   optimizer structure

NOTE: after  request  for  termination  optimizer  may   perform   several
      additional calls to user-supplied callbacks. It does  NOT  guarantee
      to stop immediately - it just guarantees that these additional calls
      will be discarded later.

NOTE: calling this function on optimizer which is NOT running will have no
      effect.

NOTE: multiple calls to this function are possible. First call is counted,
      subsequent calls are silently ignored.
ALGLIB: Copyright 08.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicrequesttermination(minbleicstate state);
</pre>
<a name=sub_minbleicrestartfrom></a><h6 class=pageheader>minbleicrestartfrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine restarts algorithm from new point.
All optimization parameters (including constraints) are left unchanged.

This  function  allows  to  solve multiple  optimization  problems  (which
must have  same number of dimensions) without object reallocation penalty.

Inputs:
    State   -   structure previously allocated with MinBLEICCreate call.
    X       -   new starting point.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicrestartfrom(minbleicstate state, real_1d_array x);
</pre>
<a name=sub_minbleicresults></a><h6 class=pageheader>minbleicresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
BLEIC results

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[0..N-1], solution
    Rep     -   optimization report. You should check Rep.TerminationType
                in  order  to  distinguish  successful  termination  from
                unsuccessful one:
                * -8    internal integrity control  detected  infinite or
                        NAN   values   in   function/gradient.   Abnormal
                        termination signalled.
                * -3   inconsistent constraints. Feasible point is
                       either nonexistent or too hard to find. Try to
                       restart optimizer with better initial approximation
                *  1   relative function improvement is no more than EpsF.
                *  2   scaled step is no more than EpsX.
                *  4   scaled gradient norm is no more than EpsG.
                *  5   MaxIts steps was taken
                *  8   terminated by user who called minbleicrequesttermination().
                       X contains point which was &quot;current accepted&quot;  when
                       termination request was submitted.
                More information about fields of this  structure  can  be
                found in the comments on MinBLEICReport datatype.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicresults(minbleicstate state, real_1d_array &amp;x, minbleicreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbleic_d_1 class=nav>minbleic_d_1</a> | <a href=#example_minbleic_d_2 class=nav>minbleic_d_2</a> | <a href=#example_minbleic_numdiff class=nav>minbleic_numdiff</a> ]</p>
<a name=sub_minbleicresultsbuf></a><h6 class=pageheader>minbleicresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
BLEIC results

Buffered implementation of MinBLEICResults() which uses pre-allocated buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicresultsbuf(minbleicstate state, real_1d_array &amp;x, minbleicreport &amp;rep);
</pre>
<a name=sub_minbleicsetbc></a><h6 class=pageheader>minbleicsetbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets boundary constraints for BLEIC optimizer.

Boundary constraints are inactive by default (after initial creation).
They are preserved after algorithm restart with MinBLEICRestartFrom().

NOTE: if you have box-only constraints (no  general  linear  constraints),
      then MinBC optimizer can be better option. It uses  special,  faster
      constraint activation method, which performs better on problems with
      multiple constraints active at the solution.

      On small-scale problems performance of MinBC is similar to  that  of
      MinBLEIC, but on large-scale ones (hundreds and thousands of  active
      constraints) it can be several times faster than MinBLEIC.

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very small number or -INF.
    BndU    -   upper bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very large number or +INF.

NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
variable will be &quot;frozen&quot; at X[i]=BndL[i]=BndU[i].

NOTE 2: this solver has following useful properties:
* bound constraints are always satisfied exactly
* function is evaluated only INSIDE area specified by  bound  constraints,
  even  when  numerical  differentiation is used (algorithm adjusts  nodes
  according to boundary constraints)
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetbc(minbleicstate state, real_1d_array bndl, real_1d_array bndu);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbleic_d_1 class=nav>minbleic_d_1</a> | <a href=#example_minbleic_numdiff class=nav>minbleic_numdiff</a> ]</p>
<a name=sub_minbleicsetcond></a><h6 class=pageheader>minbleicsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping conditions for the optimizer.

Inputs:
    State   -   structure which stores algorithm state
    EpsG    - &ge; 0
                The  subroutine  finishes  its  work   if   the  condition
                |v| &lt; EpsG is satisfied, where:
                * |.| means Euclidian norm
                * v - scaled gradient vector, v[i]=g[i]*s[i]
                * g - gradient
                * s - scaling coefficients set by MinBLEICSetScale()
    EpsF    - &ge; 0
                The  subroutine  finishes  its work if on k+1-th iteration
                the  condition  |F(k+1)-F(k)| &le; EpsF*max{|F(k)|,|F(k+1)|,1}
                is satisfied.
    EpsX    - &ge; 0
                The subroutine finishes its work if  on  k+1-th  iteration
                the condition |v| &le; EpsX is fulfilled, where:
                * |.| means Euclidian norm
                * v - scaled step vector, v[i]=dx[i]/s[i]
                * dx - step vector, dx=X(k+1)-X(k)
                * s - scaling coefficients set by MinBLEICSetScale()
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations is unlimited.

Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
to automatic stopping criterion selection.

NOTE: when SetCond() called with non-zero MaxIts, BLEIC solver may perform
      slightly more than MaxIts iterations. I.e., MaxIts  sets  non-strict
      limit on iterations count.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetcond(minbleicstate state, <b>double</b> epsg, <b>double</b> epsf, <b>double</b> epsx, ae_int_t maxits);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbleic_d_1 class=nav>minbleic_d_1</a> | <a href=#example_minbleic_d_2 class=nav>minbleic_d_2</a> | <a href=#example_minbleic_numdiff class=nav>minbleic_numdiff</a> ]</p>
<a name=sub_minbleicsetlc></a><h6 class=pageheader>minbleicsetlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets linear constraints for BLEIC optimizer.

Linear constraints are inactive by default (after initial creation).
They are preserved after algorithm restart with MinBLEICRestartFrom().

Inputs:
    State   -   structure previously allocated with MinBLEICCreate call.
    C       -   linear constraints, array[K,N+1].
                Each row of C represents one constraint, either equality
                or inequality (see below):
                * first N elements correspond to coefficients,
                * last element corresponds to the right part.
                All elements of C (including right part) must be finite.
    CT      -   type of constraints, array[K]:
                * if CT[i] &gt; 0, then I-th constraint is C[i,*]*x &ge; C[i,n]
                * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n]
                * if CT[i] &lt; 0, then I-th constraint is C[i,*]*x &le; C[i,n]
    K       -   number of equality/inequality constraints, K &ge; 0:
                * if given, only leading K elements of C/CT are used
                * if not given, automatically determined from sizes of C/CT

NOTE 1: linear (non-bound) constraints are satisfied only approximately:
* there always exists some minor violation (about Epsilon in magnitude)
  due to rounding errors
* numerical differentiation, if used, may  lead  to  function  evaluations
  outside  of the feasible  area,   because   algorithm  does  NOT  change
  numerical differentiation formula according to linear constraints.
If you want constraints to be  satisfied  exactly, try to reformulate your
problem  in  such  manner  that  all constraints will become boundary ones
(this kind of constraints is always satisfied exactly, both in  the  final
solution and in all intermediate points).
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetlc(minbleicstate state, real_2d_array c, integer_1d_array ct, ae_int_t k);
<b>void</b> minbleicsetlc(minbleicstate state, real_2d_array c, integer_1d_array ct);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minbleic_d_2 class=nav>minbleic_d_2</a> ]</p>
<a name=sub_minbleicsetprecdefault></a><h6 class=pageheader>minbleicsetprecdefault Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification of the preconditioner: preconditioning is turned off.

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetprecdefault(minbleicstate state);
</pre>
<a name=sub_minbleicsetprecdiag></a><h6 class=pageheader>minbleicsetprecdiag Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification  of  the  preconditioner:  diagonal of approximate Hessian is
used.

Inputs:
    State   -   structure which stores algorithm state
    D       -   diagonal of the approximate Hessian, array[0..N-1],
                (if larger, only leading N elements are used).

NOTE 1: D[i] should be positive. Exception will be thrown otherwise.

NOTE 2: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetprecdiag(minbleicstate state, real_1d_array d);
</pre>
<a name=sub_minbleicsetprecscale></a><h6 class=pageheader>minbleicsetprecscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification of the preconditioner: scale-based diagonal preconditioning.

This preconditioning mode can be useful when you  don't  have  approximate
diagonal of Hessian, but you know that your  variables  are  badly  scaled
(for  example,  one  variable is in [1,10], and another in [1000,100000]),
and most part of the ill-conditioning comes from different scales of vars.

In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
can greatly improve convergence.

IMPRTANT: you should set scale of your variables  with  MinBLEICSetScale()
call  (before  or after MinBLEICSetPrecScale() call). Without knowledge of
the scale of your variables scale-based preconditioner will be  just  unit
matrix.

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetprecscale(minbleicstate state);
</pre>
<a name=sub_minbleicsetscale></a><h6 class=pageheader>minbleicsetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients for BLEIC optimizer.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison with tolerances).  Scale of
the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the function

Scaling is also used by finite difference variant of the optimizer  - step
along I-th axis is equal to DiffStep*S[I].

In  most  optimizers  (and  in  the  BLEIC  too)  scaling is NOT a form of
preconditioning. It just  affects  stopping  conditions.  You  should  set
preconditioner  by  separate  call  to  one  of  the  MinBLEICSetPrec...()
functions.

There is a special  preconditioning  mode, however,  which  uses   scaling
coefficients to form diagonal preconditioning matrix. You  can  turn  this
mode on, if you want.   But  you should understand that scaling is not the
same thing as preconditioning - these are two different, although  related
forms of tuning solver.

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetscale(minbleicstate state, real_1d_array s);
</pre>
<a name=sub_minbleicsetstpmax></a><h6 class=pageheader>minbleicsetstpmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets maximum step length

IMPORTANT: this feature is hard to combine with preconditioning. You can't
set upper limit on step length, when you solve optimization  problem  with
linear (non-boundary) constraints AND preconditioner turned on.

When  non-boundary  constraints  are  present,  you  have to either a) use
preconditioner, or b) use upper limit on step length.  YOU CAN'T USE BOTH!
In this case algorithm will terminate with appropriate error code.

Inputs:
    State   -   structure which stores algorithm state
    StpMax  -   maximum step length, &ge; 0. Set StpMax to 0.0,  if you don't
                want to limit step length.

Use this subroutine when you optimize target function which contains exp()
or  other  fast  growing  functions,  and optimization algorithm makes too
large  steps  which  lead   to overflow. This function allows us to reject
steps  that  are  too  large  (and  therefore  expose  us  to the possible
overflow) without actually calculating function value at the x+stp*d.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetstpmax(minbleicstate state, <b>double</b> stpmax);
</pre>
<a name=sub_minbleicsetxrep></a><h6 class=pageheader>minbleicsetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to MinBLEICOptimize().
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetxrep(minbleicstate state, <b>bool</b> needxrep);
</pre>
<a name=example_minbleic_d_1></a><h6 class=pageheader>minbleic_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_grad(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
<font color=navy>// and its derivatives df/d0 and df/dx1</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
   grad[0] = 400*pow(x[0]+3,3);
   grad[1] = 4*pow(x[1]-3,3);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>//</font>
<font color=navy>// subject to box constraints</font>
<font color=navy>//</font>
<font color=navy>//     -1 &le; x &le; +1, -1 &le; y &le; +1</font>
<font color=navy>//</font>
<font color=navy>// using BLEIC optimizer with:</font>
<font color=navy>// * initial point x=[0,0]</font>
<font color=navy>// * unit scale being set <b>for</b> all variables (see minbleicsetscale <b>for</b> more info)</font>
<font color=navy>// * stopping criteria set to <font color=blue><b>&quot;terminate after short enough step&quot;</b></font></font>
<font color=navy>// * OptGuard integrity check being used to check problem statement</font>
<font color=navy>//   <b>for</b> some common errors like nonsmoothness or bad analytic gradient</font>
<font color=navy>//</font>
<font color=navy>// First, we create optimizer object and tune its properties:</font>
<font color=navy>// * set box constraints</font>
<font color=navy>// * set variable scales</font>
<font color=navy>// * set stopping criteria</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[-1,-1]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[+1,+1]&quot;</b></font>;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   minbleicstate state;
   minbleiccreate(x, state);
   minbleicsetbc(state, bndl, bndu);
   minbleicsetscale(state, s);
   minbleicsetcond(state, epsg, epsf, epsx, maxits);
<font color=navy>// Then we activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard monitor helps to catch common coding and problem statement</font>
<font color=navy>// issues, like:</font>
<font color=navy>// * discontinuity of the target function (C0 continuity violation)</font>
<font color=navy>// * nonsmoothness of the target function (C1 continuity violation)</font>
<font color=navy>// * erroneous analytic gradient, i.e. one inconsistent with actual</font>
<font color=navy>//   change in the target/constraints</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential <b>for</b> early prototyping stages because such</font>
<font color=navy>// problems often result in premature termination of the optimizer</font>
<font color=navy>// which is really hard to distinguish from the correct termination.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: GRADIENT VERIFICATION IS PERFORMED BY MEANS OF NUMERICAL</font>
<font color=navy>//            DIFFERENTIATION. DO NOT USE IT IN PRODUCTION CODE!!!!!!!</font>
<font color=navy>//</font>
<font color=navy>//            Other OptGuard checks add moderate overhead, but anyway</font>
<font color=navy>//            it is better to turn them off when they are not needed.</font>
   minbleicoptguardsmoothness(state);
   minbleicoptguardgradient(state, 0.001);
<font color=navy>// Optimize and evaluate results</font>
   minbleicreport rep;
   minbleicoptimize(state, function1_grad);
   minbleicresults(state, x, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 4</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-1,1]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// NOTE: want to test OptGuard? Try breaking the gradient - say, add</font>
<font color=navy>//       1.0 to some of its components.</font>
   optguardreport ogrep;
   minbleicoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minbleic_d_2></a><h6 class=pageheader>minbleic_d_2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_grad(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
<font color=navy>// and its derivatives df/d0 and df/dx1</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
   grad[0] = 400*pow(x[0]+3,3);
   grad[1] = 4*pow(x[1]-3,3);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>//</font>
<font color=navy>// subject to inequality constraints</font>
<font color=navy>//</font>
<font color=navy>// * x &ge; 2 (posed as general linear constraint),</font>
<font color=navy>// * x+y &ge; 6</font>
<font color=navy>//</font>
<font color=navy>// using BLEIC optimizer with</font>
<font color=navy>// * initial point x=[0,0]</font>
<font color=navy>// * unit scale being set <b>for</b> all variables (see minbleicsetscale <b>for</b> more info)</font>
<font color=navy>// * stopping criteria set to <font color=blue><b>&quot;terminate after short enough step&quot;</b></font></font>
<font color=navy>// * OptGuard integrity check being used to check problem statement</font>
<font color=navy>//   <b>for</b> some common errors like nonsmoothness or bad analytic gradient</font>
<font color=navy>//</font>
<font color=navy>// First, we create optimizer object and tune its properties:</font>
<font color=navy>// * set linear constraints</font>
<font color=navy>// * set variable scales</font>
<font color=navy>// * set stopping criteria</font>
   real_1d_array x = <font color=blue><b>&quot;[5,5]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_2d_array c = <font color=blue><b>&quot;[[1,0,2],[1,1,6]]&quot;</b></font>;
   integer_1d_array ct = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   minbleicstate state;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;

   minbleiccreate(x, state);
   minbleicsetlc(state, c, ct);
   minbleicsetscale(state, s);
   minbleicsetcond(state, epsg, epsf, epsx, maxits);
<font color=navy>// Then we activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard monitor helps to catch common coding and problem statement</font>
<font color=navy>// issues, like:</font>
<font color=navy>// * discontinuity of the target function (C0 continuity violation)</font>
<font color=navy>// * nonsmoothness of the target function (C1 continuity violation)</font>
<font color=navy>// * erroneous analytic gradient, i.e. one inconsistent with actual</font>
<font color=navy>//   change in the target/constraints</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential <b>for</b> early prototyping stages because such</font>
<font color=navy>// problems often result in premature termination of the optimizer</font>
<font color=navy>// which is really hard to distinguish from the correct termination.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: GRADIENT VERIFICATION IS PERFORMED BY MEANS OF NUMERICAL</font>
<font color=navy>//            DIFFERENTIATION. DO NOT USE IT IN PRODUCTION CODE!!!!!!!</font>
<font color=navy>//</font>
<font color=navy>//            Other OptGuard checks add moderate overhead, but anyway</font>
<font color=navy>//            it is better to turn them off when they are not needed.</font>
   minbleicoptguardsmoothness(state);
   minbleicoptguardgradient(state, 0.001);
<font color=navy>// Optimize and evaluate results</font>
   minbleicreport rep;
   minbleicoptimize(state, function1_grad);
   minbleicresults(state, x, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 4</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [2,4]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// NOTE: want to test OptGuard? Try breaking the gradient - say, add</font>
<font color=navy>//       1.0 to some of its components.</font>
   optguardreport ogrep;
   minbleicoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minbleic_numdiff></a><h6 class=pageheader>minbleic_numdiff Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_func(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>//</font>
<font color=navy>// subject to box constraints</font>
<font color=navy>//</font>
<font color=navy>//     -1 &le; x &le; +1, -1 &le; y &le; +1</font>
<font color=navy>//</font>
<font color=navy>// using BLEIC optimizer with:</font>
<font color=navy>// * numerical differentiation being used</font>
<font color=navy>// * initial point x=[0,0]</font>
<font color=navy>// * unit scale being set <b>for</b> all variables (see minbleicsetscale <b>for</b> more info)</font>
<font color=navy>// * stopping criteria set to <font color=blue><b>&quot;terminate after short enough step&quot;</b></font></font>
<font color=navy>// * OptGuard integrity check being used to check problem statement</font>
<font color=navy>//   <b>for</b> some common errors like nonsmoothness or bad analytic gradient</font>
<font color=navy>//</font>
<font color=navy>// First, we create optimizer object and tune its properties:</font>
<font color=navy>// * set box constraints</font>
<font color=navy>// * set variable scales</font>
<font color=navy>// * set stopping criteria</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[-1,-1]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[+1,+1]&quot;</b></font>;
   minbleicstate state;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   <b>double</b> diffstep = 1.0e-6;

   minbleiccreatef(x, diffstep, state);
   minbleicsetbc(state, bndl, bndu);
   minbleicsetscale(state, s);
   minbleicsetcond(state, epsg, epsf, epsx, maxits);
<font color=navy>// Then we activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// Numerical differentiation always produces <font color=blue><b>&quot;correct&quot;</b></font> gradient</font>
<font color=navy>// (with some truncation error, but unbiased). Thus, we just have</font>
<font color=navy>// to check smoothness properties of the target: C0 and C1 continuity.</font>
<font color=navy>//</font>
<font color=navy>// Sometimes user accidentally tries to solve nonsmooth problems</font>
<font color=navy>// with smooth optimizer. OptGuard helps to detect such situations</font>
<font color=navy>// early, at the prototyping stage.</font>
   minbleicoptguardsmoothness(state);
<font color=navy>// Optimize and evaluate results</font>
   minbleicreport rep;
   minbleicoptimize(state, function1_func);
   minbleicresults(state, x, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 4</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-1,1]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// Want to challenge OptGuard? Try to make your problem</font>
<font color=navy>// nonsmooth by replacing 100*(x+3)^4 by 100*|x+3| and</font>
<font color=navy>// re-run optimizer.</font>
   optguardreport ogrep;
   minbleicoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_mincg></a><h4 class=pageheader>8.8.3. mincg Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_mincgreport class=toc>mincgreport</a> |
<a href=#struct_mincgstate class=toc>mincgstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_mincgcreate class=toc>mincgcreate</a> |
<a href=#sub_mincgcreatef class=toc>mincgcreatef</a> |
<a href=#sub_mincgoptguardgradient class=toc>mincgoptguardgradient</a> |
<a href=#sub_mincgoptguardnonc1test0results class=toc>mincgoptguardnonc1test0results</a> |
<a href=#sub_mincgoptguardnonc1test1results class=toc>mincgoptguardnonc1test1results</a> |
<a href=#sub_mincgoptguardresults class=toc>mincgoptguardresults</a> |
<a href=#sub_mincgoptguardsmoothness class=toc>mincgoptguardsmoothness</a> |
<a href=#sub_mincgoptimize class=toc>mincgoptimize</a> |
<a href=#sub_mincgrequesttermination class=toc>mincgrequesttermination</a> |
<a href=#sub_mincgrestartfrom class=toc>mincgrestartfrom</a> |
<a href=#sub_mincgresults class=toc>mincgresults</a> |
<a href=#sub_mincgresultsbuf class=toc>mincgresultsbuf</a> |
<a href=#sub_mincgsetcgtype class=toc>mincgsetcgtype</a> |
<a href=#sub_mincgsetcond class=toc>mincgsetcond</a> |
<a href=#sub_mincgsetprecdefault class=toc>mincgsetprecdefault</a> |
<a href=#sub_mincgsetprecdiag class=toc>mincgsetprecdiag</a> |
<a href=#sub_mincgsetprecscale class=toc>mincgsetprecscale</a> |
<a href=#sub_mincgsetscale class=toc>mincgsetscale</a> |
<a href=#sub_mincgsetstpmax class=toc>mincgsetstpmax</a> |
<a href=#sub_mincgsetxrep class=toc>mincgsetxrep</a> |
<a href=#sub_mincgsuggeststep class=toc>mincgsuggeststep</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_mincg_d_1 class=toc>mincg_d_1</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization by CG</td></tr>
<tr align=left valign=top><td><a href=#example_mincg_d_2 class=toc>mincg_d_2</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization with additional settings and restarts</td></tr>
<tr align=left valign=top><td><a href=#example_mincg_numdiff class=toc>mincg_numdiff</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization by CG with numerical differentiation</td></tr>
</table>
</div>
<a name=struct_mincgreport></a><h6 class=pageheader>mincgreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure stores optimization report:
* IterationsCount           total number of inner iterations
* NFEV                      number of gradient evaluations
* TerminationType           termination type (see below)

TERMINATION CODES

TerminationType field contains completion code, which can be:
  -8    internal integrity control detected  infinite  or  NAN  values  in
        function/gradient. Abnormal termination signalled.
   1    relative function improvement is no more than EpsF.
   2    relative step is no more than EpsX.
   4    gradient norm is no more than EpsG
   5    MaxIts steps was taken
   7    stopping conditions are too stringent,
        further improvement is impossible,
        X contains best point found so far.
   8    terminated by user who called mincgrequesttermination(). X contains
        point which was &quot;current accepted&quot; when  termination  request  was
        submitted.

Other fields of this structure are not documented and should not be used!
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> mincgreport {
   ae_int_t iterationscount;
   ae_int_t nfev;
   ae_int_t terminationtype;
};
</pre>
<a name=struct_mincgstate></a><h6 class=pageheader>mincgstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores state of the nonlinear CG optimizer.

You should use ALGLIB functions to work with this object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> mincgstate {
   bool needf;
   bool needfg;
   bool xupdated;
   double f;
   real_1d_array g;
   real_1d_array x;
};
</pre>
<a name=sub_mincgcreate></a><h6 class=pageheader>mincgcreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
NONLINEAR CONJUGATE GRADIENT METHOD
The subroutine minimizes function F(x) of N arguments by using one of  the
nonlinear conjugate gradient methods.

These CG methods are globally convergent (even on non-convex functions) as
long as grad(f) is Lipschitz continuous in  a  some  neighborhood  of  the
L = { x : f(x) &le; f(x0) }.

REQUIREMENTS:
Algorithm will request following information during its operation:
* function value F and its gradient G (simultaneously) at given point X

USAGE:
1. User initializes algorithm state with MinCGCreate() call
2. User tunes solver parameters with MinCGSetCond(), MinCGSetStpMax() and
   other functions
3. User calls MinCGOptimize() function which takes algorithm  state   and
   pointer (delegate, etc.) to callback function which calculates F/G.
4. User calls MinCGResults() to get solution
5. Optionally, user may call MinCGRestartFrom() to solve another  problem
   with same N but another starting point and/or another function.
   MinCGRestartFrom() allows to reuse already initialized structure.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    X       -   starting point, array[0..N-1].

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 25.03.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgcreate(ae_int_t n, real_1d_array x, mincgstate &amp;state);
<b>void</b> mincgcreate(real_1d_array x, mincgstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mincg_d_1 class=nav>mincg_d_1</a> | <a href=#example_mincg_d_2 class=nav>mincg_d_2</a> ]</p>
<a name=sub_mincgcreatef></a><h6 class=pageheader>mincgcreatef Function</h6>
<hr width=600 align=left>
<pre class=narration>
The subroutine is finite difference variant of MinCGCreate(). It uses
finite differences in order to differentiate target function.

Description below contains information which is specific to this function
only. We recommend to read comments on MinCGCreate() in order to get more
information about creation of CG optimizer.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    X       -   starting point, array[0..N-1].
    DiffStep-   differentiation step, &gt; 0

Outputs:
    State   -   structure which stores algorithm state

NOTES:
1. algorithm uses 4-point central formula for differentiation.
2. differentiation step along I-th axis is equal to DiffStep*S[I] where
   S[] is scaling vector which can be set by MinCGSetScale() call.
3. we recommend you to use moderate values of  differentiation  step.  Too
   large step will result in too large truncation  errors, while too small
   step will result in too large numerical  errors.  1.0E-6  can  be  good
   value to start with.
4. Numerical  differentiation  is   very   inefficient  -   one   gradient
   calculation needs 4*N function evaluations. This function will work for
   any N - either small (1...10), moderate (10...100) or  large  (100...).
   However, performance penalty will be too severe for any N's except  for
   small ones.
   We should also say that code which relies on numerical  differentiation
   is  less  robust  and  precise.  L-BFGS  needs  exact  gradient values.
   Imprecise  gradient may slow down  convergence,  especially  on  highly
   nonlinear problems.
   Thus  we  recommend to use this function for fast prototyping on small-
   dimensional problems only, and to implement analytical gradient as soon
   as possible.
ALGLIB: Copyright 16.05.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgcreatef(ae_int_t n, real_1d_array x, <b>double</b> diffstep, mincgstate &amp;state);
<b>void</b> mincgcreatef(real_1d_array x, <b>double</b> diffstep, mincgstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mincg_numdiff class=nav>mincg_numdiff</a> ]</p>
<a name=sub_mincgoptguardgradient></a><h6 class=pageheader>mincgoptguardgradient Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates verification  of  the  user-supplied
analytic gradient.

Upon  activation  of  this  option  OptGuard  integrity  checker  performs
numerical differentiation of your target function  at  the  initial  point
(note: future versions may also perform check  at  the  final  point)  and
compares numerical gradient with analytic one provided by you.

If difference is too large, an error flag is set and optimization  session
continues. After optimization session is over, you can retrieve the report
which  stores  both  gradients  and  specific  components  highlighted  as
suspicious by the OptGuard.

The primary OptGuard report can be retrieved with mincgoptguardresults().

IMPORTANT: gradient check is a high-overhead option which  will  cost  you
           about 3*N additional function evaluations. In many cases it may
           cost as much as the rest of the optimization session.

           YOU SHOULD NOT USE IT IN THE PRODUCTION CODE UNLESS YOU WANT TO
           CHECK DERIVATIVES PROVIDED BY SOME THIRD PARTY.

NOTE: unlike previous incarnation of the gradient checking code,  OptGuard
      does NOT interrupt optimization even if it discovers bad gradient.

Inputs:
    State       -   structure used to store algorithm state
    TestStep    -   verification step used for numerical differentiation:
                    * TestStep=0 turns verification off
                    * TestStep &gt; 0 activates verification
                    You should carefully choose TestStep. Value  which  is
                    too large (so large that  function  behavior  is  non-
                    cubic at this scale) will lead  to  false  alarms. Too
                    short step will result in rounding  errors  dominating
                    numerical derivative.

                    You may use different step for different parameters by
                    means of setting scale with mincgsetscale().

==== EXPLANATION ====

In order to verify gradient algorithm performs following steps:
  * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    where X[i] is i-th component of the initial point and S[i] is a  scale
    of i-th parameter
  * F(X) is evaluated at these trial points
  * we perform one more evaluation in the middle point of the interval
  * we  build  cubic  model using function values and derivatives at trial
    points and we compare its prediction with actual value in  the  middle
    point
ALGLIB: Copyright 15.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgoptguardgradient(mincgstate state, <b>double</b> teststep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mincg_d_1 class=nav>mincg_d_1</a> ]</p>
<a name=sub_mincgoptguardnonc1test0results></a><h6 class=pageheader>mincgoptguardnonc1test0results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #0

Nonsmoothness (non-C1) test #0 studies  function  values  (not  gradient!)
obtained during line searches and monitors  behavior  of  the  directional
derivative estimate.

This test is less powerful than test #1, but it does  not  depend  on  the
gradient values and thus it is more robust against artifacts introduced by
numerical differentiation.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], f[] - arrays of length CNT which store step lengths and  function
  values at these points; f[i] is evaluated in x0+stp[i]*d.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #0 &quot;strong&quot; report
    LngRep  -   C1 test #0 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgoptguardnonc1test0results(mincgstate state, optguardnonc1test0report &amp;strrep, optguardnonc1test0report &amp;lngrep);
</pre>
<a name=sub_mincgoptguardnonc1test1results></a><h6 class=pageheader>mincgoptguardnonc1test1results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #1

Nonsmoothness (non-C1)  test  #1  studies  individual  components  of  the
gradient computed during line search.

When precise analytic gradient is provided this test is more powerful than
test #0  which  works  with  function  values  and  ignores  user-provided
gradient.  However,  test  #0  becomes  more   powerful   when   numerical
differentiation is employed (in such cases test #1 detects  higher  levels
of numerical noise and becomes too conservative).

This test also tells specific components of the gradient which violate  C1
continuity, which makes it more informative than #0, which just tells that
continuity is violated.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* vidx - is an index of the variable in [0,N) with nonsmooth derivative
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], g[] - arrays of length CNT which store step lengths and  gradient
  values at these points; g[i] is evaluated in  x0+stp[i]*d  and  contains
  vidx-th component of the gradient.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #1 &quot;strong&quot; report
    LngRep  -   C1 test #1 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgoptguardnonc1test1results(mincgstate state, optguardnonc1test1report &amp;strrep, optguardnonc1test1report &amp;lngrep);
</pre>
<a name=sub_mincgoptguardresults></a><h6 class=pageheader>mincgoptguardresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Results of OptGuard integrity check, should be called  after  optimization
session is over.

==== PRIMARY REPORT ====

OptGuard performs several checks which are intended to catch common errors
in the implementation of nonlinear function/gradient:
* incorrect analytic gradient
* discontinuous (non-C0) target functions (constraints)
* nonsmooth     (non-C1) target functions (constraints)

Each of these checks is activated with appropriate function:
* mincgoptguardgradient() for gradient verification
* mincgoptguardsmoothness() for C0/C1 checks

Following flags are set when these errors are suspected:
* rep.badgradsuspected, and additionally:
  * rep.badgradvidx for specific variable (gradient element) suspected
  * rep.badgradxbase, a point where gradient is tested
  * rep.badgraduser, user-provided gradient  (stored  as  2D  matrix  with
    single row in order to make  report  structure  compatible  with  more
    complex optimizers like MinNLC or MinLM)
  * rep.badgradnum,   reference    gradient    obtained    via   numerical
    differentiation (stored as  2D matrix with single row in order to make
    report structure compatible with more complex optimizers  like  MinNLC
    or MinLM)
* rep.nonc0suspected
* rep.nonc1suspected

==== ADDITIONAL REPORTS/LOGS ====

Several different tests are performed to catch C0/C1 errors, you can  find
out specific test signaled error by looking to:
* rep.nonc0test0positive, for non-C0 test #0
* rep.nonc1test0positive, for non-C1 test #0
* rep.nonc1test1positive, for non-C1 test #1

Additional information (including line search logs)  can  be  obtained  by
means of:
* mincgoptguardnonc1test0results()
* mincgoptguardnonc1test1results()
which return detailed error reports, specific points where discontinuities
were found, and so on.

Inputs:
    State   -   algorithm state

Outputs:
    Rep     -   generic OptGuard report;  more  detailed  reports  can  be
                retrieved with other functions.

NOTE: false negatives (nonsmooth problems are not identified as  nonsmooth
      ones) are possible although unlikely.

      The reason  is  that  you  need  to  make several evaluations around
      nonsmoothness  in  order  to  accumulate  enough  information  about
      function curvature. Say, if you start right from the nonsmooth point,
      optimizer simply won't get enough data to understand what  is  going
      wrong before it terminates due to abrupt changes in the  derivative.
      It is also  possible  that  &quot;unlucky&quot;  step  will  move  us  to  the
      termination too quickly.

      Our current approach is to have less than 0.1%  false  negatives  in
      our test examples  (measured  with  multiple  restarts  from  random
      points), and to have exactly 0% false positives.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgoptguardresults(mincgstate state, optguardreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mincg_d_1 class=nav>mincg_d_1</a> ]</p>
<a name=sub_mincgoptguardsmoothness></a><h6 class=pageheader>mincgoptguardsmoothness Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates nonsmoothness monitoring  option  of
the  OptGuard  integrity  checker. Smoothness  monitor  silently  observes
solution process and tries to detect ill-posed problems, i.e. ones with:
a) discontinuous target function (non-C0)
b) nonsmooth     target function (non-C1)

Smoothness monitoring does NOT interrupt optimization  even if it suspects
that your problem is nonsmooth. It just sets corresponding  flags  in  the
OptGuard report which can be retrieved after optimization is over.

Smoothness monitoring is a moderate overhead option which often adds  less
than 1% to the optimizer running time. Thus, you can use it even for large
scale problems.

NOTE: OptGuard does  NOT  guarantee  that  it  will  always  detect  C0/C1
      continuity violations.

      First, minor errors are hard to  catch - say, a 0.0001 difference in
      the model values at two sides of the gap may be due to discontinuity
      of the model - or simply because the model has changed.

      Second, C1-violations  are  especially  difficult  to  detect  in  a
      noninvasive way. The optimizer usually  performs  very  short  steps
      near the nonsmoothness, and differentiation  usually   introduces  a
      lot of numerical noise.  It  is  hard  to  tell  whether  some  tiny
      discontinuity in the slope is due to real nonsmoothness or just  due
      to numerical noise alone.

      Our top priority was to avoid false positives, so in some rare cases
      minor errors may went unnoticed (however, in most cases they can  be
      spotted with restart from different initial point).

Inputs:
    State   -   algorithm state
    Level   -   monitoring level:
                * 0 - monitoring is disabled
                * 1 - noninvasive low-overhead monitoring; function values
                      and/or gradients are recorded, but OptGuard does not
                      try to perform additional evaluations  in  order  to
                      get more information about suspicious locations.

==== EXPLANATION ====

One major source of headache during optimization  is  the  possibility  of
the coding errors in the target function/constraints (or their gradients).
Such  errors   most   often   manifest   themselves  as  discontinuity  or
nonsmoothness of the target/constraints.

Another frequent situation is when you try to optimize something involving
lots of min() and max() operations, i.e. nonsmooth target. Although not  a
coding error, it is nonsmoothness anyway - and smooth  optimizers  usually
stop right after encountering nonsmoothness, well before reaching solution.

OptGuard integrity checker helps you to catch such situations: it monitors
function values/gradients being passed  to  the  optimizer  and  tries  to
errors. Upon discovering suspicious pair of points it  raises  appropriate
flag (and allows you to continue optimization). When optimization is done,
you can study OptGuard result.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgoptguardsmoothness(mincgstate state, ae_int_t level);
<b>void</b> mincgoptguardsmoothness(mincgstate state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mincg_d_1 class=nav>mincg_d_1</a> ]</p>
<a name=sub_mincgoptimize></a><h6 class=pageheader>mincgoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear optimizer

These functions accept following parameters:
    state   -   algorithm state
    func    -   callback which calculates function (or merit function)
                value func at given point x
    grad    -   callback which calculates function (or merit function)
                value func and gradient grad at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL

NOTES:

1. This function has two different implementations: one which  uses  exact
   (analytical) user-supplied  gradient, and one which uses function value
   only  and  numerically  differentiates  function  in  order  to  obtain
   gradient.

   Depending  on  the  specific  function  used to create optimizer object
   (either MinCGCreate()  for analytical gradient  or  MinCGCreateF()  for
   numerical differentiation) you should  choose  appropriate  variant  of
   MinCGOptimize() - one which accepts function AND gradient or one  which
   accepts function ONLY.

   Be careful to choose variant of MinCGOptimize()  which  corresponds  to
   your optimization scheme! Table below lists different  combinations  of
   callback (function/gradient) passed  to  MinCGOptimize()  and  specific
   function used to create optimizer.

                  |         USER PASSED TO MinCGOptimize()
   CREATED WITH   |  function only   |  function and gradient
   ------------------------------------------------------------
   MinCGCreateF() |     work                FAIL
   MinCGCreate()  |     FAIL                work

   Here &quot;FAIL&quot; denotes inappropriate combinations  of  optimizer  creation
   function and MinCGOptimize() version. Attemps to use  such  combination
   (for  example,  to create optimizer with  MinCGCreateF()  and  to  pass
   gradient information to MinCGOptimize()) will lead to  exception  being
   thrown. Either  you  did  not  pass  gradient when it WAS needed or you
   passed gradient when it was NOT needed.
ALGLIB: Copyright 20.04.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgoptimize(mincgstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> mincgoptimize(mincgstate &amp;state, <b>void</b> (*grad)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mincg_d_1 class=nav>mincg_d_1</a> | <a href=#example_mincg_d_2 class=nav>mincg_d_2</a> | <a href=#example_mincg_numdiff class=nav>mincg_numdiff</a> ]</p>
<a name=sub_mincgrequesttermination></a><h6 class=pageheader>mincgrequesttermination Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine submits request for termination of running  optimizer.  It
should be called from user-supplied callback when user decides that it  is
time to &quot;smoothly&quot; terminate optimization process.  As  result,  optimizer
stops at point which was &quot;current accepted&quot; when termination  request  was
submitted and returns error code 8 (successful termination).

Inputs:
    State   -   optimizer structure

NOTE: after  request  for  termination  optimizer  may   perform   several
      additional calls to user-supplied callbacks. It does  NOT  guarantee
      to stop immediately - it just guarantees that these additional calls
      will be discarded later.

NOTE: calling this function on optimizer which is NOT running will have no
      effect.

NOTE: multiple calls to this function are possible. First call is counted,
      subsequent calls are silently ignored.
ALGLIB: Copyright 08.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgrequesttermination(mincgstate state);
</pre>
<a name=sub_mincgrestartfrom></a><h6 class=pageheader>mincgrestartfrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  subroutine  restarts  CG  algorithm from new point. All optimization
parameters are left unchanged.

This  function  allows  to  solve multiple  optimization  problems  (which
must have same number of dimensions) without object reallocation penalty.

Inputs:
    State   -   structure used to store algorithm state.
    X       -   new starting point.
ALGLIB: Copyright 30.07.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgrestartfrom(mincgstate state, real_1d_array x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mincg_d_2 class=nav>mincg_d_2</a> ]</p>
<a name=sub_mincgresults></a><h6 class=pageheader>mincgresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Conjugate gradient results

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[0..N-1], solution
    Rep     -   optimization report:
                * Rep.TerminationType completetion code:
                    * -8    internal integrity control  detected  infinite
                            or NAN values in  function/gradient.  Abnormal
                            termination signalled.
                    *  1    relative function improvement is no more than
                            EpsF.
                    *  2    relative step is no more than EpsX.
                    *  4    gradient norm is no more than EpsG
                    *  5    MaxIts steps was taken
                    *  7    stopping conditions are too stringent,
                            further improvement is impossible,
                            we return best X found so far
                    *  8    terminated by user
                * Rep.IterationsCount contains iterations count
                * NFEV countains number of function calculations
ALGLIB: Copyright 20.04.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgresults(mincgstate state, real_1d_array &amp;x, mincgreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mincg_d_1 class=nav>mincg_d_1</a> | <a href=#example_mincg_d_2 class=nav>mincg_d_2</a> | <a href=#example_mincg_numdiff class=nav>mincg_numdiff</a> ]</p>
<a name=sub_mincgresultsbuf></a><h6 class=pageheader>mincgresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Conjugate gradient results

Buffered implementation of MinCGResults(), which uses pre-allocated buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 20.04.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgresultsbuf(mincgstate state, real_1d_array &amp;x, mincgreport &amp;rep);
</pre>
<a name=sub_mincgsetcgtype></a><h6 class=pageheader>mincgsetcgtype Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets CG algorithm.

Inputs:
    State   -   structure which stores algorithm state
    CGType  -   algorithm type:
                * -1    automatic selection of the best algorithm
                * 0     DY (Dai and Yuan) algorithm
                * 1     Hybrid DY-HS algorithm
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgsetcgtype(mincgstate state, ae_int_t cgtype);
</pre>
<a name=sub_mincgsetcond></a><h6 class=pageheader>mincgsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping conditions for CG optimization algorithm.

Inputs:
    State   -   structure which stores algorithm state
    EpsG    - &ge; 0
                The  subroutine  finishes  its  work   if   the  condition
                |v| &lt; EpsG is satisfied, where:
                * |.| means Euclidian norm
                * v - scaled gradient vector, v[i]=g[i]*s[i]
                * g - gradient
                * s - scaling coefficients set by MinCGSetScale()
    EpsF    - &ge; 0
                The  subroutine  finishes  its work if on k+1-th iteration
                the  condition  |F(k+1)-F(k)| &le; EpsF*max{|F(k)|,|F(k+1)|,1}
                is satisfied.
    EpsX    - &ge; 0
                The subroutine finishes its work if  on  k+1-th  iteration
                the condition |v| &le; EpsX is fulfilled, where:
                * |.| means Euclidian norm
                * v - scaled step vector, v[i]=dx[i]/s[i]
                * dx - ste pvector, dx=X(k+1)-X(k)
                * s - scaling coefficients set by MinCGSetScale()
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations is unlimited.

Passing EpsG=0, EpsF=0, EpsX=0 and MaxIts=0 (simultaneously) will lead to
automatic stopping criterion selection (small EpsX).
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgsetcond(mincgstate state, <b>double</b> epsg, <b>double</b> epsf, <b>double</b> epsx, ae_int_t maxits);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_mincg_d_1 class=nav>mincg_d_1</a> | <a href=#example_mincg_d_2 class=nav>mincg_d_2</a> | <a href=#example_mincg_numdiff class=nav>mincg_numdiff</a> ]</p>
<a name=sub_mincgsetprecdefault></a><h6 class=pageheader>mincgsetprecdefault Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification of the preconditioner: preconditioning is turned off.

Inputs:
    State   -   structure which stores algorithm state

NOTE:  you  can  change  preconditioner  &quot;on  the  fly&quot;,  during algorithm
iterations.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgsetprecdefault(mincgstate state);
</pre>
<a name=sub_mincgsetprecdiag></a><h6 class=pageheader>mincgsetprecdiag Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification  of  the  preconditioner:  diagonal of approximate Hessian is
used.

Inputs:
    State   -   structure which stores algorithm state
    D       -   diagonal of the approximate Hessian, array[0..N-1],
                (if larger, only leading N elements are used).

NOTE:  you  can  change  preconditioner  &quot;on  the  fly&quot;,  during algorithm
iterations.

NOTE 2: D[i] should be positive. Exception will be thrown otherwise.

NOTE 3: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgsetprecdiag(mincgstate state, real_1d_array d);
</pre>
<a name=sub_mincgsetprecscale></a><h6 class=pageheader>mincgsetprecscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification of the preconditioner: scale-based diagonal preconditioning.

This preconditioning mode can be useful when you  don't  have  approximate
diagonal of Hessian, but you know that your  variables  are  badly  scaled
(for  example,  one  variable is in [1,10], and another in [1000,100000]),
and most part of the ill-conditioning comes from different scales of vars.

In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
can greatly improve convergence.

IMPRTANT: you should set scale of your variables with MinCGSetScale() call
(before or after MinCGSetPrecScale() call). Without knowledge of the scale
of your variables scale-based preconditioner will be just unit matrix.

Inputs:
    State   -   structure which stores algorithm state

NOTE:  you  can  change  preconditioner  &quot;on  the  fly&quot;,  during algorithm
iterations.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgsetprecscale(mincgstate state);
</pre>
<a name=sub_mincgsetscale></a><h6 class=pageheader>mincgsetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients for CG optimizer.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison with tolerances).  Scale of
the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the function

Scaling is also used by finite difference variant of CG optimizer  -  step
along I-th axis is equal to DiffStep*S[I].

In   most   optimizers  (and  in  the  CG  too)  scaling is NOT a form  of
preconditioning. It just  affects  stopping  conditions.  You  should  set
preconditioner by separate call to one of the MinCGSetPrec...() functions.

There  is  special  preconditioning  mode, however,  which  uses   scaling
coefficients to form diagonal preconditioning matrix. You  can  turn  this
mode on, if you want.   But  you should understand that scaling is not the
same thing as preconditioning - these are two different, although  related
forms of tuning solver.

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgsetscale(mincgstate state, real_1d_array s);
</pre>
<a name=sub_mincgsetstpmax></a><h6 class=pageheader>mincgsetstpmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets maximum step length

Inputs:
    State   -   structure which stores algorithm state
    StpMax  -   maximum step length, &ge; 0. Set StpMax to 0.0,  if you don't
                want to limit step length.

Use this subroutine when you optimize target function which contains exp()
or  other  fast  growing  functions,  and optimization algorithm makes too
large  steps  which  leads  to overflow. This function allows us to reject
steps  that  are  too  large  (and  therefore  expose  us  to the possible
overflow) without actually calculating function value at the x+stp*d.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgsetstpmax(mincgstate state, <b>double</b> stpmax);
</pre>
<a name=sub_mincgsetxrep></a><h6 class=pageheader>mincgsetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to MinCGOptimize().
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgsetxrep(mincgstate state, <b>bool</b> needxrep);
</pre>
<a name=sub_mincgsuggeststep></a><h6 class=pageheader>mincgsuggeststep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function allows to suggest initial step length to the CG algorithm.

Suggested  step  length  is used as starting point for the line search. It
can be useful when you have  badly  scaled  problem,  i.e.  when  ||grad||
(which is used as initial estimate for the first step) is many  orders  of
magnitude different from the desired step.

Line search  may  fail  on  such problems without good estimate of initial
step length. Imagine, for example, problem with ||grad||=10^50 and desired
step equal to 0.1 Line  search function will use 10^50  as  initial  step,
then  it  will  decrease step length by 2 (up to 20 attempts) and will get
10^44, which is still too large.

This function allows us to tell than line search should  be  started  from
some moderate step length, like 1.0, so algorithm will be able  to  detect
desired step length in a several searches.

Default behavior (when no step is suggested) is to use preconditioner,  if
it is available, to generate initial estimate of step length.

This function influences only first iteration of algorithm. It  should  be
called between MinCGCreate/MinCGRestartFrom() call and MinCGOptimize call.
Suggested step is ignored if you have preconditioner.

Inputs:
    State   -   structure used to store algorithm state.
    Stp     -   initial estimate of the step length.
                Can be zero (no estimate).
ALGLIB: Copyright 30.07.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mincgsuggeststep(mincgstate state, <b>double</b> stp);
</pre>
<a name=example_mincg_d_1></a><h6 class=pageheader>mincg_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_grad(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
<font color=navy>// and its derivatives df/d0 and df/dx1</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
   grad[0] = 400*pow(x[0]+3,3);
   grad[1] = 4*pow(x[1]-3,3);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>//</font>
<font color=navy>// using nonlinear conjugate gradient method with:</font>
<font color=navy>// * initial point x=[0,0]</font>
<font color=navy>// * unit scale being set <b>for</b> all variables (see mincgsetscale <b>for</b> more info)</font>
<font color=navy>// * stopping criteria set to <font color=blue><b>&quot;terminate after short enough step&quot;</b></font></font>
<font color=navy>// * OptGuard integrity check being used to check problem statement</font>
<font color=navy>//   <b>for</b> some common errors like nonsmoothness or bad analytic gradient</font>
<font color=navy>//</font>
<font color=navy>// First, we create optimizer object and tune its properties</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.0000000001;
   ae_int_t maxits = 0;
   mincgstate state;
   mincgcreate(x, state);
   mincgsetcond(state, epsg, epsf, epsx, maxits);
   mincgsetscale(state, s);
<font color=navy>// Activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard monitor helps to catch common coding and problem statement</font>
<font color=navy>// issues, like:</font>
<font color=navy>// * discontinuity of the target function (C0 continuity violation)</font>
<font color=navy>// * nonsmoothness of the target function (C1 continuity violation)</font>
<font color=navy>// * erroneous analytic gradient, i.e. one inconsistent with actual</font>
<font color=navy>//   change in the target/constraints</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential <b>for</b> early prototyping stages because such</font>
<font color=navy>// problems often result in premature termination of the optimizer</font>
<font color=navy>// which is really hard to distinguish from the correct termination.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: GRADIENT VERIFICATION IS PERFORMED BY MEANS OF NUMERICAL</font>
<font color=navy>//            DIFFERENTIATION. DO NOT USE IT IN PRODUCTION CODE!!!!!!!</font>
<font color=navy>//</font>
<font color=navy>//            Other OptGuard checks add moderate overhead, but anyway</font>
<font color=navy>//            it is better to turn them off when they are not needed.</font>
   mincgoptguardsmoothness(state);
   mincgoptguardgradient(state, 0.001);
<font color=navy>// Optimize and evaluate results</font>
   mincgreport rep;
   mincgoptimize(state, function1_grad);
   mincgresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,3]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// NOTE: want to test OptGuard? Try breaking the gradient - say, add</font>
<font color=navy>//       1.0 to some of its components.</font>
   optguardreport ogrep;
   mincgoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=example_mincg_d_2></a><h6 class=pageheader>mincg_d_2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_grad(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
<font color=navy>// and its derivatives df/d0 and df/dx1</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
   grad[0] = 400*pow(x[0]+3,3);
   grad[1] = 4*pow(x[1]-3,3);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>// with nonlinear conjugate gradient method.</font>
<font color=navy>//</font>
<font color=navy>// Several advanced techniques are demonstrated:</font>
<font color=navy>// * upper limit on step size</font>
<font color=navy>// * restart from new point</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.0000000001;
   <b>double</b> stpmax = 0.1;
   ae_int_t maxits = 0;
   mincgstate state;
   mincgreport rep;

<font color=navy>// create and tune optimizer</font>
   mincgcreate(x, state);
   mincgsetscale(state, s);
   mincgsetcond(state, epsg, epsf, epsx, maxits);
   mincgsetstpmax(state, stpmax);

<font color=navy>// Set up OptGuard integrity checker which catches errors</font>
<font color=navy>// like nonsmooth targets or errors in the analytic gradient.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential at the early prototyping stages.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: gradient verification needs 3*N additional function</font>
<font color=navy>//       evaluations; DO NOT USE IT IN THE PRODUCTION CODE</font>
<font color=navy>//       because it leads to unnecessary slowdown of your app.</font>
   mincgoptguardsmoothness(state);
   mincgoptguardgradient(state, 0.001);

<font color=navy>// first run</font>
   mincgoptimize(state, function1_grad);
   mincgresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,3]</font>

<font color=navy>// second run - algorithm is restarted with mincgrestartfrom()</font>
   x = <font color=blue><b>&quot;[10,10]&quot;</b></font>;
   mincgrestartfrom(state, x);
   mincgoptimize(state, function1_grad);
   mincgresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,3]</font>

<font color=navy>// check OptGuard integrity report. Why <b>do</b> we need it at all?</font>
<font color=navy>// Well, try breaking the gradient by adding 1.0 to some</font>
<font color=navy>// of its components - OptGuard should report it as error.</font>
<font color=navy>// And it may also catch unintended errors too :)</font>
   optguardreport ogrep;
   mincgoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=example_mincg_numdiff></a><h6 class=pageheader>mincg_numdiff Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_func(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>//</font>
<font color=navy>// using numerical differentiation to calculate gradient.</font>
<font color=navy>//</font>
<font color=navy>// We also show how to use OptGuard integrity checker to catch common</font>
<font color=navy>// problem statement errors like accidentally specifying nonsmooth target</font>
<font color=navy>// function.</font>
<font color=navy>//</font>
<font color=navy>// First, we set up optimizer...</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.0000000001;
   <b>double</b> diffstep = 1.0e-6;
   ae_int_t maxits = 0;
   mincgstate state;
   mincgcreatef(x, diffstep, state);
   mincgsetcond(state, epsg, epsf, epsx, maxits);
   mincgsetscale(state, s);
<font color=navy>// Then, we activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// Numerical differentiation always produces <font color=blue><b>&quot;correct&quot;</b></font> gradient</font>
<font color=navy>// (with some truncation error, but unbiased). Thus, we just have</font>
<font color=navy>// to check smoothness properties of the target: C0 and C1 continuity.</font>
<font color=navy>//</font>
<font color=navy>// Sometimes user accidentally tried to solve nonsmooth problems</font>
<font color=navy>// with smooth optimizer. OptGuard helps to detect such situations</font>
<font color=navy>// early, at the prototyping stage.</font>
   mincgoptguardsmoothness(state);
<font color=navy>// Now we are ready to run the optimization</font>
   mincgreport rep;
   mincgoptimize(state, function1_func);
   mincgresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,3]</font>
<font color=navy>//</font>
<font color=navy>// ...and to check OptGuard integrity report.</font>
<font color=navy>//</font>
<font color=navy>// Want to challenge OptGuard? Try to make your problem</font>
<font color=navy>// nonsmooth by replacing 100*(x+3)^4 by 100*|x+3| and</font>
<font color=navy>// re-run optimizer.</font>
   optguardreport ogrep;
   mincgoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_mincomp></a><h4 class=pageheader>8.8.4. mincomp Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_minasareport class=toc>minasareport</a> |
<a href=#struct_minasastate class=toc>minasastate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_minasacreate class=toc>minasacreate</a> |
<a href=#sub_minasaoptimize class=toc>minasaoptimize</a> |
<a href=#sub_minasarestartfrom class=toc>minasarestartfrom</a> |
<a href=#sub_minasaresults class=toc>minasaresults</a> |
<a href=#sub_minasaresultsbuf class=toc>minasaresultsbuf</a> |
<a href=#sub_minasasetalgorithm class=toc>minasasetalgorithm</a> |
<a href=#sub_minasasetcond class=toc>minasasetcond</a> |
<a href=#sub_minasasetstpmax class=toc>minasasetstpmax</a> |
<a href=#sub_minasasetxrep class=toc>minasasetxrep</a> |
<a href=#sub_minbleicsetbarrierdecay class=toc>minbleicsetbarrierdecay</a> |
<a href=#sub_minbleicsetbarrierwidth class=toc>minbleicsetbarrierwidth</a> |
<a href=#sub_minlbfgssetcholeskypreconditioner class=toc>minlbfgssetcholeskypreconditioner</a> |
<a href=#sub_minlbfgssetdefaultpreconditioner class=toc>minlbfgssetdefaultpreconditioner</a>
]</font>
</div>
<a name=struct_minasareport></a><h6 class=pageheader>minasareport Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> minasareport {
   ae_int_t iterationscount;
   ae_int_t nfev;
   ae_int_t terminationtype;
   ae_int_t activeconstraints;
};
</pre>
<a name=struct_minasastate></a><h6 class=pageheader>minasastate Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> minasastate {
   bool needfg;
   bool xupdated;
   double f;
   real_1d_array g;
   real_1d_array x;
};
</pre>
<a name=sub_minasacreate></a><h6 class=pageheader>minasacreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete optimization algorithm.
Was replaced by MinBLEIC subpackage.
ALGLIB: Copyright 25.03.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minasacreate(ae_int_t n, real_1d_array x, real_1d_array bndl, real_1d_array bndu, minasastate &amp;state);
<b>void</b> minasacreate(real_1d_array x, real_1d_array bndl, real_1d_array bndu, minasastate &amp;state);
</pre>
<a name=sub_minasaoptimize></a><h6 class=pageheader>minasaoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear optimizer

These functions accept following parameters:
    state   -   algorithm state
    grad    -   callback which calculates function (or merit function)
                value func and gradient grad at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL
ALGLIB: Copyright 20.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minasaoptimize(minasastate &amp;state, <b>void</b> (*grad)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<a name=sub_minasarestartfrom></a><h6 class=pageheader>minasarestartfrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete optimization algorithm.
Was replaced by MinBLEIC subpackage.
ALGLIB: Copyright 30.07.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minasarestartfrom(minasastate state, real_1d_array x, real_1d_array bndl, real_1d_array bndu);
</pre>
<a name=sub_minasaresults></a><h6 class=pageheader>minasaresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete optimization algorithm.
Was replaced by MinBLEIC subpackage.
ALGLIB: Copyright 20.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minasaresults(minasastate state, real_1d_array &amp;x, minasareport &amp;rep);
</pre>
<a name=sub_minasaresultsbuf></a><h6 class=pageheader>minasaresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete optimization algorithm.
Was replaced by MinBLEIC subpackage.
ALGLIB: Copyright 20.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minasaresultsbuf(minasastate state, real_1d_array &amp;x, minasareport &amp;rep);
</pre>
<a name=sub_minasasetalgorithm></a><h6 class=pageheader>minasasetalgorithm Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete optimization algorithm.
Was replaced by MinBLEIC subpackage.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minasasetalgorithm(minasastate state, ae_int_t algotype);
</pre>
<a name=sub_minasasetcond></a><h6 class=pageheader>minasasetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete optimization algorithm.
Was replaced by MinBLEIC subpackage.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minasasetcond(minasastate state, <b>double</b> epsg, <b>double</b> epsf, <b>double</b> epsx, ae_int_t maxits);
</pre>
<a name=sub_minasasetstpmax></a><h6 class=pageheader>minasasetstpmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete optimization algorithm.
Was replaced by MinBLEIC subpackage.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minasasetstpmax(minasastate state, <b>double</b> stpmax);
</pre>
<a name=sub_minasasetxrep></a><h6 class=pageheader>minasasetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete optimization algorithm.
Was replaced by MinBLEIC subpackage.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minasasetxrep(minasastate state, <b>bool</b> needxrep);
</pre>
<a name=sub_minbleicsetbarrierdecay></a><h6 class=pageheader>minbleicsetbarrierdecay Function</h6>
<hr width=600 align=left>
<pre class=narration>
This is obsolete function which was used by previous version of the  BLEIC
optimizer. It does nothing in the current version of BLEIC.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetbarrierdecay(minbleicstate state, <b>double</b> mudecay);
</pre>
<a name=sub_minbleicsetbarrierwidth></a><h6 class=pageheader>minbleicsetbarrierwidth Function</h6>
<hr width=600 align=left>
<pre class=narration>
This is obsolete function which was used by previous version of the  BLEIC
optimizer. It does nothing in the current version of BLEIC.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minbleicsetbarrierwidth(minbleicstate state, <b>double</b> mu);
</pre>
<a name=sub_minlbfgssetcholeskypreconditioner></a><h6 class=pageheader>minlbfgssetcholeskypreconditioner Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete function, use MinLBFGSSetCholeskyPreconditioner() instead.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetcholeskypreconditioner(minlbfgsstate state, real_2d_array p, <b>bool</b> isupper);
</pre>
<a name=sub_minlbfgssetdefaultpreconditioner></a><h6 class=pageheader>minlbfgssetdefaultpreconditioner Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete function, use MinLBFGSSetPrecDefault() instead.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetdefaultpreconditioner(minlbfgsstate state);
</pre>
<a name=unit_minlbfgs></a><h4 class=pageheader>8.8.5. minlbfgs Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_minlbfgsreport class=toc>minlbfgsreport</a> |
<a href=#struct_minlbfgsstate class=toc>minlbfgsstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_minlbfgscreate class=toc>minlbfgscreate</a> |
<a href=#sub_minlbfgscreatef class=toc>minlbfgscreatef</a> |
<a href=#sub_minlbfgsoptguardgradient class=toc>minlbfgsoptguardgradient</a> |
<a href=#sub_minlbfgsoptguardnonc1test0results class=toc>minlbfgsoptguardnonc1test0results</a> |
<a href=#sub_minlbfgsoptguardnonc1test1results class=toc>minlbfgsoptguardnonc1test1results</a> |
<a href=#sub_minlbfgsoptguardresults class=toc>minlbfgsoptguardresults</a> |
<a href=#sub_minlbfgsoptguardsmoothness class=toc>minlbfgsoptguardsmoothness</a> |
<a href=#sub_minlbfgsoptimize class=toc>minlbfgsoptimize</a> |
<a href=#sub_minlbfgsrequesttermination class=toc>minlbfgsrequesttermination</a> |
<a href=#sub_minlbfgsrestartfrom class=toc>minlbfgsrestartfrom</a> |
<a href=#sub_minlbfgsresults class=toc>minlbfgsresults</a> |
<a href=#sub_minlbfgsresultsbuf class=toc>minlbfgsresultsbuf</a> |
<a href=#sub_minlbfgssetcond class=toc>minlbfgssetcond</a> |
<a href=#sub_minlbfgssetpreccholesky class=toc>minlbfgssetpreccholesky</a> |
<a href=#sub_minlbfgssetprecdefault class=toc>minlbfgssetprecdefault</a> |
<a href=#sub_minlbfgssetprecdiag class=toc>minlbfgssetprecdiag</a> |
<a href=#sub_minlbfgssetprecscale class=toc>minlbfgssetprecscale</a> |
<a href=#sub_minlbfgssetscale class=toc>minlbfgssetscale</a> |
<a href=#sub_minlbfgssetstpmax class=toc>minlbfgssetstpmax</a> |
<a href=#sub_minlbfgssetxrep class=toc>minlbfgssetxrep</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_minlbfgs_d_1 class=toc>minlbfgs_d_1</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization by L-BFGS</td></tr>
<tr align=left valign=top><td><a href=#example_minlbfgs_d_2 class=toc>minlbfgs_d_2</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization with additional settings and restarts</td></tr>
<tr align=left valign=top><td><a href=#example_minlbfgs_numdiff class=toc>minlbfgs_numdiff</a></td><td width=15>&nbsp;</td><td>Nonlinear optimization by L-BFGS with numerical differentiation</td></tr>
</table>
</div>
<a name=struct_minlbfgsreport></a><h6 class=pageheader>minlbfgsreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure stores optimization report:
* IterationsCount           total number of inner iterations
* NFEV                      number of gradient evaluations
* TerminationType           termination type (see below)

TERMINATION CODES

TerminationType field contains completion code, which can be:
  -8    internal integrity control detected  infinite  or  NAN  values  in
        function/gradient. Abnormal termination signalled.
   1    relative function improvement is no more than EpsF.
   2    relative step is no more than EpsX.
   4    gradient norm is no more than EpsG
   5    MaxIts steps was taken
   7    stopping conditions are too stringent,
        further improvement is impossible,
        X contains best point found so far.
   8    terminated    by  user  who  called  minlbfgsrequesttermination().
        X contains point which was   &quot;current accepted&quot;  when  termination
        request was submitted.

Other fields of this structure are not documented and should not be used!
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minlbfgsreport {
   ae_int_t iterationscount;
   ae_int_t nfev;
   ae_int_t terminationtype;
};
</pre>
<a name=struct_minlbfgsstate></a><h6 class=pageheader>minlbfgsstate Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> minlbfgsstate {
   bool needf;
   bool needfg;
   bool xupdated;
   double f;
   real_1d_array g;
   real_1d_array x;
};
</pre>
<a name=sub_minlbfgscreate></a><h6 class=pageheader>minlbfgscreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
LIMITED MEMORY BFGS METHOD FOR LARGE SCALE OPTIMIZATION
The subroutine minimizes function F(x) of N arguments by  using  a  quasi-
Newton method (LBFGS scheme) which is optimized to use  a  minimum  amount
of memory.
The subroutine generates the approximation of an inverse Hessian matrix by
using information about the last M steps of the algorithm  (instead of N).
It lessens a required amount of memory from a value  of  order  N^2  to  a
value of order 2*N*M.

REQUIREMENTS:
Algorithm will request following information during its operation:
* function value F and its gradient G (simultaneously) at given point X

USAGE:
1. User initializes algorithm state with MinLBFGSCreate() call
2. User tunes solver parameters with MinLBFGSSetCond() MinLBFGSSetStpMax()
   and other functions
3. User calls MinLBFGSOptimize() function which takes algorithm  state and
   pointer (delegate, etc.) to callback function which calculates F/G.
4. User calls MinLBFGSResults() to get solution
5. Optionally user may call MinLBFGSRestartFrom() to solve another problem
   with same N/M but another starting point and/or another function.
   MinLBFGSRestartFrom() allows to reuse already initialized structure.

Inputs:
    N       -   problem dimension. N &gt; 0
    M       -   number of corrections in the BFGS scheme of Hessian
                approximation update. Recommended value:  3 &le; M &le; 7. The smaller
                value causes worse convergence, the bigger will  not  cause  a
                considerably better convergence, but will cause a fall in  the
                performance. M &le; N.
    X       -   initial solution approximation, array[0..N-1].

Outputs:
    State   -   structure which stores algorithm state

NOTES:
1. you may tune stopping conditions with MinLBFGSSetCond() function
2. if target function contains exp() or other fast growing functions,  and
   optimization algorithm makes too large steps which leads  to  overflow,
   use MinLBFGSSetStpMax() function to bound algorithm's  steps.  However,
   L-BFGS rarely needs such a tuning.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgscreate(ae_int_t n, ae_int_t m, real_1d_array x, minlbfgsstate &amp;state);
<b>void</b> minlbfgscreate(ae_int_t m, real_1d_array x, minlbfgsstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlbfgs_d_1 class=nav>minlbfgs_d_1</a> | <a href=#example_minlbfgs_d_2 class=nav>minlbfgs_d_2</a> ]</p>
<a name=sub_minlbfgscreatef></a><h6 class=pageheader>minlbfgscreatef Function</h6>
<hr width=600 align=left>
<pre class=narration>
The subroutine is finite difference variant of MinLBFGSCreate().  It  uses
finite differences in order to differentiate target function.

Description below contains information which is specific to  this function
only. We recommend to read comments on MinLBFGSCreate() in  order  to  get
more information about creation of LBFGS optimizer.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    M       -   number of corrections in the BFGS scheme of Hessian
                approximation update. Recommended value:  3 &le; M &le; 7. The smaller
                value causes worse convergence, the bigger will  not  cause  a
                considerably better convergence, but will cause a fall in  the
                performance. M &le; N.
    X       -   starting point, array[0..N-1].
    DiffStep-   differentiation step, &gt; 0

Outputs:
    State   -   structure which stores algorithm state

NOTES:
1. algorithm uses 4-point central formula for differentiation.
2. differentiation step along I-th axis is equal to DiffStep*S[I] where
   S[] is scaling vector which can be set by MinLBFGSSetScale() call.
3. we recommend you to use moderate values of  differentiation  step.  Too
   large step will result in too large truncation  errors, while too small
   step will result in too large numerical  errors.  1.0E-6  can  be  good
   value to start with.
4. Numerical  differentiation  is   very   inefficient  -   one   gradient
   calculation needs 4*N function evaluations. This function will work for
   any N - either small (1...10), moderate (10...100) or  large  (100...).
   However, performance penalty will be too severe for any N's except  for
   small ones.
   We should also say that code which relies on numerical  differentiation
   is   less  robust  and  precise.  LBFGS  needs  exact  gradient values.
   Imprecise gradient may slow  down  convergence,  especially  on  highly
   nonlinear problems.
   Thus  we  recommend to use this function for fast prototyping on small-
   dimensional problems only, and to implement analytical gradient as soon
   as possible.
ALGLIB: Copyright 16.05.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgscreatef(ae_int_t n, ae_int_t m, real_1d_array x, <b>double</b> diffstep, minlbfgsstate &amp;state);
<b>void</b> minlbfgscreatef(ae_int_t m, real_1d_array x, <b>double</b> diffstep, minlbfgsstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlbfgs_numdiff class=nav>minlbfgs_numdiff</a> ]</p>
<a name=sub_minlbfgsoptguardgradient></a><h6 class=pageheader>minlbfgsoptguardgradient Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates verification  of  the  user-supplied
analytic gradient.

Upon  activation  of  this  option  OptGuard  integrity  checker  performs
numerical differentiation of your target function  at  the  initial  point
(note: future versions may also perform check  at  the  final  point)  and
compares numerical gradient with analytic one provided by you.

If difference is too large, an error flag is set and optimization  session
continues. After optimization session is over, you can retrieve the report
which  stores  both  gradients  and  specific  components  highlighted  as
suspicious by the OptGuard.

The primary OptGuard report can be retrieved with minlbfgsoptguardresults().

IMPORTANT: gradient check is a high-overhead option which  will  cost  you
           about 3*N additional function evaluations. In many cases it may
           cost as much as the rest of the optimization session.

           YOU SHOULD NOT USE IT IN THE PRODUCTION CODE UNLESS YOU WANT TO
           CHECK DERIVATIVES PROVIDED BY SOME THIRD PARTY.

NOTE: unlike previous incarnation of the gradient checking code,  OptGuard
      does NOT interrupt optimization even if it discovers bad gradient.

Inputs:
    State       -   structure used to store algorithm state
    TestStep    -   verification step used for numerical differentiation:
                    * TestStep=0 turns verification off
                    * TestStep &gt; 0 activates verification
                    You should carefully choose TestStep. Value  which  is
                    too large (so large that  function  behavior  is  non-
                    cubic at this scale) will lead  to  false  alarms. Too
                    short step will result in rounding  errors  dominating
                    numerical derivative.

                    You may use different step for different parameters by
                    means of setting scale with minlbfgssetscale().

==== EXPLANATION ====

In order to verify gradient algorithm performs following steps:
  * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    where X[i] is i-th component of the initial point and S[i] is a  scale
    of i-th parameter
  * F(X) is evaluated at these trial points
  * we perform one more evaluation in the middle point of the interval
  * we  build  cubic  model using function values and derivatives at trial
    points and we compare its prediction with actual value in  the  middle
    point
ALGLIB: Copyright 15.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsoptguardgradient(minlbfgsstate state, <b>double</b> teststep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlbfgs_d_1 class=nav>minlbfgs_d_1</a> ]</p>
<a name=sub_minlbfgsoptguardnonc1test0results></a><h6 class=pageheader>minlbfgsoptguardnonc1test0results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #0

Nonsmoothness (non-C1) test #0 studies  function  values  (not  gradient!)
obtained during line searches and monitors  behavior  of  the  directional
derivative estimate.

This test is less powerful than test #1, but it does  not  depend  on  the
gradient values and thus it is more robust against artifacts introduced by
numerical differentiation.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], f[] - arrays of length CNT which store step lengths and  function
  values at these points; f[i] is evaluated in x0+stp[i]*d.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #0 &quot;strong&quot; report
    LngRep  -   C1 test #0 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsoptguardnonc1test0results(minlbfgsstate state, optguardnonc1test0report &amp;strrep, optguardnonc1test0report &amp;lngrep);
</pre>
<a name=sub_minlbfgsoptguardnonc1test1results></a><h6 class=pageheader>minlbfgsoptguardnonc1test1results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #1

Nonsmoothness (non-C1)  test  #1  studies  individual  components  of  the
gradient computed during line search.

When precise analytic gradient is provided this test is more powerful than
test #0  which  works  with  function  values  and  ignores  user-provided
gradient.  However,  test  #0  becomes  more   powerful   when   numerical
differentiation is employed (in such cases test #1 detects  higher  levels
of numerical noise and becomes too conservative).

This test also tells specific components of the gradient which violate  C1
continuity, which makes it more informative than #0, which just tells that
continuity is violated.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* vidx - is an index of the variable in [0,N) with nonsmooth derivative
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], g[] - arrays of length CNT which store step lengths and  gradient
  values at these points; g[i] is evaluated in  x0+stp[i]*d  and  contains
  vidx-th component of the gradient.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #1 &quot;strong&quot; report
    LngRep  -   C1 test #1 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsoptguardnonc1test1results(minlbfgsstate state, optguardnonc1test1report &amp;strrep, optguardnonc1test1report &amp;lngrep);
</pre>
<a name=sub_minlbfgsoptguardresults></a><h6 class=pageheader>minlbfgsoptguardresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Results of OptGuard integrity check, should be called  after  optimization
session is over.

==== PRIMARY REPORT ====

OptGuard performs several checks which are intended to catch common errors
in the implementation of nonlinear function/gradient:
* incorrect analytic gradient
* discontinuous (non-C0) target functions (constraints)
* nonsmooth     (non-C1) target functions (constraints)

Each of these checks is activated with appropriate function:
* minlbfgsoptguardgradient() for gradient verification
* minlbfgsoptguardsmoothness() for C0/C1 checks

Following flags are set when these errors are suspected:
* rep.badgradsuspected, and additionally:
  * rep.badgradvidx for specific variable (gradient element) suspected
  * rep.badgradxbase, a point where gradient is tested
  * rep.badgraduser, user-provided gradient  (stored  as  2D  matrix  with
    single row in order to make  report  structure  compatible  with  more
    complex optimizers like MinNLC or MinLM)
  * rep.badgradnum,   reference    gradient    obtained    via   numerical
    differentiation (stored as  2D matrix with single row in order to make
    report structure compatible with more complex optimizers  like  MinNLC
    or MinLM)
* rep.nonc0suspected
* rep.nonc1suspected

==== ADDITIONAL REPORTS/LOGS ====

Several different tests are performed to catch C0/C1 errors, you can  find
out specific test signaled error by looking to:
* rep.nonc0test0positive, for non-C0 test #0
* rep.nonc1test0positive, for non-C1 test #0
* rep.nonc1test1positive, for non-C1 test #1

Additional information (including line search logs)  can  be  obtained  by
means of:
* minlbfgsoptguardnonc1test0results()
* minlbfgsoptguardnonc1test1results()
which return detailed error reports, specific points where discontinuities
were found, and so on.

Inputs:
    State   -   algorithm state

Outputs:
    Rep     -   generic OptGuard report;  more  detailed  reports  can  be
                retrieved with other functions.

NOTE: false negatives (nonsmooth problems are not identified as  nonsmooth
      ones) are possible although unlikely.

      The reason  is  that  you  need  to  make several evaluations around
      nonsmoothness  in  order  to  accumulate  enough  information  about
      function curvature. Say, if you start right from the nonsmooth point,
      optimizer simply won't get enough data to understand what  is  going
      wrong before it terminates due to abrupt changes in the  derivative.
      It is also  possible  that  &quot;unlucky&quot;  step  will  move  us  to  the
      termination too quickly.

      Our current approach is to have less than 0.1%  false  negatives  in
      our test examples  (measured  with  multiple  restarts  from  random
      points), and to have exactly 0% false positives.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsoptguardresults(minlbfgsstate state, optguardreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlbfgs_d_1 class=nav>minlbfgs_d_1</a> ]</p>
<a name=sub_minlbfgsoptguardsmoothness></a><h6 class=pageheader>minlbfgsoptguardsmoothness Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates nonsmoothness monitoring  option  of
the  OptGuard  integrity  checker. Smoothness  monitor  silently  observes
solution process and tries to detect ill-posed problems, i.e. ones with:
a) discontinuous target function (non-C0)
b) nonsmooth     target function (non-C1)

Smoothness monitoring does NOT interrupt optimization  even if it suspects
that your problem is nonsmooth. It just sets corresponding  flags  in  the
OptGuard report which can be retrieved after optimization is over.

Smoothness monitoring is a moderate overhead option which often adds  less
than 1% to the optimizer running time. Thus, you can use it even for large
scale problems.

NOTE: OptGuard does  NOT  guarantee  that  it  will  always  detect  C0/C1
      continuity violations.

      First, minor errors are hard to  catch - say, a 0.0001 difference in
      the model values at two sides of the gap may be due to discontinuity
      of the model - or simply because the model has changed.

      Second, C1-violations  are  especially  difficult  to  detect  in  a
      noninvasive way. The optimizer usually  performs  very  short  steps
      near the nonsmoothness, and differentiation  usually   introduces  a
      lot of numerical noise.  It  is  hard  to  tell  whether  some  tiny
      discontinuity in the slope is due to real nonsmoothness or just  due
      to numerical noise alone.

      Our top priority was to avoid false positives, so in some rare cases
      minor errors may went unnoticed (however, in most cases they can  be
      spotted with restart from different initial point).

Inputs:
    State   -   algorithm state
    Level   -   monitoring level:
                * 0 - monitoring is disabled
                * 1 - noninvasive low-overhead monitoring; function values
                      and/or gradients are recorded, but OptGuard does not
                      try to perform additional evaluations  in  order  to
                      get more information about suspicious locations.

==== EXPLANATION ====

One major source of headache during optimization  is  the  possibility  of
the coding errors in the target function/constraints (or their gradients).
Such  errors   most   often   manifest   themselves  as  discontinuity  or
nonsmoothness of the target/constraints.

Another frequent situation is when you try to optimize something involving
lots of min() and max() operations, i.e. nonsmooth target. Although not  a
coding error, it is nonsmoothness anyway - and smooth  optimizers  usually
stop right after encountering nonsmoothness, well before reaching solution.

OptGuard integrity checker helps you to catch such situations: it monitors
function values/gradients being passed  to  the  optimizer  and  tries  to
errors. Upon discovering suspicious pair of points it  raises  appropriate
flag (and allows you to continue optimization). When optimization is done,
you can study OptGuard result.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsoptguardsmoothness(minlbfgsstate state, ae_int_t level);
<b>void</b> minlbfgsoptguardsmoothness(minlbfgsstate state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlbfgs_d_1 class=nav>minlbfgs_d_1</a> ]</p>
<a name=sub_minlbfgsoptimize></a><h6 class=pageheader>minlbfgsoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear optimizer

These functions accept following parameters:
    state   -   algorithm state
    func    -   callback which calculates function (or merit function)
                value func at given point x
    grad    -   callback which calculates function (or merit function)
                value func and gradient grad at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL

NOTES:

1. This function has two different implementations: one which  uses  exact
   (analytical) user-supplied gradient,  and one which uses function value
   only  and  numerically  differentiates  function  in  order  to  obtain
   gradient.

   Depending  on  the  specific  function  used to create optimizer object
   (either MinLBFGSCreate() for analytical gradient  or  MinLBFGSCreateF()
   for numerical differentiation) you should choose appropriate variant of
   MinLBFGSOptimize() - one  which  accepts  function  AND gradient or one
   which accepts function ONLY.

   Be careful to choose variant of MinLBFGSOptimize() which corresponds to
   your optimization scheme! Table below lists different  combinations  of
   callback (function/gradient) passed to MinLBFGSOptimize()  and specific
   function used to create optimizer.

                     |         USER PASSED TO MinLBFGSOptimize()
   CREATED WITH      |  function only   |  function and gradient
   ------------------------------------------------------------
   MinLBFGSCreateF() |     work                FAIL
   MinLBFGSCreate()  |     FAIL                work

   Here &quot;FAIL&quot; denotes inappropriate combinations  of  optimizer  creation
   function  and  MinLBFGSOptimize()  version.   Attemps   to   use   such
   combination (for example, to create optimizer with MinLBFGSCreateF() and
   to pass gradient information to MinCGOptimize()) will lead to exception
   being thrown. Either  you  did  not pass gradient when it WAS needed or
   you passed gradient when it was NOT needed.
ALGLIB: Copyright 20.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsoptimize(minlbfgsstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> minlbfgsoptimize(minlbfgsstate &amp;state, <b>void</b> (*grad)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlbfgs_d_1 class=nav>minlbfgs_d_1</a> | <a href=#example_minlbfgs_d_2 class=nav>minlbfgs_d_2</a> | <a href=#example_minlbfgs_numdiff class=nav>minlbfgs_numdiff</a> ]</p>
<a name=sub_minlbfgsrequesttermination></a><h6 class=pageheader>minlbfgsrequesttermination Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine submits request for termination of running  optimizer.  It
should be called from user-supplied callback when user decides that it  is
time to &quot;smoothly&quot; terminate optimization process.  As  result,  optimizer
stops at point which was &quot;current accepted&quot; when termination  request  was
submitted and returns error code 8 (successful termination).

Inputs:
    State   -   optimizer structure

NOTE: after  request  for  termination  optimizer  may   perform   several
      additional calls to user-supplied callbacks. It does  NOT  guarantee
      to stop immediately - it just guarantees that these additional calls
      will be discarded later.

NOTE: calling this function on optimizer which is NOT running will have no
      effect.

NOTE: multiple calls to this function are possible. First call is counted,
      subsequent calls are silently ignored.
ALGLIB: Copyright 08.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsrequesttermination(minlbfgsstate state);
</pre>
<a name=sub_minlbfgsrestartfrom></a><h6 class=pageheader>minlbfgsrestartfrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  subroutine restarts LBFGS algorithm from new point. All optimization
parameters are left unchanged.

This  function  allows  to  solve multiple  optimization  problems  (which
must have same number of dimensions) without object reallocation penalty.

Inputs:
    State   -   structure used to store algorithm state
    X       -   new starting point.
ALGLIB: Copyright 30.07.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsrestartfrom(minlbfgsstate state, real_1d_array x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlbfgs_d_2 class=nav>minlbfgs_d_2</a> ]</p>
<a name=sub_minlbfgsresults></a><h6 class=pageheader>minlbfgsresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
L-BFGS algorithm results

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[0..N-1], solution
    Rep     -   optimization report:
                * Rep.TerminationType completetion code:
                    * -8    internal integrity control  detected  infinite
                            or NAN values in  function/gradient.  Abnormal
                            termination signalled.
                    * -2    rounding errors prevent further improvement.
                            X contains best point found.
                    * -1    incorrect parameters were specified
                    *  1    relative function improvement is no more than
                            EpsF.
                    *  2    relative step is no more than EpsX.
                    *  4    gradient norm is no more than EpsG
                    *  5    MaxIts steps was taken
                    *  7    stopping conditions are too stringent,
                            further improvement is impossible
                    *  8    terminated by user who called minlbfgsrequesttermination().
                            X contains point which was &quot;current accepted&quot; when
                            termination request was submitted.
                * Rep.IterationsCount contains iterations count
                * NFEV countains number of function calculations
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsresults(minlbfgsstate state, real_1d_array &amp;x, minlbfgsreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlbfgs_d_1 class=nav>minlbfgs_d_1</a> | <a href=#example_minlbfgs_d_2 class=nav>minlbfgs_d_2</a> | <a href=#example_minlbfgs_numdiff class=nav>minlbfgs_numdiff</a> ]</p>
<a name=sub_minlbfgsresultsbuf></a><h6 class=pageheader>minlbfgsresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
L-BFGS algorithm results

Buffered implementation of MinLBFGSResults which uses pre-allocated buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 20.08.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgsresultsbuf(minlbfgsstate state, real_1d_array &amp;x, minlbfgsreport &amp;rep);
</pre>
<a name=sub_minlbfgssetcond></a><h6 class=pageheader>minlbfgssetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping conditions for L-BFGS optimization algorithm.

Inputs:
    State   -   structure which stores algorithm state
    EpsG    - &ge; 0
                The  subroutine  finishes  its  work   if   the  condition
                |v| &lt; EpsG is satisfied, where:
                * |.| means Euclidian norm
                * v - scaled gradient vector, v[i]=g[i]*s[i]
                * g - gradient
                * s - scaling coefficients set by MinLBFGSSetScale()
    EpsF    - &ge; 0
                The  subroutine  finishes  its work if on k+1-th iteration
                the  condition  |F(k+1)-F(k)| &le; EpsF*max{|F(k)|,|F(k+1)|,1}
                is satisfied.
    EpsX    - &ge; 0
                The subroutine finishes its work if  on  k+1-th  iteration
                the condition |v| &le; EpsX is fulfilled, where:
                * |.| means Euclidian norm
                * v - scaled step vector, v[i]=dx[i]/s[i]
                * dx - ste pvector, dx=X(k+1)-X(k)
                * s - scaling coefficients set by MinLBFGSSetScale()
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations is unlimited.

Passing EpsG=0, EpsF=0, EpsX=0 and MaxIts=0 (simultaneously) will lead to
automatic stopping criterion selection (small EpsX).
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetcond(minlbfgsstate state, <b>double</b> epsg, <b>double</b> epsf, <b>double</b> epsx, ae_int_t maxits);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlbfgs_d_1 class=nav>minlbfgs_d_1</a> | <a href=#example_minlbfgs_d_2 class=nav>minlbfgs_d_2</a> | <a href=#example_minlbfgs_numdiff class=nav>minlbfgs_numdiff</a> ]</p>
<a name=sub_minlbfgssetpreccholesky></a><h6 class=pageheader>minlbfgssetpreccholesky Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification of the preconditioner: Cholesky factorization of  approximate
Hessian is used.

Inputs:
    State   -   structure which stores algorithm state
    P       -   triangular preconditioner, Cholesky factorization of
                the approximate Hessian. array[0..N-1,0..N-1],
                (if larger, only leading N elements are used).
    IsUpper -   whether upper or lower triangle of P is given
                (other triangle is not referenced)

After call to this function preconditioner is changed to P  (P  is  copied
into the internal buffer).

NOTE:  you  can  change  preconditioner  &quot;on  the  fly&quot;,  during algorithm
iterations.

NOTE 2:  P  should  be nonsingular. Exception will be thrown otherwise.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetpreccholesky(minlbfgsstate state, real_2d_array p, <b>bool</b> isupper);
</pre>
<a name=sub_minlbfgssetprecdefault></a><h6 class=pageheader>minlbfgssetprecdefault Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification  of  the  preconditioner:  default  preconditioner    (simple
scaling, same for all elements of X) is used.

Inputs:
    State   -   structure which stores algorithm state

NOTE:  you  can  change  preconditioner  &quot;on  the  fly&quot;,  during algorithm
iterations.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetprecdefault(minlbfgsstate state);
</pre>
<a name=sub_minlbfgssetprecdiag></a><h6 class=pageheader>minlbfgssetprecdiag Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification  of  the  preconditioner:  diagonal of approximate Hessian is
used.

Inputs:
    State   -   structure which stores algorithm state
    D       -   diagonal of the approximate Hessian, array[0..N-1],
                (if larger, only leading N elements are used).

NOTE:  you  can  change  preconditioner  &quot;on  the  fly&quot;,  during algorithm
iterations.

NOTE 2: D[i] should be positive. Exception will be thrown otherwise.

NOTE 3: you should pass diagonal of approximate Hessian - NOT ITS INVERSE.
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetprecdiag(minlbfgsstate state, real_1d_array d);
</pre>
<a name=sub_minlbfgssetprecscale></a><h6 class=pageheader>minlbfgssetprecscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modification of the preconditioner: scale-based diagonal preconditioning.

This preconditioning mode can be useful when you  don't  have  approximate
diagonal of Hessian, but you know that your  variables  are  badly  scaled
(for  example,  one  variable is in [1,10], and another in [1000,100000]),
and most part of the ill-conditioning comes from different scales of vars.

In this case simple  scale-based  preconditioner,  with H[i] = 1/(s[i]^2),
can greatly improve convergence.

IMPRTANT: you should set scale of your variables  with  MinLBFGSSetScale()
call  (before  or after MinLBFGSSetPrecScale() call). Without knowledge of
the scale of your variables scale-based preconditioner will be  just  unit
matrix.

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 13.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetprecscale(minlbfgsstate state);
</pre>
<a name=sub_minlbfgssetscale></a><h6 class=pageheader>minlbfgssetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients for LBFGS optimizer.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison with tolerances).  Scale of
the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the function

Scaling is also used by finite difference variant of the optimizer  - step
along I-th axis is equal to DiffStep*S[I].

In  most  optimizers  (and  in  the  LBFGS  too)  scaling is NOT a form of
preconditioning. It just  affects  stopping  conditions.  You  should  set
preconditioner  by  separate  call  to  one  of  the  MinLBFGSSetPrec...()
functions.

There  is  special  preconditioning  mode, however,  which  uses   scaling
coefficients to form diagonal preconditioning matrix. You  can  turn  this
mode on, if you want.   But  you should understand that scaling is not the
same thing as preconditioning - these are two different, although  related
forms of tuning solver.

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetscale(minlbfgsstate state, real_1d_array s);
</pre>
<a name=sub_minlbfgssetstpmax></a><h6 class=pageheader>minlbfgssetstpmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets maximum step length

Inputs:
    State   -   structure which stores algorithm state
    StpMax  -   maximum step length, &ge; 0. Set StpMax to 0.0 (default),  if
                you don't want to limit step length.

Use this subroutine when you optimize target function which contains exp()
or  other  fast  growing  functions,  and optimization algorithm makes too
large  steps  which  leads  to overflow. This function allows us to reject
steps  that  are  too  large  (and  therefore  expose  us  to the possible
overflow) without actually calculating function value at the x+stp*d.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetstpmax(minlbfgsstate state, <b>double</b> stpmax);
</pre>
<a name=sub_minlbfgssetxrep></a><h6 class=pageheader>minlbfgssetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to MinLBFGSOptimize().
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlbfgssetxrep(minlbfgsstate state, <b>bool</b> needxrep);
</pre>
<a name=example_minlbfgs_d_1></a><h6 class=pageheader>minlbfgs_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_grad(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
<font color=navy>// and its derivatives df/d0 and df/dx1</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
   grad[0] = 400*pow(x[0]+3,3);
   grad[1] = 4*pow(x[1]-3,3);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>//</font>
<font color=navy>// using LBFGS method, with:</font>
<font color=navy>// * initial point x=[0,0]</font>
<font color=navy>// * unit scale being set <b>for</b> all variables (see minlbfgssetscale <b>for</b> more info)</font>
<font color=navy>// * stopping criteria set to <font color=blue><b>&quot;terminate after short enough step&quot;</b></font></font>
<font color=navy>// * OptGuard integrity check being used to check problem statement</font>
<font color=navy>//   <b>for</b> some common errors like nonsmoothness or bad analytic gradient</font>
<font color=navy>//</font>
<font color=navy>// First, we create optimizer object and tune its properties</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.0000000001;
   ae_int_t maxits = 0;
   minlbfgsstate state;
   minlbfgscreate(1, x, state);
   minlbfgssetcond(state, epsg, epsf, epsx, maxits);
   minlbfgssetscale(state, s);
<font color=navy>// Activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard monitor helps to catch common coding and problem statement</font>
<font color=navy>// issues, like:</font>
<font color=navy>// * discontinuity of the target function (C0 continuity violation)</font>
<font color=navy>// * nonsmoothness of the target function (C1 continuity violation)</font>
<font color=navy>// * erroneous analytic gradient, i.e. one inconsistent with actual</font>
<font color=navy>//   change in the target/constraints</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential <b>for</b> early prototyping stages because such</font>
<font color=navy>// problems often result in premature termination of the optimizer</font>
<font color=navy>// which is really hard to distinguish from the correct termination.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: GRADIENT VERIFICATION IS PERFORMED BY MEANS OF NUMERICAL</font>
<font color=navy>//            DIFFERENTIATION. DO NOT USE IT IN PRODUCTION CODE!!!!!!!</font>
<font color=navy>//</font>
<font color=navy>//            Other OptGuard checks add moderate overhead, but anyway</font>
<font color=navy>//            it is better to turn them off when they are not needed.</font>
   minlbfgsoptguardsmoothness(state);
   minlbfgsoptguardgradient(state, 0.001);
<font color=navy>// Optimize and examine results.</font>
   minlbfgsreport rep;
   minlbfgsoptimize(state, function1_grad);
   minlbfgsresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,3]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// NOTE: want to test OptGuard? Try breaking the gradient - say, add</font>
<font color=navy>//       1.0 to some of its components.</font>
   optguardreport ogrep;
   minlbfgsoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minlbfgs_d_2></a><h6 class=pageheader>minlbfgs_d_2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_grad(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
<font color=navy>// and its derivatives df/d0 and df/dx1</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
   grad[0] = 400*pow(x[0]+3,3);
   grad[1] = 4*pow(x[1]-3,3);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>// using LBFGS method.</font>
<font color=navy>//</font>
<font color=navy>// Several advanced techniques are demonstrated:</font>
<font color=navy>// * upper limit on step size</font>
<font color=navy>// * restart from new point</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsg = 0;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0.0000000001;
   <b>double</b> stpmax = 0.1;
   ae_int_t maxits = 0;
   minlbfgsstate state;
   minlbfgsreport rep;

<font color=navy>// create and tune optimizer</font>
   minlbfgscreate(1, x, state);
   minlbfgssetcond(state, epsg, epsf, epsx, maxits);
   minlbfgssetstpmax(state, stpmax);
   minlbfgssetscale(state, s);

<font color=navy>// Set up OptGuard integrity checker which catches errors</font>
<font color=navy>// like nonsmooth targets or errors in the analytic gradient.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential at the early prototyping stages.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: gradient verification needs 3*N additional function</font>
<font color=navy>//       evaluations; DO NOT USE IT IN THE PRODUCTION CODE</font>
<font color=navy>//       because it leads to unnecessary slowdown of your app.</font>
   minlbfgsoptguardsmoothness(state);
   minlbfgsoptguardgradient(state, 0.001);

<font color=navy>// first run</font>
   minlbfgsoptimize(state, function1_grad);
   minlbfgsresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,3]</font>

<font color=navy>// second run - algorithm is restarted</font>
   x = <font color=blue><b>&quot;[10,10]&quot;</b></font>;
   minlbfgsrestartfrom(state, x);
   minlbfgsoptimize(state, function1_grad);
   minlbfgsresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,3]</font>

<font color=navy>// check OptGuard integrity report. Why <b>do</b> we need it at all?</font>
<font color=navy>// Well, try breaking the gradient by adding 1.0 to some</font>
<font color=navy>// of its components - OptGuard should report it as error.</font>
<font color=navy>// And it may also catch unintended errors too :)</font>
   optguardreport ogrep;
   minlbfgsoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minlbfgs_numdiff></a><h6 class=pageheader>minlbfgs_numdiff Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_func(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of f(x,y) = 100*(x+3)^4+(y-3)^4</font>
<font color=navy>// using numerical differentiation to calculate gradient.</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   <b>double</b> epsg = 0.0000000001;
   <b>double</b> epsf = 0;
   <b>double</b> epsx = 0;
   <b>double</b> diffstep = 1.0e-6;
   ae_int_t maxits = 0;
   minlbfgsstate state;
   minlbfgsreport rep;

   minlbfgscreatef(1, x, diffstep, state);
   minlbfgssetcond(state, epsg, epsf, epsx, maxits);
   minlbfgsoptimize(state, function1_func);
   minlbfgsresults(state, x, rep);

   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 4</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,3]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_minlm></a><h4 class=pageheader>8.8.6. minlm Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_minlmreport class=toc>minlmreport</a> |
<a href=#struct_minlmstate class=toc>minlmstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_minlmcreatefgh class=toc>minlmcreatefgh</a> |
<a href=#sub_minlmcreatefgj class=toc>minlmcreatefgj</a> |
<a href=#sub_minlmcreatefj class=toc>minlmcreatefj</a> |
<a href=#sub_minlmcreatev class=toc>minlmcreatev</a> |
<a href=#sub_minlmcreatevgj class=toc>minlmcreatevgj</a> |
<a href=#sub_minlmcreatevj class=toc>minlmcreatevj</a> |
<a href=#sub_minlmoptguardgradient class=toc>minlmoptguardgradient</a> |
<a href=#sub_minlmoptguardresults class=toc>minlmoptguardresults</a> |
<a href=#sub_minlmoptimize class=toc>minlmoptimize</a> |
<a href=#sub_minlmrequesttermination class=toc>minlmrequesttermination</a> |
<a href=#sub_minlmrestartfrom class=toc>minlmrestartfrom</a> |
<a href=#sub_minlmresults class=toc>minlmresults</a> |
<a href=#sub_minlmresultsbuf class=toc>minlmresultsbuf</a> |
<a href=#sub_minlmsetacctype class=toc>minlmsetacctype</a> |
<a href=#sub_minlmsetbc class=toc>minlmsetbc</a> |
<a href=#sub_minlmsetcond class=toc>minlmsetcond</a> |
<a href=#sub_minlmsetlc class=toc>minlmsetlc</a> |
<a href=#sub_minlmsetscale class=toc>minlmsetscale</a> |
<a href=#sub_minlmsetstpmax class=toc>minlmsetstpmax</a> |
<a href=#sub_minlmsetxrep class=toc>minlmsetxrep</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_minlm_d_fgh class=toc>minlm_d_fgh</a></td><td width=15>&nbsp;</td><td>Nonlinear Hessian-based optimization for general functions</td></tr>
<tr align=left valign=top><td><a href=#example_minlm_d_restarts class=toc>minlm_d_restarts</a></td><td width=15>&nbsp;</td><td>Efficient restarts of LM optimizer</td></tr>
<tr align=left valign=top><td><a href=#example_minlm_d_v class=toc>minlm_d_v</a></td><td width=15>&nbsp;</td><td>Nonlinear least squares optimization using function vector only</td></tr>
<tr align=left valign=top><td><a href=#example_minlm_d_vb class=toc>minlm_d_vb</a></td><td width=15>&nbsp;</td><td>Bound constrained nonlinear least squares optimization</td></tr>
<tr align=left valign=top><td><a href=#example_minlm_d_vj class=toc>minlm_d_vj</a></td><td width=15>&nbsp;</td><td>Nonlinear least squares optimization using function vector and Jacobian</td></tr>
</table>
</div>
<a name=struct_minlmreport></a><h6 class=pageheader>minlmreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
Optimization report, filled by MinLMResults() function

FIELDS:
* TerminationType, completetion code:
    * -8    optimizer detected NAN/INF values either in the function itself,
            or in its Jacobian
    * -5    inappropriate solver was used:
            * solver created with minlmcreatefgh() used  on  problem  with
              general linear constraints (set with minlmsetlc() call).
    * -3    constraints are inconsistent
    *  2    relative step is no more than EpsX.
    *  5    MaxIts steps was taken
    *  7    stopping conditions are too stringent,
            further improvement is impossible
    *  8    terminated   by  user  who  called  MinLMRequestTermination().
            X contains point which was &quot;current accepted&quot; when termination
            request was submitted.
* IterationsCount, contains iterations count
* NFunc, number of function calculations
* NJac, number of Jacobi matrix calculations
* NGrad, number of gradient calculations
* NHess, number of Hessian calculations
* NCholesky, number of Cholesky decomposition calculations
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minlmreport {
   ae_int_t iterationscount;
   ae_int_t terminationtype;
   ae_int_t nfunc;
   ae_int_t njac;
   ae_int_t ngrad;
   ae_int_t nhess;
   ae_int_t ncholesky;
};
</pre>
<a name=struct_minlmstate></a><h6 class=pageheader>minlmstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
Levenberg-Marquardt optimizer.

This structure should be created using one of the MinLMCreate???()
functions. You should not access its fields directly; use ALGLIB functions
to work with it.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minlmstate {
   bool needf;
   bool needfgh;
   bool needfi;
   bool needfij;
   bool xupdated;
   double f;
   real_1d_array fi;
   real_1d_array g;
   real_2d_array h;
   real_2d_array j;
   real_1d_array x;
};
</pre>
<a name=sub_minlmcreatefgh></a><h6 class=pageheader>minlmcreatefgh Function</h6>
<hr width=600 align=left>
<pre class=narration>
LEVENBERG-MARQUARDT-LIKE METHOD FOR NON-LINEAR OPTIMIZATION
This  function  is  used  to  find  minimum  of general form (not &quot;sum-of-
-squares&quot;) function
    F = F(x[0], ..., x[N-1])
using  its  gradient  and  Hessian.  Levenberg-Marquardt modification with
L-BFGS pre-optimization and internal pre-conditioned  L-BFGS  optimization
after each Levenberg-Marquardt step is used.

REQUIREMENTS:
This algorithm will request following information during its operation:

* function value F at given point X
* F and gradient G (simultaneously) at given point X
* F, G and Hessian H (simultaneously) at given point X

There are several overloaded versions of  MinLMOptimize()  function  which
correspond  to  different LM-like optimization algorithms provided by this
unit. You should choose version which accepts func(),  grad()  and  hess()
function pointers. First pointer is used to calculate F  at  given  point,
second  one  calculates  F(x)  and  grad F(x),  third one calculates F(x),
grad F(x), hess F(x).

You can try to initialize MinLMState structure with FGH-function and  then
use incorrect version of MinLMOptimize() (for example, version which  does
not provide Hessian matrix), but it will lead to  exception  being  thrown
after first attempt to calculate Hessian.

USAGE:
1. User initializes algorithm state with MinLMCreateFGH() call
2. User tunes solver parameters with MinLMSetCond(),  MinLMSetStpMax() and
   other functions
3. User calls MinLMOptimize() function which  takes algorithm  state   and
   pointers (delegates, etc.) to callback functions.
4. User calls MinLMResults() to get solution
5. Optionally, user may call MinLMRestartFrom() to solve  another  problem
   with same N but another starting point and/or another function.
   MinLMRestartFrom() allows to reuse already initialized structure.

Inputs:
    N       -   dimension, N &gt; 1
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    X       -   initial solution, array[0..N-1]

Outputs:
    State   -   structure which stores algorithm state

NOTES:
1. you may tune stopping conditions with MinLMSetCond() function
2. if target function contains exp() or other fast growing functions,  and
   optimization algorithm makes too large steps which leads  to  overflow,
   use MinLMSetStpMax() function to bound algorithm's steps.
ALGLIB: Copyright 30.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmcreatefgh(ae_int_t n, real_1d_array x, minlmstate &amp;state);
<b>void</b> minlmcreatefgh(real_1d_array x, minlmstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlm_d_fgh class=nav>minlm_d_fgh</a> ]</p>
<a name=sub_minlmcreatefgj></a><h6 class=pageheader>minlmcreatefgj Function</h6>
<hr width=600 align=left>
<pre class=narration>
This is obsolete function.

Since ALGLIB 3.3 it is equivalent to MinLMCreateFJ().
ALGLIB: Copyright 30.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmcreatefgj(ae_int_t n, ae_int_t m, real_1d_array x, minlmstate &amp;state);
<b>void</b> minlmcreatefgj(ae_int_t m, real_1d_array x, minlmstate &amp;state);
</pre>
<a name=sub_minlmcreatefj></a><h6 class=pageheader>minlmcreatefj Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is considered obsolete since ALGLIB 3.1.0 and is present for
backward  compatibility  only.  We  recommend  to use MinLMCreateVJ, which
provides similar, but more consistent and feature-rich interface.
ALGLIB: Copyright 30.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmcreatefj(ae_int_t n, ae_int_t m, real_1d_array x, minlmstate &amp;state);
<b>void</b> minlmcreatefj(ae_int_t m, real_1d_array x, minlmstate &amp;state);
</pre>
<a name=sub_minlmcreatev></a><h6 class=pageheader>minlmcreatev Function</h6>
<hr width=600 align=left>
<pre class=narration>
IMPROVED LEVENBERG-MARQUARDT METHOD
FOR NON-LINEAR LEAST SQUARES OPTIMIZATION
This function is used to find minimum of function which is represented  as
sum of squares:
    F(x) = f[0]^2(x[0],...,x[N-1]) + ... + f[M-1]^2(x[0],...,x[N-1])
using value of function vector f[] only. Finite differences  are  used  to
calculate Jacobian.

REQUIREMENTS:
This algorithm will request following information during its operation:
* function vector f[] at given point X

There are several overloaded versions of  MinLMOptimize()  function  which
correspond  to  different LM-like optimization algorithms provided by this
unit. You should choose version which accepts fvec() callback.

You can try to initialize MinLMState structure with VJ  function and  then
use incorrect version  of  MinLMOptimize()  (for  example,  version  which
works with general form function and does not accept function vector), but
it will  lead  to  exception being thrown after first attempt to calculate
Jacobian.

USAGE:
1. User initializes algorithm state with MinLMCreateV() call
2. User tunes solver parameters with MinLMSetCond(),  MinLMSetStpMax() and
   other functions
3. User calls MinLMOptimize() function which  takes algorithm  state   and
   callback functions.
4. User calls MinLMResults() to get solution
5. Optionally, user may call MinLMRestartFrom() to solve  another  problem
   with same N/M but another starting point and/or another function.
   MinLMRestartFrom() allows to reuse already initialized structure.

Inputs:
    N       -   dimension, N &gt; 1
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    M       -   number of functions f[i]
    X       -   initial solution, array[0..N-1]
    DiffStep-   differentiation step, &gt; 0

Outputs:
    State   -   structure which stores algorithm state

See also MinLMIteration, MinLMResults.

NOTES:
1. you may tune stopping conditions with MinLMSetCond() function
2. if target function contains exp() or other fast growing functions,  and
   optimization algorithm makes too large steps which leads  to  overflow,
   use MinLMSetStpMax() function to bound algorithm's steps.
ALGLIB: Copyright 30.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmcreatev(ae_int_t n, ae_int_t m, real_1d_array x, <b>double</b> diffstep, minlmstate &amp;state);
<b>void</b> minlmcreatev(ae_int_t m, real_1d_array x, <b>double</b> diffstep, minlmstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlm_d_v class=nav>minlm_d_v</a> | <a href=#example_minlm_d_vb class=nav>minlm_d_vb</a> | <a href=#example_minlm_d_restarts class=nav>minlm_d_restarts</a> ]</p>
<a name=sub_minlmcreatevgj></a><h6 class=pageheader>minlmcreatevgj Function</h6>
<hr width=600 align=left>
<pre class=narration>
This is obsolete function.

Since ALGLIB 3.3 it is equivalent to MinLMCreateVJ().
ALGLIB: Copyright 30.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmcreatevgj(ae_int_t n, ae_int_t m, real_1d_array x, minlmstate &amp;state);
<b>void</b> minlmcreatevgj(ae_int_t m, real_1d_array x, minlmstate &amp;state);
</pre>
<a name=sub_minlmcreatevj></a><h6 class=pageheader>minlmcreatevj Function</h6>
<hr width=600 align=left>
<pre class=narration>
IMPROVED LEVENBERG-MARQUARDT METHOD
FOR NON-LINEAR LEAST SQUARES OPTIMIZATION
This function is used to find minimum of function which is represented  as
sum of squares:
    F(x) = f[0]^2(x[0],...,x[N-1]) + ... + f[M-1]^2(x[0],...,x[N-1])
using value of function vector f[] and Jacobian of f[].

REQUIREMENTS:
This algorithm will request following information during its operation:

* function vector f[] at given point X
* function vector f[] and Jacobian of f[] (simultaneously) at given point

There are several overloaded versions of  MinLMOptimize()  function  which
correspond  to  different LM-like optimization algorithms provided by this
unit. You should choose version which accepts fvec()  and jac() callbacks.
First  one  is used to calculate f[] at given point, second one calculates
f[] and Jacobian df[i]/dx[j].

You can try to initialize MinLMState structure with VJ  function and  then
use incorrect version  of  MinLMOptimize()  (for  example,  version  which
works  with  general  form function and does not provide Jacobian), but it
will  lead  to  exception  being  thrown  after first attempt to calculate
Jacobian.

USAGE:
1. User initializes algorithm state with MinLMCreateVJ() call
2. User tunes solver parameters with MinLMSetCond(),  MinLMSetStpMax() and
   other functions
3. User calls MinLMOptimize() function which  takes algorithm  state   and
   callback functions.
4. User calls MinLMResults() to get solution
5. Optionally, user may call MinLMRestartFrom() to solve  another  problem
   with same N/M but another starting point and/or another function.
   MinLMRestartFrom() allows to reuse already initialized structure.

Inputs:
    N       -   dimension, N &gt; 1
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    M       -   number of functions f[i]
    X       -   initial solution, array[0..N-1]

Outputs:
    State   -   structure which stores algorithm state

NOTES:
1. you may tune stopping conditions with MinLMSetCond() function
2. if target function contains exp() or other fast growing functions,  and
   optimization algorithm makes too large steps which leads  to  overflow,
   use MinLMSetStpMax() function to bound algorithm's steps.
ALGLIB: Copyright 30.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmcreatevj(ae_int_t n, ae_int_t m, real_1d_array x, minlmstate &amp;state);
<b>void</b> minlmcreatevj(ae_int_t m, real_1d_array x, minlmstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlm_d_vj class=nav>minlm_d_vj</a> ]</p>
<a name=sub_minlmoptguardgradient></a><h6 class=pageheader>minlmoptguardgradient Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates verification  of  the  user-supplied
analytic Jacobian.

Upon  activation  of  this  option  OptGuard  integrity  checker  performs
numerical differentiation of your target function vector  at  the  initial
point (note: future versions may also perform check  at  the final  point)
and compares numerical Jacobian with analytic one provided by you.

If difference is too large, an error flag is set and optimization  session
continues. After optimization session is over, you can retrieve the report
which stores  both  Jacobians,  and  specific  components  highlighted  as
suspicious by the OptGuard.

The OptGuard report can be retrieved with minlmoptguardresults().

IMPORTANT: gradient check is a high-overhead option which  will  cost  you
           about 3*N additional function evaluations. In many cases it may
           cost as much as the rest of the optimization session.

           YOU SHOULD NOT USE IT IN THE PRODUCTION CODE UNLESS YOU WANT TO
           CHECK DERIVATIVES PROVIDED BY SOME THIRD PARTY.

NOTE: unlike previous incarnation of the gradient checking code,  OptGuard
      does NOT interrupt optimization even if it discovers bad gradient.

Inputs:
    State       -   structure used to store algorithm state
    TestStep    -   verification step used for numerical differentiation:
                    * TestStep=0 turns verification off
                    * TestStep &gt; 0 activates verification
                    You should carefully choose TestStep. Value  which  is
                    too large (so large that  function  behavior  is  non-
                    cubic at this scale) will lead  to  false  alarms. Too
                    short step will result in rounding  errors  dominating
                    numerical derivative.

                    You may use different step for different parameters by
                    means of setting scale with minlmsetscale().

==== EXPLANATION ====

In order to verify gradient algorithm performs following steps:
  * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    where X[i] is i-th component of the initial point and S[i] is a  scale
    of i-th parameter
  * F(X) is evaluated at these trial points
  * we perform one more evaluation in the middle point of the interval
  * we  build  cubic  model using function values and derivatives at trial
    points and we compare its prediction with actual value in  the  middle
    point
ALGLIB: Copyright 15.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmoptguardgradient(minlmstate state, <b>double</b> teststep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlm_d_vj class=nav>minlm_d_vj</a> ]</p>
<a name=sub_minlmoptguardresults></a><h6 class=pageheader>minlmoptguardresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Results of OptGuard integrity check, should be called  after  optimization
session is over.

OptGuard checks analytic Jacobian  against  reference  value  obtained  by
numerical differentiation with user-specified step.

NOTE: other optimizers perform additional OptGuard checks for things  like
      C0/C1-continuity violations. However, LM optimizer  can  check  only
      for incorrect Jacobian.

      The reason is that unlike line search methods LM optimizer does  not
      perform extensive evaluations along the line. Thus, we simply do not
      have enough data to catch C0/C1-violations.

This check is activated with  minlmoptguardgradient() function.

Following flags are set when these errors are suspected:
* rep.badgradsuspected, and additionally:
  * rep.badgradfidx for specific function (Jacobian row) suspected
  * rep.badgradvidx for specific variable (Jacobian column) suspected
  * rep.badgradxbase, a point where gradient/Jacobian is tested
  * rep.badgraduser, user-provided gradient/Jacobian
  * rep.badgradnum, reference gradient/Jacobian obtained via numerical
    differentiation

Inputs:
    State   -   algorithm state

Outputs:
    Rep     -   OptGuard report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmoptguardresults(minlmstate state, optguardreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlm_d_vj class=nav>minlm_d_vj</a> ]</p>
<a name=sub_minlmoptimize></a><h6 class=pageheader>minlmoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear optimizer

These functions accept following parameters:
    state   -   algorithm state
    func    -   callback which calculates function (or merit function)
                value func at given point x
    grad    -   callback which calculates function (or merit function)
                value func and gradient grad at given point x
    hess    -   callback which calculates function (or merit function)
                value func, gradient grad and Hessian hess at given point x
    fvec    -   callback which calculates function vector fi[]
                at given point x
    jac     -   callback which calculates function vector fi[]
                and Jacobian jac at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL

NOTES:

1. Depending on function used to create state  structure,  this  algorithm
   may accept Jacobian and/or Hessian and/or gradient.  According  to  the
   said above, there ase several versions of this function,  which  accept
   different sets of callbacks.

   This flexibility opens way to subtle errors - you may create state with
   MinLMCreateFGH() (optimization using Hessian), but call function  which
   does not accept Hessian. So when algorithm will request Hessian,  there
   will be no callback to call. In this case exception will be thrown.

   Be careful to avoid such errors because there is no way to find them at
   compile time - you can see them at runtime only.
ALGLIB: Copyright 10.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmoptimize(minlmstate &amp;state, <b>void</b> (*fvec)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> minlmoptimize(minlmstate &amp;state, <b>void</b> (*fvec)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr), <b>void</b> (*jac)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> minlmoptimize(minlmstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*grad)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr), <b>void</b> (*hess)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, real_2d_array &amp;hess, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> minlmoptimize(minlmstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*jac)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> minlmoptimize(minlmstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*grad)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr), <b>void</b> (*jac)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlm_d_v class=nav>minlm_d_v</a> | <a href=#example_minlm_d_vj class=nav>minlm_d_vj</a> | <a href=#example_minlm_d_fgh class=nav>minlm_d_fgh</a> | <a href=#example_minlm_d_vb class=nav>minlm_d_vb</a> | <a href=#example_minlm_d_restarts class=nav>minlm_d_restarts</a> ]</p>
<a name=sub_minlmrequesttermination></a><h6 class=pageheader>minlmrequesttermination Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine submits request for termination of running  optimizer.  It
should be called from user-supplied callback when user decides that it  is
time to &quot;smoothly&quot; terminate optimization process.  As  result,  optimizer
stops at point which was &quot;current accepted&quot; when termination  request  was
submitted and returns error code 8 (successful termination).

Inputs:
    State   -   optimizer structure

NOTE: after  request  for  termination  optimizer  may   perform   several
      additional calls to user-supplied callbacks. It does  NOT  guarantee
      to stop immediately - it just guarantees that these additional calls
      will be discarded later.

NOTE: calling this function on optimizer which is NOT running will have no
      effect.

NOTE: multiple calls to this function are possible. First call is counted,
      subsequent calls are silently ignored.
ALGLIB: Copyright 08.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmrequesttermination(minlmstate state);
</pre>
<a name=sub_minlmrestartfrom></a><h6 class=pageheader>minlmrestartfrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  subroutine  restarts  LM  algorithm from new point. All optimization
parameters are left unchanged.

This  function  allows  to  solve multiple  optimization  problems  (which
must have same number of dimensions) without object reallocation penalty.

Inputs:
    State   -   structure used for reverse communication previously
                allocated with MinLMCreateXXX call.
    X       -   new starting point.
ALGLIB: Copyright 30.07.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmrestartfrom(minlmstate state, real_1d_array x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlm_d_restarts class=nav>minlm_d_restarts</a> ]</p>
<a name=sub_minlmresults></a><h6 class=pageheader>minlmresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Levenberg-Marquardt algorithm results

NOTE: if you activated OptGuard integrity checking functionality and  want
      to get OptGuard report,  it  can  be  retrieved  with  the  help  of
      minlmoptguardresults() function.

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[0..N-1], solution
    Rep     -   optimization  report;  includes  termination   codes   and
                additional information. Termination codes are listed below,
                see comments for this structure for more info.
                Termination code is stored in rep.terminationtype field:
                * -8    optimizer detected NAN/INF values either in the
                        function itself, or in its Jacobian
                * -3    constraints are inconsistent
                *  2    relative step is no more than EpsX.
                *  5    MaxIts steps was taken
                *  7    stopping conditions are too stringent,
                        further improvement is impossible
                *  8    terminated by user who called minlmrequesttermination().
                        X contains point which was &quot;current accepted&quot; when
                        termination request was submitted.
ALGLIB: Copyright 10.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmresults(minlmstate state, real_1d_array &amp;x, minlmreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlm_d_v class=nav>minlm_d_v</a> | <a href=#example_minlm_d_vj class=nav>minlm_d_vj</a> | <a href=#example_minlm_d_fgh class=nav>minlm_d_fgh</a> | <a href=#example_minlm_d_vb class=nav>minlm_d_vb</a> | <a href=#example_minlm_d_restarts class=nav>minlm_d_restarts</a> ]</p>
<a name=sub_minlmresultsbuf></a><h6 class=pageheader>minlmresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Levenberg-Marquardt algorithm results

Buffered implementation of MinLMResults(), which uses pre-allocated buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 10.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmresultsbuf(minlmstate state, real_1d_array &amp;x, minlmreport &amp;rep);
</pre>
<a name=sub_minlmsetacctype></a><h6 class=pageheader>minlmsetacctype Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to change acceleration settings

You can choose between three acceleration strategies:
* AccType=0, no acceleration.
* AccType=1, secant updates are used to update quadratic model after  each
  iteration. After fixed number of iterations (or after  model  breakdown)
  we  recalculate  quadratic  model  using  analytic  Jacobian  or  finite
  differences. Number of secant-based iterations depends  on  optimization
  settings: about 3 iterations - when we have analytic Jacobian, up to 2*N
  iterations - when we use finite differences to calculate Jacobian.

AccType=1 is recommended when Jacobian  calculation  cost is prohibitively
high (several Mx1 function vector calculations  followed  by  several  NxN
Cholesky factorizations are faster than calculation of one M*N  Jacobian).
It should also be used when we have no Jacobian, because finite difference
approximation takes too much time to compute.

Table below list  optimization  protocols  (XYZ  protocol  corresponds  to
MinLMCreateXYZ) and acceleration types they support (and use by  default).

ACCELERATION TYPES SUPPORTED BY OPTIMIZATION PROTOCOLS:

protocol    0   1   comment
V           +   +
VJ          +   +
FGH         +

DEFAULT VALUES:

protocol    0   1   comment
V               x   without acceleration it is so slooooooooow
VJ          x
FGH         x

NOTE: this  function should be called before optimization. Attempt to call
it during algorithm iterations may result in unexpected behavior.

NOTE: attempt to call this function with unsupported protocol/acceleration
combination will result in exception being thrown.
ALGLIB: Copyright 14.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmsetacctype(minlmstate state, ae_int_t acctype);
</pre>
<a name=sub_minlmsetbc></a><h6 class=pageheader>minlmsetbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets boundary constraints for LM optimizer

Boundary constraints are inactive by default (after initial creation).
They are preserved until explicitly turned off with another SetBC() call.

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very small number or -INF (latter is recommended because
                it will allow solver to use better algorithm).
    BndU    -   upper bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very large number or +INF (latter is recommended because
                it will allow solver to use better algorithm).

NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
variable will be &quot;frozen&quot; at X[i]=BndL[i]=BndU[i].

NOTE 2: this solver has following useful properties:
* bound constraints are always satisfied exactly
* function is evaluated only INSIDE area specified by bound constraints
  or at its boundary
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmsetbc(minlmstate state, real_1d_array bndl, real_1d_array bndu);
</pre>
<a name=sub_minlmsetcond></a><h6 class=pageheader>minlmsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping conditions for Levenberg-Marquardt optimization
algorithm.

Inputs:
    State   -   structure which stores algorithm state
    EpsX    - &ge; 0
                The subroutine finishes its work if  on  k+1-th  iteration
                the condition |v| &le; EpsX is fulfilled, where:
                * |.| means Euclidian norm
                * v - scaled step vector, v[i]=dx[i]/s[i]
                * dx - ste pvector, dx=X(k+1)-X(k)
                * s - scaling coefficients set by MinLMSetScale()
                Recommended values: 1E-9 ... 1E-12.
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations   is    unlimited.   Only   Levenberg-Marquardt
                iterations  are  counted  (L-BFGS/CG  iterations  are  NOT
                counted because their cost is very low compared to that of
                LM).

Passing  EpsX=0  and  MaxIts=0  (simultaneously)  will  lead  to automatic
stopping criterion selection (small EpsX).

NOTE: it is not recommended to set large EpsX (say, 0.001). Because LM  is
      a second-order method, it performs very precise steps anyway.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmsetcond(minlmstate state, <b>double</b> epsx, ae_int_t maxits);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlm_d_v class=nav>minlm_d_v</a> | <a href=#example_minlm_d_vj class=nav>minlm_d_vj</a> | <a href=#example_minlm_d_fgh class=nav>minlm_d_fgh</a> | <a href=#example_minlm_d_vb class=nav>minlm_d_vb</a> | <a href=#example_minlm_d_restarts class=nav>minlm_d_restarts</a> ]</p>
<a name=sub_minlmsetlc></a><h6 class=pageheader>minlmsetlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets general linear constraints for LM optimizer

Linear constraints are inactive by default (after initial creation).  They
are preserved until explicitly turned off with another minlmsetlc() call.

Inputs:
    State   -   structure stores algorithm state
    C       -   linear constraints, array[K,N+1].
                Each row of C represents one constraint, either equality
                or inequality (see below):
                * first N elements correspond to coefficients,
                * last element corresponds to the right part.
                All elements of C (including right part) must be finite.
    CT      -   type of constraints, array[K]:
                * if CT[i] &gt; 0, then I-th constraint is C[i,*]*x &ge; C[i,n+1]
                * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
                * if CT[i] &lt; 0, then I-th constraint is C[i,*]*x &le; C[i,n+1]
    K       -   number of equality/inequality constraints, K &ge; 0:
                * if given, only leading K elements of C/CT are used
                * if not given, automatically determined from sizes of C/CT

IMPORTANT: if you have linear constraints, it is strongly  recommended  to
           set scale of variables with minlmsetscale(). QP solver which is
           used to calculate linearly constrained steps heavily relies  on
           good scaling of input problems.

IMPORTANT: solvers created with minlmcreatefgh()  do  not  support  linear
           constraints.

NOTE: linear  (non-bound)  constraints are satisfied only approximately  -
      there  always  exists some violation due  to  numerical  errors  and
      algorithmic limitations.

NOTE: general linear constraints  add  significant  overhead  to  solution
      process. Although solver performs roughly same amount of  iterations
      (when compared  with  similar  box-only  constrained  problem), each
      iteration   now    involves  solution  of  linearly  constrained  QP
      subproblem, which requires ~3-5 times more Cholesky  decompositions.
      Thus, if you can reformulate your problem in such way  this  it  has
      only box constraints, it may be beneficial to do so.
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmsetlc(minlmstate state, real_2d_array c, integer_1d_array ct, ae_int_t k);
<b>void</b> minlmsetlc(minlmstate state, real_2d_array c, integer_1d_array ct);
</pre>
<a name=sub_minlmsetscale></a><h6 class=pageheader>minlmsetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients for LM optimizer.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison with tolerances).  Scale of
the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the function

Generally, scale is NOT considered to be a form of preconditioner.  But LM
optimizer is unique in that it uses scaling matrix both  in  the  stopping
condition tests and as Marquardt damping factor.

Proper scaling is very important for the algorithm performance. It is less
important for the quality of results, but still has some influence (it  is
easier  to  converge  when  variables  are  properly  scaled, so premature
stopping is possible when very badly scalled variables are  combined  with
relaxed stopping conditions).

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmsetscale(minlmstate state, real_1d_array s);
</pre>
<a name=sub_minlmsetstpmax></a><h6 class=pageheader>minlmsetstpmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets maximum step length

Inputs:
    State   -   structure which stores algorithm state
    StpMax  -   maximum step length, &ge; 0. Set StpMax to 0.0,  if you don't
                want to limit step length.

Use this subroutine when you optimize target function which contains exp()
or  other  fast  growing  functions,  and optimization algorithm makes too
large  steps  which  leads  to overflow. This function allows us to reject
steps  that  are  too  large  (and  therefore  expose  us  to the possible
overflow) without actually calculating function value at the x+stp*d.

NOTE: non-zero StpMax leads to moderate  performance  degradation  because
intermediate  step  of  preconditioned L-BFGS optimization is incompatible
with limits on step size.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmsetstpmax(minlmstate state, <b>double</b> stpmax);
</pre>
<a name=sub_minlmsetxrep></a><h6 class=pageheader>minlmsetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to MinLMOptimize(). Both Levenberg-Marquardt and internal  L-BFGS
iterations are reported.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlmsetxrep(minlmstate state, <b>bool</b> needxrep);
</pre>
<a name=example_minlm_d_fgh></a><h6 class=pageheader>minlm_d_fgh Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_func(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
}
<b>void</b> function1_grad(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
<font color=navy>// and its derivatives df/d0 and df/dx1</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
   grad[0] = 400*pow(x[0]+3,3);
   grad[1] = 4*pow(x[1]-3,3);
}
<b>void</b> function1_hess(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, real_1d_array &amp;grad, real_2d_array &amp;hess, <b>void</b> *ptr) {
<font color=navy>// this callback calculates f(x0,x1) = 100*(x0+3)^4 + (x1-3)^4</font>
<font color=navy>// its derivatives df/d0 and df/dx1</font>
<font color=navy>// and its Hessian.</font>
   func = 100*pow(x[0]+3,4) + pow(x[1]-3,4);
   grad[0] = 400*pow(x[0]+3,3);
   grad[1] = 4*pow(x[1]-3,3);
   hess[0][0] = 1200*pow(x[0]+3,2);
   hess[0][1] = 0;
   hess[1][0] = 0;
   hess[1][1] = 12*pow(x[1]-3,2);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of F(x0,x1) = 100*(x0+3)^4+(x1-3)^4</font>
<font color=navy>// using <font color=blue><b>&quot;FGH&quot;</b></font> mode of the Levenberg-Marquardt optimizer.</font>
<font color=navy>//</font>
<font color=navy>// F is treated like a monolitic function without internal structure,</font>
<font color=navy>// i.e. we <b>do</b> NOT represent it as a sum of squares.</font>
<font color=navy>//</font>
<font color=navy>// Optimization algorithm uses:</font>
<font color=navy>// * function value F(x0,x1)</font>
<font color=navy>// * gradient G={dF/dxi}</font>
<font color=navy>// * Hessian H={d2F/(dxi*dxj)}</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   <b>double</b> epsx = 0.0000000001;
   ae_int_t maxits = 0;
   minlmstate state;
   minlmreport rep;

   minlmcreatefgh(x, state);
   minlmsetcond(state, epsx, maxits);
   minlmoptimize(state, function1_func, function1_grad, function1_hess);
   minlmresults(state, x, rep);

   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,+3]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minlm_d_restarts></a><h6 class=pageheader>minlm_d_restarts Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_fvec(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>// f0(x0,x1) = 100*(x0+3)^4,</font>
<font color=navy>// f1(x0,x1) = (x1-3)^4</font>
   fi[0] = 10*pow(x[0]+3,2);
   fi[1] = pow(x[1]-3,2);
}
<b>void</b> function2_fvec(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>// f0(x0,x1) = x0^2+1</font>
<font color=navy>// f1(x0,x1) = x1-1</font>
   fi[0] = x[0]*x[0]+1;
   fi[1] = x[1]-1;
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of F(x0,x1) = f0^2+f1^2, where </font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = 10*(x0+3)^2</font>
<font color=navy>//     f1(x0,x1) = (x1-3)^2</font>
<font color=navy>//</font>
<font color=navy>// using several starting points and efficient restarts.</font>
   real_1d_array x;
   <b>double</b> epsx = 0.0000000001;
   ae_int_t maxits = 0;
   minlmstate state;
   minlmreport rep;
<font color=navy>// create optimizer using minlmcreatev()</font>
   x = <font color=blue><b>&quot;[10,10]&quot;</b></font>;
   minlmcreatev(2, x, 0.0001, state);
   minlmsetcond(state, epsx, maxits);
   minlmoptimize(state, function1_fvec);
   minlmresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,+3]</font>
<font color=navy>//</font>
<font color=navy>// restart optimizer using minlmrestartfrom()</font>
<font color=navy>//</font>
<font color=navy>// we can use different starting point, different function,</font>
<font color=navy>// different stopping conditions, but problem size</font>
<font color=navy>// must remain unchanged.</font>
   x = <font color=blue><b>&quot;[4,4]&quot;</b></font>;
   minlmrestartfrom(state, x);
   minlmoptimize(state, function2_fvec);
   minlmresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [0,1]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minlm_d_v></a><h6 class=pageheader>minlm_d_v Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_fvec(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>// f0(x0,x1) = 100*(x0+3)^4,</font>
<font color=navy>// f1(x0,x1) = (x1-3)^4</font>
   fi[0] = 10*pow(x[0]+3,2);
   fi[1] = pow(x[1]-3,2);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of F(x0,x1) = f0^2+f1^2, where </font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = 10*(x0+3)^2</font>
<font color=navy>//     f1(x0,x1) = (x1-3)^2</font>
<font color=navy>//</font>
<font color=navy>// using <font color=blue><b>&quot;V&quot;</b></font> mode of the Levenberg-Marquardt optimizer.</font>
<font color=navy>//</font>
<font color=navy>// Optimization algorithm uses:</font>
<font color=navy>// * function vector f[] = {f1,f2}</font>
<font color=navy>//</font>
<font color=navy>// No other information (Jacobian, gradient, etc.) is needed.</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsx = 0.0000000001;
   ae_int_t maxits = 0;
   minlmstate state;
   minlmreport rep;
<font color=navy>// Create optimizer, tell it to:</font>
<font color=navy>// * use numerical differentiation with step equal to 0.0001</font>
<font color=navy>// * use unit scale <b>for</b> all variables (s is a unit vector)</font>
<font color=navy>// * stop after short enough step (less than epsx)</font>
   minlmcreatev(2, x, 0.0001, state);
   minlmsetcond(state, epsx, maxits);
   minlmsetscale(state, s);
<font color=navy>// Optimize</font>
   minlmoptimize(state, function1_fvec);
<font color=navy>// Test optimization results</font>
<font color=navy>//</font>
<font color=navy>// NOTE: because we use numerical differentiation, we <b>do</b> not</font>
<font color=navy>//       verify Jacobian correctness - it is always <font color=blue><b>&quot;correct&quot;</b></font>.</font>
<font color=navy>//       However, <b>if</b> you switch to analytic gradient, consider</font>
<font color=navy>//       checking it with OptGuard (see other examples).</font>
   minlmresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,+3]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minlm_d_vb></a><h6 class=pageheader>minlm_d_vb Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_fvec(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>// f0(x0,x1) = 100*(x0+3)^4,</font>
<font color=navy>// f1(x0,x1) = (x1-3)^4</font>
   fi[0] = 10*pow(x[0]+3,2);
   fi[1] = pow(x[1]-3,2);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of F(x0,x1) = f0^2+f1^2, where </font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = 10*(x0+3)^2</font>
<font color=navy>//     f1(x0,x1) = (x1-3)^2</font>
<font color=navy>//</font>
<font color=navy>// with boundary constraints</font>
<font color=navy>//</font>
<font color=navy>//     -1 &le; x0 &le; +1</font>
<font color=navy>//     -1 &le; x1 &le; +1</font>
<font color=navy>//</font>
<font color=navy>// using <font color=blue><b>&quot;V&quot;</b></font> mode of the Levenberg-Marquardt optimizer.</font>
<font color=navy>//</font>
<font color=navy>// Optimization algorithm uses:</font>
<font color=navy>// * function vector f[] = {f1,f2}</font>
<font color=navy>//</font>
<font color=navy>// No other information (Jacobian, gradient, etc.) is needed.</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[-1,-1]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[+1,+1]&quot;</b></font>;
   <b>double</b> epsx = 0.0000000001;
   ae_int_t maxits = 0;
   minlmstate state;
<font color=navy>// Create optimizer, tell it to:</font>
<font color=navy>// * use numerical differentiation with step equal to 1.0</font>
<font color=navy>// * use unit scale <b>for</b> all variables (s is a unit vector)</font>
<font color=navy>// * stop after short enough step (less than epsx)</font>
<font color=navy>// * set box constraints</font>
   minlmcreatev(2, x, 0.0001, state);
   minlmsetbc(state, bndl, bndu);
   minlmsetcond(state, epsx, maxits);
   minlmsetscale(state, s);
<font color=navy>// Optimize</font>
   minlmoptimize(state, function1_fvec);
<font color=navy>// Test optimization results</font>
<font color=navy>//</font>
<font color=navy>// NOTE: because we use numerical differentiation, we <b>do</b> not</font>
<font color=navy>//       verify Jacobian correctness - it is always <font color=blue><b>&quot;correct&quot;</b></font>.</font>
<font color=navy>//       However, <b>if</b> you switch to analytic gradient, consider</font>
<font color=navy>//       checking it with OptGuard (see other examples).</font>
   minlmreport rep;
   minlmresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-1,+1]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minlm_d_vj></a><h6 class=pageheader>minlm_d_vj Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> function1_fvec(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>// f0(x0,x1) = 100*(x0+3)^4,</font>
<font color=navy>// f1(x0,x1) = (x1-3)^4</font>
   fi[0] = 10*pow(x[0]+3,2);
   fi[1] = pow(x[1]-3,2);
}
<b>void</b> function1_jac(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>// f0(x0,x1) = 100*(x0+3)^4,</font>
<font color=navy>// f1(x0,x1) = (x1-3)^4</font>
<font color=navy>// and Jacobian matrix J = [dfi/dxj]</font>
   fi[0] = 10*pow(x[0]+3,2);
   fi[1] = pow(x[1]-3,2);
   jac[0][0] = 20*(x[0]+3);
   jac[0][1] = 0;
   jac[1][0] = 0;
   jac[1][1] = 2*(x[1]-3);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of F(x0,x1) = f0^2+f1^2, where </font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = 10*(x0+3)^2</font>
<font color=navy>//     f1(x0,x1) = (x1-3)^2</font>
<font color=navy>//</font>
<font color=navy>// using <font color=blue><b>&quot;VJ&quot;</b></font> mode of the Levenberg-Marquardt optimizer.</font>
<font color=navy>//</font>
<font color=navy>// Optimization algorithm uses:</font>
<font color=navy>// * function vector f[] = {f1,f2}</font>
<font color=navy>// * Jacobian matrix J = {dfi/dxj}.</font>
   real_1d_array x = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsx = 0.0000000001;
   ae_int_t maxits = 0;
   minlmstate state;
<font color=navy>// Create optimizer, tell it to:</font>
<font color=navy>// * use analytic gradient provided by user</font>
<font color=navy>// * use unit scale <b>for</b> all variables (s is a unit vector)</font>
<font color=navy>// * stop after short enough step (less than epsx)</font>
   minlmcreatevj(2, x, state);
   minlmsetcond(state, epsx, maxits);
   minlmsetscale(state, s);
<font color=navy>// Activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard monitor helps to detect erroneous analytic Jacobian,</font>
<font color=navy>// i.e. one inconsistent with actual change in the target function.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential <b>for</b> early prototyping stages because such</font>
<font color=navy>// problems often result in premature termination of the optimizer</font>
<font color=navy>// which is really hard to distinguish from the correct termination.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: JACOBIAN VERIFICATION IS PERFORMED BY MEANS OF NUMERICAL</font>
<font color=navy>//            DIFFERENTIATION, THUS DO NOT USE IT IN PRODUCTION CODE!</font>
   minlmoptguardgradient(state, 0.001);
<font color=navy>// Optimize</font>
   minlmoptimize(state, function1_fvec, function1_jac);
<font color=navy>// Test optimization results</font>
   minlmreport rep;
   minlmresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [-3,+3]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// NOTE: want to test OptGuard? Try breaking the Jacobian - say, add</font>
<font color=navy>//       1.0 to some of its components.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: unfortunately, specifics of LM optimization <b>do</b> not allow us</font>
<font color=navy>//       to detect errors like nonsmoothness (like we <b>do</b> with other</font>
<font color=navy>//       optimizers). So, only Jacobian correctness is verified.</font>
   optguardreport ogrep;
   minlmoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_minlp></a><h4 class=pageheader>8.8.7. minlp Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_minlpreport class=toc>minlpreport</a> |
<a href=#struct_minlpstate class=toc>minlpstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_minlpaddlc2 class=toc>minlpaddlc2</a> |
<a href=#sub_minlpaddlc2dense class=toc>minlpaddlc2dense</a> |
<a href=#sub_minlpcreate class=toc>minlpcreate</a> |
<a href=#sub_minlpoptimize class=toc>minlpoptimize</a> |
<a href=#sub_minlpresults class=toc>minlpresults</a> |
<a href=#sub_minlpresultsbuf class=toc>minlpresultsbuf</a> |
<a href=#sub_minlpsetalgodss class=toc>minlpsetalgodss</a> |
<a href=#sub_minlpsetalgoipm class=toc>minlpsetalgoipm</a> |
<a href=#sub_minlpsetbc class=toc>minlpsetbc</a> |
<a href=#sub_minlpsetbcall class=toc>minlpsetbcall</a> |
<a href=#sub_minlpsetbci class=toc>minlpsetbci</a> |
<a href=#sub_minlpsetcost class=toc>minlpsetcost</a> |
<a href=#sub_minlpsetlc class=toc>minlpsetlc</a> |
<a href=#sub_minlpsetlc2 class=toc>minlpsetlc2</a> |
<a href=#sub_minlpsetlc2dense class=toc>minlpsetlc2dense</a> |
<a href=#sub_minlpsetscale class=toc>minlpsetscale</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_minlp_basic class=toc>minlp_basic</a></td><td width=15>&nbsp;</td><td>Basic linear programming example</td></tr>
</table>
</div>
<a name=struct_minlpreport></a><h6 class=pageheader>minlpreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure stores optimization report:
* f                         target function value
* lagbc                     Lagrange coefficients for box constraints
* laglc                     Lagrange coefficients for linear constraints
* y                         dual variables
* stats                     array[N+M], statuses of box (N) and linear (M)
                            constraints. This array is filled only by  DSS
                            algorithm because IPM always stops at INTERIOR
                            point:
                            * stats[i] &gt; 0  &rArr;  constraint at upper bound
                                                (also used for free non-basic
                                                variables set to zero)
                            * stats[i] &lt; 0  &rArr;  constraint at lower bound
                            * stats[i] = 0  &rArr;  constraint is inactive, basic
                                                variable
* primalerror               primal feasibility error
* dualerror                 dual feasibility error
* slackerror                complementary slackness error
* iterationscount           iteration count
* terminationtype           completion code (see below)

COMPLETION CODES

Completion codes:
* -4    LP problem is primal unbounded (dual infeasible)
* -3    LP problem is primal infeasible (dual unbounded)
*  1..4 successful completion
*  5    MaxIts steps was taken
*  7    stopping conditions are too stringent,
        further improvement is impossible,
        X contains best point found so far.

LAGRANGE COEFFICIENTS

Positive Lagrange coefficient means that constraint is at its upper bound.
Negative coefficient means that constraint is at its lower  bound.  It  is
expected that at solution the dual feasibility condition holds:

    C + SUM(Ei*LagBC[i],i=0..n-1) + SUM(Ai*LagLC[i],i=0..m-1) ~ 0

where
* C is a cost vector (linear term)
* Ei is a vector with 1.0 at position I and 0 in other positions
* Ai is an I-th row of linear constraint matrix
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minlpreport {
   <b>double</b> f;
   real_1d_array lagbc;
   real_1d_array laglc;
   real_1d_array y;
   integer_1d_array stats;
   <b>double</b> primalerror;
   <b>double</b> dualerror;
   <b>double</b> slackerror;
   ae_int_t iterationscount;
   ae_int_t terminationtype;
};
</pre>
<a name=struct_minlpstate></a><h6 class=pageheader>minlpstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores linear solver state.
You should use functions provided by MinLP subpackage to work with this
object
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minlpstate {
};
</pre>
<a name=sub_minlpaddlc2></a><h6 class=pageheader>minlpaddlc2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function appends two-sided linear constraint  AL &le; A*x &le; AU  to the
list of currently present constraints.

Constraint is passed in compressed format: as list of non-zero entries  of
coefficient vector A. Such approach is more efficient than  dense  storage
for highly sparse constraint vectors.

Inputs:
    State   -   structure previously allocated with minlpcreate() call.
    IdxA    -   array[NNZ], indexes of non-zero elements of A:
                * can be unsorted
                * can include duplicate indexes (corresponding entries  of
                  ValA[] will be summed)
    ValA    -   array[NNZ], values of non-zero elements of A
    NNZ     -   number of non-zero coefficients in A
    AL, AU  -   lower and upper bounds;
                * AL=AU    &rArr; equality constraint A*x
                * AL &lt; AU  &rArr; two-sided constraint AL &le; A*x &le; AU
                * AL=-INF  &rArr; one-sided constraint A*x &le; AU
                * AU=+INF  &rArr; one-sided constraint AL &le; A*x
                * AL=-INF, AU=+INF &rArr; constraint is ignored
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpaddlc2(minlpstate state, integer_1d_array idxa, real_1d_array vala, ae_int_t nnz, <b>double</b> al, <b>double</b> au);
</pre>
<a name=sub_minlpaddlc2dense></a><h6 class=pageheader>minlpaddlc2dense Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function appends two-sided linear constraint  AL &le; A*x &le; AU  to the
list of currently present constraints.

This version accepts dense constraint vector as input, but  sparsifies  it
for internal storage and processing. Thus, time to add one  constraint  in
is O(N) - we have to scan entire array of length N. Sparse version of this
function is order of magnitude faster for  constraints  with  just  a  few
nonzeros per row.

Inputs:
    State   -   structure previously allocated with minlpcreate() call.
    A       -   linear constraint coefficient, array[N], right side is NOT
                included.
    AL, AU  -   lower and upper bounds;
                * AL=AU    &rArr; equality constraint Ai*x
                * AL &lt; AU  &rArr; two-sided constraint AL &le; A*x &le; AU
                * AL=-INF  &rArr; one-sided constraint Ai*x &le; AU
                * AU=+INF  &rArr; one-sided constraint AL &le; Ai*x
                * AL=-INF, AU=+INF &rArr; constraint is ignored
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpaddlc2dense(minlpstate state, real_1d_array a, <b>double</b> al, <b>double</b> au);
</pre>
<a name=sub_minlpcreate></a><h6 class=pageheader>minlpcreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
LINEAR PROGRAMMING
The subroutine creates LP  solver.  After  initial  creation  it  contains
default optimization problem with zero cost vector and all variables being
fixed to zero values and no constraints.

In order to actually solve something you should:
* set cost vector with minlpsetcost()
* set variable bounds with minlpsetbc() or minlpsetbcall()
* specify constraint matrix with one of the following functions:
  [*] minlpsetlc()        for dense one-sided constraints
  [*] minlpsetlc2dense()  for dense two-sided constraints
  [*] minlpsetlc2()       for sparse two-sided constraints
  [*] minlpaddlc2dense()  to add one dense row to constraint matrix
  [*] minlpaddlc2()       to add one row to constraint matrix (compressed format)
* call minlpoptimize() to run the solver and  minlpresults()  to  get  the
  solution vector and additional information.

By  default,  LP  solver uses best algorithm available. As of ALGLIB 3.17,
sparse interior point (barrier) solver is used. Future releases of  ALGLIB
may introduce other solvers.

User may choose specific LP algorithm by calling:
* minlpsetalgodss() for revised dual simplex method with DSE  pricing  and
  bounds flipping ratio test (aka long dual step).  Large-scale  sparse LU
  solverwith  Forest-Tomlin update is used internally  as  linear  algebra
  driver.
* minlpsetalgoipm() for sparse interior point method

Inputs:
    N       -   problem size

Outputs:
    State   -   optimizer in the default state
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpcreate(ae_int_t n, minlpstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlp_basic class=nav>minlp_basic</a> ]</p>
<a name=sub_minlpoptimize></a><h6 class=pageheader>minlpoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function solves LP problem.

Inputs:
    State   -   algorithm state

You should use minlpresults() function to access results  after  calls  to
this function.
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpoptimize(minlpstate state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlp_basic class=nav>minlp_basic</a> ]</p>
<a name=sub_minlpresults></a><h6 class=pageheader>minlpresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
LP solver results

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[N], solution (on failure: last trial point)
    Rep     -   optimization report. You should check Rep.TerminationType,
                which contains completion code, and you may check  another
                fields which contain another information  about  algorithm
                functioning.

                Failure codes returned by algorithm are:
                * -4    LP problem is primal unbounded (dual infeasible)
                * -3    LP problem is primal infeasible (dual unbounded)
                * -2    IPM solver detected that problem is either
                        infeasible or unbounded

                Success codes:
                *  1..4 successful completion
                *  5    MaxIts steps was taken
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpresults(minlpstate state, real_1d_array &amp;x, minlpreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlp_basic class=nav>minlp_basic</a> ]</p>
<a name=sub_minlpresultsbuf></a><h6 class=pageheader>minlpresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
LP results

Buffered implementation of MinLPResults() which uses pre-allocated  buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpresultsbuf(minlpstate state, real_1d_array &amp;x, minlpreport &amp;rep);
</pre>
<a name=sub_minlpsetalgodss></a><h6 class=pageheader>minlpsetalgodss Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets LP algorithm to revised dual simplex method.

ALGLIB implementation of dual simplex method supports advanced performance
and stability improvements like DSE pricing , bounds flipping  ratio  test
(aka long dual step), Forest-Tomlin update, shifting.

Inputs:
    State   -   optimizer
    Eps     -   stopping condition, Eps &ge; 0:
                * should be small number about 1E-6 or 1E-7.
                * zero value means that solver automatically selects good
                  value (can be different in different ALGLIB versions)
                * default value is zero
                Algorithm stops when relative error is less than Eps.
ALGLIB: Copyright 08.11.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetalgodss(minlpstate state, <b>double</b> eps);
</pre>
<a name=sub_minlpsetalgoipm></a><h6 class=pageheader>minlpsetalgoipm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets LP algorithm to sparse interior point method.

ALGORITHM INFORMATION:

* this  algorithm  is  our implementation  of  interior  point  method  as
  formulated by  R.J.Vanderbei, with minor modifications to the  algorithm
  (damped Newton directions are extensively used)
* like all interior point methods, this algorithm  tends  to  converge  in
  roughly same number of iterations (between 15 and 50) independently from
  the problem dimensionality

Inputs:
    State   -   optimizer
    Eps     -   stopping condition, Eps &ge; 0:
                * should be small number about 1E-7 or 1E-8.
                * zero value means that solver automatically selects good
                  value (can be different in different ALGLIB versions)
                * default value is zero
                Algorithm  stops  when  primal  error  AND  dual error AND
                duality gap are less than Eps.
ALGLIB: Copyright 08.11.2020 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetalgoipm(minlpstate state, <b>double</b> eps);
<b>void</b> minlpsetalgoipm(minlpstate state);
</pre>
<a name=sub_minlpsetbc></a><h6 class=pageheader>minlpsetbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets box constraints for LP solver (all variables  at  once,
different constraints for different variables).

The default state of constraints is to have all variables fixed  at  zero.
You have to overwrite it by your own constraint vector. Constraint  status
is preserved until constraints are  explicitly  overwritten  with  another
minlpsetbc()  call,   overwritten   with  minlpsetbcall(),  or   partially
overwritten with minlmsetbci() call.

Following types of constraints are supported:

    DESCRIPTION         CONSTRAINT              HOW TO SPECIFY
    fixed variable      x[i]=Bnd[i]             BndL[i]=BndU[i]
    lower bound         BndL[i] &le; x[i]           BndU[i]=+INF
    upper bound         x[i] &le; BndU[i]           BndL[i]=-INF
    range               BndL[i] &le; x[i] &le; BndU[i]  ...
    free variable       -                       BndL[I]=-INF, BndU[I]+INF

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bounds, array[N].
    BndU    -   upper bounds, array[N].

NOTE: infinite values can be specified by means of Double.PositiveInfinity
      and  Double.NegativeInfinity  (in  C#)  and  +INFINITY   and
      -INFINITY (in C++).

NOTE: you may replace infinities by very small/very large values,  but  it
      is not recommended because large numbers may introduce large numerical
      errors in the algorithm.

NOTE: if constraints for all variables are same you may use minlpsetbcall()
      which allows to specify constraints without using arrays.

NOTE: BndL &gt; BndU will result in LP problem being recognized as infeasible.
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetbc(minlpstate state, real_1d_array bndl, real_1d_array bndu);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlp_basic class=nav>minlp_basic</a> ]</p>
<a name=sub_minlpsetbcall></a><h6 class=pageheader>minlpsetbcall Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets box constraints for LP solver (all variables  at  once,
same constraints for all variables)

The default state of constraints is to have all variables fixed  at  zero.
You have to overwrite it by your own constraint vector. Constraint  status
is preserved until constraints are  explicitly  overwritten  with  another
minlpsetbc() call or partially overwritten with minlpsetbcall().

Following types of constraints are supported:

    DESCRIPTION         CONSTRAINT              HOW TO SPECIFY
    fixed variable      x[i]=Bnd[i]             BndL[i]=BndU[i]
    lower bound         BndL[i] &le; x[i]           BndU[i]=+INF
    upper bound         x[i] &le; BndU[i]           BndL[i]=-INF
    range               BndL[i] &le; x[i] &le; BndU[i]  ...
    free variable       -                       BndL[I]=-INF, BndU[I]+INF

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bound, same for all variables
    BndU    -   upper bound, same for all variables

NOTE: infinite values can be specified by means of Double.PositiveInfinity
      and  Double.NegativeInfinity  (in  C#)  and  +INFINITY   and
      -INFINITY (in C++).

NOTE: you may replace infinities by very small/very large values,  but  it
      is not recommended because large numbers may introduce large numerical
      errors in the algorithm.

NOTE: minlpsetbc() can  be  used  to  specify  different  constraints  for
      different variables.

NOTE: BndL &gt; BndU will result in LP problem being recognized as infeasible.
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetbcall(minlpstate state, <b>double</b> bndl, <b>double</b> bndu);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlp_basic class=nav>minlp_basic</a> ]</p>
<a name=sub_minlpsetbci></a><h6 class=pageheader>minlpsetbci Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets box constraints for I-th variable (other variables are
not modified).

The default state of constraints is to have all variables fixed  at  zero.
You have to overwrite it by your own constraint vector.

Following types of constraints are supported:

    DESCRIPTION         CONSTRAINT              HOW TO SPECIFY
    fixed variable      x[i]=Bnd[i]             BndL[i]=BndU[i]
    lower bound         BndL[i] &le; x[i]           BndU[i]=+INF
    upper bound         x[i] &le; BndU[i]           BndL[i]=-INF
    range               BndL[i] &le; x[i] &le; BndU[i]  ...
    free variable       -                       BndL[I]=-INF, BndU[I]+INF

Inputs:
    State   -   structure stores algorithm state
    I       -   variable index, in [0,N)
    BndL    -   lower bound for I-th variable
    BndU    -   upper bound for I-th variable

NOTE: infinite values can be specified by means of Double.PositiveInfinity
      and  Double.NegativeInfinity  (in  C#)  and  +INFINITY   and
      -INFINITY (in C++).

NOTE: you may replace infinities by very small/very large values,  but  it
      is not recommended because large numbers may introduce large numerical
      errors in the algorithm.

NOTE: minlpsetbc() can  be  used  to  specify  different  constraints  for
      different variables.

NOTE: BndL &gt; BndU will result in LP problem being recognized as infeasible.
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetbci(minlpstate state, ae_int_t i, <b>double</b> bndl, <b>double</b> bndu);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlp_basic class=nav>minlp_basic</a> ]</p>
<a name=sub_minlpsetcost></a><h6 class=pageheader>minlpsetcost Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets cost term for LP solver.

By default, cost term is zero.

Inputs:
    State   -   structure which stores algorithm state
    C       -   cost term, array[N].
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetcost(minlpstate state, real_1d_array c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlp_basic class=nav>minlp_basic</a> ]</p>
<a name=sub_minlpsetlc></a><h6 class=pageheader>minlpsetlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets one-sided linear constraints A*x ~ AU, where &quot;~&quot; can be
a mix of &quot;&le;&quot;, &quot;=&quot; and &quot;&ge;&quot;.

IMPORTANT: this function is provided here for compatibility with the  rest
           of ALGLIB optimizers which accept constraints  in  format  like
           this one. Many real-life problems feature two-sided constraints
           like a0 &le; a*x &le; a1. It is really inefficient to add them as a
           pair of one-sided constraints.

           Use minlpsetlc2dense(), minlpsetlc2(), minlpaddlc2()  (or   its
           sparse version) wherever possible.

Inputs:
    State   -   structure previously allocated with minlpcreate() call.
    A       -   linear constraints, array[K,N+1]. Each row of A represents
                one constraint, with first N elements being linear coefficients,
                and last element being right side.
    CT      -   constraint types, array[K]:
                * if CT[i] &gt; 0, then I-th constraint is A[i,*]*x &ge; A[i,n]
                * if CT[i] = 0, then I-th constraint is A[i,*]*x  = A[i,n]
                * if CT[i] &lt; 0, then I-th constraint is A[i,*]*x &le; A[i,n]
    K       -   number of equality/inequality constraints,  K &ge; 0;  if  not
                given, inferred from sizes of A and CT.
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetlc(minlpstate state, real_2d_array a, integer_1d_array ct, ae_int_t k);
<b>void</b> minlpsetlc(minlpstate state, real_2d_array a, integer_1d_array ct);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlp_basic class=nav>minlp_basic</a> ]</p>
<a name=sub_minlpsetlc2></a><h6 class=pageheader>minlpsetlc2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  two-sided linear  constraints  AL &le; A*x &le; AU  with
sparse constraining matrix A. Recommended for large-scale problems.

This  function  overwrites  linear  (non-box)  constraints set by previous
calls (if such calls were made).

Inputs:
    State   -   structure previously allocated with minlpcreate() call.
    A       -   sparse matrix with size [K,N] (exactly!).
                Each row of A represents one general linear constraint.
                A can be stored in any sparse storage format.
    AL, AU  -   lower and upper bounds, array[K];
                * AL[i]=AU[i] &rArr; equality constraint Ai*x
                * AL[i] &lt; AU[i] &rArr; two-sided constraint AL[i] &le; Ai*x &le; AU[i]
                * AL[i]=-INF  &rArr; one-sided constraint Ai*x &le; AU[i]
                * AU[i]=+INF  &rArr; one-sided constraint AL[i] &le; Ai*x
                * AL[i]=-INF, AU[i]=+INF &rArr; constraint is ignored
    K       -   number  of equality/inequality constraints, K &ge; 0.  If  K=0
                is specified, A, AL, AU are ignored.
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetlc2(minlpstate state, sparsematrix a, real_1d_array al, real_1d_array au, ae_int_t k);
</pre>
<a name=sub_minlpsetlc2dense></a><h6 class=pageheader>minlpsetlc2dense Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets two-sided linear constraints AL &le; A*x &le; AU.

This version accepts dense matrix as  input;  internally  LP  solver  uses
sparse storage  anyway  (most  LP  problems  are  sparse),  but  for  your
convenience it may accept dense inputs. This  function  overwrites  linear
constraints set by previous calls (if such calls were made).

We recommend you to use sparse version of this function unless  you  solve
small-scale LP problem (less than few hundreds of variables).

NOTE: there also exist several versions of this function:
      * one-sided dense version which  accepts  constraints  in  the  same
        format as one used by QP and  NLP solvers
      * two-sided sparse version which accepts sparse matrix
      * two-sided dense  version which allows you to add constraints row by row
      * two-sided sparse version which allows you to add constraints row by row

Inputs:
    State   -   structure previously allocated with minlpcreate() call.
    A       -   linear constraints, array[K,N]. Each row of  A  represents
                one  constraint. One-sided  inequality   constraints, two-
                sided inequality  constraints,  equality  constraints  are
                supported (see below)
    AL, AU  -   lower and upper bounds, array[K];
                * AL[i]=AU[i] &rArr; equality constraint Ai*x
                * AL[i] &lt; AU[i] &rArr; two-sided constraint AL[i] &le; Ai*x &le; AU[i]
                * AL[i]=-INF  &rArr; one-sided constraint Ai*x &le; AU[i]
                * AU[i]=+INF  &rArr; one-sided constraint AL[i] &le; Ai*x
                * AL[i]=-INF, AU[i]=+INF &rArr; constraint is ignored
    K       -   number of equality/inequality constraints,  K &ge; 0;  if  not
                given, inferred from sizes of A, AL, AU.
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetlc2dense(minlpstate state, real_2d_array a, real_1d_array al, real_1d_array au, ae_int_t k);
<b>void</b> minlpsetlc2dense(minlpstate state, real_2d_array a, real_1d_array al, real_1d_array au);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minlp_basic class=nav>minlp_basic</a> ]</p>
<a name=sub_minlpsetscale></a><h6 class=pageheader>minlpsetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients.

ALGLIB optimizers use scaling matrices to test stopping  conditions and as
preconditioner.

Scale of the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the
   function

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minlpsetscale(minlpstate state, real_1d_array s);
</pre>
<a name=example_minlp_basic></a><h6 class=pageheader>minlp_basic Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates how to minimize</font>
<font color=navy>//</font>
<font color=navy>//     F(x0,x1) = -0.1*x0 - x1</font>
<font color=navy>//</font>
<font color=navy>// subject to box constraints</font>
<font color=navy>//</font>
<font color=navy>//     -1 &le; x0,x1 &le; +1 </font>
<font color=navy>//</font>
<font color=navy>// and general linear constraints</font>
<font color=navy>//</font>
<font color=navy>//     x0 - x1 &ge; -1</font>
<font color=navy>//     x0 + x1 &le;  1</font>
<font color=navy>//</font>
<font color=navy>// We use dual simplex solver provided by ALGLIB <b>for</b> this task. Box</font>
<font color=navy>// constraints are specified by means of constraint vectors bndl and</font>
<font color=navy>// bndu (we have bndl &le; x &le; bndu). General linear constraints are</font>
<font color=navy>// specified as AL &le; A*x &le; AU, with AL/AU being 2x1 vectors and A being</font>
<font color=navy>// 2x2 matrix.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: some/all components of AL/AU can be +-INF, same applies to</font>
<font color=navy>//       bndl/bndu. You can also have AL[I]=AU[i] (as well as</font>
<font color=navy>//       BndL[i]=BndU[i]).</font>
   real_2d_array a = <font color=blue><b>&quot;[[1,-1],[1,+1]]&quot;</b></font>;
   real_1d_array al = <font color=blue><b>&quot;[-1,-inf]&quot;</b></font>;
   real_1d_array au = <font color=blue><b>&quot;[+inf,+1]&quot;</b></font>;
   real_1d_array c = <font color=blue><b>&quot;[-0.1,-1]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[-1,-1]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[+1,+1]&quot;</b></font>;
   real_1d_array x;
   minlpstate state;
   minlpreport rep;
   minlpcreate(2, state);
<font color=navy>// Set cost vector, box constraints, general linear constraints.</font>
<font color=navy>//</font>
<font color=navy>// Box constraints can be set in one call to minlpsetbc() or minlpsetbcall()</font>
<font color=navy>// (latter sets same constraints <b>for</b> all variables and accepts two scalars</font>
<font color=navy>// instead of two vectors).</font>
<font color=navy>//</font>
<font color=navy>// General linear constraints can be specified in several ways:</font>
<font color=navy>// * minlpsetlc2dense() - accepts dense 2D array as input; sometimes this</font>
<font color=navy>//   approach is more convenient, although less memory-efficient.</font>
<font color=navy>// * minlpsetlc2() - accepts sparse matrix as input</font>
<font color=navy>// * minlpaddlc2dense() - appends one row to the current set of constraints;</font>
<font color=navy>//   row being appended is specified as dense vector</font>
<font color=navy>// * minlpaddlc2() - appends one row to the current set of constraints;</font>
<font color=navy>//   row being appended is specified as sparse set of elements</font>
<font color=navy>// Independently from specific function being used, LP solver uses sparse</font>
<font color=navy>// storage format <b>for</b> internal representation of constraints.</font>
   minlpsetcost(state, c);
   minlpsetbc(state, bndl, bndu);
   minlpsetlc2dense(state, a, al, au, 2);
<font color=navy>// Set scale of the parameters.</font>
<font color=navy>//</font>
<font color=navy>// It is strongly recommended that you set scale of your variables.</font>
<font color=navy>// Knowing their scales is essential <b>for</b> evaluation of stopping criteria</font>
<font color=navy>// and <b>for</b> preconditioning of the algorithm steps.</font>
<font color=navy>// You can find more information on scaling at http://www.alglib.net/optimization/scaling.php</font>
   minlpsetscale(state, s);
<font color=navy>// Solve</font>
   minlpoptimize(state);
   minlpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(3).c_str()); <font color=navy>// EXPECTED: [0,1]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_minnlc></a><h4 class=pageheader>8.8.8. minnlc Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_minnlcreport class=toc>minnlcreport</a> |
<a href=#struct_minnlcstate class=toc>minnlcstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_minnlccreate class=toc>minnlccreate</a> |
<a href=#sub_minnlccreatef class=toc>minnlccreatef</a> |
<a href=#sub_minnlcoptguardgradient class=toc>minnlcoptguardgradient</a> |
<a href=#sub_minnlcoptguardnonc1test0results class=toc>minnlcoptguardnonc1test0results</a> |
<a href=#sub_minnlcoptguardnonc1test1results class=toc>minnlcoptguardnonc1test1results</a> |
<a href=#sub_minnlcoptguardresults class=toc>minnlcoptguardresults</a> |
<a href=#sub_minnlcoptguardsmoothness class=toc>minnlcoptguardsmoothness</a> |
<a href=#sub_minnlcoptimize class=toc>minnlcoptimize</a> |
<a href=#sub_minnlcrequesttermination class=toc>minnlcrequesttermination</a> |
<a href=#sub_minnlcrestartfrom class=toc>minnlcrestartfrom</a> |
<a href=#sub_minnlcresults class=toc>minnlcresults</a> |
<a href=#sub_minnlcresultsbuf class=toc>minnlcresultsbuf</a> |
<a href=#sub_minnlcsetalgoaul class=toc>minnlcsetalgoaul</a> |
<a href=#sub_minnlcsetalgoslp class=toc>minnlcsetalgoslp</a> |
<a href=#sub_minnlcsetalgosqp class=toc>minnlcsetalgosqp</a> |
<a href=#sub_minnlcsetbc class=toc>minnlcsetbc</a> |
<a href=#sub_minnlcsetcond class=toc>minnlcsetcond</a> |
<a href=#sub_minnlcsetlc class=toc>minnlcsetlc</a> |
<a href=#sub_minnlcsetnlc class=toc>minnlcsetnlc</a> |
<a href=#sub_minnlcsetprecexactlowrank class=toc>minnlcsetprecexactlowrank</a> |
<a href=#sub_minnlcsetprecexactrobust class=toc>minnlcsetprecexactrobust</a> |
<a href=#sub_minnlcsetprecinexact class=toc>minnlcsetprecinexact</a> |
<a href=#sub_minnlcsetprecnone class=toc>minnlcsetprecnone</a> |
<a href=#sub_minnlcsetscale class=toc>minnlcsetscale</a> |
<a href=#sub_minnlcsetstpmax class=toc>minnlcsetstpmax</a> |
<a href=#sub_minnlcsetxrep class=toc>minnlcsetxrep</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_minnlc_d_equality class=toc>minnlc_d_equality</a></td><td width=15>&nbsp;</td><td>Nonlinearly constrained optimization (equality constraints)</td></tr>
<tr align=left valign=top><td><a href=#example_minnlc_d_inequality class=toc>minnlc_d_inequality</a></td><td width=15>&nbsp;</td><td>Nonlinearly constrained optimization (inequality constraints)</td></tr>
<tr align=left valign=top><td><a href=#example_minnlc_d_mixed class=toc>minnlc_d_mixed</a></td><td width=15>&nbsp;</td><td>Nonlinearly constrained optimization with mixed equality/inequality constraints</td></tr>
</table>
</div>
<a name=struct_minnlcreport></a><h6 class=pageheader>minnlcreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
These fields store optimization report:
* iterationscount           total number of inner iterations
* nfev                      number of gradient evaluations
* terminationtype           termination type (see below)

Scaled constraint violations are reported:
* bcerr                     maximum violation of the box constraints
* bcidx                     index of the most violated box  constraint (or
                            -1, if all box constraints  are  satisfied  or
                            there is no box constraint)
* lcerr                     maximum violation of the  linear  constraints,
                            computed as maximum  scaled  distance  between
                            final point and constraint boundary.
* lcidx                     index of the most violated  linear  constraint
                            (or -1, if all constraints  are  satisfied  or
                            there is no general linear constraints)
* nlcerr                    maximum violation of the nonlinear constraints
* nlcidx                    index of the most violated nonlinear constraint
                            (or -1, if all constraints  are  satisfied  or
                            there is no nonlinear constraints)

Violations of box constraints are scaled on per-component basis  according
to  the  scale  vector s[] as specified by minnlcsetscale(). Violations of
the general linear  constraints  are  also  computed  using  user-supplied
variable scaling. Violations of nonlinear constraints are computed &quot;as is&quot;

TERMINATION CODES

TerminationType field contains completion code, which can be either:

==== FAILURE CODE ====
  -8    internal integrity control detected  infinite  or  NAN  values  in
        function/gradient. Abnormal termination signaled.
  -3    box  constraints  are  infeasible.  Note: infeasibility of non-box
        constraints does NOT trigger emergency  completion;  you  have  to
        examine  bcerr/lcerr/nlcerr   to  detect   possibly   inconsistent
        constraints.

==== SUCCESS CODE ====
   2    relative step is no more than EpsX.
   5    MaxIts steps was taken
   7    stopping conditions are too stringent,
        further improvement is impossible,
        X contains best point found so far.
   8    user requested algorithm termination via minnlcrequesttermination(),
        last accepted point is returned

Other fields of this structure are not documented and should not be used!
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minnlcreport {
   ae_int_t iterationscount;
   ae_int_t nfev;
   ae_int_t terminationtype;
   <b>double</b> bcerr;
   ae_int_t bcidx;
   <b>double</b> lcerr;
   ae_int_t lcidx;
   <b>double</b> nlcerr;
   ae_int_t nlcidx;
   ae_int_t dbgphase0its;
};
</pre>
<a name=struct_minnlcstate></a><h6 class=pageheader>minnlcstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores nonlinear optimizer state.
You should use functions provided by MinNLC subpackage to work  with  this
object
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minnlcstate {
   bool needfi;
   bool needfij;
   bool xupdated;
   double f;
   real_1d_array fi;
   real_2d_array j;
   real_1d_array x;
};
</pre>
<a name=sub_minnlccreate></a><h6 class=pageheader>minnlccreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
NONLINEARLY CONSTRAINED OPTIMIZATION
WITH PRECONDITIONED AUGMENTED LAGRANGIAN ALGORITHM
The  subroutine  minimizes  function   F(x)  of N arguments subject to any
combination of:
* bound constraints
* linear inequality constraints
* linear equality constraints
* nonlinear equality constraints Gi(x)=0
* nonlinear inequality constraints Hi(x) &le; 0

REQUIREMENTS:
* user must provide function value and gradient for F(), H(), G()
* starting point X0 must be feasible or not too far away from the feasible
  set
* F(), G(), H() are continuously differentiable on the  feasible  set  and
  its neighborhood
* nonlinear constraints G() and H() must have non-zero gradient at  G(x)=0
  and at H(x)=0. Say, constraint like x^2 &ge; 1 is supported, but x^2 &ge; 0   is
  NOT supported.

USAGE:

Constrained optimization if far more complex than the  unconstrained  one.
Nonlinearly constrained optimization is one of the most esoteric numerical
procedures.

Here we give very brief outline  of  the  MinNLC  optimizer.  We  strongly
recommend you to study examples in the ALGLIB Reference Manual and to read
ALGLIB User Guide on optimization, which is available at
http://www.alglib.net/optimization/

1. User initializes algorithm state with MinNLCCreate() call  and  chooses
   what NLC solver to use. There is some solver which is used by  default,
   with default settings, but you should NOT rely on  default  choice.  It
   may change in future releases of ALGLIB without notice, and no one  can
   guarantee that new solver will be  able  to  solve  your  problem  with
   default settings.

   From the other side, if you choose solver explicitly, you can be pretty
   sure that it will work with new ALGLIB releases.

   In the current release following solvers can be used:
   * SQP solver, recommended for medium-scale problems (less than thousand
     of variables) with hard-to-evaluate target functions.  Requires  less
     function  evaluations  than  other  solvers  but  each  step involves
     solution of QP subproblem, so running time may be higher than that of
     AUL (another recommended option). Activated  with  minnlcsetalgosqp()
     function.
   * AUL solver with dense  preconditioner,  recommended  for  large-scale
     problems or for problems  with  cheap  target  function.  Needs  more
     function evaluations that SQP (about  5x-10x  times  more),  but  its
     iterations  are  much  cheaper  that  that  of  SQP.  Activated  with
     minnlcsetalgoaul() function.
   * SLP solver, successive linear programming. The slowest one,  requires
     more target function evaluations that SQP and  AUL.  However,  it  is
     somewhat more robust in tricky cases, so it can be used  as  a backup
     plan. Activated with minnlcsetalgoslp() function.

2. [optional] user activates OptGuard  integrity checker  which  tries  to
   detect possible errors in the user-supplied callbacks:
   * discontinuity/nonsmoothness of the target/nonlinear constraints
   * errors in the analytic gradient provided by user
   This feature is essential for early prototyping stages because it helps
   to catch common coding and problem statement errors.
   OptGuard can be activated with following functions (one per each  check
   performed):
   * minnlcoptguardsmoothness()
   * minnlcoptguardgradient()

3. User adds boundary and/or linear and/or nonlinear constraints by  means
   of calling one of the following functions:
   a) minnlcsetbc() for boundary constraints
   b) minnlcsetlc() for linear constraints
   c) minnlcsetnlc() for nonlinear constraints
   You may combine (a), (b) and (c) in one optimization problem.

4. User sets scale of the variables with minnlcsetscale() function. It  is
   VERY important to set  scale  of  the  variables,  because  nonlinearly
   constrained problems are hard to solve when variables are badly scaled.

5. User sets  stopping  conditions  with  minnlcsetcond(). If  NLC  solver
   uses  inner/outer  iteration  layout,  this  function   sets   stopping
   conditions for INNER iterations.

6. Finally, user calls minnlcoptimize()  function  which  takes  algorithm
   state and pointer (delegate, etc.) to callback function which calculates
   F/G/H.

7. User calls  minnlcresults()  to  get  solution;  additionally  you  can
   retrieve OptGuard report with minnlcoptguardresults(), and get detailed
   report about purported errors in the target function with:
   * minnlcoptguardnonc1test0results()
   * minnlcoptguardnonc1test1results()

8. Optionally user may call minnlcrestartfrom() to solve  another  problem
   with same N but another starting point. minnlcrestartfrom()  allows  to
   reuse already initialized structure.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size ofX
    X       -   starting point, array[N]:
                * it is better to set X to a feasible point
                * but X can be infeasible, in which case algorithm will try
                  to find feasible point first, using X as initial
                  approximation.

Outputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlccreate(ae_int_t n, real_1d_array x, minnlcstate &amp;state);
<b>void</b> minnlccreate(real_1d_array x, minnlcstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> | <a href=#example_minnlc_d_mixed class=nav>minnlc_d_mixed</a> ]</p>
<a name=sub_minnlccreatef></a><h6 class=pageheader>minnlccreatef Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine is a finite  difference variant of MinNLCCreate(). It uses
finite differences in order to differentiate target function.

Description below contains information which is specific to this  function
only. We recommend to read comments on MinNLCCreate() in order to get more
information about creation of NLC optimizer.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size ofX
    X       -   starting point, array[N]:
                * it is better to set X to a feasible point
                * but X can be infeasible, in which case algorithm will try
                  to find feasible point first, using X as initial
                  approximation.
    DiffStep-   differentiation step, &gt; 0

Outputs:
    State   -   structure stores algorithm state

NOTES:
1. algorithm uses 4-point central formula for differentiation.
2. differentiation step along I-th axis is equal to DiffStep*S[I] where
   S[] is scaling vector which can be set by MinNLCSetScale() call.
3. we recommend you to use moderate values of  differentiation  step.  Too
   large step will result in too large TRUNCATION  errors, while too small
   step will result in too large NUMERICAL  errors.  1.0E-4  can  be  good
   value to start from.
4. Numerical  differentiation  is   very   inefficient  -   one   gradient
   calculation needs 4*N function evaluations. This function will work for
   any N - either small (1...10), moderate (10...100) or  large  (100...).
   However, performance penalty will be too severe for any N's except  for
   small ones.
   We should also say that code which relies on numerical  differentiation
   is  less   robust   and  precise.  Imprecise  gradient  may  slow  down
   convergence, especially on highly nonlinear problems.
   Thus  we  recommend to use this function for fast prototyping on small-
   dimensional problems only, and to implement analytical gradient as soon
   as possible.
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlccreatef(ae_int_t n, real_1d_array x, <b>double</b> diffstep, minnlcstate &amp;state);
<b>void</b> minnlccreatef(real_1d_array x, <b>double</b> diffstep, minnlcstate &amp;state);
</pre>
<a name=sub_minnlcoptguardgradient></a><h6 class=pageheader>minnlcoptguardgradient Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates verification  of  the  user-supplied
analytic gradient/Jacobian.

Upon  activation  of  this  option  OptGuard  integrity  checker  performs
numerical differentiation of your target  function  (constraints)  at  the
initial point (note: future versions may also perform check  at  the final
point) and compares numerical gradient/Jacobian with analytic one provided
by you.

If difference is too large, an error flag is set and optimization  session
continues. After optimization session is over, you can retrieve the report
which stores both gradients/Jacobians, and specific components highlighted
as suspicious by the OptGuard.

The primary OptGuard report can be retrieved with minnlcoptguardresults().

IMPORTANT: gradient check is a high-overhead option which  will  cost  you
           about 3*N additional function evaluations. In many cases it may
           cost as much as the rest of the optimization session.

           YOU SHOULD NOT USE IT IN THE PRODUCTION CODE UNLESS YOU WANT TO
           CHECK DERIVATIVES PROVIDED BY SOME THIRD PARTY.

NOTE: unlike previous incarnation of the gradient checking code,  OptGuard
      does NOT interrupt optimization even if it discovers bad gradient.

Inputs:
    State       -   structure used to store algorithm state
    TestStep    -   verification step used for numerical differentiation:
                    * TestStep=0 turns verification off
                    * TestStep &gt; 0 activates verification
                    You should carefully choose TestStep. Value  which  is
                    too large (so large that  function  behavior  is  non-
                    cubic at this scale) will lead  to  false  alarms. Too
                    short step will result in rounding  errors  dominating
                    numerical derivative.

                    You may use different step for different parameters by
                    means of setting scale with minnlcsetscale().

==== EXPLANATION ====

In order to verify gradient algorithm performs following steps:
  * two trial steps are made to X[i]-TestStep*S[i] and X[i]+TestStep*S[i],
    where X[i] is i-th component of the initial point and S[i] is a  scale
    of i-th parameter
  * F(X) is evaluated at these trial points
  * we perform one more evaluation in the middle point of the interval
  * we  build  cubic  model using function values and derivatives at trial
    points and we compare its prediction with actual value in  the  middle
    point
ALGLIB: Copyright 15.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcoptguardgradient(minnlcstate state, <b>double</b> teststep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> ]</p>
<a name=sub_minnlcoptguardnonc1test0results></a><h6 class=pageheader>minnlcoptguardnonc1test0results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #0

Nonsmoothness (non-C1) test #0 studies  function  values  (not  gradient!)
obtained during line searches and monitors  behavior  of  the  directional
derivative estimate.

This test is less powerful than test #1, but it does  not  depend  on  the
gradient values and thus it is more robust against artifacts introduced by
numerical differentiation.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* fidx - is an index of the function (0 for  target  function, 1 or higher
  for nonlinear constraints) which is suspected of being &quot;non-C1&quot;
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], f[] - arrays of length CNT which store step lengths and  function
  values at these points; f[i] is evaluated in x0+stp[i]*d.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #0 &quot;strong&quot; report
    LngRep  -   C1 test #0 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcoptguardnonc1test0results(minnlcstate state, optguardnonc1test0report &amp;strrep, optguardnonc1test0report &amp;lngrep);
</pre>
<a name=sub_minnlcoptguardnonc1test1results></a><h6 class=pageheader>minnlcoptguardnonc1test1results Function</h6>
<hr width=600 align=left>
<pre class=narration>
Detailed results of the OptGuard integrity check for nonsmoothness test #1

Nonsmoothness (non-C1)  test  #1  studies  individual  components  of  the
gradient computed during line search.

When precise analytic gradient is provided this test is more powerful than
test #0  which  works  with  function  values  and  ignores  user-provided
gradient.  However,  test  #0  becomes  more   powerful   when   numerical
differentiation is employed (in such cases test #1 detects  higher  levels
of numerical noise and becomes too conservative).

This test also tells specific components of the gradient which violate  C1
continuity, which makes it more informative than #0, which just tells that
continuity is violated.

Two reports are returned:
* a &quot;strongest&quot; one, corresponding  to  line   search  which  had  highest
  value of the nonsmoothness indicator
* a &quot;longest&quot; one, corresponding to line search which  had  more  function
  evaluations, and thus is more detailed

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* fidx - is an index of the function (0 for  target  function, 1 or higher
  for nonlinear constraints) which is suspected of being &quot;non-C1&quot;
* vidx - is an index of the variable in [0,N) with nonsmooth derivative
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], g[] - arrays of length CNT which store step lengths and  gradient
  values at these points; g[i] is evaluated in  x0+stp[i]*d  and  contains
  vidx-th component of the gradient.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

= SHORTLY SPEAKING: build a 2D plot of (stp,f) and look at it -  you  will see where C1 continuity is violated.

Inputs:
    State   -   algorithm state

Outputs:
    StrRep  -   C1 test #1 &quot;strong&quot; report
    LngRep  -   C1 test #1 &quot;long&quot; report
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcoptguardnonc1test1results(minnlcstate state, optguardnonc1test1report &amp;strrep, optguardnonc1test1report &amp;lngrep);
</pre>
<a name=sub_minnlcoptguardresults></a><h6 class=pageheader>minnlcoptguardresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
Results of OptGuard integrity check, should be called  after  optimization
session is over.

==== PRIMARY REPORT ====

OptGuard performs several checks which are intended to catch common errors
in the implementation of nonlinear function/gradient:
* incorrect analytic gradient
* discontinuous (non-C0) target functions (constraints)
* nonsmooth     (non-C1) target functions (constraints)

Each of these checks is activated with appropriate function:
* minnlcoptguardgradient() for gradient verification
* minnlcoptguardsmoothness() for C0/C1 checks

Following flags are set when these errors are suspected:
* rep.badgradsuspected, and additionally:
  * rep.badgradfidx for specific function (Jacobian row) suspected
  * rep.badgradvidx for specific variable (Jacobian column) suspected
  * rep.badgradxbase, a point where gradient/Jacobian is tested
  * rep.badgraduser, user-provided gradient/Jacobian
  * rep.badgradnum, reference gradient/Jacobian obtained via numerical
    differentiation
* rep.nonc0suspected, and additionally:
  * rep.nonc0fidx - an index of specific function violating C0 continuity
* rep.nonc1suspected, and additionally
  * rep.nonc1fidx - an index of specific function violating C1 continuity
Here function index 0 means  target function, index 1  or  higher  denotes
nonlinear constraints.

==== ADDITIONAL REPORTS/LOGS ====

Several different tests are performed to catch C0/C1 errors, you can  find
out specific test signaled error by looking to:
* rep.nonc0test0positive, for non-C0 test #0
* rep.nonc1test0positive, for non-C1 test #0
* rep.nonc1test1positive, for non-C1 test #1

Additional information (including line search logs)  can  be  obtained  by
means of:
* minnlcoptguardnonc1test0results()
* minnlcoptguardnonc1test1results()
which return detailed error reports, specific points where discontinuities
were found, and so on.

Inputs:
    State   -   algorithm state

Outputs:
    Rep     -   generic OptGuard report;  more  detailed  reports  can  be
                retrieved with other functions.

NOTE: false negatives (nonsmooth problems are not identified as  nonsmooth
      ones) are possible although unlikely.

      The reason  is  that  you  need  to  make several evaluations around
      nonsmoothness  in  order  to  accumulate  enough  information  about
      function curvature. Say, if you start right from the nonsmooth point,
      optimizer simply won't get enough data to understand what  is  going
      wrong before it terminates due to abrupt changes in the  derivative.
      It is also  possible  that  &quot;unlucky&quot;  step  will  move  us  to  the
      termination too quickly.

      Our current approach is to have less than 0.1%  false  negatives  in
      our test examples  (measured  with  multiple  restarts  from  random
      points), and to have exactly 0% false positives.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcoptguardresults(minnlcstate state, optguardreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> ]</p>
<a name=sub_minnlcoptguardsmoothness></a><h6 class=pageheader>minnlcoptguardsmoothness Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  activates/deactivates nonsmoothness monitoring  option  of
the  OptGuard  integrity  checker. Smoothness  monitor  silently  observes
solution process and tries to detect ill-posed problems, i.e. ones with:
a) discontinuous target function (non-C0) and/or constraints
b) nonsmooth     target function (non-C1) and/or constraints

Smoothness monitoring does NOT interrupt optimization  even if it suspects
that your problem is nonsmooth. It just sets corresponding  flags  in  the
OptGuard report which can be retrieved after optimization is over.

Smoothness monitoring is a moderate overhead option which often adds  less
than 1% to the optimizer running time. Thus, you can use it even for large
scale problems.

NOTE: OptGuard does  NOT  guarantee  that  it  will  always  detect  C0/C1
      continuity violations.

      First, minor errors are hard to  catch - say, a 0.0001 difference in
      the model values at two sides of the gap may be due to discontinuity
      of the model - or simply because the model has changed.

      Second, C1-violations  are  especially  difficult  to  detect  in  a
      noninvasive way. The optimizer usually  performs  very  short  steps
      near the nonsmoothness, and differentiation  usually   introduces  a
      lot of numerical noise.  It  is  hard  to  tell  whether  some  tiny
      discontinuity in the slope is due to real nonsmoothness or just  due
      to numerical noise alone.

      Our top priority was to avoid false positives, so in some rare cases
      minor errors may went unnoticed (however, in most cases they can  be
      spotted with restart from different initial point).

Inputs:
    State   -   algorithm state
    Level   -   monitoring level:
                * 0 - monitoring is disabled
                * 1 - noninvasive low-overhead monitoring; function values
                      and/or gradients are recorded, but OptGuard does not
                      try to perform additional evaluations  in  order  to
                      get more information about suspicious locations.
                      This kind of monitoring does not work well with  SQP
                      because SQP solver needs just 1-2 function evaluations
                      per step, which is not enough for OptGuard  to  make
                      any conclusions.

==== EXPLANATION ====

One major source of headache during optimization  is  the  possibility  of
the coding errors in the target function/constraints (or their gradients).
Such  errors   most   often   manifest   themselves  as  discontinuity  or
nonsmoothness of the target/constraints.

Another frequent situation is when you try to optimize something involving
lots of min() and max() operations, i.e. nonsmooth target. Although not  a
coding error, it is nonsmoothness anyway - and smooth  optimizers  usually
stop right after encountering nonsmoothness, well before reaching solution.

OptGuard integrity checker helps you to catch such situations: it monitors
function values/gradients being passed  to  the  optimizer  and  tries  to
errors. Upon discovering suspicious pair of points it  raises  appropriate
flag (and allows you to continue optimization). When optimization is done,
you can study OptGuard result.
ALGLIB: Copyright 21.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcoptguardsmoothness(minnlcstate state, ae_int_t level);
<b>void</b> minnlcoptguardsmoothness(minnlcstate state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> ]</p>
<a name=sub_minnlcoptimize></a><h6 class=pageheader>minnlcoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear optimizer

These functions accept following parameters:
    state   -   algorithm state
    fvec    -   callback which calculates function vector fi[]
                at given point x
    jac     -   callback which calculates function vector fi[]
                and Jacobian jac at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL

NOTES:

1. This function has two different implementations: one which  uses  exact
   (analytical) user-supplied Jacobian, and one which uses  only  function
   vector and numerically  differentiates  function  in  order  to  obtain
   gradient.

   Depending  on  the  specific  function  used to create optimizer object
   you should choose appropriate variant of MinNLCOptimize() -  one  which
   accepts function AND Jacobian or one which accepts ONLY function.

   Be careful to choose variant of MinNLCOptimize()  which  corresponds to
   your optimization scheme! Table below lists different  combinations  of
   callback (function/gradient) passed to MinNLCOptimize()   and  specific
   function used to create optimizer.

                     |         USER PASSED TO MinNLCOptimize()
   CREATED WITH      |  function only   |  function and gradient
   ------------------------------------------------------------
   MinNLCCreateF()   |     works               FAILS
   MinNLCCreate()    |     FAILS               works

   Here &quot;FAILS&quot; denotes inappropriate combinations  of  optimizer creation
   function  and  MinNLCOptimize()  version.   Attemps   to    use    such
   combination will lead to exception. Either  you  did  not pass gradient
   when it WAS needed or you passed gradient when it was NOT needed.
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcoptimize(minnlcstate &amp;state, <b>void</b> (*fvec)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> minnlcoptimize(minnlcstate &amp;state, <b>void</b> (*jac)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> | <a href=#example_minnlc_d_mixed class=nav>minnlc_d_mixed</a> ]</p>
<a name=sub_minnlcrequesttermination></a><h6 class=pageheader>minnlcrequesttermination Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine submits request for termination of running  optimizer.  It
should be called from user-supplied callback when user decides that it  is
time to &quot;smoothly&quot; terminate optimization process.  As  result,  optimizer
stops at point which was &quot;current accepted&quot; when termination  request  was
submitted and returns error code 8 (successful termination).

Inputs:
    State   -   optimizer structure

NOTE: after  request  for  termination  optimizer  may   perform   several
      additional calls to user-supplied callbacks. It does  NOT  guarantee
      to stop immediately - it just guarantees that these additional calls
      will be discarded later.

NOTE: calling this function on optimizer which is NOT running will have no
      effect.

NOTE: multiple calls to this function are possible. First call is counted,
      subsequent calls are silently ignored.
ALGLIB: Copyright 08.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcrequesttermination(minnlcstate state);
</pre>
<a name=sub_minnlcrestartfrom></a><h6 class=pageheader>minnlcrestartfrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine restarts algorithm from new point.
All optimization parameters (including constraints) are left unchanged.

This  function  allows  to  solve multiple  optimization  problems  (which
must have  same number of dimensions) without object reallocation penalty.

Inputs:
    State   -   structure previously allocated with MinNLCCreate call.
    X       -   new starting point.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcrestartfrom(minnlcstate state, real_1d_array x);
</pre>
<a name=sub_minnlcresults></a><h6 class=pageheader>minnlcresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
MinNLC results:  the  solution  found,  completion  codes  and  additional
information.

If you activated OptGuard integrity checking functionality and want to get
OptGuard report, it can be retrieved with:
* minnlcoptguardresults() - for a primary report about (a) suspected C0/C1
  continuity violations and (b) errors in the analytic gradient.
* minnlcoptguardnonc1test0results() - for C1 continuity violation test #0,
  detailed line search log
* minnlcoptguardnonc1test1results() - for C1 continuity violation test #1,
  detailed line search log

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[0..N-1], solution
    Rep     -   optimization report, contains information about completion
                code, constraint violation at the solution and so on.

                You   should   check   rep.terminationtype  in  order   to
                distinguish successful termination from unsuccessful one:

                ==== FAILURE CODES ====
                * -8    internal  integrity control  detected  infinite or
                        NAN   values    in   function/gradient.   Abnormal
                        termination signalled.
                * -3    box  constraints are infeasible.
                        Note: infeasibility of  non-box  constraints  does
                              NOT trigger emergency completion;  you  have
                              to examine rep.bcerr/rep.lcerr/rep.nlcerr to
                              detect possibly inconsistent constraints.

                ==== SUCCESS CODES ====
                *  2   scaled step is no more than EpsX.
                *  5   MaxIts steps were taken.
                *  8   user   requested    algorithm    termination    via
                       minnlcrequesttermination(), last accepted point  is
                       returned.

                More information about fields of this  structure  can  be
                found in the comments on minnlcreport datatype.
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcresults(minnlcstate state, real_1d_array &amp;x, minnlcreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> | <a href=#example_minnlc_d_mixed class=nav>minnlc_d_mixed</a> ]</p>
<a name=sub_minnlcresultsbuf></a><h6 class=pageheader>minnlcresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
NLC results

Buffered implementation of MinNLCResults() which uses pre-allocated buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcresultsbuf(minnlcstate state, real_1d_array &amp;x, minnlcreport &amp;rep);
</pre>
<a name=sub_minnlcsetalgoaul></a><h6 class=pageheader>minnlcsetalgoaul Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  tells MinNLC unit to use  Augmented  Lagrangian  algorithm
for nonlinearly constrained  optimization.  This  algorithm  is  a  slight
modification of one described in &quot;A Modified Barrier-Augmented  Lagrangian
Method for  Constrained  Minimization  (1999)&quot;  by  D.GOLDFARB,  R.POLYAK,
K. SCHEINBERG, I.YUZEFOVICH.

AUL solver can be significantly faster than SQP on easy  problems  due  to
cheaper iterations, although it needs more function evaluations.

Augmented Lagrangian algorithm works by converting problem  of  minimizing
F(x) subject to equality/inequality constraints   to unconstrained problem
of the form

    min[ f(x) +
        + Rho*PENALTY_EQ(x)   + SHIFT_EQ(x,Nu1) +
        + Rho*PENALTY_INEQ(x) + SHIFT_INEQ(x,Nu2) ]

where:
* Rho is a fixed penalization coefficient
* PENALTY_EQ(x) is a penalty term, which is used to APPROXIMATELY  enforce
  equality constraints
* SHIFT_EQ(x) is a special &quot;shift&quot;  term  which  is  used  to  &quot;fine-tune&quot;
  equality constraints, greatly increasing precision
* PENALTY_INEQ(x) is a penalty term which is used to approximately enforce
  inequality constraints
* SHIFT_INEQ(x) is a special &quot;shift&quot;  term  which  is  used to &quot;fine-tune&quot;
  inequality constraints, greatly increasing precision
* Nu1/Nu2 are vectors of Lagrange coefficients which are fine-tuned during
  outer iterations of algorithm

This  version  of  AUL  algorithm  uses   preconditioner,  which   greatly
accelerates convergence. Because this  algorithm  is  similar  to  penalty
methods,  it  may  perform  steps  into  infeasible  area.  All  kinds  of
constraints (boundary, linear and nonlinear ones) may   be   violated   in
intermediate points - and in the solution.  However,  properly  configured
AUL method is significantly better at handling  constraints  than  barrier
and/or penalty methods.

The very basic outline of algorithm is given below:
1) first outer iteration is performed with &quot;default&quot;  values  of  Lagrange
   multipliers Nu1/Nu2. Solution quality is low (candidate  point  can  be
   too  far  away  from  true  solution; large violation of constraints is
   possible) and is comparable with that of penalty methods.
2) subsequent outer iterations  refine  Lagrange  multipliers  and improve
   quality of the solution.

Inputs:
    State   -   structure which stores algorithm state
    Rho     -   penalty coefficient, Rho &gt; 0:
                * large enough  that  algorithm  converges  with   desired
                  precision. Minimum value is 10*max(S'*diag(H)*S),  where
                  S is a scale matrix (set by MinNLCSetScale) and H  is  a
                  Hessian of the function being minimized. If you can  not
                  easily estimate Hessian norm,  see  our  recommendations
                  below.
                * not TOO large to prevent ill-conditioning
                * for unit-scale problems (variables and Hessian have unit
                  magnitude), Rho=100 or Rho=1000 can be used.
                * it is important to note that Rho is internally multiplied
                  by scaling matrix, i.e. optimum value of Rho depends  on
                  scale of variables specified  by  MinNLCSetScale().
    ItsCnt  -   number of outer iterations:
                * ItsCnt=0 means that small number of outer iterations  is
                  automatically chosen (10 iterations in current version).
                * ItsCnt=1 means that AUL algorithm performs just as usual
                  barrier method.
                * ItsCnt &gt; 1 means that  AUL  algorithm  performs  specified
                  number of outer iterations

HOW TO CHOOSE PARAMETERS

Nonlinear optimization is a tricky area and Augmented Lagrangian algorithm
is sometimes hard to tune. Good values of  Rho  and  ItsCnt  are  problem-
specific.  In  order  to  help  you   we   prepared   following   set   of
recommendations:

* for  unit-scale  problems  (variables  and Hessian have unit magnitude),
  Rho=100 or Rho=1000 can be used.

* start from  some  small  value of Rho and solve problem  with  just  one
  outer iteration (ItcCnt=1). In this case algorithm behaves like  penalty
  method. Increase Rho in 2x or 10x steps until you  see  that  one  outer
  iteration returns point which is &quot;rough approximation to solution&quot;.

  It is very important to have Rho so  large  that  penalty  term  becomes
  constraining i.e. modified function becomes highly convex in constrained
  directions.

  From the other side, too large Rho may prevent you  from  converging  to
  the solution. You can diagnose it by studying number of inner iterations
  performed by algorithm: too few (5-10 on  1000-dimensional  problem)  or
  too many (orders of magnitude more than  dimensionality)  usually  means
  that Rho is too large.

* with just one outer iteration you  usually  have  low-quality  solution.
  Some constraints can be violated with very  large  margin,  while  other
  ones (which are NOT violated in the true solution) can push final  point
  too far in the inner area of the feasible set.

  For example, if you have constraint x0 &ge; 0 and true solution  x0=1,  then
  merely a presence of &quot;x0 &ge; 0&quot; will introduce a bias towards larger values
  of x0. Say, algorithm may stop at x0=1.5 instead of 1.0.

* after you found good Rho, you may increase number of  outer  iterations.
  ItsCnt=10 is a good value. Subsequent outer iteration will refine values
  of  Lagrange  multipliers.  Constraints  which  were  violated  will  be
  enforced, inactive constraints will be dropped (corresponding multipliers
  will be decreased). Ideally, you  should  see  10-1000x  improvement  in
  constraint handling (constraint violation is reduced).

* if  you  see  that  algorithm  converges  to  vicinity  of solution, but
  additional outer iterations do not refine solution,  it  may  mean  that
  algorithm is unstable - it wanders around true  solution,  but  can  not
  approach it. Sometimes algorithm may be stabilized by increasing Rho one
  more time, making it 5x or 10x larger.

SCALING OF CONSTRAINTS [IMPORTANT]

AUL optimizer scales   variables   according   to   scale   specified   by
MinNLCSetScale() function, so it can handle  problems  with  badly  scaled
variables (as long as we KNOW their scales).   However,  because  function
being optimized is a mix  of  original  function and  constraint-dependent
penalty  functions, it  is   important  to   rescale  both  variables  AND
constraints.

Say,  if  you  minimize f(x)=x^2 subject to 1000000*x &ge; 0,  then  you  have
constraint whose scale is different from that of target  function (another
example is 0.000001*x &ge; 0). It is also possible to have constraints   whose
scales  are   misaligned:   1000000*x0 &ge; 0, 0.000001*x1 &le; 0.   Inappropriate
scaling may ruin convergence because minimizing x^2 subject to x &ge; 0 is NOT
same as minimizing it subject to 1000000*x &ge; 0.

Because we  know  coefficients  of  boundary/linear  constraints,  we  can
automatically rescale and normalize them. However,  there  is  no  way  to
automatically rescale nonlinear constraints Gi(x) and  Hi(x)  -  they  are
black boxes.

It means that YOU are the one who is  responsible  for  correct scaling of
nonlinear constraints  Gi(x)  and  Hi(x).  We  recommend  you  to  rescale
nonlinear constraints in such way that I-th component of dG/dX (or  dH/dx)
has magnitude approximately equal to 1/S[i] (where S  is  a  scale  set by
MinNLCSetScale() function).

WHAT IF IT DOES NOT CONVERGE?

It is possible that AUL algorithm fails to converge to precise  values  of
Lagrange multipliers. It stops somewhere around true solution, but candidate
point is still too far from solution, and some constraints  are  violated.
Such kind of failure is specific for Lagrangian algorithms -  technically,
they stop at some point, but this point is not constrained solution.

There are exist several reasons why algorithm may fail to converge:
a) too loose stopping criteria for inner iteration
b) degenerate, redundant constraints
c) target function has unconstrained extremum exactly at the  boundary  of
   some constraint
d) numerical noise in the target function

In all these cases algorithm is unstable - each outer iteration results in
large and almost random step which improves handling of some  constraints,
but violates other ones (ideally  outer iterations should form a  sequence
of progressively decreasing steps towards solution).

First reason possible is  that  too  loose  stopping  criteria  for  inner
iteration were specified. Augmented Lagrangian algorithm solves a sequence
of intermediate problems, and requries each of them to be solved with high
precision. Insufficient precision results in incorrect update of  Lagrange
multipliers.

Another reason is that you may have specified degenerate constraints: say,
some constraint was repeated twice. In most cases AUL algorithm gracefully
handles such situations, but sometimes it may spend too much time figuring
out subtle degeneracies in constraint matrix.

Third reason is tricky and hard to diagnose. Consider situation  when  you
minimize  f=x^2  subject to constraint x &ge; 0.  Unconstrained   extremum  is
located  exactly  at  the  boundary  of  constrained  area.  In  this case
algorithm will tend to oscillate between negative  and  positive  x.  Each
time it stops at x &lt; 0 it &quot;reinforces&quot; constraint x &ge; 0, and each time it  is
bounced to x &gt; 0 it &quot;relaxes&quot; constraint (and is  attracted  to  x &lt; 0).

Such situation  sometimes  happens  in  problems  with  hidden  symetries.
Algorithm  is  got  caught  in  a  loop with  Lagrange  multipliers  being
continuously increased/decreased. Luckily, such loop forms after at  least
three iterations, so this problem can be solved by  DECREASING  number  of
outer iterations down to 1-2 and increasing  penalty  coefficient  Rho  as
much as possible.

Final reason is numerical noise. AUL algorithm is robust against  moderate
noise (more robust than, say, active set methods),  but  large  noise  may
destabilize algorithm.
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetalgoaul(minnlcstate state, <b>double</b> rho, ae_int_t itscnt);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> | <a href=#example_minnlc_d_mixed class=nav>minnlc_d_mixed</a> ]</p>
<a name=sub_minnlcsetalgoslp></a><h6 class=pageheader>minnlcsetalgoslp Function</h6>
<hr width=600 align=left>
<pre class=narration>
This   function  tells  MinNLC  optimizer  to  use  SLP (Successive Linear
Programming) algorithm for  nonlinearly  constrained   optimization.  This
algorithm  is  a  slight  modification  of  one  described  in  &quot;A  Linear
programming-based optimization algorithm for solving nonlinear programming
problems&quot; (2010) by Claus Still and Tapio Westerlund.

This solver is the slowest one in ALGLIB, it requires more target function
evaluations that SQP and AUL. However it is somewhat more robust in tricky
cases, so it can be used as a backup plan. We recommend to use  this  algo
when SQP/AUL do not work (does not return  the  solution  you  expect). If
trying different approach gives same  results,  then  MAYBE  something  is
wrong with your optimization problem.

Despite its name (&quot;linear&quot; = &quot;first order method&quot;) this algorithm performs
steps similar to that of conjugate gradients method;  internally  it  uses
orthogonality/conjugacy requirement for subsequent steps  which  makes  it
closer to second order methods in terms of convergence speed.

Convergence is proved for the following case:
* function and constraints are continuously differentiable (C1 class)
* extended MangasarianâFromovitz constraint qualification  (EMFCQ)  holds;
  in the context of this algorithm EMFCQ  means  that  one  can,  for  any
  infeasible  point,  find  a  search  direction  such that the constraint
  infeasibilities are reduced.

This algorithm has following nice properties:
* no parameters to tune
* no convexity requirements for target function or constraints
* initial point can be infeasible
* algorithm respects box constraints in all intermediate points  (it  does
  not even evaluate function outside of box constrained area)
* once linear constraints are enforced, algorithm will not violate them
* no such guarantees can be provided for nonlinear constraints,  but  once
  nonlinear constraints are enforced, algorithm will try  to  respect them
  as much as possible
* numerical differentiation does not  violate  box  constraints  (although
  general linear and nonlinear ones can be violated during differentiation)
* from our experience, this algorithm is somewhat more  robust  in  really
  difficult cases

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 02.04.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetalgoslp(minnlcstate state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> | <a href=#example_minnlc_d_mixed class=nav>minnlc_d_mixed</a> ]</p>
<a name=sub_minnlcsetalgosqp></a><h6 class=pageheader>minnlcsetalgosqp Function</h6>
<hr width=600 align=left>
<pre class=narration>
This   function  tells  MinNLC  optimizer to use SQP (Successive Quadratic
Programming) algorithm for nonlinearly constrained optimization.

This algorithm needs order of magnitude (5x-10x) less function evaluations
than AUL solver, but has higher overhead because each  iteration  involves
solution of quadratic programming problem.

Convergence is proved for the following case:
* function and constraints are continuously differentiable (C1 class)

This algorithm has following nice properties:
* no parameters to tune
* no convexity requirements for target function or constraints
* initial point can be infeasible
* algorithm respects box constraints in all intermediate points  (it  does
  not even evaluate function outside of box constrained area)
* once linear constraints are enforced, algorithm will not violate them
* no such guarantees can be provided for nonlinear constraints,  but  once
  nonlinear constraints are enforced, algorithm will try  to  respect them
  as much as possible
* numerical differentiation does not  violate  box  constraints  (although
  general linear and nonlinear ones can be violated during differentiation)

We recommend this algorithm as a default option for medium-scale  problems
(less than thousand of variables) or problems with target  function  being
hard to evaluate.

For   large-scale  problems  or  ones  with very  cheap  target   function
AUL solver can be better option.

Inputs:
    State   -   structure which stores algorithm state

==== INTERACTION WITH OPTGUARD ====

OptGuard integrity  checker  allows us to catch problems  like  errors  in
gradients   and  discontinuity/nonsmoothness  of  the  target/constraints.
Latter kind of problems can be detected  by  looking  upon  line  searches
performed during optimization and searching for signs of nonsmoothness.

The problem with SQP is that it is too good for OptGuard to work - it does
not perform line searches. It typically  needs  1-2  function  evaluations
per step, and it is not enough for OptGuard to detect nonsmoothness.

So, if you suspect that your problem is nonsmooth, we recommend you to use
AUL or SLP solvers.
ALGLIB: Copyright 02.12.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetalgosqp(minnlcstate state);
</pre>
<a name=sub_minnlcsetbc></a><h6 class=pageheader>minnlcsetbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets boundary constraints for NLC optimizer.

Boundary constraints are inactive by  default  (after  initial  creation).
They are preserved after algorithm restart with  MinNLCRestartFrom().

You may combine boundary constraints with  general  linear ones - and with
nonlinear ones! Boundary constraints are  handled  more  efficiently  than
other types.  Thus,  if  your  problem  has  mixed  constraints,  you  may
explicitly specify some of them as boundary and save some time/space.

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very small number or -INF.
    BndU    -   upper bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very large number or +INF.

NOTE 1:  it is possible to specify  BndL[i]=BndU[i].  In  this  case  I-th
variable will be &quot;frozen&quot; at X[i]=BndL[i]=BndU[i].

NOTE 2:  when you solve your problem  with  augmented  Lagrangian  solver,
         boundary constraints are  satisfied  only  approximately!  It  is
         possible   that  algorithm  will  evaluate  function  outside  of
         feasible area!
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetbc(minnlcstate state, real_1d_array bndl, real_1d_array bndu);
</pre>
<a name=sub_minnlcsetcond></a><h6 class=pageheader>minnlcsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping conditions for inner iterations of  optimizer.

Inputs:
    State   -   structure which stores algorithm state
    EpsX    -   &ge; 0
                The subroutine finishes its work if  on  k+1-th  iteration
                the condition |v| &le; EpsX is fulfilled, where:
                * |.| means Euclidian norm
                * v - scaled step vector, v[i]=dx[i]/s[i]
                * dx - step vector, dx=X(k+1)-X(k)
                * s - scaling coefficients set by MinNLCSetScale()
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations is unlimited.

Passing EpsX=0 and MaxIts=0 (simultaneously) will lead to automatic
selection of the stopping condition.
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetcond(minnlcstate state, <b>double</b> epsx, ae_int_t maxits);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> | <a href=#example_minnlc_d_mixed class=nav>minnlc_d_mixed</a> ]</p>
<a name=sub_minnlcsetlc></a><h6 class=pageheader>minnlcsetlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets linear constraints for MinNLC optimizer.

Linear constraints are inactive by default (after initial creation).  They
are preserved after algorithm restart with MinNLCRestartFrom().

You may combine linear constraints with boundary ones - and with nonlinear
ones! If your problem has mixed constraints, you  may  explicitly  specify
some of them as linear. It  may  help  optimizer   to   handle  them  more
efficiently.

Inputs:
    State   -   structure previously allocated with MinNLCCreate call.
    C       -   linear constraints, array[K,N+1].
                Each row of C represents one constraint, either equality
                or inequality (see below):
                * first N elements correspond to coefficients,
                * last element corresponds to the right part.
                All elements of C (including right part) must be finite.
    CT      -   type of constraints, array[K]:
                * if CT[i] &gt; 0, then I-th constraint is C[i,*]*x &ge; C[i,n+1]
                * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
                * if CT[i] &lt; 0, then I-th constraint is C[i,*]*x &le; C[i,n+1]
    K       -   number of equality/inequality constraints, K &ge; 0:
                * if given, only leading K elements of C/CT are used
                * if not given, automatically determined from sizes of C/CT

NOTE 1: when you solve your problem  with  augmented  Lagrangian   solver,
        linear constraints are  satisfied  only   approximately!   It   is
        possible   that  algorithm  will  evaluate  function  outside   of
        feasible area!
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetlc(minnlcstate state, real_2d_array c, integer_1d_array ct, ae_int_t k);
<b>void</b> minnlcsetlc(minnlcstate state, real_2d_array c, integer_1d_array ct);
</pre>
<a name=sub_minnlcsetnlc></a><h6 class=pageheader>minnlcsetnlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets nonlinear constraints for MinNLC optimizer.

In fact, this function sets NUMBER of nonlinear  constraints.  Constraints
itself (constraint functions) are passed to MinNLCOptimize() method.  This
method requires user-defined vector function F[]  and  its  Jacobian  J[],
where:
* first component of F[] and first row  of  Jacobian  J[]  corresponds  to
  function being minimized
* next NLEC components of F[] (and rows  of  J)  correspond  to  nonlinear
  equality constraints G_i(x)=0
* next NLIC components of F[] (and rows  of  J)  correspond  to  nonlinear
  inequality constraints H_i(x) &le; 0

NOTE: you may combine nonlinear constraints with linear/boundary ones.  If
      your problem has mixed constraints, you  may explicitly specify some
      of them as linear ones. It may help optimizer to  handle  them  more
      efficiently.

Inputs:
    State   -   structure previously allocated with MinNLCCreate call.
    NLEC    -   number of Non-Linear Equality Constraints (NLEC), &ge; 0
    NLIC    -   number of Non-Linear Inquality Constraints (NLIC), &ge; 0

NOTE 1: when you solve your problem  with  augmented  Lagrangian   solver,
        nonlinear constraints are satisfied only  approximately!   It   is
        possible   that  algorithm  will  evaluate  function  outside   of
        feasible area!

NOTE 2: algorithm scales variables  according  to   scale   specified   by
        MinNLCSetScale()  function,  so  it can handle problems with badly
        scaled variables (as long as we KNOW their scales).

        However,  there  is  no  way  to  automatically  scale   nonlinear
        constraints Gi(x) and Hi(x). Inappropriate scaling  of  Gi/Hi  may
        ruin convergence. Solving problem with  constraint  &quot;1000*G0(x)=0&quot;
        is NOT same as solving it with constraint &quot;0.001*G0(x)=0&quot;.

        It  means  that  YOU  are  the  one who is responsible for correct
        scaling of nonlinear constraints Gi(x) and Hi(x). We recommend you
        to scale nonlinear constraints in such way that I-th component  of
        dG/dX (or dH/dx) has approximately unit  magnitude  (for  problems
        with unit scale)  or  has  magnitude approximately equal to 1/S[i]
        (where S is a scale set by MinNLCSetScale() function).
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetnlc(minnlcstate state, ae_int_t nlec, ae_int_t nlic);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> | <a href=#example_minnlc_d_mixed class=nav>minnlc_d_mixed</a> ]</p>
<a name=sub_minnlcsetprecexactlowrank></a><h6 class=pageheader>minnlcsetprecexactlowrank Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets preconditioner to &quot;exact low rank&quot; mode.

Preconditioning is very important for convergence of  Augmented Lagrangian
algorithm because presence of penalty term makes problem  ill-conditioned.
Difference between  performance  of  preconditioned  and  unpreconditioned
methods can be as large as 100x!

MinNLC optimizer may use following preconditioners,  each  with   its  own
benefits and drawbacks:
    a) inexact LBFGS-based, with O(N*K) evaluation time
    b) exact low rank one,  with O(N*K^2) evaluation time
    c) exact robust one,    with O(N^3+K*N^2) evaluation time
where K is a total number of general linear and nonlinear constraints (box
ones are not counted).

It also provides special unpreconditioned mode of operation which  can  be
used for test purposes. Comments below discuss low rank preconditioner.

Exact low-rank preconditioner  uses  Woodbury  matrix  identity  to  build
quadratic model of the penalized function. It has following features:
* no special assumptions about orthogonality of constraints
* preconditioner evaluation is optimized for K &lt;&lt; N. Its cost  is  O(N*K^2),
  so it may become prohibitively slow for K &ge; N.
* finally, stability of the process is guaranteed only for K &lt;&lt; N.  Woodbury
  update often fail for K &ge; N due to degeneracy of  intermediate  matrices.
  That's why we recommend to use &quot;exact robust&quot;  preconditioner  for  such
  cases.

RECOMMENDATIONS

We  recommend  to  choose  between  &quot;exact  low  rank&quot;  and &quot;exact robust&quot;
preconditioners, with &quot;low rank&quot; version being chosen  when  you  know  in
advance that total count of non-box constraints won't exceed N, and &quot;robust&quot;
version being chosen when you need bulletproof solution.

Inputs:
    State   -   structure stores algorithm state
    UpdateFreq- update frequency. Preconditioner is  rebuilt  after  every
                UpdateFreq iterations. Recommended value: 10 or higher.
                Zero value means that good default value will be used.
ALGLIB: Copyright 26.09.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetprecexactlowrank(minnlcstate state, ae_int_t updatefreq);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> | <a href=#example_minnlc_d_mixed class=nav>minnlc_d_mixed</a> ]</p>
<a name=sub_minnlcsetprecexactrobust></a><h6 class=pageheader>minnlcsetprecexactrobust Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets preconditioner to &quot;exact robust&quot; mode.

Preconditioning is very important for convergence of  Augmented Lagrangian
algorithm because presence of penalty term makes problem  ill-conditioned.
Difference between  performance  of  preconditioned  and  unpreconditioned
methods can be as large as 100x!

MinNLC optimizer may use following preconditioners,  each  with   its  own
benefits and drawbacks:
    a) inexact LBFGS-based, with O(N*K) evaluation time
    b) exact low rank one,  with O(N*K^2) evaluation time
    c) exact robust one,    with O(N^3+K*N^2) evaluation time
where K is a total number of general linear and nonlinear constraints (box
ones are not counted).

It also provides special unpreconditioned mode of operation which  can  be
used for test purposes. Comments below discuss robust preconditioner.

Exact  robust  preconditioner   uses   Cholesky  decomposition  to  invert
approximate Hessian matrix H=D+W'*C*W (where D stands for  diagonal  terms
of Hessian, combined result of initial scaling matrix and penalty from box
constraints; W stands for general linear constraints and linearization  of
nonlinear ones; C stands for diagonal matrix of penalty coefficients).

This preconditioner has following features:
* no special assumptions about constraint structure
* preconditioner is optimized  for  stability;  unlike  &quot;exact  low  rank&quot;
  version which fails for K &ge; N, this one works well for any value of K.
* the only drawback is that is takes O(N^3+K*N^2) time  to  build  it.  No
  economical  Woodbury update is applied even when it  makes  sense,  thus
  there  are  exist situations (K &lt;&lt; N) when &quot;exact low rank&quot; preconditioner
  outperforms this one.

RECOMMENDATIONS

We  recommend  to  choose  between  &quot;exact  low  rank&quot;  and &quot;exact robust&quot;
preconditioners, with &quot;low rank&quot; version being chosen  when  you  know  in
advance that total count of non-box constraints won't exceed N, and &quot;robust&quot;
version being chosen when you need bulletproof solution.

Inputs:
    State   -   structure stores algorithm state
    UpdateFreq- update frequency. Preconditioner is  rebuilt  after  every
                UpdateFreq iterations. Recommended value: 10 or higher.
                Zero value means that good default value will be used.
ALGLIB: Copyright 26.09.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetprecexactrobust(minnlcstate state, ae_int_t updatefreq);
</pre>
<a name=sub_minnlcsetprecinexact></a><h6 class=pageheader>minnlcsetprecinexact Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets preconditioner to &quot;inexact LBFGS-based&quot; mode.

Preconditioning is very important for convergence of  Augmented Lagrangian
algorithm because presence of penalty term makes problem  ill-conditioned.
Difference between  performance  of  preconditioned  and  unpreconditioned
methods can be as large as 100x!

MinNLC optimizer may use following preconditioners,  each  with   its  own
benefits and drawbacks:
    a) inexact LBFGS-based, with O(N*K) evaluation time
    b) exact low rank one,  with O(N*K^2) evaluation time
    c) exact robust one,    with O(N^3+K*N^2) evaluation time
where K is a total number of general linear and nonlinear constraints (box
ones are not counted).

Inexact  LBFGS-based  preconditioner  uses L-BFGS  formula  combined  with
orthogonality assumption to perform very fast updates. For a N-dimensional
problem with K general linear or nonlinear constraints (boundary ones  are
not counted) it has O(N*K) cost per iteration.  This   preconditioner  has
best  quality  (less  iterations)  when   general   linear  and  nonlinear
constraints are orthogonal to each other (orthogonality  with  respect  to
boundary constraints is not required). Number of iterations increases when
constraints  are  non-orthogonal, because algorithm assumes orthogonality,
but still it is better than no preconditioner at all.

Inputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 26.09.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetprecinexact(minnlcstate state);
</pre>
<a name=sub_minnlcsetprecnone></a><h6 class=pageheader>minnlcsetprecnone Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets preconditioner to &quot;turned off&quot; mode.

Preconditioning is very important for convergence of  Augmented Lagrangian
algorithm because presence of penalty term makes problem  ill-conditioned.
Difference between  performance  of  preconditioned  and  unpreconditioned
methods can be as large as 100x!

MinNLC optimizer may  utilize  two  preconditioners,  each  with  its  own
benefits and drawbacks: a) inexact LBFGS-based, and b) exact low rank one.
It also provides special unpreconditioned mode of operation which  can  be
used for test purposes.

This function activates this test mode. Do not use it in  production  code
to solve real-life problems.

Inputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 26.09.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetprecnone(minnlcstate state);
</pre>
<a name=sub_minnlcsetscale></a><h6 class=pageheader>minnlcsetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients for NLC optimizer.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison with tolerances).  Scale of
the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the function

Scaling is also used by finite difference variant of the optimizer  - step
along I-th axis is equal to DiffStep*S[I].

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 06.06.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetscale(minnlcstate state, real_1d_array s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minnlc_d_inequality class=nav>minnlc_d_inequality</a> | <a href=#example_minnlc_d_equality class=nav>minnlc_d_equality</a> | <a href=#example_minnlc_d_mixed class=nav>minnlc_d_mixed</a> ]</p>
<a name=sub_minnlcsetstpmax></a><h6 class=pageheader>minnlcsetstpmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets maximum step length (after scaling of step vector  with
respect to variable scales specified by minnlcsetscale() call).

Inputs:
    State   -   structure which stores algorithm state
    StpMax  -   maximum step length, &ge; 0. Set StpMax to 0.0 (default),  if
                you don't want to limit step length.

Use this subroutine when you optimize target function which contains exp()
or  other  fast  growing  functions,  and optimization algorithm makes too
large  steps  which  leads  to overflow. This function allows us to reject
steps  that  are  too  large  (and  therefore  expose  us  to the possible
overflow) without actually calculating function value at the x+stp*d.

NOTE: different solvers employed by MinNLC optimizer use  different  norms
      for step; AUL solver uses 2-norm, whilst SLP solver uses INF-norm.
ALGLIB: Copyright 02.04.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetstpmax(minnlcstate state, <b>double</b> stpmax);
</pre>
<a name=sub_minnlcsetxrep></a><h6 class=pageheader>minnlcsetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to MinNLCOptimize().

NOTE: algorithm passes two parameters to rep() callback  -  current  point
      and penalized function value at current point. Important -  function
      value which is returned is NOT function being minimized. It  is  sum
      of the value of the function being minimized - and penalty term.
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnlcsetxrep(minnlcstate state, <b>bool</b> needxrep);
</pre>
<a name=example_minnlc_d_equality></a><h6 class=pageheader>minnlc_d_equality Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> nlcfunc1_jac(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = -x0+x1</font>
<font color=navy>//     f1(x0,x1) = x0^2+x1^2-1</font>
<font color=navy>//</font>
<font color=navy>// and Jacobian matrix J = [dfi/dxj]</font>
   fi[0] = -x[0]+x[1];
   fi[1] = x[0]*x[0] + x[1]*x[1] - 1.0;
   jac[0][0] = -1.0;
   jac[0][1] = +1.0;
   jac[1][0] = 2*x[0];
   jac[1][1] = 2*x[1];
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x0,x1) = -x0+x1</font>
<font color=navy>//</font>
<font color=navy>// subject to nonlinear equality constraint</font>
<font color=navy>//</font>
<font color=navy>//    x0^2 + x1^2 - 1 = 0</font>
   real_1d_array x0 = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   minnlcstate state;
<font color=navy>// Create optimizer object and tune its settings:</font>
<font color=navy>// * epsx=0.000001  stopping condition <b>for</b> inner iterations</font>
<font color=navy>// * s=[1,1]        all variables have unit scale</font>
   minnlccreate(2, x0, state);
   minnlcsetcond(state, epsx, maxits);
   minnlcsetscale(state, s);
<font color=navy>// Choose one of the nonlinear programming solvers supported by minnlc</font>
<font color=navy>// optimizer:</font>
<font color=navy>// * SLP - successive linear programming NLP solver</font>
<font color=navy>// * AUL - augmented Lagrangian NLP solver</font>
<font color=navy>//</font>
<font color=navy>// Different solvers have different properties:</font>
<font color=navy>// * SLP is the most robust solver provided by ALGLIB: it can solve both</font>
<font color=navy>//   convex and nonconvex optimization problems, it respects box and</font>
<font color=navy>//   linear constraints (after you find feasible point it won't move away</font>
<font color=navy>//   from the feasible area) and tries to respect nonlinear constraints</font>
<font color=navy>//   as much as possible. It also usually needs less function evaluations</font>
<font color=navy>//   to converge than AUL.</font>
<font color=navy>//   However, it solves LP subproblems at each iterations which adds</font>
<font color=navy>//   significant overhead to its running time. Sometimes it can be as much</font>
<font color=navy>//   as 7x times slower than AUL.</font>
<font color=navy>// * AUL solver is less robust than SLP - it can violate box and linear</font>
<font color=navy>//   constraints at any moment, and it is intended <b>for</b> convex optimization</font>
<font color=navy>//   problems (although in many cases it can deal with nonconvex ones too).</font>
<font color=navy>//   Also, unlike SLP it needs some tuning (penalty factor and number of</font>
<font color=navy>//   outer iterations).</font>
<font color=navy>//   However, it is often much faster than the current version of SLP.</font>
<font color=navy>//</font>
<font color=navy>// In the code below we set solver to be AUL but then override it with SLP,</font>
<font color=navy>// so the effective choice is to use SLP. We recommend you to use SLP at</font>
<font color=navy>// least <b>for</b> early prototyping stages.</font>
<font color=navy>//</font>
<font color=navy>// You can comment out line with SLP <b>if</b> you want to solve your problem with</font>
<font color=navy>// AUL solver.</font>
   <b>double</b> rho = 1000.0;
   ae_int_t outerits = 5;
   minnlcsetalgoaul(state, rho, outerits);
   minnlcsetalgoslp(state);
<font color=navy>// Set constraints:</font>
<font color=navy>//</font>
<font color=navy>// Nonlinear constraints are tricky - you can not <font color=blue><b>&quot;pack&quot;</b></font> general</font>
<font color=navy>// nonlinear function into <b>double</b> precision array. That's why</font>
<font color=navy>// minnlcsetnlc() does not accept constraints itself - only constraint</font>
<font color=navy>// counts are passed: first parameter is number of equality constraints,</font>
<font color=navy>// second one is number of inequality constraints.</font>
<font color=navy>//</font>
<font color=navy>// As <b>for</b> constraining functions - these functions are passed as part</font>
<font color=navy>// of problem Jacobian (see below).</font>
<font color=navy>//</font>
<font color=navy>// NOTE: MinNLC optimizer supports arbitrary combination of boundary, general</font>
<font color=navy>//       linear and general nonlinear constraints. This example does not</font>
<font color=navy>//       show how to work with general linear constraints, but you can</font>
<font color=navy>//       easily find it in documentation on minnlcsetbc() and</font>
<font color=navy>//       minnlcsetlc() functions.</font>
   minnlcsetnlc(state, 1, 0);
<font color=navy>// Activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard monitor helps to catch common coding and problem statement</font>
<font color=navy>// issues, like:</font>
<font color=navy>// * discontinuity of the target/constraints (C0 continuity violation)</font>
<font color=navy>// * nonsmoothness of the target/constraints (C1 continuity violation)</font>
<font color=navy>// * erroneous analytic Jacobian, i.e. one inconsistent with actual</font>
<font color=navy>//   change in the target/constraints</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential <b>for</b> early prototyping stages because such</font>
<font color=navy>// problems often result in premature termination of the optimizer</font>
<font color=navy>// which is really hard to distinguish from the correct termination.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: GRADIENT VERIFICATION IS PERFORMED BY MEANS OF NUMERICAL</font>
<font color=navy>//            DIFFERENTIATION, THUS DO NOT USE IT IN PRODUCTION CODE!</font>
<font color=navy>//</font>
<font color=navy>//            Other OptGuard checks add moderate overhead, but anyway</font>
<font color=navy>//            it is better to turn them off when they are not needed.</font>
   minnlcoptguardsmoothness(state);
   minnlcoptguardgradient(state, 0.001);
<font color=navy>// Optimize and test results.</font>
<font color=navy>//</font>
<font color=navy>// Optimizer object accepts vector function and its Jacobian, with first</font>
<font color=navy>// component (Jacobian row) being target function, and next components</font>
<font color=navy>// (Jacobian rows) being nonlinear equality and inequality constraints.</font>
<font color=navy>//</font>
<font color=navy>// So, our vector function has form</font>
<font color=navy>//</font>
<font color=navy>//     {f0,f1} = { -x0+x1 , x0^2+x1^2-1 }</font>
<font color=navy>//</font>
<font color=navy>// with Jacobian</font>
<font color=navy>//</font>
<font color=navy>//         [  -1    +1  ]</font>
<font color=navy>//     J = [            ]</font>
<font color=navy>//         [ 2*x0  2*x1 ]</font>
<font color=navy>//</font>
<font color=navy>// with f0 being target function, f1 being constraining function. Number</font>
<font color=navy>// of equality/inequality constraints is specified by minnlcsetnlc(),</font>
<font color=navy>// with equality ones always being first, inequality ones being last.</font>
   minnlcreport rep;
   real_1d_array x1;
   minnlcoptimize(state, nlcfunc1_jac);
   minnlcresults(state, x1, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x1.tostring(2).c_str()); <font color=navy>// EXPECTED: [0.70710,-0.70710]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// NOTE: want to test OptGuard? Try breaking the Jacobian - say, add</font>
<font color=navy>//       1.0 to some of its components.</font>
   optguardreport ogrep;
   minnlcoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minnlc_d_inequality></a><h6 class=pageheader>minnlc_d_inequality Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> nlcfunc1_jac(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = -x0+x1</font>
<font color=navy>//     f1(x0,x1) = x0^2+x1^2-1</font>
<font color=navy>//</font>
<font color=navy>// and Jacobian matrix J = [dfi/dxj]</font>
   fi[0] = -x[0]+x[1];
   fi[1] = x[0]*x[0] + x[1]*x[1] - 1.0;
   jac[0][0] = -1.0;
   jac[0][1] = +1.0;
   jac[1][0] = 2*x[0];
   jac[1][1] = 2*x[1];
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x0,x1) = -x0+x1</font>
<font color=navy>//</font>
<font color=navy>// subject to box constraints</font>
<font color=navy>//</font>
<font color=navy>//    x0 &ge; 0, x1 &ge; 0</font>
<font color=navy>//</font>
<font color=navy>// and nonlinear inequality constraint</font>
<font color=navy>//</font>
<font color=navy>//    x0^2 + x1^2 - 1 &le; 0</font>
   real_1d_array x0 = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   real_1d_array bndl = <font color=blue><b>&quot;[0,0]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[+inf,+inf]&quot;</b></font>;
   minnlcstate state;
<font color=navy>// Create optimizer object and tune its settings:</font>
<font color=navy>// * epsx=0.000001  stopping condition <b>for</b> inner iterations</font>
<font color=navy>// * s=[1,1]        all variables have unit scale; it is important to</font>
<font color=navy>//                  tell optimizer about scales of your variables - it</font>
<font color=navy>//                  greatly accelerates convergence and helps to perform</font>
<font color=navy>//                  some important integrity checks.</font>
   minnlccreate(2, x0, state);
   minnlcsetcond(state, epsx, maxits);
   minnlcsetscale(state, s);
<font color=navy>// Choose one of the nonlinear programming solvers supported by minnlc</font>
<font color=navy>// optimizer:</font>
<font color=navy>// * SQP - sequential quadratic programming NLP solver</font>
<font color=navy>// * AUL - augmented Lagrangian NLP solver</font>
<font color=navy>// * SLP - successive linear programming NLP solver</font>
<font color=navy>//</font>
<font color=navy>// Different solvers have different properties:</font>
<font color=navy>// * SQP needs less function evaluations than any other solver, but it</font>
<font color=navy>//   has much higher iteration cost than other solvers (a QP subproblem</font>
<font color=navy>//   has to be solved during each step)</font>
<font color=navy>// * AUL solver has cheaper iterations, but needs more target function</font>
<font color=navy>//   evaluations</font>
<font color=navy>// * SLP is the most robust solver provided by ALGLIB, but it performs</font>
<font color=navy>//   order of magnitude more iterations than SQP.</font>
<font color=navy>//</font>
<font color=navy>// In the code below we set solver to be AUL but then override it with SLP,</font>
<font color=navy>// and then with SQP, so the effective choice is to use SLP. We recommend</font>
<font color=navy>// you to use SQP at least <b>for</b> early prototyping stages, and then switch</font>
<font color=navy>// to AUL <b>if</b> possible.</font>
<font color=navy>//</font>
   <b>double</b> rho = 1000.0;
   ae_int_t outerits = 5;
   minnlcsetalgoaul(state, rho, outerits);
   minnlcsetalgoslp(state);
   minnlcsetalgosqp(state);
<font color=navy>// Set constraints:</font>
<font color=navy>//</font>
<font color=navy>// 1. boundary constraints are passed with minnlcsetbc() call</font>
<font color=navy>//</font>
<font color=navy>// 2. nonlinear constraints are more tricky - you can not <font color=blue><b>&quot;pack&quot;</b></font> general</font>
<font color=navy>//    nonlinear function into <b>double</b> precision array. That's why</font>
<font color=navy>//    minnlcsetnlc() does not accept constraints itself - only constraint</font>
<font color=navy>//    counts are passed: first parameter is number of equality constraints,</font>
<font color=navy>//    second one is number of inequality constraints.</font>
<font color=navy>//</font>
<font color=navy>//    As <b>for</b> constraining functions - these functions are passed as part</font>
<font color=navy>//    of problem Jacobian (see below).</font>
<font color=navy>//</font>
<font color=navy>// NOTE: MinNLC optimizer supports arbitrary combination of boundary, general</font>
<font color=navy>//       linear and general nonlinear constraints. This example does not</font>
<font color=navy>//       show how to work with general linear constraints, but you can</font>
<font color=navy>//       easily find it in documentation on minnlcsetlc() function.</font>
   minnlcsetbc(state, bndl, bndu);
   minnlcsetnlc(state, 0, 1);
<font color=navy>// Activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard monitor helps to catch common coding and problem statement</font>
<font color=navy>// issues, like:</font>
<font color=navy>// * discontinuity of the target/constraints (C0 continuity violation)</font>
<font color=navy>// * nonsmoothness of the target/constraints (C1 continuity violation)</font>
<font color=navy>// * erroneous analytic Jacobian, i.e. one inconsistent with actual</font>
<font color=navy>//   change in the target/constraints</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential <b>for</b> early prototyping stages because such</font>
<font color=navy>// problems often result in premature termination of the optimizer</font>
<font color=navy>// which is really hard to distinguish from the correct termination.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: GRADIENT VERIFICATION IS PERFORMED BY MEANS OF NUMERICAL</font>
<font color=navy>//            DIFFERENTIATION, THUS DO NOT USE IT IN PRODUCTION CODE!</font>
<font color=navy>//</font>
<font color=navy>//            Other OptGuard checks add moderate overhead, but anyway</font>
<font color=navy>//            it is better to turn them off when they are not needed.</font>
   minnlcoptguardsmoothness(state);
   minnlcoptguardgradient(state, 0.001);
<font color=navy>// Optimize and test results.</font>
<font color=navy>//</font>
<font color=navy>// Optimizer object accepts vector function and its Jacobian, with first</font>
<font color=navy>// component (Jacobian row) being target function, and next components</font>
<font color=navy>// (Jacobian rows) being nonlinear equality and inequality constraints.</font>
<font color=navy>//</font>
<font color=navy>// So, our vector function has form</font>
<font color=navy>//</font>
<font color=navy>//     {f0,f1} = { -x0+x1 , x0^2+x1^2-1 }</font>
<font color=navy>//</font>
<font color=navy>// with Jacobian</font>
<font color=navy>//</font>
<font color=navy>//         [  -1    +1  ]</font>
<font color=navy>//     J = [            ]</font>
<font color=navy>//         [ 2*x0  2*x1 ]</font>
<font color=navy>//</font>
<font color=navy>// with f0 being target function, f1 being constraining function. Number</font>
<font color=navy>// of equality/inequality constraints is specified by minnlcsetnlc(),</font>
<font color=navy>// with equality ones always being first, inequality ones being last.</font>
   minnlcreport rep;
   real_1d_array x1;
   minnlcoptimize(state, nlcfunc1_jac);
   minnlcresults(state, x1, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x1.tostring(2).c_str()); <font color=navy>// EXPECTED: [1.0000,0.0000]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// NOTE: want to test OptGuard? Try breaking the Jacobian - say, add</font>
<font color=navy>//       1.0 to some of its components.</font>
   optguardreport ogrep;
   minnlcoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minnlc_d_mixed></a><h6 class=pageheader>minnlc_d_mixed Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> nlcfunc2_jac(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1,x2) = x0+x1</font>
<font color=navy>//     f1(x0,x1,x2) = x2-exp(x0)</font>
<font color=navy>//     f2(x0,x1,x2) = x0^2+x1^2-1</font>
<font color=navy>//</font>
<font color=navy>// and Jacobian matrix J = [dfi/dxj]</font>
   fi[0] = x[0]+x[1];
   fi[1] = x[2]-exp(x[0]);
   fi[2] = x[0]*x[0] + x[1]*x[1] - 1.0;
   jac[0][0] = 1.0;
   jac[0][1] = 1.0;
   jac[0][2] = 0.0;
   jac[1][0] = -exp(x[0]);
   jac[1][1] = 0.0;
   jac[1][2] = 1.0;
   jac[2][0] = 2*x[0];
   jac[2][1] = 2*x[1];
   jac[2][2] = 0.0;
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x0,x1) = x0+x1</font>
<font color=navy>//</font>
<font color=navy>// subject to nonlinear inequality constraint</font>
<font color=navy>//</font>
<font color=navy>//    x0^2 + x1^2 - 1 &le; 0</font>
<font color=navy>//</font>
<font color=navy>// and nonlinear equality constraint</font>
<font color=navy>//</font>
<font color=navy>//    x2-exp(x0) = 0</font>
   real_1d_array x0 = <font color=blue><b>&quot;[0,0,0]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1,1]&quot;</b></font>;
   <b>double</b> epsx = 0.000001;
   ae_int_t maxits = 0;
   minnlcstate state;
   minnlcreport rep;
   real_1d_array x1;
<font color=navy>// Create optimizer object and tune its settings:</font>
<font color=navy>// * epsx=0.000001  stopping condition <b>for</b> inner iterations</font>
<font color=navy>// * s=[1,1]        all variables have unit scale</font>
<font color=navy>// * upper limit on step length is specified (to avoid probing locations where exp() is large)</font>
   minnlccreate(3, x0, state);
   minnlcsetcond(state, epsx, maxits);
   minnlcsetscale(state, s);
   minnlcsetstpmax(state, 10.0);
<font color=navy>// Choose one of the nonlinear programming solvers supported by minnlc</font>
<font color=navy>// optimizer:</font>
<font color=navy>// * SLP - successive linear programming NLP solver</font>
<font color=navy>// * AUL - augmented Lagrangian NLP solver</font>
<font color=navy>//</font>
<font color=navy>// Different solvers have different properties:</font>
<font color=navy>// * SLP is the most robust solver provided by ALGLIB: it can solve both</font>
<font color=navy>//   convex and nonconvex optimization problems, it respects box and</font>
<font color=navy>//   linear constraints (after you find feasible point it won't move away</font>
<font color=navy>//   from the feasible area) and tries to respect nonlinear constraints</font>
<font color=navy>//   as much as possible. It also usually needs less function evaluations</font>
<font color=navy>//   to converge than AUL.</font>
<font color=navy>//   However, it solves LP subproblems at each iterations which adds</font>
<font color=navy>//   significant overhead to its running time. Sometimes it can be as much</font>
<font color=navy>//   as 7x times slower than AUL.</font>
<font color=navy>// * AUL solver is less robust than SLP - it can violate box and linear</font>
<font color=navy>//   constraints at any moment, and it is intended <b>for</b> convex optimization</font>
<font color=navy>//   problems (although in many cases it can deal with nonconvex ones too).</font>
<font color=navy>//   Also, unlike SLP it needs some tuning (penalty factor and number of</font>
<font color=navy>//   outer iterations).</font>
<font color=navy>//   However, it is often much faster than the current version of SLP.</font>
<font color=navy>//</font>
<font color=navy>// In the code below we set solver to be AUL but then override it with SLP,</font>
<font color=navy>// so the effective choice is to use SLP. We recommend you to use SLP at</font>
<font color=navy>// least <b>for</b> early prototyping stages.</font>
<font color=navy>//</font>
<font color=navy>// You can comment out line with SLP <b>if</b> you want to solve your problem with</font>
<font color=navy>// AUL solver.</font>
   <b>double</b> rho = 1000.0;
   ae_int_t outerits = 5;
   minnlcsetalgoaul(state, rho, outerits);
   minnlcsetalgoslp(state);
<font color=navy>// Set constraints:</font>
<font color=navy>//</font>
<font color=navy>// Nonlinear constraints are tricky - you can not <font color=blue><b>&quot;pack&quot;</b></font> general</font>
<font color=navy>// nonlinear function into <b>double</b> precision array. That's why</font>
<font color=navy>// minnlcsetnlc() does not accept constraints itself - only constraint</font>
<font color=navy>// counts are passed: first parameter is number of equality constraints,</font>
<font color=navy>// second one is number of inequality constraints.</font>
<font color=navy>//</font>
<font color=navy>// As <b>for</b> constraining functions - these functions are passed as part</font>
<font color=navy>// of problem Jacobian (see below).</font>
<font color=navy>//</font>
<font color=navy>// NOTE: MinNLC optimizer supports arbitrary combination of boundary, general</font>
<font color=navy>//       linear and general nonlinear constraints. This example does not</font>
<font color=navy>//       show how to work with boundary or general linear constraints, but you</font>
<font color=navy>//       can easily find it in documentation on minnlcsetbc() and</font>
<font color=navy>//       minnlcsetlc() functions.</font>
   minnlcsetnlc(state, 1, 1);
<font color=navy>// Activate OptGuard integrity checking.</font>
<font color=navy>//</font>
<font color=navy>// OptGuard monitor helps to catch common coding and problem statement</font>
<font color=navy>// issues, like:</font>
<font color=navy>// * discontinuity of the target/constraints (C0 continuity violation)</font>
<font color=navy>// * nonsmoothness of the target/constraints (C1 continuity violation)</font>
<font color=navy>// * erroneous analytic Jacobian, i.e. one inconsistent with actual</font>
<font color=navy>//   change in the target/constraints</font>
<font color=navy>//</font>
<font color=navy>// OptGuard is essential <b>for</b> early prototyping stages because such</font>
<font color=navy>// problems often result in premature termination of the optimizer</font>
<font color=navy>// which is really hard to distinguish from the correct termination.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: GRADIENT VERIFICATION IS PERFORMED BY MEANS OF NUMERICAL</font>
<font color=navy>//            DIFFERENTIATION, THUS DO NOT USE IT IN PRODUCTION CODE!</font>
<font color=navy>//</font>
<font color=navy>//            Other OptGuard checks add moderate overhead, but anyway</font>
<font color=navy>//            it is better to turn them off when they are not needed.</font>
   minnlcoptguardsmoothness(state);
   minnlcoptguardgradient(state, 0.001);
<font color=navy>// Optimize and test results.</font>
<font color=navy>//</font>
<font color=navy>// Optimizer object accepts vector function and its Jacobian, with first</font>
<font color=navy>// component (Jacobian row) being target function, and next components</font>
<font color=navy>// (Jacobian rows) being nonlinear equality and inequality constraints.</font>
<font color=navy>//</font>
<font color=navy>// So, our vector function has form</font>
<font color=navy>//</font>
<font color=navy>//     {f0,f1,f2} = { x0+x1 , x2-exp(x0) , x0^2+x1^2-1 }</font>
<font color=navy>//</font>
<font color=navy>// with Jacobian</font>
<font color=navy>//</font>
<font color=navy>//         [  +1      +1       0 ]</font>
<font color=navy>//     J = [-exp(x0)  0        1 ]</font>
<font color=navy>//         [ 2*x0    2*x1      0 ]</font>
<font color=navy>//</font>
<font color=navy>// with f0 being target function, f1 being equality constraint <font color=blue><b>&quot;f1=0&quot;</b></font>,</font>
<font color=navy>// f2 being inequality constraint <font color=blue><b>&quot;f2 &le; 0&quot;</b></font>. Number of equality/inequality</font>
<font color=navy>// constraints is specified by minnlcsetnlc(), with equality ones always</font>
<font color=navy>// being first, inequality ones being last.</font>
   minnlcoptimize(state, nlcfunc2_jac);
   minnlcresults(state, x1, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x1.tostring(2).c_str()); <font color=navy>// EXPECTED: [-0.70710,-0.70710,0.49306]</font>
<font color=navy>//</font>
<font color=navy>// Check that OptGuard did not report errors</font>
<font color=navy>//</font>
<font color=navy>// NOTE: want to test OptGuard? Try breaking the Jacobian - say, add</font>
<font color=navy>//       1.0 to some of its components.</font>
   optguardreport ogrep;
   minnlcoptguardresults(state, ogrep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.badgradsuspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc0suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, ogrep.nonc1suspected ? <font color=blue><b>&quot;true&quot;</b></font> : <font color=blue><b>&quot;false&quot;</b></font>); <font color=navy>// EXPECTED: false</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_minns></a><h4 class=pageheader>8.8.9. minns Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_minnsreport class=toc>minnsreport</a> |
<a href=#struct_minnsstate class=toc>minnsstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_minnscreate class=toc>minnscreate</a> |
<a href=#sub_minnscreatef class=toc>minnscreatef</a> |
<a href=#sub_minnsoptimize class=toc>minnsoptimize</a> |
<a href=#sub_minnsrequesttermination class=toc>minnsrequesttermination</a> |
<a href=#sub_minnsrestartfrom class=toc>minnsrestartfrom</a> |
<a href=#sub_minnsresults class=toc>minnsresults</a> |
<a href=#sub_minnsresultsbuf class=toc>minnsresultsbuf</a> |
<a href=#sub_minnssetalgoags class=toc>minnssetalgoags</a> |
<a href=#sub_minnssetbc class=toc>minnssetbc</a> |
<a href=#sub_minnssetcond class=toc>minnssetcond</a> |
<a href=#sub_minnssetlc class=toc>minnssetlc</a> |
<a href=#sub_minnssetnlc class=toc>minnssetnlc</a> |
<a href=#sub_minnssetscale class=toc>minnssetscale</a> |
<a href=#sub_minnssetxrep class=toc>minnssetxrep</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_minns_d_bc class=toc>minns_d_bc</a></td><td width=15>&nbsp;</td><td>Nonsmooth box constrained optimization</td></tr>
<tr align=left valign=top><td><a href=#example_minns_d_diff class=toc>minns_d_diff</a></td><td width=15>&nbsp;</td><td>Nonsmooth unconstrained optimization with numerical differentiation</td></tr>
<tr align=left valign=top><td><a href=#example_minns_d_nlc class=toc>minns_d_nlc</a></td><td width=15>&nbsp;</td><td>Nonsmooth nonlinearly constrained optimization</td></tr>
<tr align=left valign=top><td><a href=#example_minns_d_unconstrained class=toc>minns_d_unconstrained</a></td><td width=15>&nbsp;</td><td>Nonsmooth unconstrained optimization</td></tr>
</table>
</div>
<a name=struct_minnsreport></a><h6 class=pageheader>minnsreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure stores optimization report:
* IterationsCount           total number of inner iterations
* NFEV                      number of gradient evaluations
* TerminationType           termination type (see below)
* CErr                      maximum violation of all types of constraints
* LCErr                     maximum violation of linear constraints
* NLCErr                    maximum violation of nonlinear constraints

TERMINATION CODES

TerminationType field contains completion code, which can be:
  -8    internal integrity control detected  infinite  or  NAN  values  in
        function/gradient. Abnormal termination signalled.
  -3    box constraints are inconsistent
  -1    inconsistent parameters were passed:
        * penalty parameter for minnssetalgoags() is zero,
          but we have nonlinear constraints set by minnssetnlc()
   2    sampling radius decreased below epsx
   5    MaxIts steps was taken
   7    stopping conditions are too stringent,
        further improvement is impossible,
        X contains best point found so far.
   8    User requested termination via MinNSRequestTermination()

Other fields of this structure are not documented and should not be used!
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minnsreport {
   ae_int_t iterationscount;
   ae_int_t nfev;
   <b>double</b> cerr;
   <b>double</b> lcerr;
   <b>double</b> nlcerr;
   ae_int_t terminationtype;
   ae_int_t varidx;
   ae_int_t funcidx;
};
</pre>
<a name=struct_minnsstate></a><h6 class=pageheader>minnsstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores nonlinear optimizer state.
You should use functions provided by MinNS subpackage to work  with  this
object
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minnsstate {
   bool needfi;
   bool needfij;
   bool xupdated;
   double f;
   real_1d_array fi;
   real_2d_array j;
   real_1d_array x;
};
</pre>
<a name=sub_minnscreate></a><h6 class=pageheader>minnscreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
NONSMOOTH NONCONVEX OPTIMIZATION
SUBJECT TO BOX/LINEAR/NONLINEAR-NONSMOOTH CONSTRAINTS
The  subroutine  minimizes  function   F(x)  of N arguments subject to any
combination of:
* bound constraints
* linear inequality constraints
* linear equality constraints
* nonlinear equality constraints Gi(x)=0
* nonlinear inequality constraints Hi(x) &le; 0

IMPORTANT: see MinNSSetAlgoAGS for important  information  on  performance
           restrictions of AGS solver.

REQUIREMENTS:
* starting point X0 must be feasible or not too far away from the feasible
  set
* F(), G(), H() are continuous, locally Lipschitz  and  continuously  (but
  not necessarily twice) differentiable in an open dense  subset  of  R^N.
  Functions F(), G() and H() may be nonsmooth and non-convex.
  Informally speaking, it means  that  functions  are  composed  of  large
  differentiable &quot;patches&quot; with nonsmoothness having  place  only  at  the
  boundaries between these &quot;patches&quot;.
  Most real-life nonsmooth  functions  satisfy  these  requirements.  Say,
  anything which involves finite number of abs(), min() and max() is  very
  likely to pass the test.
  Say, it is possible to optimize anything of the following:
  * f=abs(x0)+2*abs(x1)
  * f=max(x0,x1)
  * f=sin(max(x0,x1)+abs(x2))
* for nonlinearly constrained problems: F()  must  be  bounded from  below
  without nonlinear constraints (this requirement is due to the fact that,
  contrary to box and linear constraints, nonlinear ones  require  special
  handling).
* user must provide function value and gradient for F(), H(), G()  at  all
  points where function/gradient can be calculated. If optimizer  requires
  value exactly at the boundary between &quot;patches&quot; (say, at x=0 for f=abs(x)),
  where gradient is not defined, user may resolve tie arbitrarily (in  our
  case - return +1 or -1 at its discretion).
* NS solver supports numerical differentiation, i.e. it may  differentiate
  your function for you,  but  it  results  in  2N  increase  of  function
  evaluations. Not recommended unless you solve really small problems. See
  minnscreatef() for more information on this functionality.

USAGE:

1. User initializes algorithm state with MinNSCreate() call  and   chooses
   what NLC solver to use. There is some solver which is used by  default,
   with default settings, but you should NOT rely on  default  choice.  It
   may change in future releases of ALGLIB without notice, and no one  can
   guarantee that new solver will be  able  to  solve  your  problem  with
   default settings.

   From the other side, if you choose solver explicitly, you can be pretty
   sure that it will work with new ALGLIB releases.

   In the current release following solvers can be used:
   * AGS solver (activated with MinNSSetAlgoAGS() function)

2. User adds boundary and/or linear and/or nonlinear constraints by  means
   of calling one of the following functions:
   a) MinNSSetBC() for boundary constraints
   b) MinNSSetLC() for linear constraints
   c) MinNSSetNLC() for nonlinear constraints
   You may combine (a), (b) and (c) in one optimization problem.

3. User sets scale of the variables with MinNSSetScale() function. It   is
   VERY important to set  scale  of  the  variables,  because  nonlinearly
   constrained problems are hard to solve when variables are badly scaled.

4. User sets stopping conditions with MinNSSetCond().

5. Finally, user calls MinNSOptimize()  function  which  takes   algorithm
   state and pointer (delegate, etc) to callback function which calculates
   F/G/H.

7. User calls MinNSResults() to get solution

8. Optionally user may call MinNSRestartFrom() to solve   another  problem
   with same N but another starting point. MinNSRestartFrom()  allows   to
   reuse already initialized structure.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    X       -   starting point, array[N]:
                * it is better to set X to a feasible point
                * but X can be infeasible, in which case algorithm will try
                  to find feasible point first, using X as initial
                  approximation.

Outputs:
    State   -   structure stores algorithm state

NOTE: minnscreatef() function may be used if  you  do  not  have  analytic
      gradient.   This   function  creates  solver  which  uses  numerical
      differentiation with user-specified step.
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnscreate(ae_int_t n, real_1d_array x, minnsstate &amp;state);
<b>void</b> minnscreate(real_1d_array x, minnsstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minns_d_unconstrained class=nav>minns_d_unconstrained</a> | <a href=#example_minns_d_bc class=nav>minns_d_bc</a> | <a href=#example_minns_d_nlc class=nav>minns_d_nlc</a> ]</p>
<a name=sub_minnscreatef></a><h6 class=pageheader>minnscreatef Function</h6>
<hr width=600 align=left>
<pre class=narration>
Version of minnscreatef() which uses numerical differentiation. I.e.,  you
do not have to calculate derivatives yourself. However, this version needs
2N times more function evaluations.

2-point differentiation formula is  used,  because  more  precise  4-point
formula is unstable when used on non-smooth functions.

Inputs:
    N       -   problem dimension, N &gt; 0:
                * if given, only leading N elements of X are used
                * if not given, automatically determined from size of X
    X       -   starting point, array[N]:
                * it is better to set X to a feasible point
                * but X can be infeasible, in which case algorithm will try
                  to find feasible point first, using X as initial
                  approximation.
    DiffStep-   differentiation  step,  DiffStep &gt; 0.   Algorithm   performs
                numerical differentiation  with  step  for  I-th  variable
                being equal to DiffStep*S[I] (here S[] is a  scale vector,
                set by minnssetscale() function).
                Do not use  too  small  steps,  because  it  may  lead  to
                catastrophic cancellation during intermediate calculations.

Outputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnscreatef(ae_int_t n, real_1d_array x, <b>double</b> diffstep, minnsstate &amp;state);
<b>void</b> minnscreatef(real_1d_array x, <b>double</b> diffstep, minnsstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minns_d_diff class=nav>minns_d_diff</a> ]</p>
<a name=sub_minnsoptimize></a><h6 class=pageheader>minnsoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear optimizer

These functions accept following parameters:
    state   -   algorithm state
    fvec    -   callback which calculates function vector fi[]
                at given point x
    jac     -   callback which calculates function vector fi[]
                and Jacobian jac at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL

NOTES:

1. This function has two different implementations: one which  uses  exact
   (analytical) user-supplied Jacobian, and one which uses  only  function
   vector and numerically  differentiates  function  in  order  to  obtain
   gradient.

   Depending  on  the  specific  function  used to create optimizer object
   you should choose appropriate variant of  minnsoptimize() -  one  which
   accepts function AND Jacobian or one which accepts ONLY function.

   Be careful to choose variant of minnsoptimize()  which  corresponds  to
   your optimization scheme! Table below lists different  combinations  of
   callback (function/gradient) passed to minnsoptimize()    and  specific
   function used to create optimizer.

                     |         USER PASSED TO minnsoptimize()
   CREATED WITH      |  function only   |  function and gradient
   ------------------------------------------------------------
   minnscreatef()    |     works               FAILS
   minnscreate()     |     FAILS               works

   Here &quot;FAILS&quot; denotes inappropriate combinations  of  optimizer creation
   function  and  minnsoptimize()  version.   Attemps   to    use     such
   combination will lead to exception. Either  you  did  not pass gradient
   when it WAS needed or you passed gradient when it was NOT needed.
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnsoptimize(minnsstate &amp;state, <b>void</b> (*fvec)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
<b>void</b> minnsoptimize(minnsstate &amp;state, <b>void</b> (*jac)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minns_d_unconstrained class=nav>minns_d_unconstrained</a> | <a href=#example_minns_d_diff class=nav>minns_d_diff</a> | <a href=#example_minns_d_bc class=nav>minns_d_bc</a> | <a href=#example_minns_d_nlc class=nav>minns_d_nlc</a> ]</p>
<a name=sub_minnsrequesttermination></a><h6 class=pageheader>minnsrequesttermination Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine submits request for termination of running  optimizer.  It
should be called from user-supplied callback when user decides that it  is
time to &quot;smoothly&quot; terminate optimization process.  As  result,  optimizer
stops at point which was &quot;current accepted&quot; when termination  request  was
submitted and returns error code 8 (successful termination).

Inputs:
    State   -   optimizer structure

NOTE: after  request  for  termination  optimizer  may   perform   several
      additional calls to user-supplied callbacks. It does  NOT  guarantee
      to stop immediately - it just guarantees that these additional calls
      will be discarded later.

NOTE: calling this function on optimizer which is NOT running will have no
      effect.

NOTE: multiple calls to this function are possible. First call is counted,
      subsequent calls are silently ignored.
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnsrequesttermination(minnsstate state);
</pre>
<a name=sub_minnsrestartfrom></a><h6 class=pageheader>minnsrestartfrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine restarts algorithm from new point.
All optimization parameters (including constraints) are left unchanged.

This  function  allows  to  solve multiple  optimization  problems  (which
must have  same number of dimensions) without object reallocation penalty.

Inputs:
    State   -   structure previously allocated with minnscreate() call.
    X       -   new starting point.
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnsrestartfrom(minnsstate state, real_1d_array x);
</pre>
<a name=sub_minnsresults></a><h6 class=pageheader>minnsresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
MinNS results

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[0..N-1], solution
    Rep     -   optimization report. You should check Rep.TerminationType
                in  order  to  distinguish  successful  termination  from
                unsuccessful one:
                * -8   internal integrity control  detected  infinite  or
                       NAN   values   in   function/gradient.    Abnormal
                       termination signalled.
                * -3   box constraints are inconsistent
                * -1   inconsistent parameters were passed:
                       * penalty parameter for minnssetalgoags() is zero,
                         but we have nonlinear constraints set by minnssetnlc()
                *  2   sampling radius decreased below epsx
                *  7    stopping conditions are too stringent,
                        further improvement is impossible,
                        X contains best point found so far.
                *  8    User requested termination via minnsrequesttermination()
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnsresults(minnsstate state, real_1d_array &amp;x, minnsreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minns_d_unconstrained class=nav>minns_d_unconstrained</a> | <a href=#example_minns_d_diff class=nav>minns_d_diff</a> | <a href=#example_minns_d_bc class=nav>minns_d_bc</a> | <a href=#example_minns_d_nlc class=nav>minns_d_nlc</a> ]</p>
<a name=sub_minnsresultsbuf></a><h6 class=pageheader>minnsresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Buffered implementation of minnsresults() which uses pre-allocated  buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnsresultsbuf(minnsstate state, real_1d_array &amp;x, minnsreport &amp;rep);
</pre>
<a name=sub_minnssetalgoags></a><h6 class=pageheader>minnssetalgoags Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function tells MinNS unit to use  AGS  (adaptive  gradient  sampling)
algorithm for nonsmooth constrained  optimization.  This  algorithm  is  a
slight modification of one described in  &quot;An  Adaptive  Gradient  Sampling
Algorithm for Nonsmooth Optimization&quot; by Frank E. Curtisy and Xiaocun Quez.

This optimizer has following benefits and drawbacks:
+ robustness; it can be used with nonsmooth and nonconvex functions.
+ relatively easy tuning; most of the metaparameters are easy to select.
- it has convergence of steepest descent, slower than CG/LBFGS.
- each iteration involves evaluation of ~2N gradient values  and  solution
  of 2Nx2N quadratic programming problem, which  limits  applicability  of
  algorithm by small-scale problems (up to 50-100).

IMPORTANT: this  algorithm  has  convergence  guarantees,   i.e.  it  will
           steadily move towards some stationary point of the function.

           However, &quot;stationary point&quot; does not  always  mean  &quot;solution&quot;.
           Nonsmooth problems often have &quot;flat spots&quot;,  i.e.  areas  where
           function do not change at all. Such &quot;flat spots&quot; are stationary
           points by definition, and algorithm may be caught here.

           Nonsmooth CONVEX tasks are not prone to  this  problem. Say, if
           your function has form f()=MAX(f0,f1,...), and f_i are  convex,
           then f() is convex too and you have guaranteed  convergence  to
           solution.

Inputs:
    State   -   structure which stores algorithm state
    Radius  -   initial sampling radius, &ge; 0.

                Internally multiplied  by  vector of  per-variable  scales
                specified by minnssetscale()).

                You should select relatively large sampling radius, roughly
                proportional to scaled length of the first  steps  of  the
                algorithm. Something close to 0.1 in magnitude  should  be
                good for most problems.

                AGS solver can automatically decrease radius, so too large
                radius is  not a problem (assuming that you  won't  choose
                so large radius that algorithm  will  sample  function  in
                too far away points, where gradient value is irrelevant).

                Too small radius won't cause algorithm to fail, but it may
                slow down algorithm (it may  have  to  perform  too  short
                steps).
    Penalty -   penalty coefficient for nonlinear constraints:
                * for problem with nonlinear constraints  should  be  some
                  problem-specific  positive   value,  large  enough  that
                  penalty term changes shape of the function.
                  Starting  from  some  problem-specific   value   penalty
                  coefficient becomes  large  enough  to  exactly  enforce
                  nonlinear constraints;  larger  values  do  not  improve
                  precision.
                  Increasing it too much may slow down convergence, so you
                  should choose it carefully.
                * can be zero for problems WITHOUT  nonlinear  constraints
                  (i.e. for unconstrained ones or ones with  just  box  or
                  linear constraints)
                * if you specify zero value for problem with at least  one
                  nonlinear  constraint,  algorithm  will  terminate  with
                  error code -1.

ALGORITHM OUTLINE

The very basic outline of unconstrained AGS algorithm is given below:

0. If sampling radius is below EpsX  or  we  performed  more  then  MaxIts
   iterations - STOP.
1. sample O(N) gradient values at random locations  around  current point;
   informally speaking, this sample is an implicit piecewise  linear model
   of the function, although algorithm formulation does  not  mention that
   explicitly
2. solve quadratic programming problem in order to find descent direction
3. if QP solver tells us that we  are  near  solution,  decrease  sampling
   radius and move to (0)
4. perform backtracking line search
5. after moving to new point, goto (0)

As for the constraints:
* box constraints are handled exactly  by  modification  of  the  function
  being minimized
* linear/nonlinear constraints are handled by adding L1  penalty.  Because
  our solver can handle nonsmoothness, we can  use  L1  penalty  function,
  which is an exact one  (i.e.  exact  solution  is  returned  under  such
  penalty).
* penalty coefficient for  linear  constraints  is  chosen  automatically;
  however, penalty coefficient for nonlinear constraints must be specified
  by user.
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnssetalgoags(minnsstate state, <b>double</b> radius, <b>double</b> penalty);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minns_d_unconstrained class=nav>minns_d_unconstrained</a> | <a href=#example_minns_d_diff class=nav>minns_d_diff</a> | <a href=#example_minns_d_bc class=nav>minns_d_bc</a> | <a href=#example_minns_d_nlc class=nav>minns_d_nlc</a> ]</p>
<a name=sub_minnssetbc></a><h6 class=pageheader>minnssetbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets boundary constraints.

Boundary constraints are inactive by default (after initial creation).
They are preserved after algorithm restart with minnsrestartfrom().

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very small number or -INF.
    BndU    -   upper bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very large number or +INF.

NOTE 1: it is possible to specify BndL[i]=BndU[i]. In this case I-th
variable will be &quot;frozen&quot; at X[i]=BndL[i]=BndU[i].

NOTE 2: AGS solver has following useful properties:
* bound constraints are always satisfied exactly
* function is evaluated only INSIDE area specified by  bound  constraints,
  even  when  numerical  differentiation is used (algorithm adjusts  nodes
  according to boundary constraints)
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnssetbc(minnsstate state, real_1d_array bndl, real_1d_array bndu);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minns_d_bc class=nav>minns_d_bc</a> ]</p>
<a name=sub_minnssetcond></a><h6 class=pageheader>minnssetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping conditions for iterations of optimizer.

Inputs:
    State   -   structure which stores algorithm state
    EpsX    - &ge; 0
                The AGS solver finishes its work if  on  k+1-th  iteration
                sampling radius decreases below EpsX.
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations is unlimited.

Passing EpsX=0  and  MaxIts=0  (simultaneously)  will  lead  to  automatic
stopping criterion selection. We do not recommend you to rely  on  default
choice in production code.
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnssetcond(minnsstate state, <b>double</b> epsx, ae_int_t maxits);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minns_d_unconstrained class=nav>minns_d_unconstrained</a> | <a href=#example_minns_d_diff class=nav>minns_d_diff</a> | <a href=#example_minns_d_bc class=nav>minns_d_bc</a> | <a href=#example_minns_d_nlc class=nav>minns_d_nlc</a> ]</p>
<a name=sub_minnssetlc></a><h6 class=pageheader>minnssetlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets linear constraints.

Linear constraints are inactive by default (after initial creation).
They are preserved after algorithm restart with minnsrestartfrom().

Inputs:
    State   -   structure previously allocated with minnscreate() call.
    C       -   linear constraints, array[K,N+1].
                Each row of C represents one constraint, either equality
                or inequality (see below):
                * first N elements correspond to coefficients,
                * last element corresponds to the right part.
                All elements of C (including right part) must be finite.
    CT      -   type of constraints, array[K]:
                * if CT[i] &gt; 0, then I-th constraint is C[i,*]*x &ge; C[i,n+1]
                * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
                * if CT[i] &lt; 0, then I-th constraint is C[i,*]*x &le; C[i,n+1]
    K       -   number of equality/inequality constraints, K &ge; 0:
                * if given, only leading K elements of C/CT are used
                * if not given, automatically determined from sizes of C/CT

NOTE: linear (non-bound) constraints are satisfied only approximately:

* there always exists some minor violation (about current sampling  radius
  in magnitude during optimization, about EpsX in the solution) due to use
  of penalty method to handle constraints.
* numerical differentiation, if used, may  lead  to  function  evaluations
  outside  of the feasible  area,   because   algorithm  does  NOT  change
  numerical differentiation formula according to linear constraints.

If you want constraints to be  satisfied  exactly, try to reformulate your
problem  in  such  manner  that  all constraints will become boundary ones
(this kind of constraints is always satisfied exactly, both in  the  final
solution and in all intermediate points).
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnssetlc(minnsstate state, real_2d_array c, integer_1d_array ct, ae_int_t k);
<b>void</b> minnssetlc(minnsstate state, real_2d_array c, integer_1d_array ct);
</pre>
<a name=sub_minnssetnlc></a><h6 class=pageheader>minnssetnlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets nonlinear constraints.

In fact, this function sets NUMBER of nonlinear  constraints.  Constraints
itself (constraint functions) are passed to minnsoptimize() method.   This
method requires user-defined vector function F[]  and  its  Jacobian  J[],
where:
* first component of F[] and first row  of  Jacobian  J[]  correspond   to
  function being minimized
* next NLEC components of F[] (and rows  of  J)  correspond  to  nonlinear
  equality constraints G_i(x)=0
* next NLIC components of F[] (and rows  of  J)  correspond  to  nonlinear
  inequality constraints H_i(x) &le; 0

NOTE: you may combine nonlinear constraints with linear/boundary ones.  If
      your problem has mixed constraints, you  may explicitly specify some
      of them as linear ones. It may help optimizer to  handle  them  more
      efficiently.

Inputs:
    State   -   structure previously allocated with minnscreate() call.
    NLEC    -   number of Non-Linear Equality Constraints (NLEC), &ge; 0
    NLIC    -   number of Non-Linear Inquality Constraints (NLIC), &ge; 0

NOTE 1: nonlinear constraints are satisfied only  approximately!   It   is
        possible   that  algorithm  will  evaluate  function  outside   of
        the feasible area!

NOTE 2: algorithm scales variables  according  to   scale   specified   by
        minnssetscale()  function,  so  it can handle problems with  badly
        scaled variables (as long as we KNOW their scales).

        However,  there  is  no  way  to  automatically  scale   nonlinear
        constraints Gi(x) and Hi(x). Inappropriate scaling  of  Gi/Hi  may
        ruin convergence. Solving problem with  constraint  &quot;1000*G0(x)=0&quot;
        is NOT same as solving it with constraint &quot;0.001*G0(x)=0&quot;.

        It  means  that  YOU  are  the  one who is responsible for correct
        scaling of nonlinear constraints Gi(x) and Hi(x). We recommend you
        to scale nonlinear constraints in such way that I-th component  of
        dG/dX (or dH/dx) has approximately unit  magnitude  (for  problems
        with unit scale)  or  has  magnitude approximately equal to 1/S[i]
        (where S is a scale set by minnssetscale() function).

NOTE 3: nonlinear constraints are always hard to handle,  no  matter  what
        algorithm you try to use. Even basic box/linear constraints modify
        function  curvature   by  adding   valleys  and  ridges.  However,
        nonlinear constraints add valleys which are very  hard  to  follow
        due to their &quot;curved&quot; nature.

        It means that optimization with single nonlinear constraint may be
        significantly slower than optimization with multiple linear  ones.
        It is normal situation, and we recommend you to  carefully  choose
        Rho parameter of minnssetalgoags(), because too  large  value  may
        slow down convergence.
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnssetnlc(minnsstate state, ae_int_t nlec, ae_int_t nlic);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minns_d_nlc class=nav>minns_d_nlc</a> ]</p>
<a name=sub_minnssetscale></a><h6 class=pageheader>minnssetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients for NLC optimizer.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison with tolerances).  Scale of
the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the function

Scaling is also used by finite difference variant of the optimizer  - step
along I-th axis is equal to DiffStep*S[I].

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 18.05.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnssetscale(minnsstate state, real_1d_array s);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minns_d_unconstrained class=nav>minns_d_unconstrained</a> | <a href=#example_minns_d_diff class=nav>minns_d_diff</a> | <a href=#example_minns_d_bc class=nav>minns_d_bc</a> | <a href=#example_minns_d_nlc class=nav>minns_d_nlc</a> ]</p>
<a name=sub_minnssetxrep></a><h6 class=pageheader>minnssetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to minnsoptimize().
ALGLIB: Copyright 28.11.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minnssetxrep(minnsstate state, <b>bool</b> needxrep);
</pre>
<a name=example_minns_d_bc></a><h6 class=pageheader>minns_d_bc Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> nsfunc1_jac(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = 2*|x0|+x1</font>
<font color=navy>//</font>
<font color=navy>// and Jacobian matrix J = [df0/dx0 df0/dx1]</font>
   fi[0] = 2*fabs(<b>double</b>(x[0]))+fabs(<b>double</b>(x[1]));
   jac[0][0] = 2*sign(x[0]);
   jac[0][1] = sign(x[1]);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x0,x1) = 2*|x0|+|x1|</font>
<font color=navy>//</font>
<font color=navy>// subject to box constraints</font>
<font color=navy>//</font>
<font color=navy>//        1 &le; x0 &lt; +INF</font>
<font color=navy>//     -INF &le; x1 &lt; +INF</font>
<font color=navy>//</font>
<font color=navy>// using nonsmooth nonlinear optimizer.</font>
   real_1d_array x0 = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[1,-inf]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[+inf,+inf]&quot;</b></font>;
   <b>double</b> epsx = 0.00001;
   <b>double</b> radius = 0.1;
   <b>double</b> rho = 0.0;
   ae_int_t maxits = 0;
   minnsstate state;
   minnsreport rep;
   real_1d_array x1;
<font color=navy>// Create optimizer object, choose AGS algorithm and tune its settings:</font>
<font color=navy>// * radius=0.1     good initial value; will be automatically decreased later.</font>
<font color=navy>// * rho=0.0        penalty coefficient <b>for</b> nonlinear constraints; can be zero</font>
<font color=navy>//                  because we <b>do</b> not have such constraints</font>
<font color=navy>// * epsx=0.000001  stopping conditions</font>
<font color=navy>// * s=[1,1]        all variables have unit scale</font>
   minnscreate(2, x0, state);
   minnssetalgoags(state, radius, rho);
   minnssetcond(state, epsx, maxits);
   minnssetscale(state, s);
<font color=navy>// Set box constraints.</font>
<font color=navy>//</font>
<font color=navy>// General linear constraints are set in similar way (see comments on</font>
<font color=navy>// minnssetlc() function <b>for</b> more information).</font>
<font color=navy>//</font>
<font color=navy>// You may combine box, linear and nonlinear constraints in one optimization</font>
<font color=navy>// problem.</font>
   minnssetbc(state, bndl, bndu);
<font color=navy>// Optimize and test results.</font>
<font color=navy>//</font>
<font color=navy>// Optimizer object accepts vector function and its Jacobian, with first</font>
<font color=navy>// component (Jacobian row) being target function, and next components</font>
<font color=navy>// (Jacobian rows) being nonlinear equality and inequality constraints</font>
<font color=navy>// (box/linear ones are passed separately by means of minnssetbc() and</font>
<font color=navy>// minnssetlc() calls).</font>
<font color=navy>//</font>
<font color=navy>// If you <b>do</b> not have nonlinear constraints (exactly our situation), then</font>
<font color=navy>// you will have one-component function vector and 1xN Jacobian matrix.</font>
<font color=navy>//</font>
<font color=navy>// So, our vector function has form</font>
<font color=navy>//</font>
<font color=navy>//     {f0} = { 2*|x0|+|x1| }</font>
<font color=navy>//</font>
<font color=navy>// with Jacobian</font>
<font color=navy>//</font>
<font color=navy>//         [                       ]</font>
<font color=navy>//     J = [ 2*sign(x0)   sign(x1) ]</font>
<font color=navy>//         [                       ]</font>
<font color=navy>//</font>
<font color=navy>// NOTE: nonsmooth optimizer requires considerably more function</font>
<font color=navy>//       evaluations than smooth solver - about 2N times more. Using</font>
<font color=navy>//       numerical differentiation introduces additional (multiplicative)</font>
<font color=navy>//       2N speedup.</font>
<font color=navy>//</font>
<font color=navy>//       It means that <b>if</b> smooth optimizer WITH user-supplied gradient</font>
<font color=navy>//       needs 100 function evaluations to solve 50-dimensional problem,</font>
<font color=navy>//       then AGS solver with user-supplied gradient will need about 10.000</font>
<font color=navy>//       function evaluations, and with numerical gradient about 1.000.000</font>
<font color=navy>//       function evaluations will be performed.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: AGS solver used by us can handle nonsmooth and nonconvex</font>
<font color=navy>//       optimization problems. It has convergence guarantees, i.e. it will</font>
<font color=navy>//       converge to stationary point of the function after running <b>for</b> some</font>
<font color=navy>//       time.</font>
<font color=navy>//</font>
<font color=navy>//       However, it is important to remember that <font color=blue><b>&quot;stationary point&quot;</b></font> is not</font>
<font color=navy>//       equal to <font color=blue><b>&quot;solution&quot;</b></font>. If your problem is convex, everything is OK.</font>
<font color=navy>//       But nonconvex optimization problems may have <font color=blue><b>&quot;flat spots&quot;</b></font> - large</font>
<font color=navy>//       areas where gradient is exactly zero, but function value is far away</font>
<font color=navy>//       from optimal. Such areas are stationary points too, and optimizer</font>
<font color=navy>//       may be trapped here.</font>
<font color=navy>//</font>
<font color=navy>//       <font color=blue><b>&quot;Flat spots&quot;</b></font> are nonsmooth equivalent of the saddle points, but with</font>
<font color=navy>//       orders of magnitude worse properties - they may be quite large and</font>
<font color=navy>//       hard to avoid. All nonsmooth optimizers are prone to this kind of the</font>
<font color=navy>//       problem, because it is impossible to automatically distinguish &quot;flat</font>
<font color=navy>//       spot&quot; from true solution.</font>
<font color=navy>//</font>
<font color=navy>//       This note is here to warn you that you should be very careful when</font>
<font color=navy>//       you solve nonsmooth optimization problems. Visual inspection of</font>
<font color=navy>//       results is essential.</font>
<font color=navy>//</font>
<font color=navy>//</font>
   minnsoptimize(state, nsfunc1_jac);
   minnsresults(state, x1, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x1.tostring(2).c_str()); <font color=navy>// EXPECTED: [1.0000,0.0000]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minns_d_diff></a><h6 class=pageheader>minns_d_diff Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> nsfunc1_fvec(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = 2*|x0|+x1</font>
   fi[0] = 2*fabs(<b>double</b>(x[0]))+fabs(<b>double</b>(x[1]));
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x0,x1) = 2*|x0|+|x1|</font>
<font color=navy>//</font>
<font color=navy>// using nonsmooth nonlinear optimizer with numerical</font>
<font color=navy>// differentiation provided by ALGLIB.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: nonsmooth optimizer requires considerably more function</font>
<font color=navy>//       evaluations than smooth solver - about 2N times more. Using</font>
<font color=navy>//       numerical differentiation introduces additional (multiplicative)</font>
<font color=navy>//       2N speedup.</font>
<font color=navy>//</font>
<font color=navy>//       It means that <b>if</b> smooth optimizer WITH user-supplied gradient</font>
<font color=navy>//       needs 100 function evaluations to solve 50-dimensional problem,</font>
<font color=navy>//       then AGS solver with user-supplied gradient will need about 10.000</font>
<font color=navy>//       function evaluations, and with numerical gradient about 1.000.000</font>
<font color=navy>//       function evaluations will be performed.</font>
   real_1d_array x0 = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsx = 0.00001;
   <b>double</b> diffstep = 0.000001;
   <b>double</b> radius = 0.1;
   <b>double</b> rho = 0.0;
   ae_int_t maxits = 0;
   minnsstate state;
   minnsreport rep;
   real_1d_array x1;
<font color=navy>// Create optimizer object, choose AGS algorithm and tune its settings:</font>
<font color=navy>// * radius=0.1     good initial value; will be automatically decreased later.</font>
<font color=navy>// * rho=0.0        penalty coefficient <b>for</b> nonlinear constraints; can be zero</font>
<font color=navy>//                  because we <b>do</b> not have such constraints</font>
<font color=navy>// * epsx=0.000001  stopping conditions</font>
<font color=navy>// * s=[1,1]        all variables have unit scale</font>
   minnscreatef(2, x0, diffstep, state);
   minnssetalgoags(state, radius, rho);
   minnssetcond(state, epsx, maxits);
   minnssetscale(state, s);
<font color=navy>// Optimize and test results.</font>
<font color=navy>//</font>
<font color=navy>// Optimizer object accepts vector function, with first component</font>
<font color=navy>// being target function, and next components being nonlinear equality</font>
<font color=navy>// and inequality constraints (box/linear ones are passed separately</font>
<font color=navy>// by means of minnssetbc() and minnssetlc() calls).</font>
<font color=navy>//</font>
<font color=navy>// If you <b>do</b> not have nonlinear constraints (exactly our situation), then</font>
<font color=navy>// you will have one-component function vector.</font>
<font color=navy>//</font>
<font color=navy>// So, our vector function has form</font>
<font color=navy>//</font>
<font color=navy>//     {f0} = { 2*|x0|+|x1| }</font>
   minnsoptimize(state, nsfunc1_fvec);
   minnsresults(state, x1, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x1.tostring(2).c_str()); <font color=navy>// EXPECTED: [0.0000,0.0000]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minns_d_nlc></a><h6 class=pageheader>minns_d_nlc Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> nsfunc2_jac(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr) {
<font color=navy>// this callback calculates function vector</font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = 2*|x0|+x1</font>
<font color=navy>//     f1(x0,x1) = x0-1</font>
<font color=navy>//     f2(x0,x1) = -x1-1</font>
<font color=navy>//</font>
<font color=navy>// and Jacobian matrix J</font>
<font color=navy>//</font>
<font color=navy>//         [ df0/dx0   df0/dx1 ]</font>
<font color=navy>//     J = [ df1/dx0   df1/dx1 ]</font>
<font color=navy>//         [ df2/dx0   df2/dx1 ]</font>
   fi[0] = 2*fabs(<b>double</b>(x[0]))+fabs(<b>double</b>(x[1]));
   jac[0][0] = 2*sign(x[0]);
   jac[0][1] = sign(x[1]);
   fi[1] = x[0]-1;
   jac[1][0] = 1;
   jac[1][1] = 0;
   fi[2] = -x[1]-1;
   jac[2][0] = 0;
   jac[2][1] = -1;
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x0,x1) = 2*|x0|+|x1|</font>
<font color=navy>//</font>
<font color=navy>// subject to combination of equality and inequality constraints</font>
<font color=navy>//</font>
<font color=navy>//      x0  =  1</font>
<font color=navy>//      x1 &ge; -1</font>
<font color=navy>//</font>
<font color=navy>// using nonsmooth nonlinear optimizer. Although these constraints</font>
<font color=navy>// are linear, we treat them as general nonlinear ones in order to</font>
<font color=navy>// demonstrate nonlinearly constrained optimization setup.</font>
   real_1d_array x0 = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsx = 0.00001;
   <b>double</b> radius = 0.1;
   <b>double</b> rho = 50.0;
   ae_int_t maxits = 0;
   minnsstate state;
   minnsreport rep;
   real_1d_array x1;
<font color=navy>// Create optimizer object, choose AGS algorithm and tune its settings:</font>
<font color=navy>// * radius=0.1     good initial value; will be automatically decreased later.</font>
<font color=navy>// * rho=50.0       penalty coefficient <b>for</b> nonlinear constraints. It is your</font>
<font color=navy>//                  responsibility to choose good one - large enough that it</font>
<font color=navy>//                  enforces constraints, but small enough in order to avoid</font>
<font color=navy>//                  extreme slowdown due to ill-conditioning.</font>
<font color=navy>// * epsx=0.000001  stopping conditions</font>
<font color=navy>// * s=[1,1]        all variables have unit scale</font>
   minnscreate(2, x0, state);
   minnssetalgoags(state, radius, rho);
   minnssetcond(state, epsx, maxits);
   minnssetscale(state, s);
<font color=navy>// Set general nonlinear constraints.</font>
<font color=navy>//</font>
<font color=navy>// This part is more tricky than working with box/linear constraints - you</font>
<font color=navy>// can not <font color=blue><b>&quot;pack&quot;</b></font> general nonlinear function into <b>double</b> precision array.</font>
<font color=navy>// That's why minnssetnlc() does not accept constraints itself - only</font>
<font color=navy>// constraint COUNTS are passed: first parameter is number of equality</font>
<font color=navy>// constraints, second one is number of inequality constraints.</font>
<font color=navy>//</font>
<font color=navy>// As <b>for</b> constraining functions - these functions are passed as part</font>
<font color=navy>// of problem Jacobian (see below).</font>
<font color=navy>//</font>
<font color=navy>// NOTE: MinNS optimizer supports arbitrary combination of boundary, general</font>
<font color=navy>//       linear and general nonlinear constraints. This example does not</font>
<font color=navy>//       show how to work with general linear constraints, but you can</font>
<font color=navy>//       easily find it in documentation on minnlcsetlc() function.</font>
   minnssetnlc(state, 1, 1);
<font color=navy>// Optimize and test results.</font>
<font color=navy>//</font>
<font color=navy>// Optimizer object accepts vector function and its Jacobian, with first</font>
<font color=navy>// component (Jacobian row) being target function, and next components</font>
<font color=navy>// (Jacobian rows) being nonlinear equality and inequality constraints</font>
<font color=navy>// (box/linear ones are passed separately by means of minnssetbc() and</font>
<font color=navy>// minnssetlc() calls).</font>
<font color=navy>//</font>
<font color=navy>// Nonlinear equality constraints have form Gi(x)=0, inequality ones</font>
<font color=navy>// have form Hi(x) &le; 0, so we may have to <font color=blue><b>&quot;normalize&quot;</b></font> constraints prior</font>
<font color=navy>// to passing them to optimizer (right side is zero, constraints are</font>
<font color=navy>// sorted, multiplied by -1 when needed).</font>
<font color=navy>//</font>
<font color=navy>// So, our vector function has form</font>
<font color=navy>//</font>
<font color=navy>//     {f0,f1,f2} = { 2*|x0|+|x1|,  x0-1, -x1-1 }</font>
<font color=navy>//</font>
<font color=navy>// with Jacobian</font>
<font color=navy>//</font>
<font color=navy>//         [ 2*sign(x0)   sign(x1) ]</font>
<font color=navy>//     J = [     1           0     ]</font>
<font color=navy>//         [     0          -1     ]</font>
<font color=navy>//</font>
<font color=navy>// which means that we have optimization problem</font>
<font color=navy>//</font>
<font color=navy>//     min{f0} subject to f1=0, f2 &le; 0</font>
<font color=navy>//</font>
<font color=navy>// which is essentially same as</font>
<font color=navy>//</font>
<font color=navy>//     min { 2*|x0|+|x1| } subject to x0=1, x1 &ge; -1</font>
<font color=navy>//</font>
<font color=navy>// NOTE: AGS solver used by us can handle nonsmooth and nonconvex</font>
<font color=navy>//       optimization problems. It has convergence guarantees, i.e. it will</font>
<font color=navy>//       converge to stationary point of the function after running <b>for</b> some</font>
<font color=navy>//       time.</font>
<font color=navy>//</font>
<font color=navy>//       However, it is important to remember that <font color=blue><b>&quot;stationary point&quot;</b></font> is not</font>
<font color=navy>//       equal to <font color=blue><b>&quot;solution&quot;</b></font>. If your problem is convex, everything is OK.</font>
<font color=navy>//       But nonconvex optimization problems may have <font color=blue><b>&quot;flat spots&quot;</b></font> - large</font>
<font color=navy>//       areas where gradient is exactly zero, but function value is far away</font>
<font color=navy>//       from optimal. Such areas are stationary points too, and optimizer</font>
<font color=navy>//       may be trapped here.</font>
<font color=navy>//</font>
<font color=navy>//       <font color=blue><b>&quot;Flat spots&quot;</b></font> are nonsmooth equivalent of the saddle points, but with</font>
<font color=navy>//       orders of magnitude worse properties - they may be quite large and</font>
<font color=navy>//       hard to avoid. All nonsmooth optimizers are prone to this kind of the</font>
<font color=navy>//       problem, because it is impossible to automatically distinguish &quot;flat</font>
<font color=navy>//       spot&quot; from true solution.</font>
<font color=navy>//</font>
<font color=navy>//       This note is here to warn you that you should be very careful when</font>
<font color=navy>//       you solve nonsmooth optimization problems. Visual inspection of</font>
<font color=navy>//       results is essential.</font>
   minnsoptimize(state, nsfunc2_jac);
   minnsresults(state, x1, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x1.tostring(2).c_str()); <font color=navy>// EXPECTED: [1.0000,0.0000]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minns_d_unconstrained></a><h6 class=pageheader>minns_d_unconstrained Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>void</b> nsfunc1_jac(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr) {
<font color=navy>// this callback calculates</font>
<font color=navy>//</font>
<font color=navy>//     f0(x0,x1) = 2*|x0|+x1</font>
<font color=navy>//</font>
<font color=navy>// and Jacobian matrix J = [df0/dx0 df0/dx1]</font>
   fi[0] = 2*fabs(<b>double</b>(x[0]))+fabs(<b>double</b>(x[1]));
   jac[0][0] = 2*sign(x[0]);
   jac[0][1] = sign(x[1]);
}

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of</font>
<font color=navy>//</font>
<font color=navy>//     f(x0,x1) = 2*|x0|+|x1|</font>
<font color=navy>//</font>
<font color=navy>// using nonsmooth nonlinear optimizer.</font>
   real_1d_array x0 = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   <b>double</b> epsx = 0.00001;
   <b>double</b> radius = 0.1;
   <b>double</b> rho = 0.0;
   ae_int_t maxits = 0;
   minnsstate state;
   minnsreport rep;
   real_1d_array x1;
<font color=navy>// Create optimizer object, choose AGS algorithm and tune its settings:</font>
<font color=navy>// * radius=0.1     good initial value; will be automatically decreased later.</font>
<font color=navy>// * rho=0.0        penalty coefficient <b>for</b> nonlinear constraints; can be zero</font>
<font color=navy>//                  because we <b>do</b> not have such constraints</font>
<font color=navy>// * epsx=0.000001  stopping conditions</font>
<font color=navy>// * s=[1,1]        all variables have unit scale</font>
   minnscreate(2, x0, state);
   minnssetalgoags(state, radius, rho);
   minnssetcond(state, epsx, maxits);
   minnssetscale(state, s);
<font color=navy>// Optimize and test results.</font>
<font color=navy>//</font>
<font color=navy>// Optimizer object accepts vector function and its Jacobian, with first</font>
<font color=navy>// component (Jacobian row) being target function, and next components</font>
<font color=navy>// (Jacobian rows) being nonlinear equality and inequality constraints</font>
<font color=navy>// (box/linear ones are passed separately by means of minnssetbc() and</font>
<font color=navy>// minnssetlc() calls).</font>
<font color=navy>//</font>
<font color=navy>// If you <b>do</b> not have nonlinear constraints (exactly our situation), then</font>
<font color=navy>// you will have one-component function vector and 1xN Jacobian matrix.</font>
<font color=navy>//</font>
<font color=navy>// So, our vector function has form</font>
<font color=navy>//</font>
<font color=navy>//     {f0} = { 2*|x0|+|x1| }</font>
<font color=navy>//</font>
<font color=navy>// with Jacobian</font>
<font color=navy>//</font>
<font color=navy>//         [                       ]</font>
<font color=navy>//     J = [ 2*sign(x0)   sign(x1) ]</font>
<font color=navy>//         [                       ]</font>
<font color=navy>//</font>
<font color=navy>// NOTE: nonsmooth optimizer requires considerably more function</font>
<font color=navy>//       evaluations than smooth solver - about 2N times more. Using</font>
<font color=navy>//       numerical differentiation introduces additional (multiplicative)</font>
<font color=navy>//       2N speedup.</font>
<font color=navy>//</font>
<font color=navy>//       It means that <b>if</b> smooth optimizer WITH user-supplied gradient</font>
<font color=navy>//       needs 100 function evaluations to solve 50-dimensional problem,</font>
<font color=navy>//       then AGS solver with user-supplied gradient will need about 10.000</font>
<font color=navy>//       function evaluations, and with numerical gradient about 1.000.000</font>
<font color=navy>//       function evaluations will be performed.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: AGS solver used by us can handle nonsmooth and nonconvex</font>
<font color=navy>//       optimization problems. It has convergence guarantees, i.e. it will</font>
<font color=navy>//       converge to stationary point of the function after running <b>for</b> some</font>
<font color=navy>//       time.</font>
<font color=navy>//</font>
<font color=navy>//       However, it is important to remember that <font color=blue><b>&quot;stationary point&quot;</b></font> is not</font>
<font color=navy>//       equal to <font color=blue><b>&quot;solution&quot;</b></font>. If your problem is convex, everything is OK.</font>
<font color=navy>//       But nonconvex optimization problems may have <font color=blue><b>&quot;flat spots&quot;</b></font> - large</font>
<font color=navy>//       areas where gradient is exactly zero, but function value is far away</font>
<font color=navy>//       from optimal. Such areas are stationary points too, and optimizer</font>
<font color=navy>//       may be trapped here.</font>
<font color=navy>//</font>
<font color=navy>//       <font color=blue><b>&quot;Flat spots&quot;</b></font> are nonsmooth equivalent of the saddle points, but with</font>
<font color=navy>//       orders of magnitude worse properties - they may be quite large and</font>
<font color=navy>//       hard to avoid. All nonsmooth optimizers are prone to this kind of the</font>
<font color=navy>//       problem, because it is impossible to automatically distinguish &quot;flat</font>
<font color=navy>//       spot&quot; from true solution.</font>
<font color=navy>//</font>
<font color=navy>//       This note is here to warn you that you should be very careful when</font>
<font color=navy>//       you solve nonsmooth optimization problems. Visual inspection of</font>
<font color=navy>//       results is essential.</font>
   minnsoptimize(state, nsfunc1_jac);
   minnsresults(state, x1, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x1.tostring(2).c_str()); <font color=navy>// EXPECTED: [0.0000,0.0000]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_minqp></a><h4 class=pageheader>8.8.10. minqp Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_minqpreport class=toc>minqpreport</a> |
<a href=#struct_minqpstate class=toc>minqpstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_minqpaddlc2 class=toc>minqpaddlc2</a> |
<a href=#sub_minqpaddlc2dense class=toc>minqpaddlc2dense</a> |
<a href=#sub_minqpcreate class=toc>minqpcreate</a> |
<a href=#sub_minqpoptimize class=toc>minqpoptimize</a> |
<a href=#sub_minqpresults class=toc>minqpresults</a> |
<a href=#sub_minqpresultsbuf class=toc>minqpresultsbuf</a> |
<a href=#sub_minqpsetalgobleic class=toc>minqpsetalgobleic</a> |
<a href=#sub_minqpsetalgodenseaul class=toc>minqpsetalgodenseaul</a> |
<a href=#sub_minqpsetalgodenseipm class=toc>minqpsetalgodenseipm</a> |
<a href=#sub_minqpsetalgoquickqp class=toc>minqpsetalgoquickqp</a> |
<a href=#sub_minqpsetalgosparseipm class=toc>minqpsetalgosparseipm</a> |
<a href=#sub_minqpsetbc class=toc>minqpsetbc</a> |
<a href=#sub_minqpsetbcall class=toc>minqpsetbcall</a> |
<a href=#sub_minqpsetbci class=toc>minqpsetbci</a> |
<a href=#sub_minqpsetlc class=toc>minqpsetlc</a> |
<a href=#sub_minqpsetlc2 class=toc>minqpsetlc2</a> |
<a href=#sub_minqpsetlc2dense class=toc>minqpsetlc2dense</a> |
<a href=#sub_minqpsetlc2mixed class=toc>minqpsetlc2mixed</a> |
<a href=#sub_minqpsetlcmixed class=toc>minqpsetlcmixed</a> |
<a href=#sub_minqpsetlcmixedlegacy class=toc>minqpsetlcmixedlegacy</a> |
<a href=#sub_minqpsetlcsparse class=toc>minqpsetlcsparse</a> |
<a href=#sub_minqpsetlinearterm class=toc>minqpsetlinearterm</a> |
<a href=#sub_minqpsetorigin class=toc>minqpsetorigin</a> |
<a href=#sub_minqpsetquadraticterm class=toc>minqpsetquadraticterm</a> |
<a href=#sub_minqpsetquadratictermsparse class=toc>minqpsetquadratictermsparse</a> |
<a href=#sub_minqpsetscale class=toc>minqpsetscale</a> |
<a href=#sub_minqpsetscaleautodiag class=toc>minqpsetscaleautodiag</a> |
<a href=#sub_minqpsetstartingpoint class=toc>minqpsetstartingpoint</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_minqp_d_bc1 class=toc>minqp_d_bc1</a></td><td width=15>&nbsp;</td><td>Bound constrained dense quadratic programming</td></tr>
<tr align=left valign=top><td><a href=#example_minqp_d_lc1 class=toc>minqp_d_lc1</a></td><td width=15>&nbsp;</td><td>Linearly constrained dense quadratic programming</td></tr>
<tr align=left valign=top><td><a href=#example_minqp_d_nonconvex class=toc>minqp_d_nonconvex</a></td><td width=15>&nbsp;</td><td>Nonconvex quadratic programming</td></tr>
<tr align=left valign=top><td><a href=#example_minqp_d_u1 class=toc>minqp_d_u1</a></td><td width=15>&nbsp;</td><td>Unconstrained dense quadratic programming</td></tr>
<tr align=left valign=top><td><a href=#example_minqp_d_u2 class=toc>minqp_d_u2</a></td><td width=15>&nbsp;</td><td>Unconstrained sparse quadratic programming</td></tr>
</table>
</div>
<a name=struct_minqpreport></a><h6 class=pageheader>minqpreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure stores optimization report:
* InnerIterationsCount      number of inner iterations
* OuterIterationsCount      number of outer iterations
* NCholesky                 number of Cholesky decomposition
* NMV                       number of matrix-vector products
                            (only products calculated as part of iterative
                            process are counted)
* TerminationType           completion code (see below)
* LagBC                     Lagrange multipliers for box constraints,
                            array[N], not filled by QP-BLEIC solver
* LagLC                     Lagrange multipliers for linear constraints,
                            array[MSparse+MDense], ignored by QP-BLEIC solver

==== COMPLETION CODES ====

Completion codes:
* -9    failure of the automatic scale evaluation:  one  of  the  diagonal
        elements of the quadratic term is non-positive.  Specify  variable
        scales manually!
* -5    inappropriate solver was used:
        * QuickQP solver for problem with general linear constraints (dense/sparse)
* -4    BLEIC-QP or QuickQP solver found unconstrained direction
        of negative curvature (function is unbounded from
        below  even  under  constraints),  no  meaningful
        minimum can be found.
* -3    inconsistent constraints (or, maybe, feasible point is
        too hard to find). If you are sure that constraints are feasible,
        try to restart optimizer with better initial approximation.
* -2    IPM solver has difficulty finding primal/dual feasible point.
        It is likely that the problem is either infeasible or unbounded,
        but it is difficult to determine exact reason for termination.
        X contains best point found so far.
*  1..4 successful completion
*  5    MaxIts steps was taken
*  7    stopping conditions are too stringent,
        further improvement is impossible,
        X contains best point found so far.

==== LAGRANGE MULTIPLIERS ====

Some  optimizers  report  values of  Lagrange  multipliers  on  successful
completion (positive completion code):
* DENSE-IPM-QP and SPARSE-IPM-QP return very precise Lagrange  multipliers
  as determined during solution process.
* DENSE-AUL-QP returns approximate Lagrange multipliers  (which  are  very
  close to &quot;true&quot;  Lagrange  multipliers  except  for  overconstrained  or
  degenerate problems)

Two arrays of multipliers are returned:
* LagBC is array[N] which is loaded with multipliers from box constraints;
  LagBC[i] &gt; 0 means that I-th constraint is at the upper bound, LagBC[I] &lt; 0
  means that I-th constraint is at the lower bound, LagBC[I]=0 means  that
  I-th box constraint is inactive.
* LagLC is array[MSparse+MDense] which is  loaded  with  multipliers  from
  general  linear  constraints  (former  MSparse  elements  corresponds to
  sparse part of the constraint matrix, latter MDense are  for  the  dense
  constraints, as was specified by user).
  LagLC[i] &gt; 0 means that I-th constraint at the upper bound, LagLC[i] &lt; 0
  means that I-th constraint is at the lower bound, LagLC[i]=0 means  that
  I-th linear constraint is inactive.

On failure (or when optimizer does not support Lagrange multipliers) these
arrays are zero-filled.

It is expected that at solution the dual feasibility condition holds:

    C+H*(Xs-X0) + SUM(Ei*LagBC[i],i=0..n-1) + SUM(Ai*LagLC[i],i=0..m-1) ~ 0

where
* C is a linear term
* H is a quadratic term
* Xs is a solution, and X0 is an origin term (zero by default)
* Ei is a vector with 1.0 at position I and 0 in other positions
* Ai is an I-th row of linear constraint matrix

NOTE: methods  from  IPM  family  may  also  return  meaningful   Lagrange
      multipliers  on  completion   with   code   -2   (infeasibility   or
      unboundedness  detected).
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minqpreport {
   ae_int_t inneriterationscount;
   ae_int_t outeriterationscount;
   ae_int_t nmv;
   ae_int_t ncholesky;
   ae_int_t terminationtype;
   real_1d_array lagbc;
   real_1d_array laglc;
};
</pre>
<a name=struct_minqpstate></a><h6 class=pageheader>minqpstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores nonlinear optimizer state.
You should use functions provided by MinQP subpackage to work with this
object
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> minqpstate {
};
</pre>
<a name=sub_minqpaddlc2></a><h6 class=pageheader>minqpaddlc2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function appends two-sided linear constraint  AL &le; A*x &le; AU  to the
list of currently present sparse constraints.

Constraint is passed in compressed format: as list of non-zero entries  of
coefficient vector A. Such approach is more efficient than  dense  storage
for highly sparse constraint vectors.

Inputs:
    State   -   structure previously allocated with minqpcreate() call.
    IdxA    -   array[NNZ], indexes of non-zero elements of A:
                * can be unsorted
                * can include duplicate indexes (corresponding entries  of
                  ValA[] will be summed)
    ValA    -   array[NNZ], values of non-zero elements of A
    NNZ     -   number of non-zero coefficients in A
    AL, AU  -   lower and upper bounds;
                * AL=AU    &rArr; equality constraint A*x
                * AL &lt; AU    &rArr; two-sided constraint AL &le; A*x &le; AU
                * AL=-INF  &rArr; one-sided constraint A*x &le; AU
                * AU=+INF  &rArr; one-sided constraint AL &le; A*x
                * AL=-INF, AU=+INF &rArr; constraint is ignored
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpaddlc2(minqpstate state, integer_1d_array idxa, real_1d_array vala, ae_int_t nnz, <b>double</b> al, <b>double</b> au);
</pre>
<a name=sub_minqpaddlc2dense></a><h6 class=pageheader>minqpaddlc2dense Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function appends two-sided linear constraint  AL &le; A*x &le; AU  to the
list of currently present dense constraints.

Inputs:
    State   -   structure previously allocated with minqpcreate() call.
    A       -   linear constraint coefficient, array[N], right side is NOT
                included.
    AL, AU  -   lower and upper bounds;
                * AL=AU    &rArr; equality constraint Ai*x
                * AL &lt; AU    &rArr; two-sided constraint AL &le; A*x &le; AU
                * AL=-INF  &rArr; one-sided constraint Ai*x &le; AU
                * AU=+INF  &rArr; one-sided constraint AL &le; Ai*x
                * AL=-INF, AU=+INF &rArr; constraint is ignored
ALGLIB: Copyright 19.07.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpaddlc2dense(minqpstate state, real_1d_array a, <b>double</b> al, <b>double</b> au);
</pre>
<a name=sub_minqpcreate></a><h6 class=pageheader>minqpcreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
CONSTRAINED QUADRATIC PROGRAMMING
The subroutine creates QP optimizer. After initial creation,  it  contains
default optimization problem with zero quadratic and linear terms  and  no
constraints.

In order to actually solve something you should:
* set cost vector with minqpsetlinearterm()
* set variable bounds with minqpsetbc() or minqpsetbcall()
* specify constraint matrix with one of the following functions:
  * modern API:
    * minqpsetlc2()       for sparse two-sided constraints AL &le; A*x &le; AU
    * minqpsetlc2dense()  for dense  two-sided constraints AL &le; A*x &le; AU
    * minqpsetlc2mixed()  for mixed  two-sided constraints AL &le; A*x &le; AU
    * minqpaddlc2dense()  to add one dense row to dense constraint submatrix
    * minqpaddlc2()       to add one sparse row to sparse constraint submatrix
  * legacy API:
    * minqpsetlc()        for dense one-sided equality/inequality constraints
    * minqpsetlcsparse()  for sparse one-sided equality/inequality constraints
    * minqpsetlcmixed()   for mixed dense/sparse one-sided equality/inequality constraints
* choose appropriate QP solver and set it  and  its stopping  criteria  by
  means of minqpsetalgo??????() function
* call minqpoptimize() to run the solver and  minqpresults()  to  get  the
  solution vector and additional information.

Following solvers are recommended for convex and semidefinite problems:
* QuickQP for dense problems with box-only constraints (or no constraints
  at all)
* DENSE-IPM-QP for  convex  or  semidefinite  problems  with   medium  (up
  to several thousands) variable count, dense/sparse  quadratic  term  and
  any number  (up  to  many  thousands)  of  dense/sparse  general  linear
  constraints
* SPARSE-IPM-QP for convex  or  semidefinite  problems  with   large (many
  thousands) variable count, sparse quadratic term AND linear constraints.

If your problem happens to be nonconvex,  but  either  (a) is  effectively
convexified under constraints,  or  (b)  has  unique  solution  even  with
nonconvex target, then you can use:
* QuickQP for dense nonconvex problems with box-only constraints
* DENSE-AUL-QP  for   dense   nonconvex   problems  which  are effectively
  convexified under constraints with up to several thousands of  variables
  and any (small or large) number of general linear constraints
* QP-BLEIC for dense/sparse problems with small (up to  several  hundreds)
  number of general linear  constraints  and  arbitrarily  large  variable
  count.

Inputs:
    N       -   problem size

Outputs:
    State   -   optimizer with zero quadratic/linear terms
                and no constraints
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpcreate(ae_int_t n, minqpstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minqp_d_u1 class=nav>minqp_d_u1</a> | <a href=#example_minqp_d_bc1 class=nav>minqp_d_bc1</a> | <a href=#example_minqp_d_lc1 class=nav>minqp_d_lc1</a> | <a href=#example_minqp_d_u2 class=nav>minqp_d_u2</a> | <a href=#example_minqp_d_nonconvex class=nav>minqp_d_nonconvex</a> ]</p>
<a name=sub_minqpoptimize></a><h6 class=pageheader>minqpoptimize Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function solves quadratic programming problem.

Prior to calling this function you should choose solver by means of one of
the following functions:

* minqpsetalgoquickqp()     - for QuickQP solver
* minqpsetalgobleic()       - for BLEIC-QP solver
* minqpsetalgodenseaul()    - for Dense-AUL-QP solver
* minqpsetalgodenseipm()    - for Dense-IPM-QP solver

These functions also allow you to control stopping criteria of the solver.
If you did not set solver,  MinQP  subpackage  will  automatically  select
solver for your problem and will run it with default stopping criteria.

However, it is better to set explicitly solver and its stopping criteria.

Inputs:
    State   -   algorithm state

You should use MinQPResults() function to access results after calls
to this function.
     Special thanks to Elvira Illarionova  for  important  suggestions  on
     the linearly constrained QP algorithm.
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpoptimize(minqpstate state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minqp_d_u1 class=nav>minqp_d_u1</a> | <a href=#example_minqp_d_bc1 class=nav>minqp_d_bc1</a> | <a href=#example_minqp_d_lc1 class=nav>minqp_d_lc1</a> | <a href=#example_minqp_d_u2 class=nav>minqp_d_u2</a> | <a href=#example_minqp_d_nonconvex class=nav>minqp_d_nonconvex</a> ]</p>
<a name=sub_minqpresults></a><h6 class=pageheader>minqpresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
QP solver results

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[0..N-1], solution (on failure - the best point found
                so far).
    Rep     -   optimization report, contains:
                * completion code in Rep.TerminationType (positive  values
                  denote some kind of success, negative - failures)
                * Lagrange multipliers - for QP solvers which support them
                * other statistics
                See comments on minqpreport structure for more information

Following completion codes are returned in Rep.TerminationType:
* -9    failure of the automatic scale evaluation:  one  of  the  diagonal
        elements of the quadratic term is non-positive.  Specify  variable
        scales manually!
* -5    inappropriate solver was used:
        * QuickQP solver for problem with general linear constraints
* -4    the function is unbounded from below even under constraints,
        no meaningful minimum can be found.
* -3    inconsistent constraints (or, maybe, feasible point is too hard to
        find).
* -2    IPM solver has difficulty finding primal/dual feasible point.
        It is likely that the problem is either infeasible or unbounded,
        but it is difficult to determine exact reason for termination.
        X contains best point found so far.
*  &gt;0   success
*  7    stopping conditions are too stringent,
        further improvement is impossible,
        X contains best point found so far.
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpresults(minqpstate state, real_1d_array &amp;x, minqpreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minqp_d_u1 class=nav>minqp_d_u1</a> | <a href=#example_minqp_d_bc1 class=nav>minqp_d_bc1</a> | <a href=#example_minqp_d_lc1 class=nav>minqp_d_lc1</a> | <a href=#example_minqp_d_u2 class=nav>minqp_d_u2</a> | <a href=#example_minqp_d_nonconvex class=nav>minqp_d_nonconvex</a> ]</p>
<a name=sub_minqpresultsbuf></a><h6 class=pageheader>minqpresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
QP results

Buffered implementation of MinQPResults() which uses pre-allocated  buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpresultsbuf(minqpstate state, real_1d_array &amp;x, minqpreport &amp;rep);
</pre>
<a name=sub_minqpsetalgobleic></a><h6 class=pageheader>minqpsetalgobleic Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function tells solver to use BLEIC-based algorithm and sets  stopping
criteria for the algorithm.

This algorithm is intended for large-scale  problems,  possibly nonconvex,
with small number of general linear constraints. Feasible initial point is
essential for good performance.

IMPORTANT: when DENSE-IPM (or DENSE-AUL for  nonconvex  problems)  solvers
           are applicable, their performance is much better than  that  of
           BLEIC-QP.
           We recommend  you to use BLEIC only when other solvers can  not
           be used.

ALGORITHM FEATURES:

* supports dense and sparse QP problems
* supports box and general linear equality/inequality constraints
* can solve all types of problems  (convex,  semidefinite,  nonconvex)  as
  long as they are bounded from below under constraints.
  Say, it is possible to solve &quot;min{-x^2} subject to -1 &le; x &le; +1&quot;.
  Of course, global  minimum  is found only  for  positive  definite   and
  semidefinite  problems.  As  for indefinite ones - only local minimum is
  found.

ALGORITHM OUTLINE:

* BLEIC-QP solver is just a driver function for MinBLEIC solver; it solves
  quadratic  programming   problem   as   general   linearly   constrained
  optimization problem, which is solved by means of BLEIC solver  (part of
  ALGLIB, active set method).

ALGORITHM LIMITATIONS:
* This algorithm is inefficient on  problems with hundreds  and  thousands
  of general inequality constraints and infeasible initial point.  Initial
  feasibility detection stage may take too long on such constraint sets.
  Consider using DENSE-IPM or DENSE-AUL instead.
* unlike QuickQP solver, this algorithm does not perform Newton steps  and
  does not use Level 3 BLAS. Being general-purpose active set  method,  it
  can activate constraints only one-by-one. Thus, its performance is lower
  than that of QuickQP.
* its precision is also a bit  inferior  to  that  of   QuickQP.  BLEIC-QP
  performs only LBFGS steps (no Newton steps), which are good at detecting
  neighborhood of the solution, buy needs many iterations to find solution
  with more than 6 digits of precision.

Inputs:
    State   -   structure which stores algorithm state
    EpsG    - &ge; 0
                The  subroutine  finishes  its  work   if   the  condition
                |v| &lt; EpsG is satisfied, where:
                * |.| means Euclidian norm
                * v - scaled constrained gradient vector, v[i]=g[i]*s[i]
                * g - gradient
                * s - scaling coefficients set by MinQPSetScale()
    EpsF    - &ge; 0
                The  subroutine  finishes its work if exploratory steepest
                descent  step  on  k+1-th iteration  satisfies   following
                condition:  |F(k+1)-F(k)| &le; EpsF*max{|F(k)|,|F(k+1)|,1}
    EpsX    - &ge; 0
                The  subroutine  finishes its work if exploratory steepest
                descent  step  on  k+1-th iteration  satisfies   following
                condition:
                * |.| means Euclidian norm
                * v - scaled step vector, v[i]=dx[i]/s[i]
                * dx - step vector, dx=X(k+1)-X(k)
                * s - scaling coefficients set by MinQPSetScale()
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations is unlimited. NOTE: this  algorithm uses  LBFGS
                iterations,  which  are  relatively  cheap,  but   improve
                function value only a bit. So you will need many iterations
                to converge - from 0.1*N to 10*N, depending  on  problem's
                condition number.

IT IS VERY IMPORTANT TO CALL MinQPSetScale() WHEN YOU USE THIS  ALGORITHM
BECAUSE ITS STOPPING CRITERIA ARE SCALE-DEPENDENT!

Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
to automatic stopping criterion selection (presently it is  small    step
length, but it may change in the future versions of ALGLIB).
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetalgobleic(minqpstate state, <b>double</b> epsg, <b>double</b> epsf, <b>double</b> epsx, ae_int_t maxits);
</pre>
<a name=sub_minqpsetalgodenseaul></a><h6 class=pageheader>minqpsetalgodenseaul Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function tells QP solver to use DENSE-AUL algorithm and sets stopping
criteria for the algorithm.

This  algorithm  is  intended  for  non-convex problems with moderate  (up
to several thousands) variable count and arbitrary number  of  constraints
which are either (a) effectively convexified under constraints or (b) have
unique solution even with nonconvex target.

IMPORTANT: when DENSE-IPM solver is applicable, its performance is usually
           much better than that of DENSE-AUL.
           We recommend  you to use DENSE-AUL only when other solvers  can
           not be used.

ALGORITHM FEATURES:

* supports  box  and  dense/sparse  general   linear   equality/inequality
  constraints
* convergence is theoretically proved for positive-definite  (convex)   QP
  problems. Semidefinite and non-convex problems can be solved as long  as
  they  are   bounded  from  below  under  constraints,  although  without
  theoretical guarantees.

ALGORITHM OUTLINE:

* this  algorithm   is   an   augmented   Lagrangian   method  with  dense
  preconditioner (hence  its  name).
* it performs several outer iterations in order to refine  values  of  the
  Lagrange multipliers. Single outer  iteration  is  a  solution  of  some
  unconstrained optimization problem: first  it  performs  dense  Cholesky
  factorization of the Hessian in order to build preconditioner  (adaptive
  regularization is applied to enforce positive  definiteness),  and  then
  it uses L-BFGS optimizer to solve optimization problem.
* typically you need about 5-10 outer iterations to converge to solution

ALGORITHM LIMITATIONS:

* because dense Cholesky driver is used, this algorithm has O(N^2)  memory
  requirements and O(OuterIterations*N^3) minimum running time.  From  the
  practical  point  of  view,  it  limits  its  applicability  by  several
  thousands of variables.
  From  the  other  side,  variables  count  is  the most limiting factor,
  and dependence on constraint count is  much  more  lower. Assuming  that
  constraint matrix is sparse, it may handle tens of thousands  of general
  linear constraints.

Inputs:
    State   -   structure which stores algorithm state
    EpsX    - &ge; 0, stopping criteria for inner optimizer.
                Inner  iterations  are  stopped  when  step  length  (with
                variable scaling being applied) is less than EpsX.
                See  minqpsetscale()  for  more  information  on  variable
                scaling.
    Rho     -   penalty coefficient, Rho &gt; 0:
                * large enough  that  algorithm  converges  with   desired
                  precision.
                * not TOO large to prevent ill-conditioning
                * recommended values are 100, 1000 or 10000
    ItsCnt  -   number of outer iterations:
                * recommended values: 10-15 (although  in  most  cases  it
                  converges within 5 iterations, you may need a  few  more
                  to be sure).
                * ItsCnt=0 means that small number of outer iterations  is
                  automatically chosen (10 iterations in current version).
                * ItsCnt=1 means that AUL algorithm performs just as usual
                  penalty method.
                * ItsCnt &gt; 1 means that  AUL  algorithm  performs  specified
                  number of outer iterations

IT IS VERY IMPORTANT TO CALL minqpsetscale() WHEN YOU USE THIS  ALGORITHM
BECAUSE ITS CONVERGENCE PROPERTIES AND STOPPING CRITERIA ARE SCALE-DEPENDENT!

NOTE: Passing  EpsX=0  will  lead  to  automatic  step  length  selection
      (specific step length chosen may change in the future  versions  of
      ALGLIB, so it is better to specify step length explicitly).
ALGLIB: Copyright 20.08.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetalgodenseaul(minqpstate state, <b>double</b> epsx, <b>double</b> rho, ae_int_t itscnt);
</pre>
<a name=sub_minqpsetalgodenseipm></a><h6 class=pageheader>minqpsetalgodenseipm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function tells QP solver to  use  DENSE-IPM  QP  algorithm  and  sets
stopping criteria for the algorithm.

This  algorithm  is  intended  for convex and semidefinite  problems  with
moderate (up to several thousands) variable count and arbitrary number  of
constraints.

IMPORTANT: this algorithm won't work for nonconvex problems, use DENSE-AUL
           or BLEIC-QP instead. If you try to  run  DENSE-IPM  on  problem
           with  indefinite  matrix  (matrix having  at least one negative
           eigenvalue) then depending on circumstances it may  either  (a)
           stall at some  arbitrary  point,  or  (b)  throw  exception  on
           failure of Cholesky decomposition.

ALGORITHM FEATURES:

* supports  box  and  dense/sparse  general   linear   equality/inequality
  constraints

ALGORITHM OUTLINE:

* this  algorithm  is  our implementation  of  interior  point  method  as
  formulated by  R.J.Vanderbei, with minor modifications to the  algorithm
  (damped Newton directions are extensively used)
* like all interior point methods, this algorithm  tends  to  converge  in
  roughly same number of iterations (between 15 and 50) independently from
  the problem dimensionality

ALGORITHM LIMITATIONS:

* because dense Cholesky driver is used, for  N-dimensional  problem  with
  M dense constaints this algorithm has O(N^2+N*M) memory requirements and
  O(N^3+N*M^2) running time.
  Having sparse constraints with Z nonzeros per row  relaxes  storage  and
  running time down to O(N^2+M*Z) and O(N^3+N*Z^2)
  From the practical  point  of  view,  it  limits  its  applicability  by
  several thousands of variables.
  From  the  other  side,  variables  count  is  the most limiting factor,
  and dependence on constraint count is  much  more  lower. Assuming  that
  constraint matrix is sparse, it may handle tens of thousands  of general
  linear constraints.

Inputs:
    State   -   structure which stores algorithm state
    Eps     - &ge; 0, stopping criteria. The algorithm stops  when   primal
                and dual infeasiblities as well as complementarity gap are
                less than Eps.

IT IS VERY IMPORTANT TO CALL minqpsetscale() WHEN YOU USE THIS  ALGORITHM
BECAUSE ITS CONVERGENCE PROPERTIES AND STOPPING CRITERIA ARE SCALE-DEPENDENT!

NOTE: Passing EpsX=0 will lead to automatic selection of small epsilon.
ALGLIB: Copyright 01.11.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetalgodenseipm(minqpstate state, <b>double</b> eps);
</pre>
<a name=sub_minqpsetalgoquickqp></a><h6 class=pageheader>minqpsetalgoquickqp Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function tells solver to use QuickQP  algorithm:  special  extra-fast
algorithm for problems with box-only constrants. It may  solve  non-convex
problems as long as they are bounded from below under constraints.

ALGORITHM FEATURES:
* several times faster than DENSE-IPM when running on box-only problem
* utilizes accelerated methods for activation of constraints.
* supports dense and sparse QP problems
* supports ONLY box constraints; general linear constraints are NOT
  supported by this solver
* can solve all types of problems  (convex,  semidefinite,  nonconvex)  as
  long as they are bounded from below under constraints.
  Say, it is possible to solve &quot;min{-x^2} subject to -1 &le; x &le; +1&quot;.
  In convex/semidefinite case global minimum  is  returned,  in  nonconvex
  case - algorithm returns one of the local minimums.

ALGORITHM OUTLINE:

* algorithm  performs  two kinds of iterations: constrained CG  iterations
  and constrained Newton iterations
* initially it performs small number of constrained CG  iterations,  which
  can efficiently activate/deactivate multiple constraints
* after CG phase algorithm tries to calculate Cholesky  decomposition  and
  to perform several constrained Newton steps. If  Cholesky  decomposition
  failed (matrix is indefinite even under constraints),  we  perform  more
  CG iterations until we converge to such set of constraints  that  system
  matrix becomes  positive  definite.  Constrained  Newton  steps  greatly
  increase convergence speed and precision.
* algorithm interleaves CG and Newton iterations which  allows  to  handle
  indefinite matrices (CG phase) and quickly converge after final  set  of
  constraints is found (Newton phase). Combination of CG and Newton phases
  is called &quot;outer iteration&quot;.
* it is possible to turn off Newton  phase  (beneficial  for  semidefinite
  problems - Cholesky decomposition will fail too often)

ALGORITHM LIMITATIONS:

* algorithm does not support general  linear  constraints;  only  box ones
  are supported
* Cholesky decomposition for sparse problems  is  performed  with  Skyline
  Cholesky solver, which is intended for low-profile matrices. No profile-
  reducing reordering of variables is performed in this version of ALGLIB.
* problems with near-zero negative eigenvalues (or exacty zero  ones)  may
  experience about 2-3x performance penalty. The reason is  that  Cholesky
  decomposition can not be performed until we identify directions of  zero
  and negative curvature and activate corresponding boundary constraints -
  but we need a lot of trial and errors because these directions  are hard
  to notice in the matrix spectrum.
  In this case you may turn off Newton phase of algorithm.
  Large negative eigenvalues  are  not  an  issue,  so  highly  non-convex
  problems can be solved very efficiently.

Inputs:
    State   -   structure which stores algorithm state
    EpsG    - &ge; 0
                The  subroutine  finishes  its  work   if   the  condition
                |v| &lt; EpsG is satisfied, where:
                * |.| means Euclidian norm
                * v - scaled constrained gradient vector, v[i]=g[i]*s[i]
                * g - gradient
                * s - scaling coefficients set by MinQPSetScale()
    EpsF    - &ge; 0
                The  subroutine  finishes its work if exploratory steepest
                descent  step  on  k+1-th iteration  satisfies   following
                condition:  |F(k+1)-F(k)| &le; EpsF*max{|F(k)|,|F(k+1)|,1}
    EpsX    - &ge; 0
                The  subroutine  finishes its work if exploratory steepest
                descent  step  on  k+1-th iteration  satisfies   following
                condition:
                * |.| means Euclidian norm
                * v - scaled step vector, v[i]=dx[i]/s[i]
                * dx - step vector, dx=X(k+1)-X(k)
                * s - scaling coefficients set by MinQPSetScale()
    MaxOuterIts-maximum number of OUTER iterations.  One  outer  iteration
                includes some amount of CG iterations (from 5 to  ~N)  and
                one or several (usually small amount) Newton steps.  Thus,
                one outer iteration has high cost, but can greatly  reduce
                funcation value.
                Use 0 if you do not want to limit number of outer iterations.
    UseNewton-  use Newton phase or not:
                * Newton phase improves performance of  positive  definite
                  dense problems (about 2 times improvement can be observed)
                * can result in some performance penalty  on  semidefinite
                  or slightly negative definite  problems  -  each  Newton
                  phase will bring no improvement (Cholesky failure),  but
                  still will require computational time.
                * if you doubt, you can turn off this  phase  -  optimizer
                  will retain its most of its high speed.

IT IS VERY IMPORTANT TO CALL MinQPSetScale() WHEN YOU USE THIS  ALGORITHM
BECAUSE ITS STOPPING CRITERIA ARE SCALE-DEPENDENT!

Passing EpsG=0, EpsF=0 and EpsX=0 and MaxIts=0 (simultaneously) will lead
to automatic stopping criterion selection (presently it is  small    step
length, but it may change in the future versions of ALGLIB).
ALGLIB: Copyright 22.05.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetalgoquickqp(minqpstate state, <b>double</b> epsg, <b>double</b> epsf, <b>double</b> epsx, ae_int_t maxouterits, <b>bool</b> usenewton);
</pre>
<a name=sub_minqpsetalgosparseipm></a><h6 class=pageheader>minqpsetalgosparseipm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function tells QP solver to  use  SPARSE-IPM  QP algorithm  and  sets
stopping criteria for the algorithm.

This  algorithm  is  intended  for convex and semidefinite  problems  with
large  variable  and  constraint  count  and  sparse  quadratic  term  and
constraints. It is possible to have  some  limited  set  of  dense  linear
constraints - they will be handled separately by dense BLAS - but the more
dense constraints you have, the more time solver needs.

IMPORTANT: internally this solver performs large  and  sparse  (N+M)x(N+M)
           triangular factorization. So it expects both quadratic term and
           constraints to be highly sparse. However, its  running  time is
           influenced by BOTH fill factor and sparsity pattern.

           Generally we expect that no more than few nonzero  elements per
           row are present. However different sparsity patterns may result
           in completely different running  times  even  given  same  fill
           factor.

           In many cases this algorithm outperforms DENSE-IPM by order  of
           magnitude. However, in some cases you may  get  better  results
           with DENSE-IPM even when solving sparse task.

IMPORTANT: this algorithm won't work for nonconvex problems, use DENSE-AUL
           or BLEIC-QP instead. If you try to  run  DENSE-IPM  on  problem
           with  indefinite  matrix  (matrix having  at least one negative
           eigenvalue) then depending on circumstances it may  either  (a)
           stall at some  arbitrary  point,  or  (b)  throw  exception  on
           failure of Cholesky decomposition.

ALGORITHM FEATURES:

* supports  box  and  dense/sparse  general   linear   equality/inequality
  constraints
* specializes on large-scale sparse problems

ALGORITHM OUTLINE:

* this  algorithm  is  our implementation  of  interior  point  method  as
  formulated by  R.J.Vanderbei, with minor modifications to the  algorithm
  (damped Newton directions are extensively used)
* like all interior point methods, this algorithm  tends  to  converge  in
  roughly same number of iterations (between 15 and 50) independently from
  the problem dimensionality

ALGORITHM LIMITATIONS:

* this algorithm may handle moderate number  of dense constraints, usually
  no more than a thousand of dense ones without losing its efficiency.

Inputs:
    State   -   structure which stores algorithm state
    Eps     - &ge; 0, stopping criteria. The algorithm stops  when   primal
                and dual infeasiblities as well as complementarity gap are
                less than Eps.

IT IS VERY IMPORTANT TO CALL minqpsetscale() WHEN YOU USE THIS  ALGORITHM
BECAUSE ITS CONVERGENCE PROPERTIES AND STOPPING CRITERIA ARE SCALE-DEPENDENT!

NOTE: Passing EpsX=0 will lead to automatic selection of small epsilon.
ALGLIB: Copyright 01.11.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetalgosparseipm(minqpstate state, <b>double</b> eps);
</pre>
<a name=sub_minqpsetbc></a><h6 class=pageheader>minqpsetbc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets box constraints for QP solver

Box constraints are inactive by default (after  initial  creation).  After
being  set,  they are  preserved until explicitly overwritten with another
minqpsetbc()  or  minqpsetbcall()  call,  or  partially  overwritten  with
minqpsetbci() call.

Following types of constraints are supported:

    DESCRIPTION         CONSTRAINT              HOW TO SPECIFY
    fixed variable      x[i]=Bnd[i]             BndL[i]=BndU[i]
    lower bound         BndL[i] &le; x[i]           BndU[i]=+INF
    upper bound         x[i] &le; BndU[i]           BndL[i]=-INF
    range               BndL[i] &le; x[i] &le; BndU[i]  ...
    free variable       -                       BndL[I]=-INF, BndU[I]+INF

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very small number or -INF (latter is recommended because
                it will allow solver to use better algorithm).
    BndU    -   upper bounds, array[N].
                If some (all) variables are unbounded, you may specify
                very large number or +INF (latter is recommended because
                it will allow solver to use better algorithm).

NOTE: infinite values can be specified by means of Double.PositiveInfinity
      and  Double.NegativeInfinity  (in  C#)  and  +INFINITY   and
      -INFINITY (in C++).

NOTE: you may replace infinities by very small/very large values,  but  it
      is not recommended because large numbers may introduce large numerical
      errors in the algorithm.

NOTE: if constraints for all variables are same you may use minqpsetbcall()
      which allows to specify constraints without using arrays.

NOTE: BndL &gt; BndU will result in QP problem being recognized as infeasible.
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetbc(minqpstate state, real_1d_array bndl, real_1d_array bndu);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minqp_d_bc1 class=nav>minqp_d_bc1</a> ]</p>
<a name=sub_minqpsetbcall></a><h6 class=pageheader>minqpsetbcall Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets box constraints for QP solver (all variables  at  once,
same constraints for all variables)

Box constraints are inactive by default (after  initial  creation).  After
being  set,  they are  preserved until explicitly overwritten with another
minqpsetbc()  or  minqpsetbcall()  call,  or  partially  overwritten  with
minqpsetbci() call.

Following types of constraints are supported:

    DESCRIPTION         CONSTRAINT              HOW TO SPECIFY
    fixed variable      x[i]=Bnd                BndL=BndU
    lower bound         BndL &le; x[i]              BndU=+INF
    upper bound         x[i] &le; BndU              BndL=-INF
    range               BndL &le; x[i] &le; BndU        ...
    free variable       -                       BndL=-INF, BndU+INF

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bound, same for all variables
    BndU    -   upper bound, same for all variables

NOTE: infinite values can be specified by means of Double.PositiveInfinity
      and  Double.NegativeInfinity  (in  C#)  and  +INFINITY   and
      -INFINITY (in C++).

NOTE: you may replace infinities by very small/very large values,  but  it
      is not recommended because large numbers may introduce large numerical
      errors in the algorithm.

NOTE: BndL &gt; BndU will result in QP problem being recognized as infeasible.
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetbcall(minqpstate state, <b>double</b> bndl, <b>double</b> bndu);
</pre>
<a name=sub_minqpsetbci></a><h6 class=pageheader>minqpsetbci Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets box constraints for I-th variable (other variables are
not modified).

Following types of constraints are supported:

    DESCRIPTION         CONSTRAINT              HOW TO SPECIFY
    fixed variable      x[i]=Bnd                BndL=BndU
    lower bound         BndL &le; x[i]              BndU=+INF
    upper bound         x[i] &le; BndU              BndL=-INF
    range               BndL &le; x[i] &le; BndU        ...
    free variable       -                       BndL=-INF, BndU+INF

Inputs:
    State   -   structure stores algorithm state
    BndL    -   lower bound
    BndU    -   upper bound

NOTE: infinite values can be specified by means of Double.PositiveInfinity
      and  Double.NegativeInfinity  (in  C#)  and  +INFINITY   and
      -INFINITY (in C++).

NOTE: you may replace infinities by very small/very large values,  but  it
      is not recommended because large numbers may introduce large numerical
      errors in the algorithm.

NOTE: BndL &gt; BndU will result in QP problem being recognized as infeasible.
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetbci(minqpstate state, ae_int_t i, <b>double</b> bndl, <b>double</b> bndu);
</pre>
<a name=sub_minqpsetlc></a><h6 class=pageheader>minqpsetlc Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets dense linear constraints for QP optimizer.

This  function  overrides  results  of  previous  calls  to  minqpsetlc(),
minqpsetlcsparse() and minqpsetlcmixed().  After  call  to  this  function
all non-box constraints are dropped, and you have only  those  constraints
which were specified in the present call.

If you want  to  specify  mixed  (with  dense  and  sparse  terms)  linear
constraints, you should call minqpsetlcmixed().

Inputs:
    State   -   structure previously allocated with MinQPCreate call.
    C       -   linear constraints, array[K,N+1].
                Each row of C represents one constraint, either equality
                or inequality (see below):
                * first N elements correspond to coefficients,
                * last element corresponds to the right part.
                All elements of C (including right part) must be finite.
    CT      -   type of constraints, array[K]:
                * if CT[i] &gt; 0, then I-th constraint is C[i,*]*x &ge; C[i,n+1]
                * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
                * if CT[i] &lt; 0, then I-th constraint is C[i,*]*x &le; C[i,n+1]
    K       -   number of equality/inequality constraints, K &ge; 0:
                * if given, only leading K elements of C/CT are used
                * if not given, automatically determined from sizes of C/CT

NOTE 1: linear (non-bound) constraints are satisfied only approximately  -
        there always exists some violation due  to  numerical  errors  and
        algorithmic limitations (BLEIC-QP solver is most  precise,  AUL-QP
        solver is less precise).
ALGLIB: Copyright 19.06.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetlc(minqpstate state, real_2d_array c, integer_1d_array ct, ae_int_t k);
<b>void</b> minqpsetlc(minqpstate state, real_2d_array c, integer_1d_array ct);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minqp_d_lc1 class=nav>minqp_d_lc1</a> ]</p>
<a name=sub_minqpsetlc2></a><h6 class=pageheader>minqpsetlc2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  two-sided linear  constraints  AL &le; A*x &le; AU  with
sparse constraining matrix A. Recommended for large-scale problems.

This  function  overwrites  linear  (non-box)  constraints set by previous
calls (if such calls were made).

Inputs:
    State   -   structure previously allocated with minqpcreate() call.
    A       -   sparse matrix with size [K,N] (exactly!).
                Each row of A represents one general linear constraint.
                A can be stored in any sparse storage format.
    AL, AU  -   lower and upper bounds, array[K];
                * AL[i]=AU[i] &rArr; equality constraint Ai*x
                * AL[i] &lt; AU[i] &rArr; two-sided constraint AL[i] &le; Ai*x &le; AU[i]
                * AL[i]=-INF  &rArr; one-sided constraint Ai*x &le; AU[i]
                * AU[i]=+INF  &rArr; one-sided constraint AL[i] &le; Ai*x
                * AL[i]=-INF, AU[i]=+INF &rArr; constraint is ignored
    K       -   number  of equality/inequality constraints, K &ge; 0.  If  K=0
                is specified, A, AL, AU are ignored.
ALGLIB: Copyright 01.11.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetlc2(minqpstate state, sparsematrix a, real_1d_array al, real_1d_array au, ae_int_t k);
</pre>
<a name=sub_minqpsetlc2dense></a><h6 class=pageheader>minqpsetlc2dense Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets two-sided linear constraints AL &le; A*x &le; AU with dense
constraint matrix A.

NOTE: knowing  that  constraint  matrix  is  dense  helps  some QP solvers
      (especially modern IPM method) to utilize efficient  dense  Level  3
      BLAS for dense parts of the problem. If your problem has both  dense
      and sparse constraints, you  can  use  minqpsetlc2mixed()  function,
      which will result in dense algebra being applied to dense terms, and
      sparse sparse linear algebra applied to sparse terms.

Inputs:
    State   -   structure previously allocated with minqpcreate() call.
    A       -   linear constraints, array[K,N]. Each row of  A  represents
                one  constraint. One-sided  inequality   constraints, two-
                sided inequality  constraints,  equality  constraints  are
                supported (see below)
    AL, AU  -   lower and upper bounds, array[K];
                * AL[i]=AU[i] &rArr; equality constraint Ai*x
                * AL[i] &lt; AU[i] &rArr; two-sided constraint AL[i] &le; Ai*x &le; AU[i]
                * AL[i]=-INF  &rArr; one-sided constraint Ai*x &le; AU[i]
                * AU[i]=+INF  &rArr; one-sided constraint AL[i] &le; Ai*x
                * AL[i]=-INF, AU[i]=+INF &rArr; constraint is ignored
    K       -   number of equality/inequality constraints,  K &ge; 0;  if  not
                given, inferred from sizes of A, AL, AU.
ALGLIB: Copyright 01.11.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetlc2dense(minqpstate state, real_2d_array a, real_1d_array al, real_1d_array au, ae_int_t k);
<b>void</b> minqpsetlc2dense(minqpstate state, real_2d_array a, real_1d_array al, real_1d_array au);
</pre>
<a name=sub_minqpsetlc2mixed></a><h6 class=pageheader>minqpsetlc2mixed Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  two-sided linear  constraints  AL &le; A*x &le; AU  with
mixed constraining matrix A including sparse part (first SparseK rows) and
dense part (last DenseK rows). Recommended for large-scale problems.

This  function  overwrites  linear  (non-box)  constraints set by previous
calls (if such calls were made).

This function may be useful if constraint matrix includes large number  of
both types of rows - dense and sparse. If you have just a few sparse rows,
you  may  represent  them  in  dense  format  without losing performance.
Similarly, if you have just a few dense rows, you may store them in sparse
format with almost same performance.

Inputs:
    State   -   structure previously allocated with minqpcreate() call.
    SparseA -   sparse matrix with size [K,N] (exactly!).
                Each row of A represents one general linear constraint.
                A can be stored in any sparse storage format.
    SparseK -   number of sparse constraints, SparseK &ge; 0
    DenseA  -   linear constraints, array[K,N], set of dense constraints.
                Each row of A represents one general linear constraint.
    DenseK  -   number of dense constraints, DenseK &ge; 0
    AL, AU  -   lower and upper bounds, array[SparseK+DenseK], with former
                SparseK elements corresponding to sparse constraints,  and
                latter DenseK elements corresponding to dense constraints;
                * AL[i]=AU[i] &rArr; equality constraint Ai*x
                * AL[i] &lt; AU[i] &rArr; two-sided constraint AL[i] &le; Ai*x &le; AU[i]
                * AL[i]=-INF  &rArr; one-sided constraint Ai*x &le; AU[i]
                * AU[i]=+INF  &rArr; one-sided constraint AL[i] &le; Ai*x
                * AL[i]=-INF, AU[i]=+INF &rArr; constraint is ignored
    K       -   number  of equality/inequality constraints, K &ge; 0.  If  K=0
                is specified, A, AL, AU are ignored.
ALGLIB: Copyright 01.11.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetlc2mixed(minqpstate state, sparsematrix sparsea, ae_int_t ksparse, real_2d_array densea, ae_int_t kdense, real_1d_array al, real_1d_array au);
</pre>
<a name=sub_minqpsetlcmixed></a><h6 class=pageheader>minqpsetlcmixed Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets mixed linear constraints, which include a set of  dense
rows, and a set of sparse rows.

This  function  overrides  results  of  previous  calls  to  minqpsetlc(),
minqpsetlcsparse() and minqpsetlcmixed().

This function may be useful if constraint matrix includes large number  of
both types of rows - dense and sparse. If you have just a few sparse rows,
you  may  represent  them  in  dense  format  without losing performance.
Similarly, if you have just a few dense rows, you may store them in sparse
format with almost same performance.

Inputs:
    State   -   structure previously allocated with MinQPCreate call.
    SparseC -   linear constraints, sparse  matrix with dimensions EXACTLY
                EQUAL TO [SparseK,N+1].  Each  row  of  C  represents  one
                constraint, either equality or inequality (see below):
                * first N elements correspond to coefficients,
                * last element corresponds to the right part.
                All elements of C (including right part) must be finite.
    SparseCT-   type of sparse constraints, array[K]:
                * if SparseCT[i] &gt; 0, then I-th constraint is SparseC[i,*]*x &ge; SparseC[i,n+1]
                * if SparseCT[i]=0, then I-th constraint is SparseC[i,*]*x  = SparseC[i,n+1]
                * if SparseCT[i] &lt; 0, then I-th constraint is SparseC[i,*]*x &le; SparseC[i,n+1]
    SparseK -   number of sparse equality/inequality constraints, K &ge; 0
    DenseC  -   dense linear constraints, array[K,N+1].
                Each row of DenseC represents one constraint, either equality
                or inequality (see below):
                * first N elements correspond to coefficients,
                * last element corresponds to the right part.
                All elements of DenseC (including right part) must be finite.
    DenseCT -   type of constraints, array[K]:
                * if DenseCT[i] &gt; 0, then I-th constraint is DenseC[i,*]*x &ge; DenseC[i,n+1]
                * if DenseCT[i]=0, then I-th constraint is DenseC[i,*]*x  = DenseC[i,n+1]
                * if DenseCT[i] &lt; 0, then I-th constraint is DenseC[i,*]*x &le; DenseC[i,n+1]
    DenseK  -   number of equality/inequality constraints, DenseK &ge; 0

NOTE 1: linear (non-box) constraints  are  satisfied only approximately  -
        there always exists some violation due  to  numerical  errors  and
        algorithmic limitations (BLEIC-QP solver is most  precise,  AUL-QP
        solver is less precise).

NOTE 2: due to backward compatibility reasons SparseC can be  larger  than
        [SparseK,N+1]. In this case only leading  [SparseK,N+1]  submatrix
        will be  used.  However,  the  rest  of  ALGLIB  has  more  strict
        requirements on the input size, so we recommend you to pass sparse
        term whose size exactly matches algorithm expectations.
ALGLIB: Copyright 22.08.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetlcmixed(minqpstate state, sparsematrix sparsec, integer_1d_array sparsect, ae_int_t sparsek, real_2d_array densec, integer_1d_array densect, ae_int_t densek);
</pre>
<a name=sub_minqpsetlcmixedlegacy></a><h6 class=pageheader>minqpsetlcmixedlegacy Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function provides legacy API for specification of mixed  dense/sparse
linear constraints.

New conventions used by ALGLIB since release  3.16.0  state  that  set  of
sparse constraints comes first,  followed  by  set  of  dense  ones.  This
convention is essential when you talk about things like order of  Lagrange
multipliers.

However, legacy API accepted mixed  constraints  in  reverse  order.  This
function is here to simplify situation with code relying on legacy API. It
simply accepts constraints in one order (old) and passes them to new  API,
now in correct order.
ALGLIB: Copyright 01.11.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetlcmixedlegacy(minqpstate state, real_2d_array densec, integer_1d_array densect, ae_int_t densek, sparsematrix sparsec, integer_1d_array sparsect, ae_int_t sparsek);
</pre>
<a name=sub_minqpsetlcsparse></a><h6 class=pageheader>minqpsetlcsparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets sparse linear constraints for QP optimizer.

This  function  overrides  results  of  previous  calls  to  minqpsetlc(),
minqpsetlcsparse() and minqpsetlcmixed().  After  call  to  this  function
all non-box constraints are dropped, and you have only  those  constraints
which were specified in the present call.

If you want  to  specify  mixed  (with  dense  and  sparse  terms)  linear
constraints, you should call minqpsetlcmixed().

Inputs:
    State   -   structure previously allocated with MinQPCreate call.
    C       -   linear  constraints,  sparse  matrix  with  dimensions  at
                least [K,N+1]. If matrix has  larger  size,  only  leading
                Kx(N+1) rectangle is used.
                Each row of C represents one constraint, either equality
                or inequality (see below):
                * first N elements correspond to coefficients,
                * last element corresponds to the right part.
                All elements of C (including right part) must be finite.
    CT      -   type of constraints, array[K]:
                * if CT[i] &gt; 0, then I-th constraint is C[i,*]*x &ge; C[i,n+1]
                * if CT[i]=0, then I-th constraint is C[i,*]*x  = C[i,n+1]
                * if CT[i] &lt; 0, then I-th constraint is C[i,*]*x &le; C[i,n+1]
    K       -   number of equality/inequality constraints, K &ge; 0

NOTE 1: linear (non-bound) constraints are satisfied only approximately  -
        there always exists some violation due  to  numerical  errors  and
        algorithmic limitations (BLEIC-QP solver is most  precise,  AUL-QP
        solver is less precise).
ALGLIB: Copyright 22.08.2016 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetlcsparse(minqpstate state, sparsematrix c, integer_1d_array ct, ae_int_t k);
</pre>
<a name=sub_minqpsetlinearterm></a><h6 class=pageheader>minqpsetlinearterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets linear term for QP solver.

By default, linear term is zero.

Inputs:
    State   -   structure which stores algorithm state
    B       -   linear term, array[N].
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetlinearterm(minqpstate state, real_1d_array b);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minqp_d_u1 class=nav>minqp_d_u1</a> | <a href=#example_minqp_d_bc1 class=nav>minqp_d_bc1</a> | <a href=#example_minqp_d_lc1 class=nav>minqp_d_lc1</a> | <a href=#example_minqp_d_u2 class=nav>minqp_d_u2</a> | <a href=#example_minqp_d_nonconvex class=nav>minqp_d_nonconvex</a> ]</p>
<a name=sub_minqpsetorigin></a><h6 class=pageheader>minqpsetorigin Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function sets origin for QP solver. By default, following QP program
is solved:

    min(0.5*x'*A*x+b'*x)

This function allows to solve different problem:

    min(0.5*(x-x_origin)'*A*(x-x_origin)+b'*(x-x_origin))

Specification of non-zero origin affects function being minimized, but not
constraints. Box and  linear  constraints  are  still  calculated  without
origin.

Inputs:
    State   -   structure which stores algorithm state
    XOrigin -   origin, array[N].
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetorigin(minqpstate state, real_1d_array xorigin);
</pre>
<a name=sub_minqpsetquadraticterm></a><h6 class=pageheader>minqpsetquadraticterm Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  dense  quadratic  term  for  QP solver. By  default,
quadratic term is zero.

IMPORTANT:

This solver minimizes following  function:
    f(x) = 0.5*x'*A*x + b'*x.
Note that quadratic term has 0.5 before it. So if  you  want  to  minimize
    f(x) = x^2 + x
you should rewrite your problem as follows:
    f(x) = 0.5*(2*x^2) + x
and your matrix A will be equal to [[2.0]], not to [[1.0]]

Inputs:
    State   -   structure which stores algorithm state
    A       -   matrix, array[N,N]
    IsUpper -   (optional) storage type:
                * if True, symmetric matrix  A  is  given  by  its  upper
                  triangle, and the lower triangle isn't used
                * if False, symmetric matrix  A  is  given  by  its lower
                  triangle, and the upper triangle isn't used
                * if not given, both lower and upper  triangles  must  be
                  filled.
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetquadraticterm(minqpstate state, real_2d_array a, <b>bool</b> isupper);
<b>void</b> minqpsetquadraticterm(minqpstate state, real_2d_array a);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minqp_d_u1 class=nav>minqp_d_u1</a> | <a href=#example_minqp_d_bc1 class=nav>minqp_d_bc1</a> | <a href=#example_minqp_d_lc1 class=nav>minqp_d_lc1</a> | <a href=#example_minqp_d_nonconvex class=nav>minqp_d_nonconvex</a> ]</p>
<a name=sub_minqpsetquadratictermsparse></a><h6 class=pageheader>minqpsetquadratictermsparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  sets  sparse  quadratic  term  for  QP solver. By default,
quadratic  term  is  zero.  This  function  overrides  previous  calls  to
minqpsetquadraticterm() or minqpsetquadratictermsparse().

NOTE: dense solvers like DENSE-AUL-QP or DENSE-IPM-QP  will  convert  this
      matrix to dense storage anyway.

IMPORTANT:

This solver minimizes following  function:
    f(x) = 0.5*x'*A*x + b'*x.
Note that quadratic term has 0.5 before it. So if  you  want  to  minimize
    f(x) = x^2 + x
you should rewrite your problem as follows:
    f(x) = 0.5*(2*x^2) + x
and your matrix A will be equal to [[2.0]], not to [[1.0]]

Inputs:
    State   -   structure which stores algorithm state
    A       -   matrix, array[N,N]
    IsUpper -   (optional) storage type:
                * if True, symmetric matrix  A  is  given  by  its  upper
                  triangle, and the lower triangle isn't used
                * if False, symmetric matrix  A  is  given  by  its lower
                  triangle, and the upper triangle isn't used
                * if not given, both lower and upper  triangles  must  be
                  filled.
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetquadratictermsparse(minqpstate state, sparsematrix a, <b>bool</b> isupper);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minqp_d_u2 class=nav>minqp_d_u2</a> ]</p>
<a name=sub_minqpsetscale></a><h6 class=pageheader>minqpsetscale Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets scaling coefficients.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison  with  tolerances)  and  as
preconditioner.

Scale of the I-th variable is a translation invariant measure of:
a) &quot;how large&quot; the variable is
b) how large the step should be to make significant changes in the
   function

If you do not know how to choose scales of your variables, you can:
* read www.alglib.net/optimization/scaling.php article
* use minqpsetscaleautodiag(), which calculates scale  using  diagonal  of
  the  quadratic  term:  S  is  set to 1/sqrt(diag(A)), which works well
  sometimes.

Inputs:
    State   -   structure stores algorithm state
    S       -   array[N], non-zero scaling coefficients
                S[i] may be negative, sign doesn't matter.
ALGLIB: Copyright 14.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetscale(minqpstate state, real_1d_array s);
</pre>
<a name=sub_minqpsetscaleautodiag></a><h6 class=pageheader>minqpsetscaleautodiag Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets automatic evaluation of variable scaling.

IMPORTANT: this function works only for  matrices  with positive  diagonal
           elements! Zero or negative elements will  result  in  -9  error
           code  being  returned.  Specify  scale  vector  manually   with
           minqpsetscale() in such cases.

ALGLIB optimizers use scaling matrices to test stopping  conditions  (step
size and gradient are scaled before comparison  with  tolerances)  and  as
preconditioner.

The  best  way  to  set  scaling  is  to manually specify variable scales.
However, sometimes you just need quick-and-dirty solution  -  either  when
you perform fast prototyping, or when you know your problem well  and  you
are 100% sure that this quick solution is robust enough in your case.

One such solution is to evaluate scale of I-th variable as 1/sqrt(A[i,i]),
where A[i,i] is an I-th diagonal element of the quadratic term.

Such approach works well sometimes, but you have to be careful here.

Inputs:
    State   -   structure stores algorithm state
ALGLIB: Copyright 26.12.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetscaleautodiag(minqpstate state);
</pre>
<a name=sub_minqpsetstartingpoint></a><h6 class=pageheader>minqpsetstartingpoint Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets starting point for QP solver. It is useful to have good
initial approximation to the solution, because it will increase  speed  of
convergence and identification of active constraints.

NOTE: interior point solvers ignore initial point provided by user.

Inputs:
    State   -   structure which stores algorithm state
    X       -   starting point, array[N].
ALGLIB: Copyright 11.01.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> minqpsetstartingpoint(minqpstate state, real_1d_array x);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_minqp_d_u1 class=nav>minqp_d_u1</a> | <a href=#example_minqp_d_bc1 class=nav>minqp_d_bc1</a> | <a href=#example_minqp_d_lc1 class=nav>minqp_d_lc1</a> | <a href=#example_minqp_d_u2 class=nav>minqp_d_u2</a> | <a href=#example_minqp_d_nonconvex class=nav>minqp_d_nonconvex</a> ]</p>
<a name=example_minqp_d_bc1></a><h6 class=pageheader>minqp_d_bc1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of F(x0,x1) = x0^2 + x1^2 -6*x0 - 4*x1</font>
<font color=navy>// subject to bound constraints 0 &le; x0 &le; 2.5, 0 &le; x1 &le; 2.5</font>
<font color=navy>//</font>
<font color=navy>// Exact solution is [x0,x1] = [2.5,2]</font>
<font color=navy>//</font>
<font color=navy>// We provide algorithm with starting point. With such small problem good starting</font>
<font color=navy>// point is not really necessary, but with high-dimensional problem it can save us</font>
<font color=navy>// a lot of time.</font>
<font color=navy>//</font>
<font color=navy>// Several QP solvers are tried: QuickQP, BLEIC, DENSE-AUL.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: this solver minimizes  following  function:</font>
<font color=navy>//     f(x) = 0.5*x'*A*x + b'*x.</font>
<font color=navy>// Note that quadratic term has 0.5 before it. So <b>if</b> you want to minimize</font>
<font color=navy>// quadratic function, you should rewrite it in such way that quadratic term</font>
<font color=navy>// is multiplied by 0.5 too.</font>
<font color=navy>// For example, our function is f(x)=x0^2+x1^2+..., but we rewrite it as </font>
<font color=navy>//     f(x) = 0.5*(2*x0^2+2*x1^2) + ....</font>
<font color=navy>// and pass diag(2,2) as quadratic term - NOT diag(1,1)!</font>
   real_2d_array a = <font color=blue><b>&quot;[[2,0],[0,2]]&quot;</b></font>;
   real_1d_array b = <font color=blue><b>&quot;[-6,-4]&quot;</b></font>;
   real_1d_array x0 = <font color=blue><b>&quot;[0,1]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[0.0,0.0]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[2.5,2.5]&quot;</b></font>;
   real_1d_array x;
   minqpstate state;
   minqpreport rep;
<font color=navy>// create solver, set quadratic/linear terms</font>
   minqpcreate(2, state);
   minqpsetquadraticterm(state, a);
   minqpsetlinearterm(state, b);
   minqpsetstartingpoint(state, x0);
   minqpsetbc(state, bndl, bndu);
<font color=navy>// Set scale of the parameters.</font>
<font color=navy>// It is strongly recommended that you set scale of your variables.</font>
<font color=navy>// Knowing their scales is essential <b>for</b> evaluation of stopping criteria</font>
<font color=navy>// and <b>for</b> preconditioning of the algorithm steps.</font>
<font color=navy>// You can find more information on scaling at http://www.alglib.net/optimization/scaling.php</font>
<font color=navy>//</font>
<font color=navy>// NOTE: <b>for</b> convex problems you may try using minqpsetscaleautodiag()</font>
<font color=navy>//       which automatically determines variable scales.</font>
   minqpsetscale(state, s);
<font color=navy>// Solve problem with QuickQP solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is intended <b>for</b> medium and large-scale problems with box</font>
<font color=navy>// constraints (general linear constraints are not supported).</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used, Newton phase is active.</font>
   minqpsetalgoquickqp(state, 0.0, 0.0, 0.0, 0, true);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 4</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [2.5,2]</font>
<font color=navy>//</font>
<font color=navy>// Solve problem with BLEIC-based QP solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is intended <b>for</b> problems with moderate (up to 50) number</font>
<font color=navy>// of general linear constraints and unlimited number of box constraints.</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used.</font>
   minqpsetalgobleic(state, 0.0, 0.0, 0.0, 0);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [2.5,2]</font>
<font color=navy>//</font>
<font color=navy>// Solve problem with DENSE-AUL solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is optimized <b>for</b> problems with up to several thousands of</font>
<font color=navy>// variables and large amount of general linear constraints. Problems with</font>
<font color=navy>// less than 50 general linear constraints can be efficiently solved with</font>
<font color=navy>// BLEIC, problems with box-only constraints can be solved with QuickQP.</font>
<font color=navy>// However, DENSE-AUL will work in any (including unconstrained) case.</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used.</font>
   minqpsetalgodenseaul(state, 1.0e-9, 1.0e+4, 5);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [2.5,2]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minqp_d_lc1></a><h6 class=pageheader>minqp_d_lc1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of F(x0,x1) = x0^2 + x1^2 -6*x0 - 4*x1</font>
<font color=navy>// subject to linear constraint x0+x1 &le; 2</font>
<font color=navy>//</font>
<font color=navy>// Exact solution is [x0,x1] = [1.5,0.5]</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: this solver minimizes  following  function:</font>
<font color=navy>//     f(x) = 0.5*x'*A*x + b'*x.</font>
<font color=navy>// Note that quadratic term has 0.5 before it. So <b>if</b> you want to minimize</font>
<font color=navy>// quadratic function, you should rewrite it in such way that quadratic term</font>
<font color=navy>// is multiplied by 0.5 too.</font>
<font color=navy>// For example, our function is f(x)=x0^2+x1^2+..., but we rewrite it as </font>
<font color=navy>//     f(x) = 0.5*(2*x0^2+2*x1^2) + ....</font>
<font color=navy>// and pass diag(2,2) as quadratic term - NOT diag(1,1)!</font>
   real_2d_array a = <font color=blue><b>&quot;[[2,0],[0,2]]&quot;</b></font>;
   real_1d_array b = <font color=blue><b>&quot;[-6,-4]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_2d_array c = <font color=blue><b>&quot;[[1.0,1.0,2.0]]&quot;</b></font>;
   integer_1d_array ct = <font color=blue><b>&quot;[-1]&quot;</b></font>;
   real_1d_array x;
   minqpstate state;
   minqpreport rep;
<font color=navy>// create solver, set quadratic/linear terms</font>
   minqpcreate(2, state);
   minqpsetquadraticterm(state, a);
   minqpsetlinearterm(state, b);
   minqpsetlc(state, c, ct);
<font color=navy>// Set scale of the parameters.</font>
<font color=navy>// It is strongly recommended that you set scale of your variables.</font>
<font color=navy>// Knowing their scales is essential <b>for</b> evaluation of stopping criteria</font>
<font color=navy>// and <b>for</b> preconditioning of the algorithm steps.</font>
<font color=navy>// You can find more information on scaling at http://www.alglib.net/optimization/scaling.php</font>
<font color=navy>//</font>
<font color=navy>// NOTE: <b>for</b> convex problems you may try using minqpsetscaleautodiag()</font>
<font color=navy>//       which automatically determines variable scales.</font>
   minqpsetscale(state, s);
<font color=navy>// Solve problem with BLEIC-based QP solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is intended <b>for</b> problems with moderate (up to 50) number</font>
<font color=navy>// of general linear constraints and unlimited number of box constraints.</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used.</font>
   minqpsetalgobleic(state, 0.0, 0.0, 0.0, 0);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.500,0.500]</font>
<font color=navy>//</font>
<font color=navy>// Solve problem with DENSE-AUL solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is optimized <b>for</b> problems with up to several thousands of</font>
<font color=navy>// variables and large amount of general linear constraints. Problems with</font>
<font color=navy>// less than 50 general linear constraints can be efficiently solved with</font>
<font color=navy>// BLEIC, problems with box-only constraints can be solved with QuickQP.</font>
<font color=navy>// However, DENSE-AUL will work in any (including unconstrained) case.</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used.</font>
   minqpsetalgodenseaul(state, 1.0e-9, 1.0e+4, 5);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(1).c_str()); <font color=navy>// EXPECTED: [1.500,0.500]</font>
<font color=navy>//</font>
<font color=navy>// Solve problem with QuickQP solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is intended <b>for</b> medium and large-scale problems with box</font>
<font color=navy>// constraints, and...</font>
<font color=navy>//</font>
<font color=navy>// ...Oops! It does not support general linear constraints, -5 returned as completion code!</font>
   minqpsetalgoquickqp(state, 0.0, 0.0, 0.0, 0, true);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: -5</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minqp_d_nonconvex></a><h6 class=pageheader>minqp_d_nonconvex Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of nonconvex function</font>
<font color=navy>//     F(x0,x1) = -(x0^2+x1^2)</font>
<font color=navy>// subject to constraints x0,x1 in [1.0,2.0]</font>
<font color=navy>// Exact solution is [x0,x1] = [2,2].</font>
<font color=navy>//</font>
<font color=navy>// Non-convex problems are harded to solve than convex ones, and they</font>
<font color=navy>// may have more than one local minimum. However, ALGLIB solves may deal</font>
<font color=navy>// with such problems (altough they <b>do</b> not guarantee convergence to</font>
<font color=navy>// global minimum).</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: this solver minimizes  following  function:</font>
<font color=navy>//     f(x) = 0.5*x'*A*x + b'*x.</font>
<font color=navy>// Note that quadratic term has 0.5 before it. So <b>if</b> you want to minimize</font>
<font color=navy>// quadratic function, you should rewrite it in such way that quadratic term</font>
<font color=navy>// is multiplied by 0.5 too.</font>
<font color=navy>//</font>
<font color=navy>// For example, our function is f(x)=-(x0^2+x1^2), but we rewrite it as </font>
<font color=navy>//     f(x) = 0.5*(-2*x0^2-2*x1^2)</font>
<font color=navy>// and pass diag(-2,-2) as quadratic term - NOT diag(-1,-1)!</font>
   real_2d_array a = <font color=blue><b>&quot;[[-2,0],[0,-2]]&quot;</b></font>;
   real_1d_array x0 = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array bndl = <font color=blue><b>&quot;[1.0,1.0]&quot;</b></font>;
   real_1d_array bndu = <font color=blue><b>&quot;[2.0,2.0]&quot;</b></font>;
   real_1d_array x;
   minqpstate state;
   minqpreport rep;
<font color=navy>// create solver, set quadratic/linear terms, constraints</font>
   minqpcreate(2, state);
   minqpsetquadraticterm(state, a);
   minqpsetstartingpoint(state, x0);
   minqpsetbc(state, bndl, bndu);
<font color=navy>// Set scale of the parameters.</font>
<font color=navy>// It is strongly recommended that you set scale of your variables.</font>
<font color=navy>// Knowing their scales is essential <b>for</b> evaluation of stopping criteria</font>
<font color=navy>// and <b>for</b> preconditioning of the algorithm steps.</font>
<font color=navy>// You can find more information on scaling at http://www.alglib.net/optimization/scaling.php</font>
<font color=navy>//</font>
<font color=navy>// NOTE: there also exists minqpsetscaleautodiag() function</font>
<font color=navy>//       which automatically determines variable scales; however,</font>
<font color=navy>//       it does NOT work <b>for</b> non-convex problems.</font>
   minqpsetscale(state, s);
<font color=navy>// Solve problem with BLEIC-based QP solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is intended <b>for</b> problems with moderate (up to 50) number</font>
<font color=navy>// of general linear constraints and unlimited number of box constraints.</font>
<font color=navy>//</font>
<font color=navy>// It may solve non-convex problems as <b>long</b> as they are bounded from</font>
<font color=navy>// below under constraints.</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used.</font>
   minqpsetalgobleic(state, 0.0, 0.0, 0.0, 0);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [2,2]</font>
<font color=navy>//</font>
<font color=navy>// Solve problem with DENSE-AUL solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is optimized <b>for</b> problems with up to several thousands of</font>
<font color=navy>// variables and large amount of general linear constraints. Problems with</font>
<font color=navy>// less than 50 general linear constraints can be efficiently solved with</font>
<font color=navy>// BLEIC, problems with box-only constraints can be solved with QuickQP.</font>
<font color=navy>// However, DENSE-AUL will work in any (including unconstrained) case.</font>
<font color=navy>//</font>
<font color=navy>// Algorithm convergence is guaranteed only <b>for</b> convex case, but you may</font>
<font color=navy>// expect that it will work <b>for</b> non-convex problems too (because near the</font>
<font color=navy>// solution they are locally convex).</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used.</font>
   minqpsetalgodenseaul(state, 1.0e-9, 1.0e+4, 5);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [2,2]</font>
<font color=navy>// Hmm... this problem is bounded from below (has solution) only under constraints.</font>
<font color=navy>// What it we remove them?</font>
<font color=navy>//</font>
<font color=navy>// You may see that BLEIC algorithm detects unboundedness of the problem, </font>
<font color=navy>// -4 is returned as completion code. However, DENSE-AUL is unable to detect</font>
<font color=navy>// such situation and it will cycle forever (we <b>do</b> not test it here).</font>
   real_1d_array nobndl = <font color=blue><b>&quot;[-inf,-inf]&quot;</b></font>;
   real_1d_array nobndu = <font color=blue><b>&quot;[+inf,+inf]&quot;</b></font>;
   minqpsetbc(state, nobndl, nobndu);
   minqpsetalgobleic(state, 0.0, 0.0, 0.0, 0);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: -4</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minqp_d_u1></a><h6 class=pageheader>minqp_d_u1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of F(x0,x1) = x0^2 + x1^2 -6*x0 - 4*x1</font>
<font color=navy>//</font>
<font color=navy>// Exact solution is [x0,x1] = [3,2]</font>
<font color=navy>//</font>
<font color=navy>// We provide algorithm with starting point, although in this case</font>
<font color=navy>// (dense matrix, no constraints) it can work without such information.</font>
<font color=navy>//</font>
<font color=navy>// Several QP solvers are tried: QuickQP, BLEIC, DENSE-AUL.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: this solver minimizes  following  function:</font>
<font color=navy>//     f(x) = 0.5*x'*A*x + b'*x.</font>
<font color=navy>// Note that quadratic term has 0.5 before it. So <b>if</b> you want to minimize</font>
<font color=navy>// quadratic function, you should rewrite it in such way that quadratic term</font>
<font color=navy>// is multiplied by 0.5 too.</font>
<font color=navy>//</font>
<font color=navy>// For example, our function is f(x)=x0^2+x1^2+..., but we rewrite it as </font>
<font color=navy>//     f(x) = 0.5*(2*x0^2+2*x1^2) + .... </font>
<font color=navy>// and pass diag(2,2) as quadratic term - NOT diag(1,1)!</font>
   real_2d_array a = <font color=blue><b>&quot;[[2,0],[0,2]]&quot;</b></font>;
   real_1d_array b = <font color=blue><b>&quot;[-6,-4]&quot;</b></font>;
   real_1d_array x0 = <font color=blue><b>&quot;[0,1]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array x;
   minqpstate state;
   minqpreport rep;
<font color=navy>// create solver, set quadratic/linear terms</font>
   minqpcreate(2, state);
   minqpsetquadraticterm(state, a);
   minqpsetlinearterm(state, b);
   minqpsetstartingpoint(state, x0);
<font color=navy>// Set scale of the parameters.</font>
<font color=navy>// It is strongly recommended that you set scale of your variables.</font>
<font color=navy>// Knowing their scales is essential <b>for</b> evaluation of stopping criteria</font>
<font color=navy>// and <b>for</b> preconditioning of the algorithm steps.</font>
<font color=navy>// You can find more information on scaling at http://www.alglib.net/optimization/scaling.php</font>
<font color=navy>//</font>
<font color=navy>// NOTE: <b>for</b> convex problems you may try using minqpsetscaleautodiag()</font>
<font color=navy>//       which automatically determines variable scales.</font>
   minqpsetscale(state, s);
<font color=navy>// Solve problem with QuickQP solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is intended <b>for</b> medium and large-scale problems with box</font>
<font color=navy>// constraints (general linear constraints are not supported), but it can</font>
<font color=navy>// also be efficiently used on unconstrained problems.</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used, Newton phase is active.</font>
   minqpsetalgoquickqp(state, 0.0, 0.0, 0.0, 0, true);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [3,2]</font>
<font color=navy>//</font>
<font color=navy>// Solve problem with BLEIC-based QP solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is intended <b>for</b> problems with moderate (up to 50) number</font>
<font color=navy>// of general linear constraints and unlimited number of box constraints.</font>
<font color=navy>// Of course, unconstrained problems can be solved too.</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used.</font>
   minqpsetalgobleic(state, 0.0, 0.0, 0.0, 0);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [3,2]</font>
<font color=navy>//</font>
<font color=navy>// Solve problem with DENSE-AUL solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is optimized <b>for</b> problems with up to several thousands of</font>
<font color=navy>// variables and large amount of general linear constraints. Problems with</font>
<font color=navy>// less than 50 general linear constraints can be efficiently solved with</font>
<font color=navy>// BLEIC, problems with box-only constraints can be solved with QuickQP.</font>
<font color=navy>// However, DENSE-AUL will work in any (including unconstrained) case.</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used.</font>
   minqpsetalgodenseaul(state, 1.0e-9, 1.0e+4, 5);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [3,2]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_minqp_d_u2></a><h6 class=pageheader>minqp_d_u2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Optimization.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates minimization of F(x0,x1) = x0^2 + x1^2 -6*x0 - 4*x1,</font>
<font color=navy>// with quadratic term given by sparse matrix structure.</font>
<font color=navy>//</font>
<font color=navy>// Exact solution is [x0,x1] = [3,2]</font>
<font color=navy>//</font>
<font color=navy>// We provide algorithm with starting point, although in this case</font>
<font color=navy>// (dense matrix, no constraints) it can work without such information.</font>
<font color=navy>//</font>
<font color=navy>// IMPORTANT: this solver minimizes  following  function:</font>
<font color=navy>//     f(x) = 0.5*x'*A*x + b'*x.</font>
<font color=navy>// Note that quadratic term has 0.5 before it. So <b>if</b> you want to minimize</font>
<font color=navy>// quadratic function, you should rewrite it in such way that quadratic term</font>
<font color=navy>// is multiplied by 0.5 too.</font>
<font color=navy>//</font>
<font color=navy>// For example, our function is f(x)=x0^2+x1^2+..., but we rewrite it as </font>
<font color=navy>//     f(x) = 0.5*(2*x0^2+2*x1^2) + ....</font>
<font color=navy>// and pass diag(2,2) as quadratic term - NOT diag(1,1)!</font>
   sparsematrix a;
   real_1d_array b = <font color=blue><b>&quot;[-6,-4]&quot;</b></font>;
   real_1d_array x0 = <font color=blue><b>&quot;[0,1]&quot;</b></font>;
   real_1d_array s = <font color=blue><b>&quot;[1,1]&quot;</b></font>;
   real_1d_array x;
   minqpstate state;
   minqpreport rep;
<font color=navy>// initialize sparsematrix structure</font>
   sparsecreate(2, 2, 0, a);
   sparseset(a, 0, 0, 2.0);
   sparseset(a, 1, 1, 2.0);
<font color=navy>// create solver, set quadratic/linear terms</font>
   minqpcreate(2, state);
   minqpsetquadratictermsparse(state, a, true);
   minqpsetlinearterm(state, b);
   minqpsetstartingpoint(state, x0);
<font color=navy>// Set scale of the parameters.</font>
<font color=navy>// It is strongly recommended that you set scale of your variables.</font>
<font color=navy>// Knowing their scales is essential <b>for</b> evaluation of stopping criteria</font>
<font color=navy>// and <b>for</b> preconditioning of the algorithm steps.</font>
<font color=navy>// You can find more information on scaling at http://www.alglib.net/optimization/scaling.php</font>
<font color=navy>//</font>
<font color=navy>// NOTE: <b>for</b> convex problems you may try using minqpsetscaleautodiag()</font>
<font color=navy>//       which automatically determines variable scales.</font>
   minqpsetscale(state, s);
<font color=navy>// Solve problem with BLEIC-based QP solver.</font>
<font color=navy>//</font>
<font color=navy>// This solver is intended <b>for</b> problems with moderate (up to 50) number</font>
<font color=navy>// of general linear constraints and unlimited number of box constraints.</font>
<font color=navy>// It also supports sparse problems.</font>
<font color=navy>//</font>
<font color=navy>// Default stopping criteria are used.</font>
   minqpsetalgobleic(state, 0.0, 0.0, 0.0, 0);
   minqpoptimize(state);
   minqpresults(state, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [3,2]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_optguardapi></a><h4 class=pageheader>8.8.11. optguardapi Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_optguardnonc0report class=toc>optguardnonc0report</a> |
<a href=#struct_optguardnonc1test0report class=toc>optguardnonc1test0report</a> |
<a href=#struct_optguardnonc1test1report class=toc>optguardnonc1test1report</a> |
<a href=#struct_optguardreport class=toc>optguardreport</a>
]</font>
</div>
<a name=struct_optguardnonc0report></a><h6 class=pageheader>optguardnonc0report Class</h6>
<hr width=600 align=left>
<pre class=narration>
This  structure  is  used  for  detailed   reporting  about  suspected  C0
continuity violation.

==== WHAT IS TESTED ====

C0 test  studies  function  values (not gradient!)  obtained  during  line
searches and monitors estimate of the Lipschitz  constant.  Sudden  spikes
usually indicate that discontinuity was detected.

==== WHAT IS REPORTED ====

Actually, report retrieval function returns TWO report structures:

* one for most suspicious point found so far (one with highest  change  in
  the function value), so called &quot;strongest&quot; report
* another one for most detailed line search (more function  evaluations  =
  easier to understand what's going on) which triggered  test #0 criteria,
  so called &quot;longest&quot; report

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* fidx - is an index of the function (0 for  target  function, 1 or higher
  for nonlinear constraints) which is suspected of being &quot;non-C1&quot;
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], f[] - arrays of length CNT which store step lengths and  function
  values at these points; f[i] is evaluated in x0+stp[i]*d.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

You can plot function values stored in stp[]  and  f[]  arrays  and  study
behavior of your function by your own eyes, just  to  be  sure  that  test
correctly reported C1 violation.
ALGLIB: Copyright 19.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> optguardnonc0report {
   <b>bool</b> positive;
   ae_int_t fidx;
   real_1d_array x0;
   real_1d_array d;
   ae_int_t n;
   real_1d_array stp;
   real_1d_array f;
   ae_int_t cnt;
   ae_int_t stpidxa;
   ae_int_t stpidxb;
};
</pre>
<a name=struct_optguardnonc1test0report></a><h6 class=pageheader>optguardnonc1test0report Class</h6>
<hr width=600 align=left>
<pre class=narration>
This  structure  is  used  for  detailed   reporting  about  suspected  C1
continuity violation as flagged by C1 test #0 (OptGuard  has several tests
for C1 continuity, this report is used by #0).

==== WHAT IS TESTED ====

C1 test #0 studies function values (not gradient!)  obtained  during  line
searches and monitors behavior of directional  derivative  estimate.  This
test is less powerful than test #1, but it does  not  depend  on  gradient
values  and  thus  it  is  more  robust  against  artifacts  introduced by
numerical differentiation.

==== WHAT IS REPORTED ====

Actually, report retrieval function returns TWO report structures:

* one for most suspicious point found so far (one with highest  change  in
  the directional derivative), so called &quot;strongest&quot; report
* another one for most detailed line search (more function  evaluations  =
  easier to understand what's going on) which triggered  test #0 criteria,
  so called &quot;longest&quot; report

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* fidx - is an index of the function (0 for  target  function, 1 or higher
  for nonlinear constraints) which is suspected of being &quot;non-C1&quot;
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], f[] - arrays of length CNT which store step lengths and  function
  values at these points; f[i] is evaluated in x0+stp[i]*d.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

You can plot function values stored in stp[]  and  f[]  arrays  and  study
behavior of your function by your own eyes, just  to  be  sure  that  test
correctly reported C1 violation.
ALGLIB: Copyright 19.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> optguardnonc1test0report {
   <b>bool</b> positive;
   ae_int_t fidx;
   real_1d_array x0;
   real_1d_array d;
   ae_int_t n;
   real_1d_array stp;
   real_1d_array f;
   ae_int_t cnt;
   ae_int_t stpidxa;
   ae_int_t stpidxb;
};
</pre>
<a name=struct_optguardnonc1test1report></a><h6 class=pageheader>optguardnonc1test1report Class</h6>
<hr width=600 align=left>
<pre class=narration>
This  structure  is  used  for  detailed   reporting  about  suspected  C1
continuity violation as flagged by C1 test #1 (OptGuard  has several tests
for C1 continuity, this report is used by #1).

==== WHAT IS TESTED ====

C1 test #1 studies individual  components  of  the  gradient  as  recorded
during line searches. Upon discovering discontinuity in the gradient  this
test records specific component which was suspected (or  one  with  highest
indication of discontinuity if multiple components are suspected).

When precise analytic gradient is provided this test is more powerful than
test #0  which  works  with  function  values  and  ignores  user-provided
gradient.  However,  test  #0  becomes  more   powerful   when   numerical
differentiation is employed (in such cases test #1 detects  higher  levels
of numerical noise and becomes too conservative).

This test also tells specific components of the gradient which violate  C1
continuity, which makes it more informative than #0, which just tells that
continuity is violated.

==== WHAT IS REPORTED ====

Actually, report retrieval function returns TWO report structures:

* one for most suspicious point found so far (one with highest  change  in
  the directional derivative), so called &quot;strongest&quot; report
* another one for most detailed line search (more function  evaluations  =
  easier to understand what's going on) which triggered  test #1 criteria,
  so called &quot;longest&quot; report

In both cases following fields are returned:

* positive - is TRUE  when test flagged suspicious point;  FALSE  if  test
  did not notice anything (in the latter cases fields below are empty).
* fidx - is an index of the function (0 for  target  function, 1 or higher
  for nonlinear constraints) which is suspected of being &quot;non-C1&quot;
* vidx - is an index of the variable in [0,N) with nonsmooth derivative
* x0[], d[] - arrays of length N which store initial point  and  direction
  for line search (d[] can be normalized, but does not have to)
* stp[], g[] - arrays of length CNT which store step lengths and  gradient
  values at these points; g[i] is evaluated in  x0+stp[i]*d  and  contains
  vidx-th component of the gradient.
* stpidxa, stpidxb - we  suspect  that  function  violates  C1  continuity
  between steps #stpidxa and #stpidxb (usually we have  stpidxb=stpidxa+3,
  with  most  likely  position  of  the  violation  between  stpidxa+1 and
  stpidxa+2.

You can plot function values stored in stp[]  and  g[]  arrays  and  study
behavior of your function by your own eyes, just  to  be  sure  that  test
correctly reported C1 violation.
ALGLIB: Copyright 19.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> optguardnonc1test1report {
   <b>bool</b> positive;
   ae_int_t fidx;
   ae_int_t vidx;
   real_1d_array x0;
   real_1d_array d;
   ae_int_t n;
   real_1d_array stp;
   real_1d_array g;
   ae_int_t cnt;
   ae_int_t stpidxa;
   ae_int_t stpidxb;
};
</pre>
<a name=struct_optguardreport></a><h6 class=pageheader>optguardreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure is used to store  OptGuard  report,  i.e.  report  on   the
properties of the nonlinear function being optimized with ALGLIB.

After you tell your optimizer to activate OptGuard  this technology starts
to silently monitor function values and gradients/Jacobians  being  passed
all around during your optimization session. Depending on specific set  of
checks enabled OptGuard may perform additional function evaluations  (say,
about 3*N evaluations if you want to check analytic gradient for errors).

Upon discovering that something strange happens  (function  values  and/or
gradient components change too sharply and/or unexpectedly) OptGuard  sets
one of the &quot;suspicion  flags&quot; (without interrupting optimization session).
After optimization is done, you can examine OptGuard report.

Following report fields can be set:
* nonc0suspected
* nonc1suspected
* badgradsuspected

==== WHAT CAN BE DETECTED WITH OptGuard INTEGRITY CHECKER ====

Following  types  of  errors  in your target function (constraints) can be
caught:
a) discontinuous functions (&quot;non-C0&quot; part of the report)
b) functions with discontinuous derivative (&quot;non-C1&quot; part of the report)
c) errors in the analytic gradient provided by user

These types of errors result in optimizer  stopping  well  before reaching
solution (most often - right after encountering discontinuity).

Type A errors are usually  coding  errors  during  implementation  of  the
target function. Most &quot;normal&quot; problems involve continuous functions,  and
anyway you can't reliably optimize discontinuous function.

Type B errors are either coding errors or (in case code itself is correct)
evidence of the fact  that  your  problem  is  an  &quot;incorrect&quot;  one.  Most
optimizers (except for ones provided by MINNS subpackage) do  not  support
nonsmooth problems.

Type C errors are coding errors which often prevent optimizer from  making
even one step  or result in optimizing stopping  too  early,  as  soon  as
actual descent direction becomes too different from one suggested by user-
supplied gradient.

==== WHAT IS REPORTED ====

Following set of report fields deals with discontinuous  target functions,
ones not belonging to C0 continuity class:

* nonc0suspected - is a flag which is set upon discovering some indication
  of the discontinuity. If this flag is false, the rest of &quot;non-C0&quot; fields
  should be ignored
* nonc0fidx - is an index of the function (0 for  target  function,  1  or
  higher for nonlinear constraints) which is suspected of being &quot;non-C0&quot;
* nonc0lipshitzc - a Lipchitz constant for a function which was  suspected
  of being non-continuous.
* nonc0test0positive -  set  to  indicate  specific  test  which  detected
  continuity violation (test #0)

Following set of report fields deals with discontinuous gradient/Jacobian,
i.e. with functions violating C1 continuity:

* nonc1suspected - is a flag which is set upon discovering some indication
  of the discontinuity. If this flag is false, the rest of &quot;non-C1&quot; fields
  should be ignored
* nonc1fidx - is an index of the function (0 for  target  function,  1  or
  higher for nonlinear constraints) which is suspected of being &quot;non-C1&quot;
* nonc1lipshitzc - a Lipchitz constant for a function gradient  which  was
  suspected of being non-smooth.
* nonc1test0positive -  set  to  indicate  specific  test  which  detected
  continuity violation (test #0)
* nonc1test1positive -  set  to  indicate  specific  test  which  detected
  continuity violation (test #1)

Following set of report fields deals with errors in the gradient:
* badgradsuspected - is a flad which is set upon discovering an  error  in
  the analytic gradient supplied by user
* badgradfidx - index  of   the  function  with bad gradient (0 for target
  function, 1 or higher for nonlinear constraints)
* badgradvidx - index of the variable
* badgradxbase - location where Jacobian is tested
* following  matrices  store  user-supplied  Jacobian  and  its  numerical
  differentiation version (which is assumed to be  free  from  the  coding
  errors), both of them computed near the initial point:
  * badgraduser, an array[K,N], analytic Jacobian supplied by user
  * badgradnum,  an array[K,N], numeric  Jacobian computed by ALGLIB
  Here K is a total number of  nonlinear  functions  (target  +  nonlinear
  constraints), N is a variable number.
  The  element  of  badgraduser[] with index [badgradfidx,badgradvidx]  is
  assumed to be wrong.

More detailed error log can  be  obtained  from  optimizer  by  explicitly
requesting reports for tests C0.0, C1.0, C1.1.
ALGLIB: Copyright 19.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> optguardreport {
   <b>bool</b> nonc0suspected;
   <b>bool</b> nonc0test0positive;
   ae_int_t nonc0fidx;
   <b>double</b> nonc0lipschitzc;
   <b>bool</b> nonc1suspected;
   <b>bool</b> nonc1test0positive;
   <b>bool</b> nonc1test1positive;
   ae_int_t nonc1fidx;
   <b>double</b> nonc1lipschitzc;
   <b>bool</b> badgradsuspected;
   ae_int_t badgradfidx;
   ae_int_t badgradvidx;
   real_1d_array badgradxbase;
   real_2d_array badgraduser;
   real_2d_array badgradnum;
};
</pre>
</p>
<p>
<a name=pck_Solvers class=sheader></a><h3>8.9. Solvers Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_directdensesolvers class=toc>directdensesolvers</a></td><td>Direct dense linear solvers</td></tr>
<tr align=left valign=top><td><a href=#unit_directsparsesolvers class=toc>directsparsesolvers</a></td><td>Direct sparse linear solvers</td></tr>
<tr align=left valign=top><td><a href=#unit_lincg class=toc>lincg</a></td><td>Sparse linear CG solver</td></tr>
<tr align=left valign=top><td><a href=#unit_linlsqr class=toc>linlsqr</a></td><td>Sparse linear LSQR solver</td></tr>
<tr align=left valign=top><td><a href=#unit_nleq class=toc>nleq</a></td><td>Solvers for nonlinear equations</td></tr>
<tr align=left valign=top><td><a href=#unit_polynomialsolver class=toc>polynomialsolver</a></td><td>Polynomial solver</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_directdensesolvers></a><h4 class=pageheader>8.9.1. directdensesolvers Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_densesolverlsreport class=toc>densesolverlsreport</a> |
<a href=#struct_densesolverreport class=toc>densesolverreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_cmatrixlusolve class=toc>cmatrixlusolve</a> |
<a href=#sub_cmatrixlusolvefast class=toc>cmatrixlusolvefast</a> |
<a href=#sub_cmatrixlusolvem class=toc>cmatrixlusolvem</a> |
<a href=#sub_cmatrixlusolvemfast class=toc>cmatrixlusolvemfast</a> |
<a href=#sub_cmatrixmixedsolve class=toc>cmatrixmixedsolve</a> |
<a href=#sub_cmatrixmixedsolvem class=toc>cmatrixmixedsolvem</a> |
<a href=#sub_cmatrixsolve class=toc>cmatrixsolve</a> |
<a href=#sub_cmatrixsolvefast class=toc>cmatrixsolvefast</a> |
<a href=#sub_cmatrixsolvem class=toc>cmatrixsolvem</a> |
<a href=#sub_cmatrixsolvemfast class=toc>cmatrixsolvemfast</a> |
<a href=#sub_hpdmatrixcholeskysolve class=toc>hpdmatrixcholeskysolve</a> |
<a href=#sub_hpdmatrixcholeskysolvefast class=toc>hpdmatrixcholeskysolvefast</a> |
<a href=#sub_hpdmatrixcholeskysolvem class=toc>hpdmatrixcholeskysolvem</a> |
<a href=#sub_hpdmatrixcholeskysolvemfast class=toc>hpdmatrixcholeskysolvemfast</a> |
<a href=#sub_hpdmatrixsolve class=toc>hpdmatrixsolve</a> |
<a href=#sub_hpdmatrixsolvefast class=toc>hpdmatrixsolvefast</a> |
<a href=#sub_hpdmatrixsolvem class=toc>hpdmatrixsolvem</a> |
<a href=#sub_hpdmatrixsolvemfast class=toc>hpdmatrixsolvemfast</a> |
<a href=#sub_rmatrixlusolve class=toc>rmatrixlusolve</a> |
<a href=#sub_rmatrixlusolvefast class=toc>rmatrixlusolvefast</a> |
<a href=#sub_rmatrixlusolvem class=toc>rmatrixlusolvem</a> |
<a href=#sub_rmatrixlusolvemfast class=toc>rmatrixlusolvemfast</a> |
<a href=#sub_rmatrixmixedsolve class=toc>rmatrixmixedsolve</a> |
<a href=#sub_rmatrixmixedsolvem class=toc>rmatrixmixedsolvem</a> |
<a href=#sub_rmatrixsolve class=toc>rmatrixsolve</a> |
<a href=#sub_rmatrixsolvefast class=toc>rmatrixsolvefast</a> |
<a href=#sub_rmatrixsolvels class=toc>rmatrixsolvels</a> |
<a href=#sub_rmatrixsolvem class=toc>rmatrixsolvem</a> |
<a href=#sub_rmatrixsolvemfast class=toc>rmatrixsolvemfast</a> |
<a href=#sub_spdmatrixcholeskysolve class=toc>spdmatrixcholeskysolve</a> |
<a href=#sub_spdmatrixcholeskysolvefast class=toc>spdmatrixcholeskysolvefast</a> |
<a href=#sub_spdmatrixcholeskysolvem class=toc>spdmatrixcholeskysolvem</a> |
<a href=#sub_spdmatrixcholeskysolvemfast class=toc>spdmatrixcholeskysolvemfast</a> |
<a href=#sub_spdmatrixsolve class=toc>spdmatrixsolve</a> |
<a href=#sub_spdmatrixsolvefast class=toc>spdmatrixsolvefast</a> |
<a href=#sub_spdmatrixsolvem class=toc>spdmatrixsolvem</a> |
<a href=#sub_spdmatrixsolvemfast class=toc>spdmatrixsolvemfast</a>
]</font>
</div>
<a name=struct_densesolverlsreport></a><h6 class=pageheader>densesolverlsreport Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> densesolverlsreport {
   <b>double</b> r2;
   real_2d_array cx;
   ae_int_t n;
   ae_int_t k;
};
</pre>
<a name=struct_densesolverreport></a><h6 class=pageheader>densesolverreport Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> densesolverreport {
   <b>double</b> r1;
   <b>double</b> rinf;
};
</pre>
<a name=sub_cmatrixlusolve></a><h6 class=pageheader>cmatrixlusolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complex dense linear solver for A*x=b with complex N*N A  given  by its LU
decomposition and N*1 vectors x and b. This is  &quot;slow-but-robust&quot;  version
of  the  complex  linear  solver  with  additional  features   which   add
significant performance overhead. Faster version  is  CMatrixLUSolveFast()
function.

Algorithm features:
* automatic detection of degenerate cases
* O(N^2) complexity
* condition number estimation

No iterative refinement is provided because exact form of original matrix
is not known to subroutine. Use CMatrixSolve or CMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which results in 10-15x  performance  penalty  when  compared
           ! with &quot;fast&quot; version which just calls triangular solver.
           !
           ! This performance penalty is insignificant  when compared with
           ! cost of large LU decomposition.  However,  if you  call  this
           ! function many times for the same  left  side,  this  overhead
           ! BECOMES significant. It  also  becomes significant for small-
           ! scale problems.
           !
           ! In such cases we strongly recommend you to use faster solver,
           ! CMatrixLUSolveFast() function.

Inputs:
    LUA     -   array[0..N-1,0..N-1], LU decomposition, CMatrixLU result
    P       -   array[0..N-1], pivots array, CMatrixLU result
    N       -   size of A
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixlusolve(complex_2d_array lua, integer_1d_array p, ae_int_t n, complex_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, complex_1d_array &amp;x);
</pre>
<a name=sub_cmatrixlusolvefast></a><h6 class=pageheader>cmatrixlusolvefast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complex dense linear solver for A*x=b with N*N complex A given by  its  LU
decomposition and N*1 vectors x and b. This is  fast  lightweight  version
of solver, which is significantly faster than CMatrixLUSolve(),  but  does
not provide additional information (like condition numbers).

Algorithm features:
* O(N^2) complexity
* no additional time-consuming features, just triangular solver

Inputs:
    LUA     -   array[0..N-1,0..N-1], LU decomposition, CMatrixLU result
    P       -   array[0..N-1], pivots array, CMatrixLU result
    N       -   size of A
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is exactly singular (ill conditioned matrices
                        are not recognized).
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[N]:
                * info &gt; 0  &rArr; overwritten by solution
                * info = -3 &rArr; filled by zeros

NOTE: unlike  CMatrixLUSolve(),  this   function   does   NOT   check  for
      near-degeneracy of input matrix. It  checks  for  EXACT  degeneracy,
      because this check is easy to do. However,  very  badly  conditioned
      matrices may went unnoticed.
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixlusolvefast(complex_2d_array lua, integer_1d_array p, ae_int_t n, complex_1d_array b, ae_int_t &amp;info);
</pre>
<a name=sub_cmatrixlusolvem></a><h6 class=pageheader>cmatrixlusolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B with N*N complex A given by its  LU  decomposition,
and N*M matrices X and B (multiple right sides).   &quot;Slow-but-feature-rich&quot;
version of the solver.

Algorithm features:
* automatic detection of degenerate cases
* O(M*N^2) complexity
* condition number estimation

No iterative refinement  is provided because exact form of original matrix
is not known to subroutine. Use CMatrixSolve or CMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which  results  in  significant  performance   penalty   when
           ! compared with &quot;fast&quot;  version  which  just  calls  triangular
           ! solver.
           !
           ! This performance penalty is especially apparent when you  use
           ! ALGLIB parallel capabilities (condition number estimation  is
           ! inherently  sequential).  It   also   becomes significant for
           ! small-scale problems.
           !
           ! In such cases we strongly recommend you to use faster solver,
           ! CMatrixLUSolveMFast() function.

Inputs:
    LUA     -   array[0..N-1,0..N-1], LU decomposition, RMatrixLU result
    P       -   array[0..N-1], pivots array, RMatrixLU result
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N,M], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixlusolvem(complex_2d_array lua, integer_1d_array p, ae_int_t n, complex_2d_array b, ae_int_t m, ae_int_t &amp;info, densesolverreport &amp;rep, complex_2d_array &amp;x);
</pre>
<a name=sub_cmatrixlusolvemfast></a><h6 class=pageheader>cmatrixlusolvemfast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B with N*N complex A given by its  LU  decomposition,
and N*M matrices X and B (multiple  right  sides).  &quot;Fast-but-lightweight&quot;
version of the solver.

Algorithm features:
* O(M*N^2) complexity
* no additional time-consuming features

Inputs:
    LUA     -   array[0..N-1,0..N-1], LU decomposition, RMatrixLU result
    P       -   array[0..N-1], pivots array, RMatrixLU result
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    matrix is exactly singular (ill conditioned matrices
                        are not recognized).
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[N,M]:
                * info &gt; 0  &rArr; overwritten by solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixlusolvemfast(complex_2d_array lua, integer_1d_array p, ae_int_t n, complex_2d_array b, ae_int_t m, ae_int_t &amp;info);
</pre>
<a name=sub_cmatrixmixedsolve></a><h6 class=pageheader>cmatrixmixedsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver. Same as RMatrixMixedSolve(), but for complex matrices.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* iterative refinement
* O(N^2) complexity

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    LUA     -   array[0..N-1,0..N-1], LU decomposition, CMatrixLU result
    P       -   array[0..N-1], pivots array, CMatrixLU result
    N       -   size of A
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixmixedsolve(complex_2d_array a, complex_2d_array lua, integer_1d_array p, ae_int_t n, complex_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, complex_1d_array &amp;x);
</pre>
<a name=sub_cmatrixmixedsolvem></a><h6 class=pageheader>cmatrixmixedsolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver. Same as RMatrixMixedSolveM(), but for complex matrices.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* iterative refinement
* O(M*N^2) complexity

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    LUA     -   array[0..N-1,0..N-1], LU decomposition, CMatrixLU result
    P       -   array[0..N-1], pivots array, CMatrixLU result
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N,M], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixmixedsolvem(complex_2d_array a, complex_2d_array lua, integer_1d_array p, ae_int_t n, complex_2d_array b, ae_int_t m, ae_int_t &amp;info, densesolverreport &amp;rep, complex_2d_array &amp;x);
</pre>
<a name=sub_cmatrixsolve></a><h6 class=pageheader>cmatrixsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complex dense solver for A*x=B with N*N complex matrix A and  N*1  complex
vectors x and b. &quot;Slow-but-feature-rich&quot; version of the solver.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* iterative refinement
* O(N^3) complexity

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear  system
           ! and  performs  iterative   refinement,   which   results   in
           ! significant performance penalty  when  compared  with  &quot;fast&quot;
           ! version  which  just  performs  LU  decomposition  and  calls
           ! triangular solver.
           !
           ! This  performance  penalty  is  especially  visible  in   the
           ! multithreaded mode, because both condition number  estimation
           ! and   iterative    refinement   are   inherently   sequential
           ! calculations.
           !
           ! Thus, if you need high performance and if you are pretty sure
           ! that your system is well conditioned, we  strongly  recommend
           ! you to use faster solver, CMatrixSolveFast() function.

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixsolve(complex_2d_array a, ae_int_t n, complex_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, complex_1d_array &amp;x);
</pre>
<a name=sub_cmatrixsolvefast></a><h6 class=pageheader>cmatrixsolvefast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complex dense solver for A*x=B with N*N complex matrix A and  N*1  complex
vectors x and b. &quot;Fast-but-lightweight&quot; version of the solver.

Algorithm features:
* O(N^3) complexity
* no additional time consuming features, just triangular solver

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is exactly singular (ill conditioned matrices
                        are not recognized).
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[N]:
                * info &gt; 0  &rArr; overwritten by solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixsolvefast(complex_2d_array a, ae_int_t n, complex_1d_array b, ae_int_t &amp;info);
</pre>
<a name=sub_cmatrixsolvem></a><h6 class=pageheader>cmatrixsolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complex dense solver for A*X=B with N*N  complex  matrix  A,  N*M  complex
matrices  X  and  B.  &quot;Slow-but-feature-rich&quot;   version   which   provides
additional functions, at the cost of slower  performance.  Faster  version
may be invoked with CMatrixSolveMFast() function.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* iterative refinement
* O(N^3+M*N^2) complexity

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear  system
           ! and  performs  iterative   refinement,   which   results   in
           ! significant performance penalty  when  compared  with  &quot;fast&quot;
           ! version  which  just  performs  LU  decomposition  and  calls
           ! triangular solver.
           !
           ! This  performance  penalty  is  especially  visible  in   the
           ! multithreaded mode, because both condition number  estimation
           ! and   iterative    refinement   are   inherently   sequential
           ! calculations.
           !
           ! Thus, if you need high performance and if you are pretty sure
           ! that your system is well conditioned, we  strongly  recommend
           ! you to use faster solver, CMatrixSolveMFast() function.

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size
    RFS     -   iterative refinement switch:
                * True - refinement is used.
                  Less performance, more precision.
                * False - refinement is not used.
                  More performance, less precision.

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N,M], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixsolvem(complex_2d_array a, ae_int_t n, complex_2d_array b, ae_int_t m, <b>bool</b> rfs, ae_int_t &amp;info, densesolverreport &amp;rep, complex_2d_array &amp;x);
</pre>
<a name=sub_cmatrixsolvemfast></a><h6 class=pageheader>cmatrixsolvemfast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complex dense solver for A*X=B with N*N  complex  matrix  A,  N*M  complex
matrices  X  and  B.  &quot;Fast-but-lightweight&quot; version which  provides  just
triangular solver - and no additional functions like iterative  refinement
or condition number estimation.

Algorithm features:
* O(N^3+M*N^2) complexity
* no additional time consuming functions

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    matrix is exactly singular (ill conditioned matrices
                        are not recognized).
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[N,M]:
                * info &gt; 0  &rArr; overwritten by solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 16.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> cmatrixsolvemfast(complex_2d_array a, ae_int_t n, complex_2d_array b, ae_int_t m, ae_int_t &amp;info);
</pre>
<a name=sub_hpdmatrixcholeskysolve></a><h6 class=pageheader>hpdmatrixcholeskysolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*x=b with N*N Hermitian positive definite matrix A given
by its Cholesky decomposition, and N*1 complex vectors x and  b.  This  is
&quot;slow-but-feature-rich&quot; version of the solver  which  estimates  condition
number of the system.

Algorithm features:
* automatic detection of degenerate cases
* O(N^2) complexity
* condition number estimation
* matrix is represented by its upper or lower triangle

No iterative refinement is provided because such partial representation of
matrix does not allow efficient calculation of extra-precise  matrix-vector
products for large matrices. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which results in 10-15x  performance  penalty  when  compared
           ! with &quot;fast&quot; version which just calls triangular solver.
           !
           ! This performance penalty is insignificant  when compared with
           ! cost of large LU decomposition.  However,  if you  call  this
           ! function many times for the same  left  side,  this  overhead
           ! BECOMES significant. It  also  becomes significant for small-
           ! scale problems (N &lt; 50).
           !
           ! In such cases we strongly recommend you to use faster solver,
           ! HPDMatrixCholeskySolveFast() function.

Inputs:
    CHA     -   array[0..N-1,0..N-1], Cholesky decomposition,
                SPDMatrixCholesky result
    N       -   size of A
    IsUpper -   what half of CHA is provided
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    A is is exactly singular or ill conditioned
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N]:
                * for info &gt; 0  - solution
                * for info = -3 - filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixcholeskysolve(complex_2d_array cha, ae_int_t n, <b>bool</b> isupper, complex_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, complex_1d_array &amp;x);
</pre>
<a name=sub_hpdmatrixcholeskysolvefast></a><h6 class=pageheader>hpdmatrixcholeskysolvefast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*x=b with N*N Hermitian positive definite matrix A given
by its Cholesky decomposition, and N*1 complex vectors x and  b.  This  is
&quot;fast-but-lightweight&quot; version of the solver.

Algorithm features:
* O(N^2) complexity
* matrix is represented by its upper or lower triangle
* no additional time-consuming features

Inputs:
    CHA     -   array[0..N-1,0..N-1], Cholesky decomposition,
                SPDMatrixCholesky result
    N       -   size of A
    IsUpper -   what half of CHA is provided
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    A is is exactly singular or ill conditioned
                        B is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[N]:
                * for info &gt; 0  - overwritten by solution
                * for info = -3 - filled by zeros
ALGLIB: Copyright 18.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixcholeskysolvefast(complex_2d_array cha, ae_int_t n, <b>bool</b> isupper, complex_1d_array b, ae_int_t &amp;info);
</pre>
<a name=sub_hpdmatrixcholeskysolvem></a><h6 class=pageheader>hpdmatrixcholeskysolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B with N*N Hermitian positive definite matrix A given
by its Cholesky decomposition and N*M complex matrices X  and  B.  This is
&quot;slow-but-feature-rich&quot; version of the solver which, in  addition  to  the
solution, estimates condition number of the system.

Algorithm features:
* automatic detection of degenerate cases
* O(M*N^2) complexity
* condition number estimation
* matrix is represented by its upper or lower triangle

No iterative refinement is provided because such partial representation of
matrix does not allow efficient calculation of extra-precise  matrix-vector
products for large matrices. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which  results  in  significant  performance   penalty   when
           ! compared with &quot;fast&quot;  version  which  just  calls  triangular
           ! solver. Amount of  overhead  introduced  depends  on  M  (the
           ! larger - the more efficient).
           !
           ! This performance penalty is insignificant  when compared with
           ! cost of large Cholesky decomposition.  However,  if  you call
           ! this  function  many  times  for  the same  left  side,  this
           ! overhead BECOMES significant. It  also   becomes  significant
           ! for small-scale problems (N &lt; 50).
           !
           ! In such cases we strongly recommend you to use faster solver,
           ! HPDMatrixCholeskySolveMFast() function.

Inputs:
    CHA     -   array[N,N], Cholesky decomposition,
                HPDMatrixCholesky result
    N       -   size of CHA
    IsUpper -   what half of CHA is provided
    B       -   array[N,M], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    A is singular, or VERY close to singular.
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task was solved
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N]:
                * for info &gt; 0 contains solution
                * for info = -3 filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixcholeskysolvem(complex_2d_array cha, ae_int_t n, <b>bool</b> isupper, complex_2d_array b, ae_int_t m, ae_int_t &amp;info, densesolverreport &amp;rep, complex_2d_array &amp;x);
</pre>
<a name=sub_hpdmatrixcholeskysolvemfast></a><h6 class=pageheader>hpdmatrixcholeskysolvemfast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B with N*N Hermitian positive definite matrix A given
by its Cholesky decomposition and N*M complex matrices X  and  B.  This is
&quot;fast-but-lightweight&quot; version of the solver.

Algorithm features:
* O(M*N^2) complexity
* matrix is represented by its upper or lower triangle
* no additional time-consuming features

Inputs:
    CHA     -   array[N,N], Cholesky decomposition,
                HPDMatrixCholesky result
    N       -   size of CHA
    IsUpper -   what half of CHA is provided
    B       -   array[N,M], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    A is singular, or VERY close to singular.
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task was solved
    B       -   array[N]:
                * for info &gt; 0 overwritten by solution
                * for info = -3 filled by zeros
ALGLIB: Copyright 18.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixcholeskysolvemfast(complex_2d_array cha, ae_int_t n, <b>bool</b> isupper, complex_2d_array b, ae_int_t m, ae_int_t &amp;info);
</pre>
<a name=sub_hpdmatrixsolve></a><h6 class=pageheader>hpdmatrixsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*x=b, with N*N Hermitian positive definite matrix A, and
N*1 complex vectors  x  and  b.  &quot;Slow-but-feature-rich&quot;  version  of  the
solver.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* O(N^3) complexity
* matrix is represented by its upper or lower triangle

No iterative refinement is provided because such partial representation of
matrix does not allow efficient calculation of extra-precise  matrix-vector
products for large matrices. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which  results  in  significant   performance   penalty  when
           ! compared with &quot;fast&quot; version  which  just  performs  Cholesky
           ! decomposition and calls triangular solver.
           !
           ! This  performance  penalty  is  especially  visible  in   the
           ! multithreaded mode, because both condition number  estimation
           ! and   iterative    refinement   are   inherently   sequential
           ! calculations.
           !
           ! Thus, if you need high performance and if you are pretty sure
           ! that your system is well conditioned, we  strongly  recommend
           ! you to use faster solver, HPDMatrixSolveFast() function.

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    IsUpper -   what half of A is provided
    B       -   array[0..N-1], right part

Outputs:
    Info    -   same as in RMatrixSolve
                Returns -3 for non-HPD matrices.
    Rep     -   same as in RMatrixSolve
    X       -   same as in RMatrixSolve
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixsolve(complex_2d_array a, ae_int_t n, <b>bool</b> isupper, complex_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, complex_1d_array &amp;x);
</pre>
<a name=sub_hpdmatrixsolvefast></a><h6 class=pageheader>hpdmatrixsolvefast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*x=b, with N*N Hermitian positive definite matrix A, and
N*1 complex vectors  x  and  b.  &quot;Fast-but-lightweight&quot;  version  of   the
solver without additional functions.

Algorithm features:
* O(N^3) complexity
* matrix is represented by its upper or lower triangle
* no additional time consuming functions

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    IsUpper -   what half of A is provided
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    A is is exactly singular or not positive definite
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task was solved
    B       -   array[0..N-1]:
                * overwritten by solution
                * zeros, if A is exactly singular (diagonal of its LU
                  decomposition has exact zeros).
ALGLIB: Copyright 17.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixsolvefast(complex_2d_array a, ae_int_t n, <b>bool</b> isupper, complex_1d_array b, ae_int_t &amp;info);
</pre>
<a name=sub_hpdmatrixsolvem></a><h6 class=pageheader>hpdmatrixsolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B, with N*N Hermitian positive definite matrix A  and
N*M  complex  matrices  X  and  B.  &quot;Slow-but-feature-rich&quot; version of the
solver.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* O(N^3+M*N^2) complexity
* matrix is represented by its upper or lower triangle

No iterative refinement is provided because such partial representation of
matrix does not allow efficient calculation of extra-precise  matrix-vector
products for large matrices. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which  results  in  significant  performance   penalty   when
           ! compared with &quot;fast&quot;  version  which  just  calls  triangular
           ! solver.
           !
           ! This performance penalty is especially apparent when you  use
           ! ALGLIB parallel capabilities (condition number estimation  is
           ! inherently  sequential).  It   also   becomes significant for
           ! small-scale problems (N &lt; 100).
           !
           ! In such cases we strongly recommend you to use faster solver,
           ! HPDMatrixSolveMFast() function.

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    IsUpper -   what half of A is provided
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   same as in RMatrixSolve.
                Returns -3 for non-HPD matrices.
    Rep     -   same as in RMatrixSolve
    X       -   same as in RMatrixSolve
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixsolvem(complex_2d_array a, ae_int_t n, <b>bool</b> isupper, complex_2d_array b, ae_int_t m, ae_int_t &amp;info, densesolverreport &amp;rep, complex_2d_array &amp;x);
</pre>
<a name=sub_hpdmatrixsolvemfast></a><h6 class=pageheader>hpdmatrixsolvemfast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B, with N*N Hermitian positive definite matrix A  and
N*M complex matrices X and B. &quot;Fast-but-lightweight&quot; version of the solver.

Algorithm features:
* O(N^3+M*N^2) complexity
* matrix is represented by its upper or lower triangle
* no additional time consuming features like condition number estimation

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    IsUpper -   what half of A is provided
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    A is is exactly  singular or is not positive definite.
                        B is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[0..N-1]:
                * overwritten by solution
                * zeros, if problem was not solved
ALGLIB: Copyright 17.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hpdmatrixsolvemfast(complex_2d_array a, ae_int_t n, <b>bool</b> isupper, complex_2d_array b, ae_int_t m, ae_int_t &amp;info);
</pre>
<a name=sub_rmatrixlusolve></a><h6 class=pageheader>rmatrixlusolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

This  subroutine  solves  a  system  A*x=b,  where A is NxN non-denegerate
real matrix given by its LU decomposition, x and b are real vectors.  This
is &quot;slow-but-robust&quot; version of the linear LU-based solver. Faster version
is RMatrixLUSolveFast() function.

Algorithm features:
* automatic detection of degenerate cases
* O(N^2) complexity
* condition number estimation

No iterative refinement  is provided because exact form of original matrix
is not known to subroutine. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which results in 10-15x  performance  penalty  when  compared
           ! with &quot;fast&quot; version which just calls triangular solver.
           !
           ! This performance penalty is insignificant  when compared with
           ! cost of large LU decomposition.  However,  if you  call  this
           ! function many times for the same  left  side,  this  overhead
           ! BECOMES significant. It  also  becomes significant for small-
           ! scale problems.
           !
           ! In such cases we strongly recommend you to use faster solver,
           ! RMatrixLUSolveFast() function.

Inputs:
    LUA     -   array[N,N], LU decomposition, RMatrixLU result
    P       -   array[N], pivots array, RMatrixLU result
    N       -   size of A
    B       -   array[N], right part

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixlusolve(real_2d_array lua, integer_1d_array p, ae_int_t n, real_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, real_1d_array &amp;x);
</pre>
<a name=sub_rmatrixlusolvefast></a><h6 class=pageheader>rmatrixlusolvefast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

This  subroutine  solves  a  system  A*x=b,  where A is NxN non-denegerate
real matrix given by its LU decomposition, x and b are real vectors.  This
is &quot;fast-without-any-checks&quot; version of the linear LU-based solver. Slower
but more robust version is RMatrixLUSolve() function.

Algorithm features:
* O(N^2) complexity
* fast algorithm without ANY additional checks, just triangular solver

Inputs:
    LUA     -   array[0..N-1,0..N-1], LU decomposition, RMatrixLU result
    P       -   array[0..N-1], pivots array, RMatrixLU result
    N       -   size of A
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is exactly singular (ill conditioned matrices
                        are not recognized).
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[N]:
                * info &gt; 0  &rArr; overwritten by solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 18.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixlusolvefast(real_2d_array lua, integer_1d_array p, ae_int_t n, real_1d_array b, ae_int_t &amp;info);
</pre>
<a name=sub_rmatrixlusolvem></a><h6 class=pageheader>rmatrixlusolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

Similar to RMatrixLUSolve() but solves  task  with  multiple  right  parts
(where b and x are NxM matrices). This  is  &quot;robust-but-slow&quot;  version  of
LU-based solver which performs additional  checks  for  non-degeneracy  of
inputs (condition number estimation). If you need  best  performance,  use
&quot;fast-without-any-checks&quot; version, RMatrixLUSolveMFast().

Algorithm features:
* automatic detection of degenerate cases
* O(M*N^2) complexity
* condition number estimation

No iterative refinement  is provided because exact form of original matrix
is not known to subroutine. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which  results  in  significant  performance   penalty   when
           ! compared with &quot;fast&quot;  version  which  just  calls  triangular
           ! solver.
           !
           ! This performance penalty is especially apparent when you  use
           ! ALGLIB parallel capabilities (condition number estimation  is
           ! inherently  sequential).  It   also   becomes significant for
           ! small-scale problems.
           !
           ! In such cases we strongly recommend you to use faster solver,
           ! RMatrixLUSolveMFast() function.

Inputs:
    LUA     -   array[N,N], LU decomposition, RMatrixLU result
    P       -   array[N], pivots array, RMatrixLU result
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N,M], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixlusolvem(real_2d_array lua, integer_1d_array p, ae_int_t n, real_2d_array b, ae_int_t m, ae_int_t &amp;info, densesolverreport &amp;rep, real_2d_array &amp;x);
</pre>
<a name=sub_rmatrixlusolvemfast></a><h6 class=pageheader>rmatrixlusolvemfast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

Similar to RMatrixLUSolve() but solves  task  with  multiple  right parts,
where b and x are NxM matrices.  This is &quot;fast-without-any-checks&quot; version
of LU-based solver. It does not estimate  condition number  of  a  system,
so it is extremely fast. If you need better detection  of  near-degenerate
cases, use RMatrixLUSolveM() function.

Algorithm features:
* O(M*N^2) complexity
* fast algorithm without ANY additional checks, just triangular solver

Inputs:
    LUA     -   array[0..N-1,0..N-1], LU decomposition, RMatrixLU result
    P       -   array[0..N-1], pivots array, RMatrixLU result
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    matrix is exactly singular (ill conditioned matrices
                        are not recognized).
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[N,M]:
                * info &gt; 0  &rArr; overwritten by solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 18.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixlusolvemfast(real_2d_array lua, integer_1d_array p, ae_int_t n, real_2d_array b, ae_int_t m, ae_int_t &amp;info);
</pre>
<a name=sub_rmatrixmixedsolve></a><h6 class=pageheader>rmatrixmixedsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

This  subroutine  solves  a  system  A*x=b,  where BOTH ORIGINAL A AND ITS
LU DECOMPOSITION ARE KNOWN. You can use it if for some  reasons  you  have
both A and its LU decomposition.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* iterative refinement
* O(N^2) complexity

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    LUA     -   array[0..N-1,0..N-1], LU decomposition, RMatrixLU result
    P       -   array[0..N-1], pivots array, RMatrixLU result
    N       -   size of A
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixmixedsolve(real_2d_array a, real_2d_array lua, integer_1d_array p, ae_int_t n, real_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, real_1d_array &amp;x);
</pre>
<a name=sub_rmatrixmixedsolvem></a><h6 class=pageheader>rmatrixmixedsolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

Similar to RMatrixMixedSolve() but  solves task with multiple right  parts
(where b and x are NxM matrices).

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* iterative refinement
* O(M*N^2) complexity

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    LUA     -   array[0..N-1,0..N-1], LU decomposition, RMatrixLU result
    P       -   array[0..N-1], pivots array, RMatrixLU result
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N,M], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixmixedsolvem(real_2d_array a, real_2d_array lua, integer_1d_array p, ae_int_t n, real_2d_array b, ae_int_t m, ae_int_t &amp;info, densesolverreport &amp;rep, real_2d_array &amp;x);
</pre>
<a name=sub_rmatrixsolve></a><h6 class=pageheader>rmatrixsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*x=b with N*N real matrix A and N*1 real vectorx  x  and
b. This is &quot;slow-but-feature rich&quot; version of the  linear  solver.  Faster
version is RMatrixSolveFast() function.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* iterative refinement
* O(N^3) complexity

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear  system
           ! and  performs  iterative   refinement,   which   results   in
           ! significant performance penalty  when  compared  with  &quot;fast&quot;
           ! version  which  just  performs  LU  decomposition  and  calls
           ! triangular solver.
           !
           ! This  performance  penalty  is  especially  visible  in   the
           ! multithreaded mode, because both condition number  estimation
           ! and   iterative    refinement   are   inherently   sequential
           ! calculations. It is also very significant on small matrices.
           !
           ! Thus, if you need high performance and if you are pretty sure
           ! that your system is well conditioned, we  strongly  recommend
           ! you to use faster solver, RMatrixSolveFast() function.

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or exactly singular.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixsolve(real_2d_array a, ae_int_t n, real_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, real_1d_array &amp;x);
</pre>
<a name=sub_rmatrixsolvefast></a><h6 class=pageheader>rmatrixsolvefast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

This  subroutine  solves  a  system  A*x=b,  where A is NxN non-denegerate
real matrix, x  and  b  are  vectors.  This is a &quot;fast&quot; version of  linear
solver which does NOT provide  any  additional  functions  like  condition
number estimation or iterative refinement.

Algorithm features:
* efficient algorithm O(N^3) complexity
* no performance overhead from additional functionality

If you need condition number estimation or iterative refinement, use  more
feature-rich version - RMatrixSolve().

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is exactly singular (ill conditioned matrices
                        are not recognized).
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[N]:
                * info &gt; 0  &rArr; overwritten by solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 16.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixsolvefast(real_2d_array a, ae_int_t n, real_1d_array b, ae_int_t &amp;info);
</pre>
<a name=sub_rmatrixsolvels></a><h6 class=pageheader>rmatrixsolvels Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

This subroutine finds solution of the linear system A*X=B with non-square,
possibly degenerate A.  System  is  solved in the least squares sense, and
general least squares solution  X = X0 + CX*y  which  minimizes |A*X-B| is
returned. If A is non-degenerate, solution in the usual sense is returned.

Algorithm features:
* automatic detection (and correct handling!) of degenerate cases
* iterative refinement
* O(N^3) complexity

Inputs:
    A       -   array[0..NRows-1,0..NCols-1], system matrix
    NRows   -   vertical size of A
    NCols   -   horizontal size of A
    B       -   array[0..NCols-1], right part
    Threshold-  a number in [0,1]. Singular values  beyond  Threshold  are
                considered  zero.  Set  it to 0.0, if you don't understand
                what it means, so the solver will choose good value on its
                own.

Outputs:
    Info    -   return code:
                * -4    SVD subroutine failed
                * -1    if NRows &le; 0 or NCols &le; 0 or Threshold &lt; 0 was passed
                *  1    if task is solved
    Rep     -   solver report, see below for more info
    X       -   array[0..N-1,0..M-1], it contains:
                * solution of A*X=B (even for singular A)
                * zeros, if SVD subroutine failed

SOLVER REPORT

Subroutine sets following fields of the Rep structure:
* R2        reciprocal of condition number: 1/cond(A), 2-norm.
* N         = NCols
* K         dim(Null(A))
* CX        array[0..N-1,0..K-1], kernel of A.
            Columns of CX store such vectors that A*CX[i]=0.
ALGLIB: Copyright 24.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixsolvels(real_2d_array a, ae_int_t nrows, ae_int_t ncols, real_1d_array b, <b>double</b> threshold, ae_int_t &amp;info, densesolverlsreport &amp;rep, real_1d_array &amp;x);
</pre>
<a name=sub_rmatrixsolvem></a><h6 class=pageheader>rmatrixsolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

Similar to RMatrixSolve() but solves task with multiple right parts (where
b and x are NxM matrices). This is  &quot;slow-but-robust&quot;  version  of  linear
solver with additional functionality  like  condition  number  estimation.
There also exists faster version - RMatrixSolveMFast().

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* optional iterative refinement
* O(N^3+M*N^2) complexity

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear  system
           ! and  performs  iterative   refinement,   which   results   in
           ! significant performance penalty  when  compared  with  &quot;fast&quot;
           ! version  which  just  performs  LU  decomposition  and  calls
           ! triangular solver.
           !
           ! This  performance  penalty  is  especially  visible  in   the
           ! multithreaded mode, because both condition number  estimation
           ! and   iterative    refinement   are   inherently   sequential
           ! calculations. It also very significant on small matrices.
           !
           ! Thus, if you need high performance and if you are pretty sure
           ! that your system is well conditioned, we  strongly  recommend
           ! you to use faster solver, RMatrixSolveMFast() function.

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size
    RFS     -   iterative refinement switch:
                * True - refinement is used.
                  Less performance, more precision.
                * False - refinement is not used.
                  More performance, less precision.

Outputs:
    Info    -   return code:
                * -3    A is ill conditioned or singular.
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixsolvem(real_2d_array a, ae_int_t n, real_2d_array b, ae_int_t m, <b>bool</b> rfs, ae_int_t &amp;info, densesolverreport &amp;rep, real_2d_array &amp;x);
</pre>
<a name=sub_rmatrixsolvemfast></a><h6 class=pageheader>rmatrixsolvemfast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver.

Similar to RMatrixSolve() but solves task with multiple right parts (where
b and x are NxM matrices). This is &quot;fast&quot; version of linear  solver  which
does NOT offer additional functions like condition  number  estimation  or
iterative refinement.

Algorithm features:
* O(N^3+M*N^2) complexity
* no additional functionality, highest performance

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size
    RFS     -   iterative refinement switch:
                * True - refinement is used.
                  Less performance, more precision.
                * False - refinement is not used.
                  More performance, less precision.

Outputs:
    Info    -   return code:
                * -3    matrix is exactly singular (ill conditioned matrices
                        are not recognized).
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    B       -   array[N]:
                * info &gt; 0  &rArr; overwritten by solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rmatrixsolvemfast(real_2d_array a, ae_int_t n, real_2d_array b, ae_int_t m, ae_int_t &amp;info);
</pre>
<a name=sub_spdmatrixcholeskysolve></a><h6 class=pageheader>spdmatrixcholeskysolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*x=b with N*N symmetric positive definite matrix A given
by its Cholesky decomposition, and N*1 real vectors x and b. This is &quot;slow-
but-feature-rich&quot;  version  of  the  solver  which,  in  addition  to  the
solution, performs condition number estimation.

Algorithm features:
* automatic detection of degenerate cases
* O(N^2) complexity
* condition number estimation
* matrix is represented by its upper or lower triangle

No iterative refinement is provided because such partial representation of
matrix does not allow efficient calculation of extra-precise  matrix-vector
products for large matrices. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which results in 10-15x  performance  penalty  when  compared
           ! with &quot;fast&quot; version which just calls triangular solver.
           !
           ! This performance penalty is insignificant  when compared with
           ! cost of large LU decomposition.  However,  if you  call  this
           ! function many times for the same  left  side,  this  overhead
           ! BECOMES significant. It  also  becomes significant for small-
           ! scale problems (N &lt; 50).
           !
           ! In such cases we strongly recommend you to use faster solver,
           ! SPDMatrixCholeskySolveFast() function.

Inputs:
    CHA     -   array[N,N], Cholesky decomposition,
                SPDMatrixCholesky result
    N       -   size of A
    IsUpper -   what half of CHA is provided
    B       -   array[N], right part

Outputs:
    Info    -   return code:
                * -3    A is is exactly singular or ill conditioned
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N]:
                * for info &gt; 0  - solution
                * for info = -3 - filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixcholeskysolve(real_2d_array cha, ae_int_t n, <b>bool</b> isupper, real_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, real_1d_array &amp;x);
</pre>
<a name=sub_spdmatrixcholeskysolvefast></a><h6 class=pageheader>spdmatrixcholeskysolvefast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*x=b with N*N symmetric positive definite matrix A given
by its Cholesky decomposition, and N*1 real vectors x and b. This is &quot;fast-
but-lightweight&quot; version of the solver.

Algorithm features:
* O(N^2) complexity
* matrix is represented by its upper or lower triangle
* no additional features

Inputs:
    CHA     -   array[N,N], Cholesky decomposition,
                SPDMatrixCholesky result
    N       -   size of A
    IsUpper -   what half of CHA is provided
    B       -   array[N], right part

Outputs:
    Info    -   return code:
                * -3    A is is exactly singular or ill conditioned
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task is solved
    B       -   array[N]:
                * for info &gt; 0  - overwritten by solution
                * for info = -3 - filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixcholeskysolvefast(real_2d_array cha, ae_int_t n, <b>bool</b> isupper, real_1d_array b, ae_int_t &amp;info);
</pre>
<a name=sub_spdmatrixcholeskysolvem></a><h6 class=pageheader>spdmatrixcholeskysolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B with N*N symmetric positive definite matrix A given
by its Cholesky decomposition, and N*M vectors X and B. It  is  &quot;slow-but-
feature-rich&quot; version of the solver which estimates  condition  number  of
the system.

Algorithm features:
* automatic detection of degenerate cases
* O(M*N^2) complexity
* condition number estimation
* matrix is represented by its upper or lower triangle

No iterative refinement is provided because such partial representation of
matrix does not allow efficient calculation of extra-precise  matrix-vector
products for large matrices. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which  results  in  significant  performance   penalty   when
           ! compared with &quot;fast&quot;  version  which  just  calls  triangular
           ! solver. Amount of  overhead  introduced  depends  on  M  (the
           ! larger - the more efficient).
           !
           ! This performance penalty is insignificant  when compared with
           ! cost of large LU decomposition.  However,  if you  call  this
           ! function many times for the same  left  side,  this  overhead
           ! BECOMES significant. It  also  becomes significant for small-
           ! scale problems (N &lt; 50).
           !
           ! In such cases we strongly recommend you to use faster solver,
           ! SPDMatrixCholeskySolveMFast() function.

Inputs:
    CHA     -   array[0..N-1,0..N-1], Cholesky decomposition,
                SPDMatrixCholesky result
    N       -   size of CHA
    IsUpper -   what half of CHA is provided
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    A is is exactly singular or badly conditioned
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task was solved
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N]:
                * for info &gt; 0 contains solution
                * for info = -3 filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixcholeskysolvem(real_2d_array cha, ae_int_t n, <b>bool</b> isupper, real_2d_array b, ae_int_t m, ae_int_t &amp;info, densesolverreport &amp;rep, real_2d_array &amp;x);
</pre>
<a name=sub_spdmatrixcholeskysolvemfast></a><h6 class=pageheader>spdmatrixcholeskysolvemfast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B with N*N symmetric positive definite matrix A given
by its Cholesky decomposition, and N*M vectors X and B. It  is  &quot;fast-but-
lightweight&quot; version of  the  solver  which  just  solves  linear  system,
without any additional functions.

Algorithm features:
* O(M*N^2) complexity
* matrix is represented by its upper or lower triangle
* no additional functionality

Inputs:
    CHA     -   array[N,N], Cholesky decomposition,
                SPDMatrixCholesky result
    N       -   size of CHA
    IsUpper -   what half of CHA is provided
    B       -   array[N,M], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    A is is exactly singular or badly conditioned
                        X is filled by zeros in such cases.
                * -1    N &le; 0 was passed
                *  1    task was solved
    B       -   array[N]:
                * for info &gt; 0 overwritten by solution
                * for info = -3 filled by zeros
ALGLIB: Copyright 18.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixcholeskysolvemfast(real_2d_array cha, ae_int_t n, <b>bool</b> isupper, real_2d_array b, ae_int_t m, ae_int_t &amp;info);
</pre>
<a name=sub_spdmatrixsolve></a><h6 class=pageheader>spdmatrixsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense linear solver for A*x=b with N*N real  symmetric  positive  definite
matrix A,  N*1 vectors x and b.  &quot;Slow-but-feature-rich&quot;  version  of  the
solver.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* O(N^3) complexity
* matrix is represented by its upper or lower triangle

No iterative refinement is provided because such partial representation of
matrix does not allow efficient calculation of extra-precise  matrix-vector
products for large matrices. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which  results  in  significant   performance   penalty  when
           ! compared with &quot;fast&quot; version  which  just  performs  Cholesky
           ! decomposition and calls triangular solver.
           !
           ! This  performance  penalty  is  especially  visible  in   the
           ! multithreaded mode, because both condition number  estimation
           ! and   iterative    refinement   are   inherently   sequential
           ! calculations.
           !
           ! Thus, if you need high performance and if you are pretty sure
           ! that your system is well conditioned, we  strongly  recommend
           ! you to use faster solver, SPDMatrixSolveFast() function.

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    IsUpper -   what half of A is provided
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or non-SPD.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixsolve(real_2d_array a, ae_int_t n, <b>bool</b> isupper, real_1d_array b, ae_int_t &amp;info, densesolverreport &amp;rep, real_1d_array &amp;x);
</pre>
<a name=sub_spdmatrixsolvefast></a><h6 class=pageheader>spdmatrixsolvefast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense linear solver for A*x=b with N*N real  symmetric  positive  definite
matrix A,  N*1 vectors x and  b.  &quot;Fast-but-lightweight&quot;  version  of  the
solver.

Algorithm features:
* O(N^3) complexity
* matrix is represented by its upper or lower triangle
* no additional time consuming features like condition number estimation

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    IsUpper -   what half of A is provided
    B       -   array[0..N-1], right part

Outputs:
    Info    -   return code:
                * -3    A is is exactly singular or non-SPD
                * -1    N &le; 0 was passed
                *  1    task was solved
    B       -   array[N], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 17.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixsolvefast(real_2d_array a, ae_int_t n, <b>bool</b> isupper, real_1d_array b, ae_int_t &amp;info);
</pre>
<a name=sub_spdmatrixsolvem></a><h6 class=pageheader>spdmatrixsolvem Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B with N*N symmetric positive definite matrix A,  and
N*M vectors X and B. It is &quot;slow-but-feature-rich&quot; version of the solver.

Algorithm features:
* automatic detection of degenerate cases
* condition number estimation
* O(N^3+M*N^2) complexity
* matrix is represented by its upper or lower triangle

No iterative refinement is provided because such partial representation of
matrix does not allow efficient calculation of extra-precise  matrix-vector
products for large matrices. Use RMatrixSolve or RMatrixMixedSolve  if  you
need iterative refinement.

IMPORTANT: ! this function is NOT the most efficient linear solver provided
           ! by ALGLIB. It estimates condition  number  of  linear system,
           ! which  results  in  significant   performance   penalty  when
           ! compared with &quot;fast&quot; version  which  just  performs  Cholesky
           ! decomposition and calls triangular solver.
           !
           ! This  performance  penalty  is  especially  visible  in   the
           ! multithreaded mode, because both condition number  estimation
           ! and   iterative    refinement   are   inherently   sequential
           ! calculations.
           !
           ! Thus, if you need high performance and if you are pretty sure
           ! that your system is well conditioned, we  strongly  recommend
           ! you to use faster solver, SPDMatrixSolveMFast() function.

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    IsUpper -   what half of A is provided
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    matrix is very badly conditioned or non-SPD.
                * -1    N &le; 0 was passed
                *  1    task is solved (but matrix A may be ill-conditioned,
                        check R1/RInf parameters for condition numbers).
    Rep     -   additional report, following fields are set:
                * rep.r1    condition number in 1-norm
                * rep.rinf  condition number in inf-norm
    X       -   array[N,M], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 27.01.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixsolvem(real_2d_array a, ae_int_t n, <b>bool</b> isupper, real_2d_array b, ae_int_t m, ae_int_t &amp;info, densesolverreport &amp;rep, real_2d_array &amp;x);
</pre>
<a name=sub_spdmatrixsolvemfast></a><h6 class=pageheader>spdmatrixsolvemfast Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dense solver for A*X=B with N*N symmetric positive definite matrix A,  and
N*M vectors X and B. It is &quot;fast-but-lightweight&quot; version of the solver.

Algorithm features:
* O(N^3+M*N^2) complexity
* matrix is represented by its upper or lower triangle
* no additional time consuming features

Inputs:
    A       -   array[0..N-1,0..N-1], system matrix
    N       -   size of A
    IsUpper -   what half of A is provided
    B       -   array[0..N-1,0..M-1], right part
    M       -   right part size

Outputs:
    Info    -   return code:
                * -3    A is is exactly singular
                * -1    N &le; 0 was passed
                *  1    task was solved
    B       -   array[N,M], it contains:
                * info &gt; 0  &rArr; solution
                * info = -3 &rArr; filled by zeros
ALGLIB: Copyright 17.03.2015 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spdmatrixsolvemfast(real_2d_array a, ae_int_t n, <b>bool</b> isupper, real_2d_array b, ae_int_t m, ae_int_t &amp;info);
</pre>
<a name=unit_directsparsesolvers></a><h4 class=pageheader>8.9.2. directsparsesolvers Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_sparsesolverreport class=toc>sparsesolverreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_sparselusolve class=toc>sparselusolve</a> |
<a href=#sub_sparsesolve class=toc>sparsesolve</a> |
<a href=#sub_sparsespdcholeskysolve class=toc>sparsespdcholeskysolve</a> |
<a href=#sub_sparsespdsolve class=toc>sparsespdsolve</a> |
<a href=#sub_sparsespdsolvesks class=toc>sparsespdsolvesks</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_solvesks_d_1 class=toc>solvesks_d_1</a></td><td width=15>&nbsp;</td><td>Solving positive definite sparse system using Skyline (SKS) solver</td></tr>
</table>
</div>
<a name=struct_sparsesolverreport></a><h6 class=pageheader>sparsesolverreport Class</h6>
<hr width=600 align=left>
<pre class=narration>
This structure is a sparse solver report.

Following fields can be accessed by users:
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> sparsesolverreport {
   ae_int_t terminationtype;
};
</pre>
<a name=sub_sparselusolve></a><h6 class=pageheader>sparselusolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse linear solver for A*x=b with general (nonsymmetric) N*N sparse real
matrix A given by its LU factorization, N*1 vectors x and b.

IMPORTANT: this solver requires input matrix  to  be  in  the  CRS  sparse
           storage format. An exception will  be  generated  if  you  pass
           matrix in some other format (HASH or SKS).

Inputs:
    A       -   LU factorization of the sparse matrix, must be NxN exactly
                in CRS storage format
    P, Q    -   pivot indexes from LU factorization
    B       -   array[0..N-1], right part

Outputs:
    X       -   array[N], it contains:
                * rep.terminationtype &gt; 0    &rArr;  solution
                * rep.terminationtype=-3   &rArr;  filled by zeros
    Rep     -   solver report, following fields are set:
                * rep.terminationtype - solver status; &gt; 0 for success,
                  set to -3 on failure (degenerate system).
ALGLIB: Copyright 26.12.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparselusolve(sparsematrix a, integer_1d_array p, integer_1d_array q, real_1d_array b, real_1d_array &amp;x, sparsesolverreport &amp;rep);
</pre>
<a name=sub_sparsesolve></a><h6 class=pageheader>sparsesolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse linear solver for A*x=b with general (nonsymmetric) N*N sparse real
matrix A, N*1 vectors x and b.

This solver converts input matrix to CRS format, performs LU factorization
and uses sparse triangular solvers to get solution of the original system.

Inputs:
    A       -   sparse matrix, must be NxN exactly, any storage format
    B       -   array[0..N-1], right part

Outputs:
    X       -   array[N], it contains:
                * rep.terminationtype &gt; 0    &rArr;  solution
                * rep.terminationtype=-3   &rArr;  filled by zeros
    Rep     -   solver report, following fields are set:
                * rep.terminationtype - solver status; &gt; 0 for success,
                  set to -3 on failure (degenerate system).
ALGLIB: Copyright 26.12.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsesolve(sparsematrix a, real_1d_array b, real_1d_array &amp;x, sparsesolverreport &amp;rep);
</pre>
<a name=sub_sparsespdcholeskysolve></a><h6 class=pageheader>sparsespdcholeskysolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse linear solver for A*x=b with N*N real  symmetric  positive definite
matrix A given by its Cholesky decomposition, and N*1 vectors x and b.

IMPORTANT: this solver requires input matrix to be in  the  SKS  (Skyline)
           or CRS (compressed row storage) format. An  exception  will  be
           generated if you pass matrix in some other format.

Inputs:
    A       -   sparse NxN matrix stored in CRS or SKS format, must be NxN
                exactly
    IsUpper -   which half of A is provided (another half is ignored)
    B       -   array[N], right part

Outputs:
    X       -   array[N], it contains:
                * rep.terminationtype &gt; 0    &rArr;  solution
                * rep.terminationtype=-3   &rArr;  filled by zeros
    Rep     -   solver report, following fields are set:
                * rep.terminationtype - solver status; &gt; 0 for success,
                  set to -3 on failure (degenerate or non-SPD system).
ALGLIB: Copyright 26.12.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsespdcholeskysolve(sparsematrix a, <b>bool</b> isupper, real_1d_array b, real_1d_array &amp;x, sparsesolverreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_solvesks_d_1 class=nav>solvesks_d_1</a> ]</p>
<a name=sub_sparsespdsolve></a><h6 class=pageheader>sparsespdsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse linear solver for A*x=b with N*N  sparse  real  symmetric  positive
definite matrix A, N*1 vectors x and b.

This solver  converts  input  matrix  to  CRS  format,  performs  Cholesky
factorization using supernodal Cholesky  decomposition  with  permutation-
reducing ordering and uses sparse triangular solver to get solution of the
original system.

Inputs:
    A       -   sparse matrix, must be NxN exactly
    IsUpper -   which half of A is provided (another half is ignored)
    B       -   array[N], right part

Outputs:
    X       -   array[N], it contains:
                * rep.terminationtype &gt; 0  &rArr;  solution
                * rep.terminationtype=-3   &rArr;  filled by zeros
    Rep     -   solver report, following fields are set:
                * rep.terminationtype - solver status; &gt; 0 for success,
                  set to -3 on failure (degenerate or non-SPD system).
ALGLIB: Copyright 26.12.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsespdsolve(sparsematrix a, <b>bool</b> isupper, real_1d_array b, real_1d_array &amp;x, sparsesolverreport &amp;rep);
</pre>
<a name=sub_sparsespdsolvesks></a><h6 class=pageheader>sparsespdsolvesks Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sparse linear solver for A*x=b with N*N  sparse  real  symmetric  positive
definite matrix A, N*1 vectors x and b.

This solver  converts  input  matrix  to  SKS  format,  performs  Cholesky
factorization using  SKS  Cholesky  subroutine  (works  well  for  limited
bandwidth matrices) and uses sparse triangular solvers to get solution  of
the original system.

Inputs:
    A       -   sparse matrix, must be NxN exactly
    IsUpper -   which half of A is provided (another half is ignored)
    B       -   array[0..N-1], right part

Outputs:
    X       -   array[N], it contains:
                * rep.terminationtype &gt; 0    &rArr;  solution
                * rep.terminationtype=-3   &rArr;  filled by zeros
    Rep     -   solver report, following fields are set:
                * rep.terminationtype - solver status; &gt; 0 for success,
                  set to -3 on failure (degenerate or non-SPD system).
ALGLIB: Copyright 26.12.2017 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sparsespdsolvesks(sparsematrix a, <b>bool</b> isupper, real_1d_array b, real_1d_array &amp;x, sparsesolverreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_solvesks_d_1 class=nav>solvesks_d_1</a> ]</p>
<a name=example_solvesks_d_1></a><h6 class=pageheader>solvesks_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Solvers.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example demonstrates creation/initialization of the sparse matrix</font>
<font color=navy>// in the SKS (Skyline) storage format and solution using SKS-based direct</font>
<font color=navy>// solver.</font>
<font color=navy>//</font>
<font color=navy>// First, we have to create matrix and initialize it. Matrix is created</font>
<font color=navy>// in the SKS format, using fixed bandwidth initialization function.</font>
<font color=navy>// Several points should be noted:</font>
<font color=navy>//</font>
<font color=navy>// 1. SKS sparse storage format also allows variable bandwidth matrices;</font>
<font color=navy>//    we just <b>do</b> not want to overcomplicate this example.</font>
<font color=navy>//</font>
<font color=navy>// 2. SKS format requires you to specify matrix geometry prior to</font>
<font color=navy>//    initialization of its elements with sparseset(). If you specified</font>
<font color=navy>//    bandwidth=1, you can not change your mind afterwards and call</font>
<font color=navy>//    sparseset() <b>for</b> non-existent elements.</font>
<font color=navy>// </font>
<font color=navy>// 3. Because SKS solver need just one triangle of SPD matrix, we can</font>
<font color=navy>//    omit initialization of the lower triangle of our matrix.</font>
   ae_int_t n = 4;
   ae_int_t bandwidth = 1;
   sparsematrix s;
   sparsecreatesksband(n, n, bandwidth, s);
   sparseset(s, 0, 0, 2.0);
   sparseset(s, 0, 1, 1.0);
   sparseset(s, 1, 1, 3.0);
   sparseset(s, 1, 2, 1.0);
   sparseset(s, 2, 2, 3.0);
   sparseset(s, 2, 3, 1.0);
   sparseset(s, 3, 3, 2.0);
<font color=navy>// Now we have symmetric positive definite 4x4 system width bandwidth=1:</font>
<font color=navy>//</font>
<font color=navy>//     [ 2 1     ]   [ x0]]   [  4 ]</font>
<font color=navy>//     [ 1 3 1   ]   [ x1 ]   [ 10 ]</font>
<font color=navy>//     [   1 3 1 ] * [ x2 ] = [ 15 ]</font>
<font color=navy>//     [     1 2 ]   [ x3 ]   [ 11 ]</font>
<font color=navy>//</font>
<font color=navy>// After successful creation we can call SKS solver.</font>
   real_1d_array b = <font color=blue><b>&quot;[4,10,15,11]&quot;</b></font>;
   sparsesolverreport rep;
   real_1d_array x;
   <b>bool</b> isuppertriangle = true;
   sparsespdsolvesks(s, isuppertriangle, b, x, rep);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(4).c_str()); <font color=navy>// EXPECTED: [1.0000, 2.0000, 3.0000, 4.0000]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_lincg></a><h4 class=pageheader>8.9.3. lincg Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_lincgreport class=toc>lincgreport</a> |
<a href=#struct_lincgstate class=toc>lincgstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_lincgcreate class=toc>lincgcreate</a> |
<a href=#sub_lincgresults class=toc>lincgresults</a> |
<a href=#sub_lincgsetb class=toc>lincgsetb</a> |
<a href=#sub_lincgsetcond class=toc>lincgsetcond</a> |
<a href=#sub_lincgsetprecdiag class=toc>lincgsetprecdiag</a> |
<a href=#sub_lincgsetprecunit class=toc>lincgsetprecunit</a> |
<a href=#sub_lincgsetrestartfreq class=toc>lincgsetrestartfreq</a> |
<a href=#sub_lincgsetrupdatefreq class=toc>lincgsetrupdatefreq</a> |
<a href=#sub_lincgsetstartingpoint class=toc>lincgsetstartingpoint</a> |
<a href=#sub_lincgsetxrep class=toc>lincgsetxrep</a> |
<a href=#sub_lincgsolvesparse class=toc>lincgsolvesparse</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_lincg_d_1 class=toc>lincg_d_1</a></td><td width=15>&nbsp;</td><td>Solution of sparse linear systems with CG</td></tr>
</table>
</div>
<a name=struct_lincgreport></a><h6 class=pageheader>lincgreport Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> lincgreport {
   ae_int_t iterationscount;
   ae_int_t nmv;
   ae_int_t terminationtype;
   <b>double</b> r2;
};
</pre>
<a name=struct_lincgstate></a><h6 class=pageheader>lincgstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores state of the linear CG method.
You should use ALGLIB functions to work with this object.
Never try to access its fields directly!
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> lincgstate {
};
</pre>
<a name=sub_lincgcreate></a><h6 class=pageheader>lincgcreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function initializes linear CG Solver. This solver is used  to  solve
symmetric positive definite problems. If you want  to  solve  nonsymmetric
(or non-positive definite) problem you may use LinLSQR solver provided  by
ALGLIB.

USAGE:
1. User initializes algorithm state with LinCGCreate() call
2. User tunes solver parameters with  LinCGSetCond() and other functions
3. Optionally, user sets starting point with LinCGSetStartingPoint()
4. User  calls LinCGSolveSparse() function which takes algorithm state and
   SparseMatrix object.
5. User calls LinCGResults() to get solution
6. Optionally, user may call LinCGSolveSparse()  again  to  solve  another
   problem  with different matrix and/or right part without reinitializing
   LinCGState structure.

Inputs:
    N       -   problem dimension, N &gt; 0

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 14.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgcreate(ae_int_t n, lincgstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lincg_d_1 class=nav>lincg_d_1</a> ]</p>
<a name=sub_lincgresults></a><h6 class=pageheader>lincgresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
CG-solver: results.

This function must be called after LinCGSolve

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[N], solution
    Rep     -   optimization report:
                * Rep.TerminationType completetion code:
                    * -5    input matrix is either not positive definite,
                            too large or too small
                    * -4    overflow/underflow during solution
                            (ill conditioned problem)
                    *  1    ||residual|| &le; EpsF*||b||
                    *  5    MaxIts steps was taken
                    *  7    rounding errors prevent further progress,
                            best point found is returned
                * Rep.IterationsCount contains iterations count
                * NMV countains number of matrix-vector calculations
ALGLIB: Copyright 14.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgresults(lincgstate state, real_1d_array &amp;x, lincgreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lincg_d_1 class=nav>lincg_d_1</a> ]</p>
<a name=sub_lincgsetb></a><h6 class=pageheader>lincgsetb Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets right part. By default, right part is zero.

Inputs:
    B       -   right part, array[N].

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 14.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgsetb(lincgstate state, real_1d_array b);
</pre>
<a name=sub_lincgsetcond></a><h6 class=pageheader>lincgsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping criteria.

Inputs:
    EpsF    -   algorithm will be stopped if norm of residual is less than
                EpsF*||b||.
    MaxIts  -   algorithm will be stopped if number of iterations is  more
                than MaxIts.

Outputs:
    State   -   structure which stores algorithm state

NOTES:
If  both  EpsF  and  MaxIts  are  zero then small EpsF will be set to small
value.
ALGLIB: Copyright 14.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgsetcond(lincgstate state, <b>double</b> epsf, ae_int_t maxits);
</pre>

<a name=sub_lincgsetprecdiag></a><h6 class=pageheader>lincgsetprecdiag Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  changes  preconditioning  settings  of  LinCGSolveSparse()
function.  LinCGSolveSparse() will use diagonal of the  system  matrix  as
preconditioner. This preconditioning mode is active by default.

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 19.11.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgsetprecdiag(lincgstate state);
</pre>
<a name=sub_lincgsetprecunit></a><h6 class=pageheader>lincgsetprecunit Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  changes  preconditioning  settings  of  LinCGSolveSparse()
function. By default, SolveSparse() uses diagonal preconditioner,  but  if
you want to use solver without preconditioning, you can call this function
which forces solver to use unit matrix for preconditioning.

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 19.11.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgsetprecunit(lincgstate state);
</pre>
<a name=sub_lincgsetrestartfreq></a><h6 class=pageheader>lincgsetrestartfreq Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets restart frequency. By default, algorithm  is  restarted
after N subsequent iterations.
ALGLIB: Copyright 14.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgsetrestartfreq(lincgstate state, ae_int_t srf);
</pre>
<a name=sub_lincgsetrupdatefreq></a><h6 class=pageheader>lincgsetrupdatefreq Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets frequency of residual recalculations.

Algorithm updates residual r_k using iterative formula,  but  recalculates
it from scratch after each 10 iterations. It is done to avoid accumulation
of numerical errors and to stop algorithm when r_k starts to grow.

Such low update frequence (1/10) gives very  little  overhead,  but  makes
algorithm a bit more robust against numerical errors. However, you may
change it

Inputs:
    Freq    -   desired update frequency, Freq &ge; 0.
                Zero value means that no updates will be done.
ALGLIB: Copyright 14.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgsetrupdatefreq(lincgstate state, ae_int_t freq);
</pre>
<a name=sub_lincgsetstartingpoint></a><h6 class=pageheader>lincgsetstartingpoint Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets starting point.
By default, zero starting point is used.

Inputs:
    X       -   starting point, array[N]

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 14.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgsetstartingpoint(lincgstate state, real_1d_array x);
</pre>
<a name=sub_lincgsetxrep></a><h6 class=pageheader>lincgsetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to MinCGOptimize().
ALGLIB: Copyright 14.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgsetxrep(lincgstate state, <b>bool</b> needxrep);
</pre>
<a name=sub_lincgsolvesparse></a><h6 class=pageheader>lincgsolvesparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Procedure for solution of A*x=b with sparse A.

Inputs:
    State   -   algorithm state
    A       -   sparse matrix in the CRS format (you MUST contvert  it  to
                CRS format by calling SparseConvertToCRS() function).
    IsUpper -   whether upper or lower triangle of A is used:
                * IsUpper=True  &rArr; only upper triangle is used and lower
                                   triangle is not referenced at all
                * IsUpper=False &rArr; only lower triangle is used and upper
                                   triangle is not referenced at all
    B       -   right part, array[N]

Result:
    This function returns no result.
    You can get solution by calling LinCGResults()

NOTE: this function uses lightweight preconditioning -  multiplication  by
      inverse of diag(A). If you want, you can turn preconditioning off by
      calling LinCGSetPrecUnit(). However, preconditioning cost is low and
      preconditioner  is  very  important  for  solution  of  badly scaled
      problems.
ALGLIB: Copyright 14.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> lincgsolvesparse(lincgstate state, sparsematrix a, <b>bool</b> isupper, real_1d_array b);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_lincg_d_1 class=nav>lincg_d_1</a> ]</p>
<a name=example_lincg_d_1></a><h6 class=pageheader>lincg_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Solvers.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example illustrates solution of sparse linear systems with</font>
<font color=navy>// conjugate gradient method.</font>
<font color=navy>// </font>
<font color=navy>// Suppose that we have linear system A*x=b with sparse symmetric</font>
<font color=navy>// positive definite A (represented by sparsematrix object)</font>
<font color=navy>//         [ 5 1       ]</font>
<font color=navy>//         [ 1 7 2     ]</font>
<font color=navy>//     A = [   2 8 1   ]</font>
<font color=navy>//         [     1 4 1 ]</font>
<font color=navy>//         [       1 4 ]</font>
<font color=navy>// and right part b</font>
<font color=navy>//     [  7 ]</font>
<font color=navy>//     [ 17 ]</font>
<font color=navy>// b = [ 14 ]</font>
<font color=navy>//     [ 10 ]</font>
<font color=navy>//     [  6 ]</font>
<font color=navy>// and we want to solve this system using sparse linear CG. In order</font>
<font color=navy>// to <b>do</b> so, we have to create left part (sparsematrix object) and</font>
<font color=navy>// right part (dense array).</font>
<font color=navy>//</font>
<font color=navy>// Initially, sparse matrix is created in the Hash-Table format,</font>
<font color=navy>// which allows easy initialization, but <b>do</b> not allow matrix to be</font>
<font color=navy>// used in the linear solvers. So after construction you should convert</font>
<font color=navy>// sparse matrix to CRS format (one suited <b>for</b> linear operations).</font>
<font color=navy>//</font>
<font color=navy>// It is important to note that in our example we initialize full</font>
<font color=navy>// matrix A, both lower and upper triangles. However, it is symmetric</font>
<font color=navy>// and sparse solver needs just one half of the matrix. So you may</font>
<font color=navy>// save about half of the space by filling only one of the triangles.</font>
   sparsematrix a;
   sparsecreate(5, 5, a);
   sparseset(a, 0, 0, 5.0);
   sparseset(a, 0, 1, 1.0);
   sparseset(a, 1, 0, 1.0);
   sparseset(a, 1, 1, 7.0);
   sparseset(a, 1, 2, 2.0);
   sparseset(a, 2, 1, 2.0);
   sparseset(a, 2, 2, 8.0);
   sparseset(a, 2, 3, 1.0);
   sparseset(a, 3, 2, 1.0);
   sparseset(a, 3, 3, 4.0);
   sparseset(a, 3, 4, 1.0);
   sparseset(a, 4, 3, 1.0);
   sparseset(a, 4, 4, 4.0);
<font color=navy>// Now our matrix is fully initialized, but we have to <b>do</b> one more</font>
<font color=navy>// step - convert it from Hash-Table format to CRS format (see</font>
<font color=navy>// documentation on sparse matrices <b>for</b> more information about these</font>
<font color=navy>// formats).</font>
<font color=navy>//</font>
<font color=navy>// If you omit this call, ALGLIB will generate exception on the first</font>
<font color=navy>// attempt to use A in linear operations. </font>
   sparseconverttocrs(a);
<font color=navy>// Initialization of the right part</font>
   real_1d_array b = <font color=blue><b>&quot;[7,17,14,10,6]&quot;</b></font>;
<font color=navy>// Now we have to create linear solver object and to use it <b>for</b> the</font>
<font color=navy>// solution of the linear system.</font>
<font color=navy>//</font>
<font color=navy>// NOTE: lincgsolvesparse() accepts additional parameter which tells</font>
<font color=navy>//       what triangle of the symmetric matrix should be used - upper</font>
<font color=navy>//       or lower. Because we've filled both parts of the matrix, we</font>
<font color=navy>//       can use any part - upper or lower.</font>
   lincgstate s;
   lincgreport rep;
   real_1d_array x;
   lincgcreate(5, s);
   lincgsolvesparse(s, a, true, b);
   lincgresults(s, x, rep);

   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 1</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [1.000,2.000,1.000,2.000,1.000]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_linlsqr></a><h4 class=pageheader>8.9.4. linlsqr Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_linlsqrreport class=toc>linlsqrreport</a> |
<a href=#struct_linlsqrstate class=toc>linlsqrstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_linlsqrcreate class=toc>linlsqrcreate</a> |
<a href=#sub_linlsqrcreatebuf class=toc>linlsqrcreatebuf</a> |
<a href=#sub_linlsqrpeekiterationscount class=toc>linlsqrpeekiterationscount</a> |
<a href=#sub_linlsqrrequesttermination class=toc>linlsqrrequesttermination</a> |
<a href=#sub_linlsqrresults class=toc>linlsqrresults</a> |
<a href=#sub_linlsqrsetcond class=toc>linlsqrsetcond</a> |
<a href=#sub_linlsqrsetlambdai class=toc>linlsqrsetlambdai</a> |
<a href=#sub_linlsqrsetprecdiag class=toc>linlsqrsetprecdiag</a> |
<a href=#sub_linlsqrsetprecunit class=toc>linlsqrsetprecunit</a> |
<a href=#sub_linlsqrsetxrep class=toc>linlsqrsetxrep</a> |
<a href=#sub_linlsqrsolvesparse class=toc>linlsqrsolvesparse</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_linlsqr_d_1 class=toc>linlsqr_d_1</a></td><td width=15>&nbsp;</td><td>Solution of sparse linear systems with CG</td></tr>
</table>
</div>
<a name=struct_linlsqrreport></a><h6 class=pageheader>linlsqrreport Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> linlsqrreport {
   ae_int_t iterationscount;
   ae_int_t nmv;
   ae_int_t terminationtype;
};
</pre>
<a name=struct_linlsqrstate></a><h6 class=pageheader>linlsqrstate Class</h6>
<hr width=600 align=left>
<pre class=narration>
This object stores state of the LinLSQR method.
You should use ALGLIB functions to work with this object.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>class</b> linlsqrstate {
};
</pre>
<a name=sub_linlsqrcreate></a><h6 class=pageheader>linlsqrcreate Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function initializes linear LSQR Solver. This solver is used to solve
non-symmetric (and, possibly, non-square) problems. Least squares solution
is returned for non-compatible systems.

USAGE:
1. User initializes algorithm state with LinLSQRCreate() call
2. User tunes solver parameters with  LinLSQRSetCond() and other functions
3. User  calls  LinLSQRSolveSparse()  function which takes algorithm state
   and SparseMatrix object.
4. User calls LinLSQRResults() to get solution
5. Optionally, user may call LinLSQRSolveSparse() again to  solve  another
   problem  with different matrix and/or right part without reinitializing
   LinLSQRState structure.

Inputs:
    M       -   number of rows in A
    N       -   number of variables, N &gt; 0

Outputs:
    State   -   structure which stores algorithm state

NOTE: see also linlsqrcreatebuf()  for  version  which  reuses  previously
      allocated place as much as possible.
ALGLIB: Copyright 30.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrcreate(ae_int_t m, ae_int_t n, linlsqrstate &amp;state);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_linlsqr_d_1 class=nav>linlsqr_d_1</a> ]</p>
<a name=sub_linlsqrcreatebuf></a><h6 class=pageheader>linlsqrcreatebuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function initializes linear LSQR Solver.  It  provides  exactly  same
functionality as linlsqrcreate(), but reuses  previously  allocated  space
as much as possible.

Inputs:
    M       -   number of rows in A
    N       -   number of variables, N &gt; 0

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 14.11.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrcreatebuf(ae_int_t m, ae_int_t n, linlsqrstate state);
</pre>
<a name=sub_linlsqrpeekiterationscount></a><h6 class=pageheader>linlsqrpeekiterationscount Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function is used to peek into LSQR solver and get  current  iteration
counter. You can safely &quot;peek&quot; into the solver from another thread.

Inputs:
    S           -   solver object

Result:
    iteration counter, in [0,INF)
ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
ae_int_t linlsqrpeekiterationscount(linlsqrstate s);
</pre>
<a name=sub_linlsqrrequesttermination></a><h6 class=pageheader>linlsqrrequesttermination Function</h6>
<hr width=600 align=left>
<pre class=narration>
This subroutine submits request for termination of the running solver.  It
can be called from some other thread which wants LSQR solver to  terminate
(obviously, the  thread  running  LSQR  solver can not request termination
because it is already busy working on LSQR).

As result, solver  stops  at  point  which  was  &quot;current  accepted&quot;  when
termination  request  was  submitted  and returns error code 8 (successful
termination).  Such   termination   is  a smooth  process  which  properly
deallocates all temporaries.

Inputs:
    State   -   solver structure

NOTE: calling this function on solver which is NOT running  will  have  no
      effect.

NOTE: multiple calls to this function are possible. First call is counted,
      subsequent calls are silently ignored.

NOTE: solver clears termination flag on its start, it means that  if  some
      other thread will request termination too soon, its request will went
      unnoticed.
ALGLIB: Copyright 08.10.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrrequesttermination(linlsqrstate state);
</pre>
<a name=sub_linlsqrresults></a><h6 class=pageheader>linlsqrresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
LSQR solver: results.

This function must be called after LinLSQRSolve

Inputs:
    State   -   algorithm state

Outputs:
    X       -   array[N], solution
    Rep     -   optimization report:
                * Rep.TerminationType completetion code:
                    *  1    ||Rk|| &le; EpsB*||B||
                    *  4    ||A^T*Rk||/(||A||*||Rk||) &le; EpsA
                    *  5    MaxIts steps was taken
                    *  7    rounding errors prevent further progress,
                            X contains best point found so far.
                            (sometimes returned on singular systems)
                    *  8    user requested termination via calling
                            linlsqrrequesttermination()
                * Rep.IterationsCount contains iterations count
                * NMV countains number of matrix-vector calculations
ALGLIB: Copyright 30.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrresults(linlsqrstate state, real_1d_array &amp;x, linlsqrreport &amp;rep);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_linlsqr_d_1 class=nav>linlsqr_d_1</a> ]</p>
<a name=sub_linlsqrsetcond></a><h6 class=pageheader>linlsqrsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping criteria.

Inputs:
    EpsA    -   algorithm will be stopped if ||A^T*Rk||/(||A||*||Rk||) &le; EpsA.
    EpsB    -   algorithm will be stopped if ||Rk|| &le; EpsB*||B||
    MaxIts  -   algorithm will be stopped if number of iterations
                more than MaxIts.

Outputs:
    State   -   structure which stores algorithm state

NOTE: if EpsA,EpsB,EpsC and MaxIts are zero then these variables will
be setted as default values.
ALGLIB: Copyright 30.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrsetcond(linlsqrstate state, <b>double</b> epsa, <b>double</b> epsb, ae_int_t maxits);
</pre>
<a name=sub_linlsqrsetlambdai></a><h6 class=pageheader>linlsqrsetlambdai Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets optional Tikhonov regularization coefficient.
It is zero by default.

Inputs:
    LambdaI -   regularization factor, LambdaI &ge; 0

Outputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 30.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrsetlambdai(linlsqrstate state, <b>double</b> lambdai);
</pre>
<a name=sub_linlsqrsetprecdiag></a><h6 class=pageheader>linlsqrsetprecdiag Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  changes  preconditioning  settings  of  LinCGSolveSparse()
function.  LinCGSolveSparse() will use diagonal of the  system  matrix  as
preconditioner. This preconditioning mode is active by default.

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 19.11.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrsetprecdiag(linlsqrstate state);
</pre>
<a name=sub_linlsqrsetprecunit></a><h6 class=pageheader>linlsqrsetprecunit Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  function  changes  preconditioning  settings of LinLSQQSolveSparse()
function. By default, SolveSparse() uses diagonal preconditioner,  but  if
you want to use solver without preconditioning, you can call this function
which forces solver to use unit matrix for preconditioning.

Inputs:
    State   -   structure which stores algorithm state
ALGLIB: Copyright 19.11.2012 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrsetprecunit(linlsqrstate state);
</pre>
<a name=sub_linlsqrsetxrep></a><h6 class=pageheader>linlsqrsetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to MinCGOptimize().
ALGLIB: Copyright 30.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrsetxrep(linlsqrstate state, <b>bool</b> needxrep);
</pre>
<a name=sub_linlsqrsolvesparse></a><h6 class=pageheader>linlsqrsolvesparse Function</h6>
<hr width=600 align=left>
<pre class=narration>
Procedure for solution of A*x=b with sparse A.

Inputs:
    State   -   algorithm state
    A       -   sparse M*N matrix in the CRS format (you MUST contvert  it
                to CRS format  by  calling  SparseConvertToCRS()  function
                BEFORE you pass it to this function).
    B       -   right part, array[M]

Result:
    This function returns no result.
    You can get solution by calling LinCGResults()

NOTE: this function uses lightweight preconditioning -  multiplication  by
      inverse of diag(A). If you want, you can turn preconditioning off by
      calling LinLSQRSetPrecUnit(). However, preconditioning cost is   low
      and preconditioner is very important for solution  of  badly  scaled
      problems.
ALGLIB: Copyright 30.11.2011 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> linlsqrsolvesparse(linlsqrstate state, sparsematrix a, real_1d_array b);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_linlsqr_d_1 class=nav>linlsqr_d_1</a> ]</p>
<a name=example_linlsqr_d_1></a><h6 class=pageheader>linlsqr_d_1 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Solvers.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// This example illustrates solution of sparse linear least squares problem</font>
<font color=navy>// with LSQR algorithm.</font>
<font color=navy>// </font>
<font color=navy>// Suppose that we have least squares problem min|A*x-b| with sparse A</font>
<font color=navy>// represented by sparsematrix object</font>
<font color=navy>//         [ 1 1 ]</font>
<font color=navy>//         [ 1 1 ]</font>
<font color=navy>//     A = [ 2 1 ]</font>
<font color=navy>//         [ 1   ]</font>
<font color=navy>//         [   1 ]</font>
<font color=navy>// and right part b</font>
<font color=navy>//     [ 4 ]</font>
<font color=navy>//     [ 2 ]</font>
<font color=navy>// b = [ 4 ]</font>
<font color=navy>//     [ 1 ]</font>
<font color=navy>//     [ 2 ]</font>
<font color=navy>// and we want to solve this system in the least squares sense using</font>
<font color=navy>// LSQR algorithm. In order to <b>do</b> so, we have to create left part</font>
<font color=navy>// (sparsematrix object) and right part (dense array).</font>
<font color=navy>//</font>
<font color=navy>// Initially, sparse matrix is created in the Hash-Table format,</font>
<font color=navy>// which allows easy initialization, but <b>do</b> not allow matrix to be</font>
<font color=navy>// used in the linear solvers. So after construction you should convert</font>
<font color=navy>// sparse matrix to CRS format (one suited <b>for</b> linear operations).</font>
   sparsematrix a;
   sparsecreate(5, 2, a);
   sparseset(a, 0, 0, 1.0);
   sparseset(a, 0, 1, 1.0);
   sparseset(a, 1, 0, 1.0);
   sparseset(a, 1, 1, 1.0);
   sparseset(a, 2, 0, 2.0);
   sparseset(a, 2, 1, 1.0);
   sparseset(a, 3, 0, 1.0);
   sparseset(a, 4, 1, 1.0);
<font color=navy>// Now our matrix is fully initialized, but we have to <b>do</b> one more</font>
<font color=navy>// step - convert it from Hash-Table format to CRS format (see</font>
<font color=navy>// documentation on sparse matrices <b>for</b> more information about these</font>
<font color=navy>// formats).</font>
<font color=navy>//</font>
<font color=navy>// If you omit this call, ALGLIB will generate exception on the first</font>
<font color=navy>// attempt to use A in linear operations. </font>
   sparseconverttocrs(a);
<font color=navy>// Initialization of the right part</font>
   real_1d_array b = <font color=blue><b>&quot;[4,2,4,1,2]&quot;</b></font>;
<font color=navy>// Now we have to create linear solver object and to use it <b>for</b> the</font>
<font color=navy>// solution of the linear system.</font>
   linlsqrstate s;
   linlsqrreport rep;
   real_1d_array x;
   linlsqrcreate(5, 2, s);
   linlsqrsolvesparse(s, a, b);
   linlsqrresults(s, x, rep);

   printf(<font color=blue><b>&quot;%d\n&quot;</b></font>, <b>int</b>(rep.terminationtype)); <font color=navy>// EXPECTED: 4</font>
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, x.tostring(2).c_str()); <font color=navy>// EXPECTED: [1.000,2.000]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_nleq></a><h4 class=pageheader>8.9.5. nleq Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_nleqreport class=toc>nleqreport</a> |
<a href=#struct_nleqstate class=toc>nleqstate</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_nleqcreatelm class=toc>nleqcreatelm</a> |
<a href=#sub_nleqrestartfrom class=toc>nleqrestartfrom</a> |
<a href=#sub_nleqresults class=toc>nleqresults</a> |
<a href=#sub_nleqresultsbuf class=toc>nleqresultsbuf</a> |
<a href=#sub_nleqsetcond class=toc>nleqsetcond</a> |
<a href=#sub_nleqsetstpmax class=toc>nleqsetstpmax</a> |
<a href=#sub_nleqsetxrep class=toc>nleqsetxrep</a> |
<a href=#sub_nleqsolve class=toc>nleqsolve</a>
]</font>
</div>
<a name=struct_nleqreport></a><h6 class=pageheader>nleqreport Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> nleqreport {
   ae_int_t iterationscount;
   ae_int_t nfunc;
   ae_int_t njac;
   ae_int_t terminationtype;
};
</pre>
<a name=struct_nleqstate></a><h6 class=pageheader>nleqstate Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> nleqstate {
   bool needf;
   bool needfij;
   bool xupdated;
   double f;
   real_1d_array fi;
   real_2d_array j;
   real_1d_array x;
};
</pre>
<a name=sub_nleqcreatelm></a><h6 class=pageheader>nleqcreatelm Function</h6>
<hr width=600 align=left>
<pre class=narration>
LEVENBERG-MARQUARDT-LIKE NONLINEAR SOLVER
This algorithm solves a system of nonlinear equations
    F[0](x[0], ..., x[N-1])   = 0
    F[1](x[0], ..., x[N-1])   = 0
    ...
    F[M-1](x[0], ..., x[N-1]) = 0
where M/N do not necessarily coincide. The algorithm converges quadratically
under following conditions:
    * the solution set XS is nonempty
    * for some xs in XS there exist such neighbourhood N(xs) that:
      * vector function F(x) and its Jacobian J(x) are continuously
        differentiable on N
      * ||F(x)|| provides local error bound on N, i.e. there  exists  such
        c1, that ||F(x)|| &gt; c1*distance(x,XS)
Note that these conditions are much more weaker than usual non-singularity
conditions. For example, algorithm will converge for any  affine  function
F (whether its Jacobian singular or not).

REQUIREMENTS:
Algorithm will request following information during its operation:
* function vector F[] and Jacobian matrix at given point X
* value of merit function f(x)=F[0]^2(x)+...+F[M-1]^2(x) at given point X

USAGE:
1. User initializes algorithm state with NLEQCreateLM() call
2. User tunes solver parameters with  NLEQSetCond(),  NLEQSetStpMax()  and
   other functions
3. User  calls  NLEQSolve()  function  which  takes  algorithm  state  and
   pointers (delegates, etc.) to callback functions which calculate  merit
   function value and Jacobian.
4. User calls NLEQResults() to get solution
5. Optionally, user may call NLEQRestartFrom() to  solve  another  problem
   with same parameters (N/M) but another starting  point  and/or  another
   function vector. NLEQRestartFrom() allows to reuse already  initialized
   structure.

Inputs:
    N       -   space dimension, N &gt; 1:
                * if provided, only leading N elements of X are used
                * if not provided, determined automatically from size of X
    M       -   system size
    X       -   starting point

Outputs:
    State   -   structure which stores algorithm state

NOTES:
1. you may tune stopping conditions with NLEQSetCond() function
2. if target function contains exp() or other fast growing functions,  and
   optimization algorithm makes too large steps which leads  to  overflow,
   use NLEQSetStpMax() function to bound algorithm's steps.
3. this  algorithm  is  a  slightly  modified implementation of the method
   described  in  'Levenberg-Marquardt  method  for constrained  nonlinear
   equations with strong local convergence properties' by Christian Kanzow
   Nobuo Yamashita and Masao Fukushima and further  developed  in  'On the
   convergence of a New Levenberg-Marquardt Method'  by  Jin-yan  Fan  and
   Ya-Xiang Yuan.
ALGLIB: Copyright 20.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nleqcreatelm(ae_int_t n, ae_int_t m, real_1d_array x, nleqstate &amp;state);
<b>void</b> nleqcreatelm(ae_int_t m, real_1d_array x, nleqstate &amp;state);
</pre>
<a name=sub_nleqrestartfrom></a><h6 class=pageheader>nleqrestartfrom Function</h6>
<hr width=600 align=left>
<pre class=narration>
This  subroutine  restarts  CG  algorithm from new point. All optimization
parameters are left unchanged.

This  function  allows  to  solve multiple  optimization  problems  (which
must have same number of dimensions) without object reallocation penalty.

Inputs:
    State   -   structure used for reverse communication previously
                allocated with MinCGCreate call.
    X       -   new starting point.
    BndL    -   new lower bounds
    BndU    -   new upper bounds
ALGLIB: Copyright 30.07.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nleqrestartfrom(nleqstate state, real_1d_array x);
</pre>
<a name=sub_nleqresults></a><h6 class=pageheader>nleqresults Function</h6>
<hr width=600 align=left>
<pre class=narration>
NLEQ solver results

Inputs:
    State   -   algorithm state.

Outputs:
    X       -   array[0..N-1], solution
    Rep     -   optimization report:
                * Rep.TerminationType completetion code:
                    * -4    ERROR:  algorithm   has   converged   to   the
                            stationary point Xf which is local minimum  of
                            f=F[0]^2+...+F[m-1]^2, but is not solution  of
                            nonlinear system.
                    *  1    sqrt(f) &le; EpsF.
                    *  5    MaxIts steps was taken
                    *  7    stopping conditions are too stringent,
                            further improvement is impossible
                * Rep.IterationsCount contains iterations count
                * NFEV countains number of function calculations
                * ActiveConstraints contains number of active constraints
ALGLIB: Copyright 20.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nleqresults(nleqstate state, real_1d_array &amp;x, nleqreport &amp;rep);
</pre>
<a name=sub_nleqresultsbuf></a><h6 class=pageheader>nleqresultsbuf Function</h6>
<hr width=600 align=left>
<pre class=narration>
NLEQ solver results

Buffered implementation of NLEQResults(), which uses pre-allocated  buffer
to store X[]. If buffer size is  too  small,  it  resizes  buffer.  It  is
intended to be used in the inner cycles of performance critical algorithms
where array reallocation penalty is too large to be ignored.
ALGLIB: Copyright 20.08.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nleqresultsbuf(nleqstate state, real_1d_array &amp;x, nleqreport &amp;rep);
</pre>
<a name=sub_nleqsetcond></a><h6 class=pageheader>nleqsetcond Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets stopping conditions for the nonlinear solver

Inputs:
    State   -   structure which stores algorithm state
    EpsF    - &ge; 0
                The subroutine finishes  its work if on k+1-th iteration
                the condition ||F|| &le; EpsF is satisfied
    MaxIts  -   maximum number of iterations. If MaxIts=0, the  number  of
                iterations is unlimited.

Passing EpsF=0 and MaxIts=0 simultaneously will lead to  automatic
stopping criterion selection (small EpsF).

NOTES:
ALGLIB: Copyright 20.08.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nleqsetcond(nleqstate state, <b>double</b> epsf, ae_int_t maxits);
</pre>
<a name=sub_nleqsetstpmax></a><h6 class=pageheader>nleqsetstpmax Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function sets maximum step length

Inputs:
    State   -   structure which stores algorithm state
    StpMax  -   maximum step length, &ge; 0. Set StpMax to 0.0,  if you don't
                want to limit step length.

Use this subroutine when target function  contains  exp()  or  other  fast
growing functions, and algorithm makes  too  large  steps  which  lead  to
overflow. This function allows us to reject steps that are too large  (and
therefore expose us to the possible overflow) without actually calculating
function value at the x+stp*d.
ALGLIB: Copyright 20.08.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nleqsetstpmax(nleqstate state, <b>double</b> stpmax);
</pre>
<a name=sub_nleqsetxrep></a><h6 class=pageheader>nleqsetxrep Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function turns on/off reporting.

Inputs:
    State   -   structure which stores algorithm state
    NeedXRep-   whether iteration reports are needed or not

If NeedXRep is True, algorithm will call rep() callback function if  it is
provided to NLEQSolve().
ALGLIB: Copyright 20.08.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nleqsetxrep(nleqstate state, <b>bool</b> needxrep);
</pre>
<a name=sub_nleqsolve></a><h6 class=pageheader>nleqsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
This family of functions is used to launch iterations of nonlinear solver

These functions accept following parameters:
    state   -   algorithm state
    func    -   callback which calculates function (or merit function)
                value func at given point x
    jac     -   callback which calculates function vector fi[]
                and Jacobian jac at given point x
    rep     -   optional callback which is called after each iteration
                can be NULL
    ptr     -   optional pointer which is passed to func/grad/hess/jac/rep
                can be NULL
ALGLIB: Copyright 20.03.2009 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> nleqsolve(nleqstate &amp;state, <b>void</b> (*func)(<b>const</b> real_1d_array &amp;x, <b>double</b> &amp;func, <b>void</b> *ptr), <b>void</b> (*jac)(<b>const</b> real_1d_array &amp;x, real_1d_array &amp;fi, real_2d_array &amp;jac, <b>void</b> *ptr), <b>void</b> (*rep)(<b>const</b> real_1d_array &amp;x, <b>double</b> func, <b>void</b> *ptr) = NULL, <b>void</b> *ptr = NULL);
</pre>
<a name=unit_polynomialsolver></a><h4 class=pageheader>8.9.6. polynomialsolver Subpackage</h4>
<div class=pagecontent>
<h5>Classes</h5><font size=2>[
<a href=#struct_polynomialsolverreport class=toc>polynomialsolverreport</a>
]</font>
<h5>Functions</h5><font size=2>[
<a href=#sub_polynomialsolve class=toc>polynomialsolve</a>
]</font>
</div>
<a name=struct_polynomialsolverreport></a><h6 class=pageheader>polynomialsolverreport Class</h6>
<hr width=600 align=left>
<pre class=narration></pre>
<pre class=declaration>
<b>class</b> polynomialsolverreport {
   <b>double</b> maxerr;
};
</pre>
<a name=sub_polynomialsolve></a><h6 class=pageheader>polynomialsolve Function</h6>
<hr width=600 align=left>
<pre class=narration>
Polynomial root finding.

This function returns all roots of the polynomial
    P(x) = a0 + a1*x + a2*x^2 + ... + an*x^n
Both real and complex roots are returned (see below).

Inputs:
    A       -   array[N+1], polynomial coefficients:
                * A[0] is constant term
                * A[N] is a coefficient of X^N
    N       -   polynomial degree

Outputs:
    X       -   array of complex roots:
                * for isolated real root, X[I] is strictly real: IMAGE(X[I])=0
                * complex roots are always returned in pairs - roots occupy
                  positions I and I+1, with:
                  * X[I+1]=Conj(X[I])
                  * IMAGE(X[I]) &gt; 0
                  * IMAGE(X[I+1]) = -IMAGE(X[I]) &lt; 0
                * multiple real roots may have non-zero imaginary part due
                  to roundoff errors. There is no reliable way to distinguish
                  real root of multiplicity 2 from two  complex  roots  in
                  the presence of roundoff errors.
    Rep     -   report, additional information, following fields are set:
                * Rep.MaxErr - max( |P(xi)| )  for  i=0..N-1.  This  field
                  allows to quickly estimate &quot;quality&quot; of the roots  being
                  returned.

NOTE:   this function uses companion matrix method to find roots. In  case
        internal EVD  solver  fails  do  find  eigenvalues,  exception  is
        generated.

NOTE:   roots are not &quot;polished&quot; and  no  matrix  balancing  is  performed
        for them.
ALGLIB: Copyright 24.02.2014 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> polynomialsolve(real_1d_array a, ae_int_t n, complex_1d_array &amp;x, polynomialsolverreport &amp;rep);
</pre>
</p>
<p>
<a name=pck_SpecialFunctions class=sheader></a><h3>8.10. SpecialFunctions Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_airyf class=toc>airyf</a></td><td>Airy functions</td></tr>
<tr align=left valign=top><td><a href=#unit_bessel class=toc>bessel</a></td><td>Bessel functions</td></tr>
<tr align=left valign=top><td><a href=#unit_betaf class=toc>betaf</a></td><td>Beta function</td></tr>
<tr align=left valign=top><td><a href=#unit_binomialdistr class=toc>binomialdistr</a></td><td>Binomial distribution</td></tr>
<tr align=left valign=top><td><a href=#unit_chebyshev class=toc>chebyshev</a></td><td>Chebyshev polynomials</td></tr>
<tr align=left valign=top><td><a href=#unit_chisquaredistr class=toc>chisquaredistr</a></td><td>Chi-Square distribution</td></tr>
<tr align=left valign=top><td><a href=#unit_dawson class=toc>dawson</a></td><td>Dawson integral</td></tr>
<tr align=left valign=top><td><a href=#unit_elliptic class=toc>elliptic</a></td><td>Elliptic integrals</td></tr>
<tr align=left valign=top><td><a href=#unit_expintegrals class=toc>expintegrals</a></td><td>Exponential integrals</td></tr>
<tr align=left valign=top><td><a href=#unit_fdistr class=toc>fdistr</a></td><td>F-distribution</td></tr>
<tr align=left valign=top><td><a href=#unit_fresnel class=toc>fresnel</a></td><td>Fresnel integrals</td></tr>
<tr align=left valign=top><td><a href=#unit_gammafunc class=toc>gammafunc</a></td><td>Gamma function</td></tr>
<tr align=left valign=top><td><a href=#unit_hermite class=toc>hermite</a></td><td>Hermite polynomials</td></tr>
<tr align=left valign=top><td><a href=#unit_ibetaf class=toc>ibetaf</a></td><td>Incomplete beta function</td></tr>
<tr align=left valign=top><td><a href=#unit_igammaf class=toc>igammaf</a></td><td>Incomplete gamma function</td></tr>
<tr align=left valign=top><td><a href=#unit_jacobianelliptic class=toc>jacobianelliptic</a></td><td>Jacobian elliptic functions</td></tr>
<tr align=left valign=top><td><a href=#unit_laguerre class=toc>laguerre</a></td><td>Laguerre polynomials</td></tr>
<tr align=left valign=top><td><a href=#unit_legendre class=toc>legendre</a></td><td>Legendre polynomials</td></tr>
<tr align=left valign=top><td><a href=#unit_normaldistr class=toc>normaldistr</a></td><td>Univarite and bivariate normal distribution PDF and CDF</td></tr>
<tr align=left valign=top><td><a href=#unit_poissondistr class=toc>poissondistr</a></td><td>Poisson distribution</td></tr>
<tr align=left valign=top><td><a href=#unit_psif class=toc>psif</a></td><td>Psi function</td></tr>
<tr align=left valign=top><td><a href=#unit_studenttdistr class=toc>studenttdistr</a></td><td>Student's t-distribution</td></tr>
<tr align=left valign=top><td><a href=#unit_trigintegrals class=toc>trigintegrals</a></td><td>Trigonometric integrals</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_airyf></a><h4 class=pageheader>8.10.1. airyf Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_airy class=toc>airy</a>
]</font>
</div>
<a name=sub_airy></a><h6 class=pageheader>airy Function</h6>
<hr width=600 align=left>
<pre class=narration>
Airy function

Solution of the differential equation

y&quot;(x) = xy.

The function returns the two independent solutions Ai, Bi
and their first derivatives Ai'(x), Bi'(x).

Evaluation is by power series summation for small x,
by rational minimax approximations for large x.

ACCURACY:
Error criterion is absolute when function &le; 1, relative
when function &gt; 1, except * denotes relative error criterion.
For large negative x, the absolute error increases as x^1.5.
For large positive x, the relative error increases as x^1.5.

Arithmetic  domain   function  # trials      peak         rms
IEEE        -10, 0     Ai        10000       1.6e-15     2.7e-16
IEEE          0, 10    Ai        10000       2.3e-14*    1.8e-15*
IEEE        -10, 0     Ai'       10000       4.6e-15     7.6e-16
IEEE          0, 10    Ai'       10000       1.8e-14*    1.5e-15*
IEEE        -10, 10    Bi        30000       4.2e-15     5.3e-16
IEEE        -10, 10    Bi'       30000       4.9e-15     7.3e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1989, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> airy(<b>double</b> x, <b>double</b> &amp;ai, <b>double</b> &amp;aip, <b>double</b> &amp;bi, <b>double</b> &amp;bip);
</pre>
<a name=unit_bessel></a><h4 class=pageheader>8.10.2. bessel Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_besseli0 class=toc>besseli0</a> |
<a href=#sub_besseli1 class=toc>besseli1</a> |
<a href=#sub_besselj0 class=toc>besselj0</a> |
<a href=#sub_besselj1 class=toc>besselj1</a> |
<a href=#sub_besseljn class=toc>besseljn</a> |
<a href=#sub_besselk0 class=toc>besselk0</a> |
<a href=#sub_besselk1 class=toc>besselk1</a> |
<a href=#sub_besselkn class=toc>besselkn</a> |
<a href=#sub_bessely0 class=toc>bessely0</a> |
<a href=#sub_bessely1 class=toc>bessely1</a> |
<a href=#sub_besselyn class=toc>besselyn</a>
]</font>
</div>
<a name=sub_besseli0></a><h6 class=pageheader>besseli0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modified Bessel function of order zero

Returns modified Bessel function of order zero of the
argument.

The function is defined as i0(x) = j0( ix ).

The range is partitioned into the two intervals [0,8] and
(8, infinity).  Chebyshev polynomial expansions are employed
in each interval.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0,30        30000       5.8e-16     1.4e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> besseli0(<b>double</b> x);
</pre>
<a name=sub_besseli1></a><h6 class=pageheader>besseli1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modified Bessel function of order one

Returns modified Bessel function of order one of the
argument.

The function is defined as i1(x) = -i j1( ix ).

The range is partitioned into the two intervals [0,8] and
(8, infinity).  Chebyshev polynomial expansions are employed
in each interval.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0, 30       30000       1.9e-15     2.1e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1985, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> besseli1(<b>double</b> x);
</pre>
<a name=sub_besselj0></a><h6 class=pageheader>besselj0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bessel function of order zero

Returns Bessel function of order zero of the argument.

The domain is divided into the intervals [0, 5] and
(5, infinity). In the first interval the following rational
approximation is used:

       2         2
(w - r  ) (w - r  ) P (w) / Q (w)
      1         2    3       8

           2
where w = x  and the two r's are zeros of the function.

In the second interval, the Hankel asymptotic expansion
is employed with two rational functions of degree 6/6
and 7/7.

ACCURACY:

                     Absolute error:
arithmetic   domain     # trials      peak         rms
   IEEE      0, 30       60000       4.2e-16     1.1e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1989, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> besselj0(<b>double</b> x);
</pre>
<a name=sub_besselj1></a><h6 class=pageheader>besselj1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bessel function of order one

Returns Bessel function of order one of the argument.

The domain is divided into the intervals [0, 8] and
(8, infinity). In the first interval a 24 term Chebyshev
expansion is used. In the second, the asymptotic
trigonometric representation is employed using two
rational functions of degree 5/5.

ACCURACY:

                     Absolute error:
arithmetic   domain      # trials      peak         rms
   IEEE      0, 30       30000       2.6e-16     1.1e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1989, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> besselj1(<b>double</b> x);
</pre>
<a name=sub_besseljn></a><h6 class=pageheader>besseljn Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bessel function of integer order

Returns Bessel function of order n, where n is a
(possibly negative) integer.

The ratio of jn(x) to j0(x) is computed by backward
recurrence.  First the ratio jn/jn-1 is found by a
continued fraction expansion.  Then the recurrence
relating successive orders is applied until j0 or j1 is
reached.

If n = 0 or 1 the routine for j0 or j1 is called
directly.

ACCURACY:

                     Absolute error:
arithmetic   range      # trials      peak         rms
   IEEE      0, 30        5000       4.4e-16     7.9e-17

Not suitable for large n or x. Use jv() (fractional order) instead.

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> besseljn(ae_int_t n, <b>double</b> x);
</pre>
<a name=sub_besselk0></a><h6 class=pageheader>besselk0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modified Bessel function, second kind, order zero

Returns modified Bessel function of the second kind
of order zero of the argument.

The range is partitioned into the two intervals [0,8] and
(8, infinity).  Chebyshev polynomial expansions are employed
in each interval.

ACCURACY:

Tested at 2000 random points between 0 and 8.  Peak absolute
error (relative when K0 &gt; 1) was 1.46e-14; rms, 4.26e-15.
                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0, 30       30000       1.2e-15     1.6e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> besselk0(<b>double</b> x);
</pre>
<a name=sub_besselk1></a><h6 class=pageheader>besselk1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modified Bessel function, second kind, order one

Computes the modified Bessel function of the second kind
of order one of the argument.

The range is partitioned into the two intervals [0,2] and
(2, infinity).  Chebyshev polynomial expansions are employed
in each interval.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0, 30       30000       1.2e-15     1.6e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> besselk1(<b>double</b> x);
</pre>
<a name=sub_besselkn></a><h6 class=pageheader>besselkn Function</h6>
<hr width=600 align=left>
<pre class=narration>
Modified Bessel function, second kind, integer order

Returns modified Bessel function of the second kind
of order n of the argument.

The range is partitioned into the two intervals [0,9.55] and
(9.55, infinity).  An ascending power series is used in the
low range, and an asymptotic expansion in the high range.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0,30        90000       1.8e-8      3.0e-10

Error is high only near the crossover point x = 9.55
between the two expansions used.

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1988, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> besselkn(ae_int_t nn, <b>double</b> x);
</pre>
<a name=sub_bessely0></a><h6 class=pageheader>bessely0 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bessel function of the second kind, order zero

Returns Bessel function of the second kind, of order
zero, of the argument.

The domain is divided into the intervals [0, 5] and
(5, infinity). In the first interval a rational approximation
R(x) is employed to compute
  y0(x)  = R(x)  +   2 * log(x) * j0(x) / PI.
Thus a call to j0() is required.

In the second interval, the Hankel asymptotic expansion
is employed with two rational functions of degree 6/6
and 7/7.

ACCURACY:

 Absolute error, when y0(x) &lt; 1; else relative error:

arithmetic   domain     # trials      peak         rms
   IEEE      0, 30       30000       1.3e-15     1.6e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1989, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> bessely0(<b>double</b> x);
</pre>
<a name=sub_bessely1></a><h6 class=pageheader>bessely1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bessel function of second kind of order one

Returns Bessel function of the second kind of order one
of the argument.

The domain is divided into the intervals [0, 8] and
(8, infinity). In the first interval a 25 term Chebyshev
expansion is used, and a call to j1() is required.
In the second, the asymptotic trigonometric representation
is employed using two rational functions of degree 5/5.

ACCURACY:

                     Absolute error:
arithmetic   domain      # trials      peak         rms
   IEEE      0, 30       30000       1.0e-15     1.3e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1989, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> bessely1(<b>double</b> x);
</pre>
<a name=sub_besselyn></a><h6 class=pageheader>besselyn Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bessel function of second kind of integer order

Returns Bessel function of order n, where n is a
(possibly negative) integer.

The function is evaluated by forward recurrence on
n, starting with values computed by the routines
y0() and y1().

If n = 0 or 1 the routine for y0 or y1 is called
directly.

ACCURACY:
                     Absolute error, except relative
                     when y &gt; 1:
arithmetic   domain     # trials      peak         rms
   IEEE      0, 30       30000       3.4e-15     4.3e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> besselyn(ae_int_t n, <b>double</b> x);
</pre>
<a name=unit_betaf></a><h4 class=pageheader>8.10.3. betaf Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_beta class=toc>beta</a>
]</font>
</div>
<a name=sub_beta></a><h6 class=pageheader>beta Function</h6>
<hr width=600 align=left>
<pre class=narration>
Beta function

                  -     -
                 | (a) | (b)
beta( a, b )  =  -----------.
                    -
                   | (a+b)

For large arguments the logarithm of the function is
evaluated using lgam(), then exponentiated.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE       0,30       30000       8.1e-14     1.1e-14

Cephes Math Library Release 2.0:  April, 1987
Copyright 1984, 1987 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> beta(<b>double</b> a, <b>double</b> b);
</pre>
<a name=unit_binomialdistr></a><h4 class=pageheader>8.10.4. binomialdistr Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_binomialcdistribution class=toc>binomialcdistribution</a> |
<a href=#sub_binomialdistribution class=toc>binomialdistribution</a> |
<a href=#sub_invbinomialdistribution class=toc>invbinomialdistribution</a>
]</font>
</div>
<a name=sub_binomialcdistribution></a><h6 class=pageheader>binomialcdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complemented binomial distribution

Returns the sum of the terms k+1 through n of the Binomial
probability density:

  n
  --  ( n )   j      n-j
  &gt;   (   )  p  (1-p)
  --  ( j )
 j=k+1

The terms are not summed directly; instead the incomplete
beta integral is employed, according to the formula

y = bdtrc( k, n, p ) = incbet( k+1, n-k, p ).

The arguments must be positive, with p ranging from 0 to 1.

ACCURACY:

Tested at random points (a,b,p).

              a,b                     Relative error:
arithmetic  domain     # trials      peak         rms
 For p between 0.001 and 1:
   IEEE     0,100       100000      6.7e-15     8.2e-16
 For p between 0 and .001:
   IEEE     0,100       100000      1.5e-13     2.7e-15

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> binomialcdistribution(ae_int_t k, ae_int_t n, <b>double</b> p);
</pre>
<a name=sub_binomialdistribution></a><h6 class=pageheader>binomialdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Binomial distribution

Returns the sum of the terms 0 through k of the Binomial
probability density:

  k
  --  ( n )   j      n-j
  &gt;   (   )  p  (1-p)
  --  ( j )
 j=0

The terms are not summed directly; instead the incomplete
beta integral is employed, according to the formula

y = bdtr( k, n, p ) = incbet( n-k, k+1, 1-p ).

The arguments must be positive, with p ranging from 0 to 1.

ACCURACY:

Tested at random points (a,b,p), with p between 0 and 1.

              a,b                     Relative error:
arithmetic  domain     # trials      peak         rms
 For p between 0.001 and 1:
   IEEE     0,100       100000      4.3e-15     2.6e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> binomialdistribution(ae_int_t k, ae_int_t n, <b>double</b> p);
</pre>
<a name=sub_invbinomialdistribution></a><h6 class=pageheader>invbinomialdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse binomial distribution

Finds the event probability p such that the sum of the
terms 0 through k of the Binomial probability density
is equal to the given cumulative probability y.

This is accomplished using the inverse beta integral
function and the relation

1 - p = incbi( n-k, k+1, y ).

ACCURACY:

Tested at random points (a,b,p).

              a,b                     Relative error:
arithmetic  domain     # trials      peak         rms
 For p between 0.001 and 1:
   IEEE     0,100       100000      2.3e-14     6.4e-16
   IEEE     0,10000     100000      6.6e-12     1.2e-13
 For p between 10^-6 and 0.001:
   IEEE     0,100       100000      2.0e-12     1.3e-14
   IEEE     0,10000     100000      1.5e-12     3.2e-14

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> invbinomialdistribution(ae_int_t k, ae_int_t n, <b>double</b> y);
</pre>
<a name=unit_chebyshev></a><h4 class=pageheader>8.10.5. chebyshev Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_chebyshevcalculate class=toc>chebyshevcalculate</a> |
<a href=#sub_chebyshevcoefficients class=toc>chebyshevcoefficients</a> |
<a href=#sub_chebyshevsum class=toc>chebyshevsum</a> |
<a href=#sub_fromchebyshev class=toc>fromchebyshev</a>
]</font>
</div>
<a name=sub_chebyshevcalculate></a><h6 class=pageheader>chebyshevcalculate Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the value of the Chebyshev polynomials of the
first and second kinds.

Parameters:
    r   -   polynomial kind, either 1 or 2.
    n   -   degree, n &ge; 0
    x   -   argument, -1 &le; x &le; 1

Result:
    the value of the Chebyshev polynomial at x
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> chebyshevcalculate(ae_int_t r, ae_int_t n, <b>double</b> x);
</pre>
<a name=sub_chebyshevcoefficients></a><h6 class=pageheader>chebyshevcoefficients Function</h6>
<hr width=600 align=left>
<pre class=narration>
Representation of Tn as C[0] + C[1]*X + ... + C[N]*X^N

Inputs:
    N   -   polynomial degree, n &ge; 0

Outputs:
    C   -   coefficients
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> chebyshevcoefficients(ae_int_t n, real_1d_array &amp;c);
</pre>
<a name=sub_chebyshevsum></a><h6 class=pageheader>chebyshevsum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Summation of Chebyshev polynomials using Clenshaw's recurrence formula.

This routine calculates
    c[0]*T0(x) + c[1]*T1(x) + ... + c[N]*TN(x)
or
    c[0]*U0(x) + c[1]*U1(x) + ... + c[N]*UN(x)
depending on the R.

Parameters:
    r   -   polynomial kind, either 1 or 2.
    n   -   degree, n &ge; 0
    x   -   argument

Result:
    the value of the Chebyshev polynomial at x
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> chebyshevsum(real_1d_array c, ae_int_t r, ae_int_t n, <b>double</b> x);
</pre>
<a name=sub_fromchebyshev></a><h6 class=pageheader>fromchebyshev Function</h6>
<hr width=600 align=left>
<pre class=narration>
Conversion of a series of Chebyshev polynomials to a power series.

Represents A[0]*T0(x) + A[1]*T1(x) + ... + A[N]*Tn(x) as
B[0] + B[1]*X + ... + B[N]*X^N.

Inputs:
    A   -   Chebyshev series coefficients
    N   -   degree, N &ge; 0

Outputs:
    B   -   power series coefficients
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fromchebyshev(real_1d_array a, ae_int_t n, real_1d_array &amp;b);
</pre>
<a name=unit_chisquaredistr></a><h4 class=pageheader>8.10.6. chisquaredistr Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_chisquarecdistribution class=toc>chisquarecdistribution</a> |
<a href=#sub_chisquaredistribution class=toc>chisquaredistribution</a> |
<a href=#sub_invchisquaredistribution class=toc>invchisquaredistribution</a>
]</font>
</div>
<a name=sub_chisquarecdistribution></a><h6 class=pageheader>chisquarecdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complemented Chi-square distribution

Returns the area under the right hand tail (from x to
infinity) of the Chi square probability density function
with v degrees of freedom:

                                 inf.
                                   -
                       1          | |  v/2-1  -t/2
 P( x | v )   =   -----------     |   t      e     dt
                   v/2  -       | |
                  2    | (v/2)   -
                                  x

where x is the Chi-square variable.

The incomplete gamma integral is used, according to the
formula

y = chdtr( v, x ) = igamc( v/2.0, x/2.0 ).

The arguments must both be positive.

ACCURACY:

See incomplete gamma function

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> chisquarecdistribution(<b>double</b> v, <b>double</b> x);
</pre>
<a name=sub_chisquaredistribution></a><h6 class=pageheader>chisquaredistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Chi-square distribution

Returns the area under the left hand tail (from 0 to x)
of the Chi square probability density function with
v degrees of freedom.

                                  x
                                   -
                       1          | |  v/2-1  -t/2
 P( x | v )   =   -----------     |   t      e     dt
                   v/2  -       | |
                  2    | (v/2)   -
                                  0

where x is the Chi-square variable.

The incomplete gamma integral is used, according to the
formula

y = chdtr( v, x ) = igam( v/2.0, x/2.0 ).

The arguments must both be positive.

ACCURACY:

See incomplete gamma function

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> chisquaredistribution(<b>double</b> v, <b>double</b> x);
</pre>
<a name=sub_invchisquaredistribution></a><h6 class=pageheader>invchisquaredistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse of complemented Chi-square distribution

Finds the Chi-square argument x such that the integral
from x to infinity of the Chi-square density is equal
to the given cumulative probability y.

This is accomplished using the inverse gamma integral
function and the relation

   x/2 = igami( df/2, y );

ACCURACY:

See inverse incomplete gamma function

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> invchisquaredistribution(<b>double</b> v, <b>double</b> y);
</pre>
<a name=unit_dawson></a><h4 class=pageheader>8.10.7. dawson Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_dawsonintegral class=toc>dawsonintegral</a>
]</font>
</div>
<a name=sub_dawsonintegral></a><h6 class=pageheader>dawsonintegral Function</h6>
<hr width=600 align=left>
<pre class=narration>
Dawson's Integral

Approximates the integral

                            x
                            -
                     2     | |        2
 dawsn(x)  =  exp( -x  )   |    exp( t  ) dt
                         | |
                          -
                          0

Three different rational approximations are employed, for
the intervals 0 to 3.25; 3.25 to 6.25; and 6.25 up.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0,10        10000       6.9e-16     1.0e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1989, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> dawsonintegral(<b>double</b> x);
</pre>
<a name=unit_elliptic></a><h4 class=pageheader>8.10.8. elliptic Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_ellipticintegrale class=toc>ellipticintegrale</a> |
<a href=#sub_ellipticintegralk class=toc>ellipticintegralk</a> |
<a href=#sub_ellipticintegralkhighprecision class=toc>ellipticintegralkhighprecision</a> |
<a href=#sub_incompleteellipticintegrale class=toc>incompleteellipticintegrale</a> |
<a href=#sub_incompleteellipticintegralk class=toc>incompleteellipticintegralk</a>
]</font>
</div>
<a name=sub_ellipticintegrale></a><h6 class=pageheader>ellipticintegrale Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complete elliptic integral of the second kind

Approximates the integral

           pi/2
            -
           | |                 2
E(m)  =    |    sqrt( 1 - m sin t ) dt
         | |
          -
           0

using the approximation

     P(x)  -  x log x Q(x).

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE       0, 1       10000       2.1e-16     7.3e-17

Cephes Math Library, Release 2.8: June, 2000
Copyright 1984, 1987, 1989, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> ellipticintegrale(<b>double</b> m);
</pre>
<a name=sub_ellipticintegralk></a><h6 class=pageheader>ellipticintegralk Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complete elliptic integral of the first kind

Approximates the integral

           pi/2
            -
           | |
           |           dt
K(m)  =    |    ------------------
           |                   2
         | |    sqrt( 1 - m sin t )
          -
           0

using the approximation

    P(x)  -  log x Q(x).

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE       0,1        30000       2.5e-16     6.8e-17

Cephes Math Library, Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> ellipticintegralk(<b>double</b> m);
</pre>
<a name=sub_ellipticintegralkhighprecision></a><h6 class=pageheader>ellipticintegralkhighprecision Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complete elliptic integral of the first kind

Approximates the integral

           pi/2
            -
           | |
           |           dt
K(m)  =    |    ------------------
           |                   2
         | |    sqrt( 1 - m sin t )
          -
           0

where m = 1 - m1, using the approximation

    P(x)  -  log x Q(x).

The argument m1 is used rather than m so that the logarithmic
singularity at m = 1 will be shifted to the origin; this
preserves maximum accuracy.

K(0) = pi/2.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE       0,1        30000       2.5e-16     6.8e-17

Cephes Math Library, Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> ellipticintegralkhighprecision(<b>double</b> m1);
</pre>
<a name=sub_incompleteellipticintegrale></a><h6 class=pageheader>incompleteellipticintegrale Function</h6>
<hr width=600 align=left>
<pre class=narration>
Incomplete elliptic integral of the second kind

Approximates the integral

               phi
                -
               | |
               |                   2
E(phi_\m)  =    |    sqrt( 1 - m sin t ) dt
               |
             | |
              -
               0

of amplitude phi and modulus m, using the arithmetic -
geometric mean algorithm.

ACCURACY:

Tested at random arguments with phi in [-10, 10] and m in
[0, 1].
                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE     -10,10      150000       3.3e-15     1.4e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1993, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> incompleteellipticintegrale(<b>double</b> phi, <b>double</b> m);
</pre>
<a name=sub_incompleteellipticintegralk></a><h6 class=pageheader>incompleteellipticintegralk Function</h6>
<hr width=600 align=left>
<pre class=narration>
Incomplete elliptic integral of the first kind F(phi|m)

Approximates the integral

               phi
                -
               | |
               |           dt
F(phi_\m)  =    |    ------------------
               |                   2
             | |    sqrt( 1 - m sin t )
              -
               0

of amplitude phi and modulus m, using the arithmetic -
geometric mean algorithm.

ACCURACY:

Tested at random points with m in [0, 1] and phi as indicated.

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE     -10,10       200000      7.4e-16     1.0e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> incompleteellipticintegralk(<b>double</b> phi, <b>double</b> m);
</pre>
<a name=unit_expintegrals></a><h4 class=pageheader>8.10.9. expintegrals Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_exponentialintegralei class=toc>exponentialintegralei</a> |
<a href=#sub_exponentialintegralen class=toc>exponentialintegralen</a>
]</font>
</div>
<a name=sub_exponentialintegralei></a><h6 class=pageheader>exponentialintegralei Function</h6>
<hr width=600 align=left>
<pre class=narration>
Exponential integral Ei(x)

              x
               -     t
              | |   e
   Ei(x) =   -|-   ---  dt .
            | |     t
             -
            -inf

Not defined for x &le; 0.
See also expn.c.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE       0,100       50000      8.6e-16     1.3e-16

Cephes Math Library Release 2.8:  May, 1999
Copyright 1999 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> exponentialintegralei(<b>double</b> x);
</pre>
<a name=sub_exponentialintegralen></a><h6 class=pageheader>exponentialintegralen Function</h6>
<hr width=600 align=left>
<pre class=narration>
Exponential integral En(x)

Evaluates the exponential integral

                inf.
                  -
                 | |   -xt
                 |    e
     E (x)  =    |    ----  dt.
      n          |      n
               | |     t
                -
                 1

Both n and x must be nonnegative.

The routine employs either a power series, a continued
fraction, or an asymptotic formula depending on the
relative values of n and x.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0, 30       10000       1.7e-15     3.6e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1985, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> exponentialintegralen(<b>double</b> x, ae_int_t n);
</pre>
<a name=unit_fdistr></a><h4 class=pageheader>8.10.10. fdistr Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_fcdistribution class=toc>fcdistribution</a> |
<a href=#sub_fdistribution class=toc>fdistribution</a> |
<a href=#sub_invfdistribution class=toc>invfdistribution</a>
]</font>
</div>
<a name=sub_fcdistribution></a><h6 class=pageheader>fcdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complemented F distribution

Returns the area from x to infinity under the F density
function (also known as Snedcor's density or the
variance ratio density).

                     inf.
                      -
             1       | |  a-1      b-1
1-P(x)  =  ------    |   t    (1-t)    dt
           B(a,b)  | |
                    -
                     x

The incomplete beta integral is used, according to the
formula

P(x) = incbet( df2/2, df1/2, (df2/(df2 + df1*x) ).

ACCURACY:

Tested at random points (a,b,x) in the indicated intervals.
               x     a,b                     Relative error:
arithmetic  domain  domain     # trials      peak         rms
   IEEE      0,1    1,100       100000      3.7e-14     5.9e-16
   IEEE      1,5    1,100       100000      8.0e-15     1.6e-15
   IEEE      0,1    1,10000     100000      1.8e-11     3.5e-13
   IEEE      1,5    1,10000     100000      2.0e-11     3.0e-12

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> fcdistribution(ae_int_t a, ae_int_t b, <b>double</b> x);
</pre>
<a name=sub_fdistribution></a><h6 class=pageheader>fdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
F distribution

Returns the area from zero to x under the F density
function (also known as Snedcor's density or the
variance ratio density).  This is the density
of x = (u1/df1)/(u2/df2), where u1 and u2 are random
variables having Chi square distributions with df1
and df2 degrees of freedom, respectively.
The incomplete beta integral is used, according to the
formula

P(x) = incbet( df1/2, df2/2, (df1*x/(df2 + df1*x) ).

The arguments a and b are greater than zero, and x is
nonnegative.

ACCURACY:

Tested at random points (a,b,x).

               x     a,b                     Relative error:
arithmetic  domain  domain     # trials      peak         rms
   IEEE      0,1    0,100       100000      9.8e-15     1.7e-15
   IEEE      1,5    0,100       100000      6.5e-15     3.5e-16
   IEEE      0,1    1,10000     100000      2.2e-11     3.3e-12
   IEEE      1,5    1,10000     100000      1.1e-11     1.7e-13

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> fdistribution(ae_int_t a, ae_int_t b, <b>double</b> x);
</pre>
<a name=sub_invfdistribution></a><h6 class=pageheader>invfdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse of complemented F distribution

Finds the F density argument x such that the integral
from x to infinity of the F density is equal to the
given probability p.

This is accomplished using the inverse beta integral
function and the relations

     z = incbi( df2/2, df1/2, p )
     x = df2 (1-z) / (df1 z).

Note: the following relations hold for the inverse of
the uncomplemented F distribution:

     z = incbi( df1/2, df2/2, p )
     x = df2 z / (df1 (1-z)).

ACCURACY:

Tested at random points (a,b,p).

             a,b                     Relative error:
arithmetic  domain     # trials      peak         rms
 For p between .001 and 1:
   IEEE     1,100       100000      8.3e-15     4.7e-16
   IEEE     1,10000     100000      2.1e-11     1.4e-13
 For p between 10^-6 and 10^-3:
   IEEE     1,100        50000      1.3e-12     8.4e-15
   IEEE     1,10000      50000      3.0e-12     4.8e-14

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> invfdistribution(ae_int_t a, ae_int_t b, <b>double</b> y);
</pre>
<a name=unit_fresnel></a><h4 class=pageheader>8.10.11. fresnel Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_fresnelintegral class=toc>fresnelintegral</a>
]</font>
</div>
<a name=sub_fresnelintegral></a><h6 class=pageheader>fresnelintegral Function</h6>
<hr width=600 align=left>
<pre class=narration>
Fresnel integral

Evaluates the Fresnel integrals

          x
          -
         | |
C(x) =   |   cos(pi/2 t**2) dt,
       | |
        -
         0

          x
          -
         | |
S(x) =   |   sin(pi/2 t**2) dt.
       | |
        -
         0

The integrals are evaluated by a power series for x &lt; 1.
For x &ge; 1 auxiliary functions f(x) and g(x) are employed
such that

C(x) = 0.5 + f(x) sin( pi/2 x**2 ) - g(x) cos( pi/2 x**2 )
S(x) = 0.5 - f(x) cos( pi/2 x**2 ) - g(x) sin( pi/2 x**2 )

ACCURACY:

 Relative error.

Arithmetic  function   domain     # trials      peak         rms
  IEEE       S(x)      0, 10       10000       2.0e-15     3.2e-16
  IEEE       C(x)      0, 10       10000       1.8e-15     3.3e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1989, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> fresnelintegral(<b>double</b> x, <b>double</b> &amp;c, <b>double</b> &amp;s);
</pre>
<a name=unit_gammafunc></a><h4 class=pageheader>8.10.12. gammafunc Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_gammafunction class=toc>gammafunction</a> |
<a href=#sub_lngamma class=toc>lngamma</a>
]</font>
</div>
<a name=sub_gammafunction></a><h6 class=pageheader>gammafunction Function</h6>
<hr width=600 align=left>
<pre class=narration>
Gamma function

Inputs:
    X   -   argument

Domain:
    0 &lt; X &lt; 171.6
    -170 &lt; X &lt; 0, X is not an integer.

Relative error:
 arithmetic   domain     # trials      peak         rms
    IEEE    -170,-33      20000       2.3e-15     3.3e-16
    IEEE     -33,  33     20000       9.4e-16     2.2e-16
    IEEE      33, 171.6   20000       2.3e-15     3.2e-16

Cephes Math Library Release 2.8:  June, 2000
Original copyright 1984, 1987, 1989, 1992, 2000 by Stephen L. Moshier
Translated to AlgoPascal by Sergey Bochkanov (2005, 2006, 2007).
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> gammafunction(<b>double</b> x);
</pre>
<a name=sub_lngamma></a><h6 class=pageheader>lngamma Function</h6>
<hr width=600 align=left>
<pre class=narration>
Natural logarithm of gamma function

Inputs:
    X       -   argument

Result:
    logarithm of the absolute value of the Gamma(X).

Outputs:
    SgnGam  -   sign(Gamma(X))

Domain:
    0 &lt; X &lt; 2.55e305
    -2.55e305 &lt; X &lt; 0, X is not an integer.

ACCURACY:
arithmetic      domain        # trials     peak         rms
   IEEE    0, 3                 28000     5.4e-16     1.1e-16
   IEEE    2.718, 2.556e305     40000     3.5e-16     8.3e-17
The error criterion was relative when the function magnitude
was greater than one but absolute when it was less than one.

The following test used the relative error criterion, though
at certain points the relative error could be much higher than
indicated.
   IEEE    -200, -4             10000     4.8e-16     1.3e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1989, 1992, 2000 by Stephen L. Moshier
Translated to AlgoPascal by Sergey Bochkanov (2005, 2006, 2007).
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> lngamma(<b>double</b> x, <b>double</b> &amp;sgngam);
</pre>
<a name=unit_hermite></a><h4 class=pageheader>8.10.13. hermite Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_hermitecalculate class=toc>hermitecalculate</a> |
<a href=#sub_hermitecoefficients class=toc>hermitecoefficients</a> |
<a href=#sub_hermitesum class=toc>hermitesum</a>
]</font>
</div>
<a name=sub_hermitecalculate></a><h6 class=pageheader>hermitecalculate Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the value of the Hermite polynomial.

Parameters:
    n   -   degree, n &ge; 0
    x   -   argument

Result:
    the value of the Hermite polynomial Hn at x
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hermitecalculate(ae_int_t n, <b>double</b> x);
</pre>
<a name=sub_hermitecoefficients></a><h6 class=pageheader>hermitecoefficients Function</h6>
<hr width=600 align=left>
<pre class=narration>
Representation of Hn as C[0] + C[1]*X + ... + C[N]*X^N

Inputs:
    N   -   polynomial degree, n &ge; 0

Outputs:
    C   -   coefficients
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hermitecoefficients(ae_int_t n, real_1d_array &amp;c);
</pre>
<a name=sub_hermitesum></a><h6 class=pageheader>hermitesum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Summation of Hermite polynomials using Clenshaw's recurrence formula.

This routine calculates
    c[0]*H0(x) + c[1]*H1(x) + ... + c[N]*HN(x)

Parameters:
    n   -   degree, n &ge; 0
    x   -   argument

Result:
    the value of the Hermite polynomial at x
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> hermitesum(real_1d_array c, ae_int_t n, <b>double</b> x);
</pre>
<a name=unit_ibetaf></a><h4 class=pageheader>8.10.14. ibetaf Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_incompletebeta class=toc>incompletebeta</a> |
<a href=#sub_invincompletebeta class=toc>invincompletebeta</a>
]</font>
</div>
<a name=sub_incompletebeta></a><h6 class=pageheader>incompletebeta Function</h6>
<hr width=600 align=left>
<pre class=narration>
Incomplete beta integral

Returns incomplete beta integral of the arguments, evaluated
from zero to x.  The function is defined as

                 x
    -            -
   | (a+b)      | |  a-1     b-1
 -----------    |   t   (1-t)   dt.
  -     -     | |
 | (a) | (b)   -
                0

The domain of definition is 0 &le; x &le; 1.  In this
implementation a and b are restricted to positive values.
The integral from x to 1 may be obtained by the symmetry
relation

   1 - incbet( a, b, x )  =  incbet( b, a, 1-x ).

The integral is evaluated by a continued fraction expansion
or, when b*x is small, by a power series.

ACCURACY:

Tested at uniformly distributed random points (a,b,x) with a and b
in &quot;domain&quot; and x between 0 and 1.
                                       Relative error
arithmetic   domain     # trials      peak         rms
   IEEE      0,5         10000       6.9e-15     4.5e-16
   IEEE      0,85       250000       2.2e-13     1.7e-14
   IEEE      0,1000      30000       5.3e-12     6.3e-13
   IEEE      0,10000    250000       9.3e-11     7.1e-12
   IEEE      0,100000    10000       8.7e-10     4.8e-11
Outputs smaller than the IEEE gradual underflow threshold
were excluded from these statistics.

Cephes Math Library, Release 2.8:  June, 2000
Copyright 1984, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> incompletebeta(<b>double</b> a, <b>double</b> b, <b>double</b> x);
</pre>
<a name=sub_invincompletebeta></a><h6 class=pageheader>invincompletebeta Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse of imcomplete beta integral

Given y, the function finds x such that

 incbet( a, b, x ) = y .

The routine performs interval halving or Newton iterations to find the
root of incbet(a,b,x) - y = 0.

ACCURACY:

                     Relative error:
               x     a,b
arithmetic   domain  domain  # trials    peak       rms
   IEEE      0,1    .5,10000   50000    5.8e-12   1.3e-13
   IEEE      0,1   .25,100    100000    1.8e-13   3.9e-15
   IEEE      0,1     0,5       50000    1.1e-12   5.5e-15
With a and b constrained to half-integer or integer values:
   IEEE      0,1    .5,10000   50000    5.8e-12   1.1e-13
   IEEE      0,1    .5,100    100000    1.7e-14   7.9e-16
With a = .5, b constrained to half-integer or integer values:
   IEEE      0,1    .5,10000   10000    8.3e-11   1.0e-11

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1996, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> invincompletebeta(<b>double</b> a, <b>double</b> b, <b>double</b> y);
</pre>
<a name=unit_igammaf></a><h4 class=pageheader>8.10.15. igammaf Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_incompletegamma class=toc>incompletegamma</a> |
<a href=#sub_incompletegammac class=toc>incompletegammac</a> |
<a href=#sub_invincompletegammac class=toc>invincompletegammac</a>
]</font>
</div>
<a name=sub_incompletegamma></a><h6 class=pageheader>incompletegamma Function</h6>
<hr width=600 align=left>
<pre class=narration>
Incomplete gamma integral

The function is defined by

                          x
                           -
                  1       | |  -t  a-1
 igam(a,x)  =   -----     |   e   t   dt.
                 -      | |
                | (a)    -
                          0

In this implementation both arguments must be positive.
The integral is evaluated by either a power series or
continued fraction expansion, depending on the relative
values of a and x.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0,30       200000       3.6e-14     2.9e-15
   IEEE      0,100      300000       9.9e-14     1.5e-14

Cephes Math Library Release 2.8:  June, 2000
Copyright 1985, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> incompletegamma(<b>double</b> a, <b>double</b> x);
</pre>
<a name=sub_incompletegammac></a><h6 class=pageheader>incompletegammac Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complemented incomplete gamma integral

The function is defined by

 igamc(a,x)   =   1 - igam(a,x)

                           inf.
                             -
                    1       | |  -t  a-1
              =   -----     |   e   t   dt.
                   -      | |
                  | (a)    -
                            x

In this implementation both arguments must be positive.
The integral is evaluated by either a power series or
continued fraction expansion, depending on the relative
values of a and x.

ACCURACY:

Tested at random a, x.
               a         x                      Relative error:
arithmetic   domain   domain     # trials      peak         rms
   IEEE     0.5,100   0,100      200000       1.9e-14     1.7e-15
   IEEE     0.01,0.5  0,100      200000       1.4e-13     1.6e-15

Cephes Math Library Release 2.8:  June, 2000
Copyright 1985, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> incompletegammac(<b>double</b> a, <b>double</b> x);
</pre>
<a name=sub_invincompletegammac></a><h6 class=pageheader>invincompletegammac Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse of complemented imcomplete gamma integral

Given p, the function finds x such that

 igamc( a, x ) = p.

Starting with the approximate value

        3
 x = a t

 where

 t = 1 - d - ndtri(p) sqrt(d)

and

 d = 1/9a,

the routine performs up to 10 Newton iterations to find the
root of igamc(a,x) - p = 0.

ACCURACY:

Tested at random a, p in the intervals indicated.

               a        p                      Relative error:
arithmetic   domain   domain     # trials      peak         rms
   IEEE     0.5,100   0,0.5       100000       1.0e-14     1.7e-15
   IEEE     0.01,0.5  0,0.5       100000       9.0e-14     3.4e-15
   IEEE    0.5,10000  0,0.5        20000       2.3e-13     3.8e-14

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> invincompletegammac(<b>double</b> a, <b>double</b> y0);
</pre>
<a name=unit_jacobianelliptic></a><h4 class=pageheader>8.10.16. jacobianelliptic Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_jacobianellipticfunctions class=toc>jacobianellipticfunctions</a>
]</font>
</div>
<a name=sub_jacobianellipticfunctions></a><h6 class=pageheader>jacobianellipticfunctions Function</h6>
<hr width=600 align=left>
<pre class=narration>
Jacobian Elliptic Functions

Evaluates the Jacobian elliptic functions sn(u|m), cn(u|m),
and dn(u|m) of parameter m between 0 and 1, and real
argument u.

These functions are periodic, with quarter-period on the
real axis equal to the complete elliptic integral
ellpk(1.0-m).

Relation to incomplete elliptic integral:
If u = ellik(phi,m), then sn(u|m) = sin(phi),
and cn(u|m) = cos(phi).  Phi is called the amplitude of u.

Computation is by means of the arithmetic-geometric mean
algorithm, except when m is within 1e-9 of 0 or 1.  In the
latter case with m close to 1, the approximation applies
only for phi &lt; pi/2.

ACCURACY:

Tested at random points with u between 0 and 10, m between
0 and 1.

           Absolute error (* = relative error):
arithmetic   function   # trials      peak         rms
   IEEE      phi         10000       9.2e-16*    1.4e-16*
   IEEE      sn          50000       4.1e-15     4.6e-16
   IEEE      cn          40000       3.6e-15     4.4e-16
   IEEE      dn          10000       1.3e-12     1.8e-14

 Peak error observed in consistency check using addition
theorem for sn(u+v) was 4e-16 (absolute).  Also tested by
the above relation to the incomplete elliptic integral.
Accuracy deteriorates when u is large.

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> jacobianellipticfunctions(<b>double</b> u, <b>double</b> m, <b>double</b> &amp;sn, <b>double</b> &amp;cn, <b>double</b> &amp;dn, <b>double</b> &amp;ph);
</pre>
<a name=unit_laguerre></a><h4 class=pageheader>8.10.17. laguerre Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_laguerrecalculate class=toc>laguerrecalculate</a> |
<a href=#sub_laguerrecoefficients class=toc>laguerrecoefficients</a> |
<a href=#sub_laguerresum class=toc>laguerresum</a>
]</font>
</div>
<a name=sub_laguerrecalculate></a><h6 class=pageheader>laguerrecalculate Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the value of the Laguerre polynomial.

Parameters:
    n   -   degree, n &ge; 0
    x   -   argument

Result:
    the value of the Laguerre polynomial Ln at x
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> laguerrecalculate(ae_int_t n, <b>double</b> x);
</pre>
<a name=sub_laguerrecoefficients></a><h6 class=pageheader>laguerrecoefficients Function</h6>
<hr width=600 align=left>
<pre class=narration>
Representation of Ln as C[0] + C[1]*X + ... + C[N]*X^N

Inputs:
    N   -   polynomial degree, n &ge; 0

Outputs:
    C   -   coefficients
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> laguerrecoefficients(ae_int_t n, real_1d_array &amp;c);
</pre>
<a name=sub_laguerresum></a><h6 class=pageheader>laguerresum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Summation of Laguerre polynomials using Clenshaw's recurrence formula.

This routine calculates c[0]*L0(x) + c[1]*L1(x) + ... + c[N]*LN(x)

Parameters:
    n   -   degree, n &ge; 0
    x   -   argument

Result:
    the value of the Laguerre polynomial at x
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> laguerresum(real_1d_array c, ae_int_t n, <b>double</b> x);
</pre>
<a name=unit_legendre></a><h4 class=pageheader>8.10.18. legendre Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_legendrecalculate class=toc>legendrecalculate</a> |
<a href=#sub_legendrecoefficients class=toc>legendrecoefficients</a> |
<a href=#sub_legendresum class=toc>legendresum</a>
]</font>
</div>
<a name=sub_legendrecalculate></a><h6 class=pageheader>legendrecalculate Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the value of the Legendre polynomial Pn.

Parameters:
    n   -   degree, n &ge; 0
    x   -   argument

Result:
    the value of the Legendre polynomial Pn at x
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> legendrecalculate(ae_int_t n, <b>double</b> x);
</pre>
<a name=sub_legendrecoefficients></a><h6 class=pageheader>legendrecoefficients Function</h6>
<hr width=600 align=left>
<pre class=narration>
Representation of Pn as C[0] + C[1]*X + ... + C[N]*X^N

Inputs:
    N   -   polynomial degree, n &ge; 0

Outputs:
    C   -   coefficients
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> legendrecoefficients(ae_int_t n, real_1d_array &amp;c);
</pre>
<a name=sub_legendresum></a><h6 class=pageheader>legendresum Function</h6>
<hr width=600 align=left>
<pre class=narration>
Summation of Legendre polynomials using Clenshaw's recurrence formula.

This routine calculates
    c[0]*P0(x) + c[1]*P1(x) + ... + c[N]*PN(x)

Parameters:
    n   -   degree, n &ge; 0
    x   -   argument

Result:
    the value of the Legendre polynomial at x
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> legendresum(real_1d_array c, ae_int_t n, <b>double</b> x);
</pre>
<a name=unit_normaldistr></a><h4 class=pageheader>8.10.19. normaldistr Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_bivariatenormalcdf class=toc>bivariatenormalcdf</a> |
<a href=#sub_bivariatenormalpdf class=toc>bivariatenormalpdf</a> |
<a href=#sub_errorfunction class=toc>errorfunction</a> |
<a href=#sub_errorfunctionc class=toc>errorfunctionc</a> |
<a href=#sub_inverf class=toc>inverf</a> |
<a href=#sub_invnormalcdf class=toc>invnormalcdf</a> |
<a href=#sub_invnormaldistribution class=toc>invnormaldistribution</a> |
<a href=#sub_normalcdf class=toc>normalcdf</a> |
<a href=#sub_normaldistribution class=toc>normaldistribution</a> |
<a href=#sub_normalpdf class=toc>normalpdf</a>
]</font>
</div>
<a name=sub_bivariatenormalcdf></a><h6 class=pageheader>bivariatenormalcdf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bivariate normal CDF

Returns the area under the bivariate Gaussian  PDF  with  correlation
parameter equal to Rho, integrated from minus infinity to (x,y):

                                          x      y
                                          -      -
                            1            | |    | |
    bvn(x,y,rho) = -------------------   |      |   f(u,v,rho)*du*dv
                    2pi*sqrt(1-rho^2)  | |    | |
                                        -      -
                                       -INF   -INF

where

                      (    u^2 - 2*rho*u*v + v^2  )
    f(u,v,rho)   = exp( - ----------------------- )
                      (        2*(1-rho^2)        )

with -1 &lt; rho &lt; +1 and arbitrary x, y.

This subroutine uses high-precision approximation scheme proposed  by
Alan Genz in &quot;Numerical  Computation  of  Rectangular  Bivariate  and
Trivariate Normal and  t  probabilities&quot;,  which  computes  CDF  with
absolute error roughly equal to 1e-14.

This function won't fail as long as Rho is in (-1,+1) range.
ALGLIB: Copyright 15.11.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> bivariatenormalcdf(<b>double</b> x, <b>double</b> y, <b>double</b> rho);
</pre>
<a name=sub_bivariatenormalpdf></a><h6 class=pageheader>bivariatenormalpdf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Bivariate normal PDF

Returns probability density function of the bivariate  Gaussian  with
correlation parameter equal to Rho:

                         1              (    x^2 - 2*rho*x*y + y^2  )
    f(x,y,rho) = ----------------- * exp( - ----------------------- )
                 2pi*sqrt(1-rho^2)      (        2*(1-rho^2)        )

with -1 &lt; rho &lt; +1 and arbitrary x, y.

This function won't fail as long as Rho is in (-1,+1) range.
ALGLIB: Copyright 15.11.2019 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> bivariatenormalpdf(<b>double</b> x, <b>double</b> y, <b>double</b> rho);
</pre>
<a name=sub_errorfunction></a><h6 class=pageheader>errorfunction Function</h6>
<hr width=600 align=left>
<pre class=narration>
Error function

The integral is

                          x
                           -
                2         | |          2
  erf(x)  =  --------     |    exp( - t  ) dt.
             sqrt(pi)   | |
                         -
                          0

For 0 &le; |x| &lt; 1, erf(x) = x * P4(x**2)/Q5(x**2); otherwise
erf(x) = 1 - erfc(x).

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0,1         30000       3.7e-16     1.0e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1988, 1992, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> errorfunction(<b>double</b> x);
</pre>
<a name=sub_errorfunctionc></a><h6 class=pageheader>errorfunctionc Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complementary error function

 1 - erf(x) =

                          inf.
                            -
                 2         | |          2
  erfc(x)  =  --------     |    exp( - t  ) dt
              sqrt(pi)   | |
                          -
                           x

For small x, erfc(x) = 1 - erf(x); otherwise rational
approximations are computed.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE      0,26.6417   30000       5.7e-14     1.5e-14

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1988, 1992, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> errorfunctionc(<b>double</b> x);
</pre>
<a name=sub_inverf></a><h6 class=pageheader>inverf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse of the error function

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1988, 1992, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> inverf(<b>double</b> e);
</pre>
<a name=sub_invnormalcdf></a><h6 class=pageheader>invnormalcdf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse of Normal CDF

Returns the argument, x, for which the area under the
Gaussian probability density function (integrated from
minus infinity to x) is equal to y.

For small arguments 0 &lt; y &lt; exp(-2), the program computes
z = sqrt( -2.0 * log(y) );  then the approximation is
x = z - log(z)/z  - (1/z) P(1/z) / Q(1/z).
There are two rational functions P/Q, one for 0 &lt; y &lt; exp(-32)
and the other for y up to exp(-2).  For larger arguments,
w = y - 0.5, and  x/sqrt(2pi) = w + w**3 R(w**2)/S(w**2)).

ACCURACY:

                     Relative error:
arithmetic   domain        # trials      peak         rms
   IEEE     0.125, 1        20000       7.2e-16     1.3e-16
   IEEE     3e-308, 0.135   50000       4.6e-16     9.8e-17

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1988, 1992, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> invnormalcdf(<b>double</b> y0);
</pre>
<a name=sub_invnormaldistribution></a><h6 class=pageheader>invnormaldistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same as invnormalcdf(), deprecated name
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> invnormaldistribution(<b>double</b> y0);
</pre>
<a name=sub_normalcdf></a><h6 class=pageheader>normalcdf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Normal distribution CDF

Returns the area under the Gaussian probability density
function, integrated from minus infinity to x:

                           x
                            -
                  1        | |          2
   ndtr(x)  = ---------    |    exp( - t /2 ) dt
              sqrt(2pi)  | |
                          -
                         -inf.

            =  ( 1 + erf(z) ) / 2
            =  erfc(z) / 2

where z = x/sqrt(2). Computation is via the functions
erf and erfc.

ACCURACY:

                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE     -13,0        30000       3.4e-14     6.7e-15

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1988, 1992, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> normalcdf(<b>double</b> x);
</pre>
<a name=sub_normaldistribution></a><h6 class=pageheader>normaldistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Same as normalcdf(), obsolete name.
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> normaldistribution(<b>double</b> x);
</pre>
<a name=sub_normalpdf></a><h6 class=pageheader>normalpdf Function</h6>
<hr width=600 align=left>
<pre class=narration>
Normal distribution PDF

Returns Gaussian probability density function:

               1
   f(x)  = --------- * exp(-x^2/2)
           sqrt(2pi)

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1988, 1992, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> normalpdf(<b>double</b> x);
</pre>
<a name=unit_poissondistr></a><h4 class=pageheader>8.10.20. poissondistr Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_invpoissondistribution class=toc>invpoissondistribution</a> |
<a href=#sub_poissoncdistribution class=toc>poissoncdistribution</a> |
<a href=#sub_poissondistribution class=toc>poissondistribution</a>
]</font>
</div>
<a name=sub_invpoissondistribution></a><h6 class=pageheader>invpoissondistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Inverse Poisson distribution

Finds the Poisson variable x such that the integral
from 0 to x of the Poisson density is equal to the
given probability y.

This is accomplished using the inverse gamma integral
function and the relation

   m = igami( k+1, y ).

ACCURACY:

See inverse incomplete gamma function

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> invpoissondistribution(ae_int_t k, <b>double</b> y);
</pre>
<a name=sub_poissoncdistribution></a><h6 class=pageheader>poissoncdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Complemented Poisson distribution

Returns the sum of the terms k+1 to infinity of the Poisson
distribution:

 inf.       j
  --   -m  m
  &gt;   e    --
  --       j!
 j=k+1

The terms are not summed directly; instead the incomplete
gamma integral is employed, according to the formula

y = pdtrc( k, m ) = igam( k+1, m ).

The arguments must both be positive.

ACCURACY:

See incomplete gamma function

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> poissoncdistribution(ae_int_t k, <b>double</b> m);
</pre>
<a name=sub_poissondistribution></a><h6 class=pageheader>poissondistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Poisson distribution

Returns the sum of the first k+1 terms of the Poisson
distribution:

  k         j
  --   -m  m
  &gt;   e    --
  --       j!
 j=0

The terms are not summed directly; instead the incomplete
gamma integral is employed, according to the relation

y = pdtr( k, m ) = igamc( k+1, m ).

The arguments must both be positive.

ACCURACY:

See incomplete gamma function

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> poissondistribution(ae_int_t k, <b>double</b> m);
</pre>
<a name=unit_psif></a><h4 class=pageheader>8.10.21. psif Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_psi class=toc>psi</a>
]</font>
</div>
<a name=sub_psi></a><h6 class=pageheader>psi Function</h6>
<hr width=600 align=left>
<pre class=narration>
Psi (digamma) function

             d      -
  psi(x)  =  -- ln | (x)
             dx

is the logarithmic derivative of the gamma function.
For integer x,
                  n-1
                   -
psi(n) = -EUL  +   &gt;  1/k.
                   -
                  k=1

This formula is used for 0 &lt; n &le; 10.  If x is negative, it
is transformed to a positive argument by the reflection
formula  psi(1-x) = psi(x) + pi cot(pi x).
For general positive x, the argument is made greater than 10
using the recurrence  psi(x+1) = psi(x) + 1/x.
Then the following asymptotic expansion is applied:

                          inf.   B
                           -      2k
psi(x) = log(x) - 1/2x -   &gt;   -------
                           -        2k
                          k=1   2k x

where the B2k are Bernoulli numbers.

ACCURACY:
   Relative error (except absolute when |psi| &lt; 1):
arithmetic   domain     # trials      peak         rms
   IEEE      0,30        30000       1.3e-15     1.4e-16
   IEEE      -30,0       40000       1.5e-15     2.2e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1992, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> psi(<b>double</b> x);
</pre>
<a name=unit_studenttdistr></a><h4 class=pageheader>8.10.22. studenttdistr Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_invstudenttdistribution class=toc>invstudenttdistribution</a> |
<a href=#sub_studenttdistribution class=toc>studenttdistribution</a>
]</font>
</div>
<a name=sub_invstudenttdistribution></a><h6 class=pageheader>invstudenttdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Functional inverse of Student's t distribution

Given probability p, finds the argument t such that stdtr(k,t)
is equal to p.

ACCURACY:

Tested at random 1 &le; k &le; 100.  The &quot;domain&quot; refers to p:
                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE    .001,.999     25000       5.7e-15     8.0e-16
   IEEE    10^-6,.001    25000       2.0e-12     2.9e-14

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> invstudenttdistribution(ae_int_t k, <b>double</b> p);
</pre>
<a name=sub_studenttdistribution></a><h6 class=pageheader>studenttdistribution Function</h6>
<hr width=600 align=left>
<pre class=narration>
Student's t distribution

Computes the integral from minus infinity to t of the Student
t distribution with integer k &gt; 0 degrees of freedom:

                                     t
                                     -
                                    | |
             -                      |         2   -(k+1)/2
            | ( (k+1)/2 )           |  (     x   )
      ----------------------        |  ( 1 + --- )        dx
                    -               |  (      k  )
      sqrt( k pi ) | ( k/2 )        |
                                  | |
                                   -
                                  -inf.

Relation to incomplete beta integral:

       1 - stdtr(k,t) = 0.5 * incbet( k/2, 1/2, z )
where
       z = k/(k + t**2).

For t &lt; -2, this is the method of computation.  For higher t,
a direct method is derived from integration by parts.
Since the function is symmetric about t=0, the area under the
right tail of the density is found by calling the function
with -t instead of t.

ACCURACY:

Tested at random 1 &le; k &le; 25.  The &quot;domain&quot; refers to t.
                     Relative error:
arithmetic   domain     # trials      peak         rms
   IEEE     -100,-2      50000       5.9e-15     1.4e-15
   IEEE     -2,100      500000       2.7e-15     4.9e-17

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 1995, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> studenttdistribution(ae_int_t k, <b>double</b> t);
</pre>
<a name=unit_trigintegrals></a><h4 class=pageheader>8.10.23. trigintegrals Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_hyperbolicsinecosineintegrals class=toc>hyperbolicsinecosineintegrals</a> |
<a href=#sub_sinecosineintegrals class=toc>sinecosineintegrals</a>
]</font>
</div>
<a name=sub_hyperbolicsinecosineintegrals></a><h6 class=pageheader>hyperbolicsinecosineintegrals Function</h6>
<hr width=600 align=left>
<pre class=narration>
Hyperbolic sine and cosine integrals

Approximates the integrals

                           x
                           -
                          | |   cosh t - 1
  Chi(x) = eul + ln x +   |    -----------  dt,
                        | |          t
                         -
                         0

              x
              -
             | |  sinh t
  Shi(x) =   |    ------  dt
           | |       t
            -
            0

where eul = 0.57721566490153286061 is Euler's constant.
The integrals are evaluated by power series for x &lt; 8
and by Chebyshev expansions for x between 8 and 88.
For large x, both functions approach exp(x)/2x.
Arguments greater than 88 in magnitude return MAXNUM.

ACCURACY:

Test interval 0 to 88.
                     Relative error:
arithmetic   function  # trials      peak         rms
   IEEE         Shi      30000       6.9e-16     1.6e-16
       Absolute error, except relative when |Chi| &gt; 1:
   IEEE         Chi      30000       8.4e-16     1.4e-16

Cephes Math Library Release 2.8:  June, 2000
Copyright 1984, 1987, 2000 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> hyperbolicsinecosineintegrals(<b>double</b> x, <b>double</b> &amp;shi, <b>double</b> &amp;chi);
</pre>
<a name=sub_sinecosineintegrals></a><h6 class=pageheader>sinecosineintegrals Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sine and cosine integrals

Evaluates the integrals

                         x
                         -
                        |  cos t - 1
  Ci(x) = eul + ln x +  |  --------- dt,
                        |      t
                       -
                        0
            x
            -
           |  sin t
  Si(x) =  |  ----- dt
           |    t
          -
           0

where eul = 0.57721566490153286061 is Euler's constant.
The integrals are approximated by rational functions.
For x &gt; 8 auxiliary functions f(x) and g(x) are employed
such that

Ci(x) = f(x) sin(x) - g(x) cos(x)
Si(x) = pi/2 - f(x) cos(x) - g(x) sin(x)

ACCURACY:
   Test interval = [0,50].
Absolute error, except relative when &gt; 1:
arithmetic   function   # trials      peak         rms
   IEEE        Si        30000       4.4e-16     7.3e-17
   IEEE        Ci        30000       6.9e-16     5.1e-17

Cephes Math Library Release 2.1:  January, 1989
Copyright 1984, 1987, 1989 by Stephen L. Moshier
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sinecosineintegrals(<b>double</b> x, <b>double</b> &amp;si, <b>double</b> &amp;ci);
</pre>
</p>
<p>
<a name=pck_Statistics class=sheader></a><h3>8.11. Statistics Package</h3>
<table align=center border=1><tbody>
<tr align=left valign=top><td><a href=#unit_basestat class=toc>basestat</a></td><td>Mean, variance, covariance, correlation, etc.</td></tr>
<tr align=left valign=top><td><a href=#unit_correlationtests class=toc>correlationtests</a></td><td>Hypothesis testing: correlation tests</td></tr>
<tr align=left valign=top><td><a href=#unit_jarquebera class=toc>jarquebera</a></td><td>Hypothesis testing: Jarque-Bera test</td></tr>
<tr align=left valign=top><td><a href=#unit_mannwhitneyu class=toc>mannwhitneyu</a></td><td>Hypothesis testing: Mann-Whitney-U test</td></tr>
<tr align=left valign=top><td><a href=#unit_stest class=toc>stest</a></td><td>Hypothesis testing: sign test</td></tr>
<tr align=left valign=top><td><a href=#unit_studentttests class=toc>studentttests</a></td><td>Hypothesis testing: Student's t-test</td></tr>
<tr align=left valign=top><td><a href=#unit_variancetests class=toc>variancetests</a></td><td>Hypothesis testing: F-test and one-sample variance test</td></tr>
<tr align=left valign=top><td><a href=#unit_wsr class=toc>wsr</a></td><td>Hypothesis testing: Wilcoxon signed rank test</td></tr>
</tbody></table>
</p>
<p>
<a name=unit_basestat></a><h4 class=pageheader>8.11.1. basestat Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_cov2 class=toc>cov2</a> |
<a href=#sub_covm class=toc>covm</a> |
<a href=#sub_covm2 class=toc>covm2</a> |
<a href=#sub_pearsoncorr2 class=toc>pearsoncorr2</a> |
<a href=#sub_pearsoncorrelation class=toc>pearsoncorrelation</a> |
<a href=#sub_pearsoncorrm class=toc>pearsoncorrm</a> |
<a href=#sub_pearsoncorrm2 class=toc>pearsoncorrm2</a> |
<a href=#sub_rankdata class=toc>rankdata</a> |
<a href=#sub_rankdatacentered class=toc>rankdatacentered</a> |
<a href=#sub_sampleadev class=toc>sampleadev</a> |
<a href=#sub_samplekurtosis class=toc>samplekurtosis</a> |
<a href=#sub_samplemean class=toc>samplemean</a> |
<a href=#sub_samplemedian class=toc>samplemedian</a> |
<a href=#sub_samplemoments class=toc>samplemoments</a> |
<a href=#sub_samplepercentile class=toc>samplepercentile</a> |
<a href=#sub_sampleskewness class=toc>sampleskewness</a> |
<a href=#sub_samplevariance class=toc>samplevariance</a> |
<a href=#sub_spearmancorr2 class=toc>spearmancorr2</a> |
<a href=#sub_spearmancorrm class=toc>spearmancorrm</a> |
<a href=#sub_spearmancorrm2 class=toc>spearmancorrm2</a> |
<a href=#sub_spearmanrankcorrelation class=toc>spearmanrankcorrelation</a>
]</font>
<h5>Examples</h5>
<table border=0 cellspacing=0 class=pagecontent>
<tr align=left valign=top><td><a href=#example_basestat_d_base class=toc>basestat_d_base</a></td><td width=15>&nbsp;</td><td>Basic functionality (moments, adev, median, percentile)</td></tr>
<tr align=left valign=top><td><a href=#example_basestat_d_c2 class=toc>basestat_d_c2</a></td><td width=15>&nbsp;</td><td>Correlation (covariance) between two random variables</td></tr>
<tr align=left valign=top><td><a href=#example_basestat_d_cm class=toc>basestat_d_cm</a></td><td width=15>&nbsp;</td><td>Correlation (covariance) between components of random vector</td></tr>
<tr align=left valign=top><td><a href=#example_basestat_d_cm2 class=toc>basestat_d_cm2</a></td><td width=15>&nbsp;</td><td>Correlation (covariance) between two random vectors</td></tr>
</table>
</div>
<a name=sub_cov2></a><h6 class=pageheader>cov2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
2-sample covariance

Inputs:
    X       -   sample 1 (array indexes: [0..N-1])
    Y       -   sample 2 (array indexes: [0..N-1])
    N       -   N &ge; 0, sample size:
                * if given, only N leading elements of X/Y are processed
                * if not given, automatically determined from input sizes

Result:
    covariance (zero for N=0 or N=1)
ALGLIB: Copyright 28.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> cov2(real_1d_array x, real_1d_array y, ae_int_t n);
<b>double</b> cov2(real_1d_array x, real_1d_array y);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_c2 class=nav>basestat_d_c2</a> ]</p>
<a name=sub_covm></a><h6 class=pageheader>covm Function</h6>
<hr width=600 align=left>
<pre class=narration>
Covariance matrix

Inputs:
    X   -   array[N,M], sample matrix:
            * J-th column corresponds to J-th variable
            * I-th row corresponds to I-th observation
    N   -   N &ge; 0, number of observations:
            * if given, only leading N rows of X are used
            * if not given, automatically determined from input size
    M   -   M &gt; 0, number of variables:
            * if given, only leading M columns of X are used
            * if not given, automatically determined from input size

Outputs:
    C   -   array[M,M], covariance matrix (zero if N=0 or N=1)
ALGLIB: Copyright 28.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> covm(real_2d_array x, ae_int_t n, ae_int_t m, real_2d_array &amp;c);
<b>void</b> covm(real_2d_array x, real_2d_array &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_cm class=nav>basestat_d_cm</a> ]</p>
<a name=sub_covm2></a><h6 class=pageheader>covm2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Cross-covariance matrix

Inputs:
    X   -   array[N,M1], sample matrix:
            * J-th column corresponds to J-th variable
            * I-th row corresponds to I-th observation
    Y   -   array[N,M2], sample matrix:
            * J-th column corresponds to J-th variable
            * I-th row corresponds to I-th observation
    N   -   N &ge; 0, number of observations:
            * if given, only leading N rows of X/Y are used
            * if not given, automatically determined from input sizes
    M1  -   M1 &gt; 0, number of variables in X:
            * if given, only leading M1 columns of X are used
            * if not given, automatically determined from input size
    M2  -   M2 &gt; 0, number of variables in Y:
            * if given, only leading M1 columns of X are used
            * if not given, automatically determined from input size

Outputs:
    C   -   array[M1,M2], cross-covariance matrix (zero if N=0 or N=1)
ALGLIB: Copyright 28.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> covm2(real_2d_array x, real_2d_array y, ae_int_t n, ae_int_t m1, ae_int_t m2, real_2d_array &amp;c);
<b>void</b> covm2(real_2d_array x, real_2d_array y, real_2d_array &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_cm2 class=nav>basestat_d_cm2</a> ]</p>
<a name=sub_pearsoncorr2></a><h6 class=pageheader>pearsoncorr2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Pearson product-moment correlation coefficient

Inputs:
    X       -   sample 1 (array indexes: [0..N-1])
    Y       -   sample 2 (array indexes: [0..N-1])
    N       -   N &ge; 0, sample size:
                * if given, only N leading elements of X/Y are processed
                * if not given, automatically determined from input sizes

Result:
    Pearson product-moment correlation coefficient
    (zero for N=0 or N=1)
ALGLIB: Copyright 28.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> pearsoncorr2(real_1d_array x, real_1d_array y, ae_int_t n);
<b>double</b> pearsoncorr2(real_1d_array x, real_1d_array y);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_c2 class=nav>basestat_d_c2</a> ]</p>
<a name=sub_pearsoncorrelation></a><h6 class=pageheader>pearsoncorrelation Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete function, we recommend to use PearsonCorr2().
ALGLIB: Copyright 09.04.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> pearsoncorrelation(real_1d_array x, real_1d_array y, ae_int_t n);
</pre>
<a name=sub_pearsoncorrm></a><h6 class=pageheader>pearsoncorrm Function</h6>
<hr width=600 align=left>
<pre class=narration>
Pearson product-moment correlation matrix

Inputs:
    X   -   array[N,M], sample matrix:
            * J-th column corresponds to J-th variable
            * I-th row corresponds to I-th observation
    N   -   N &ge; 0, number of observations:
            * if given, only leading N rows of X are used
            * if not given, automatically determined from input size
    M   -   M &gt; 0, number of variables:
            * if given, only leading M columns of X are used
            * if not given, automatically determined from input size

Outputs:
    C   -   array[M,M], correlation matrix (zero if N=0 or N=1)
ALGLIB: Copyright 28.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pearsoncorrm(real_2d_array x, ae_int_t n, ae_int_t m, real_2d_array &amp;c);
<b>void</b> pearsoncorrm(real_2d_array x, real_2d_array &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_cm class=nav>basestat_d_cm</a> ]</p>
<a name=sub_pearsoncorrm2></a><h6 class=pageheader>pearsoncorrm2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Pearson product-moment cross-correlation matrix

Inputs:
    X   -   array[N,M1], sample matrix:
            * J-th column corresponds to J-th variable
            * I-th row corresponds to I-th observation
    Y   -   array[N,M2], sample matrix:
            * J-th column corresponds to J-th variable
            * I-th row corresponds to I-th observation
    N   -   N &ge; 0, number of observations:
            * if given, only leading N rows of X/Y are used
            * if not given, automatically determined from input sizes
    M1  -   M1 &gt; 0, number of variables in X:
            * if given, only leading M1 columns of X are used
            * if not given, automatically determined from input size
    M2  -   M2 &gt; 0, number of variables in Y:
            * if given, only leading M1 columns of X are used
            * if not given, automatically determined from input size

Outputs:
    C   -   array[M1,M2], cross-correlation matrix (zero if N=0 or N=1)
ALGLIB: Copyright 28.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pearsoncorrm2(real_2d_array x, real_2d_array y, ae_int_t n, ae_int_t m1, ae_int_t m2, real_2d_array &amp;c);
<b>void</b> pearsoncorrm2(real_2d_array x, real_2d_array y, real_2d_array &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_cm2 class=nav>basestat_d_cm2</a> ]</p>
<a name=sub_rankdata></a><h6 class=pageheader>rankdata Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function replaces data in XY by their ranks:
* XY is processed row-by-row
* rows are processed separately
* tied data are correctly handled (tied ranks are calculated)
* ranking starts from 0, ends at NFeatures-1
* sum of within-row values is equal to (NFeatures-1)*NFeatures/2

Inputs:
    XY      -   array[NPoints,NFeatures], dataset
    NPoints -   number of points
    NFeatures-  number of features

Outputs:
    XY      -   data are replaced by their within-row ranks;
                ranking starts from 0, ends at NFeatures-1
ALGLIB: Copyright 18.04.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rankdata(real_2d_array xy, ae_int_t npoints, ae_int_t nfeatures);
<b>void</b> rankdata(real_2d_array xy);
</pre>
<a name=sub_rankdatacentered></a><h6 class=pageheader>rankdatacentered Function</h6>
<hr width=600 align=left>
<pre class=narration>
This function replaces data in XY by their CENTERED ranks:
* XY is processed row-by-row
* rows are processed separately
* tied data are correctly handled (tied ranks are calculated)
* centered ranks are just usual ranks, but centered in such way  that  sum
  of within-row values is equal to 0.0.
* centering is performed by subtracting mean from each row, i.e. it changes
  mean value, but does NOT change higher moments

Inputs:
    XY      -   array[NPoints,NFeatures], dataset
    NPoints -   number of points
    NFeatures-  number of features

Outputs:
    XY      -   data are replaced by their within-row ranks;
                ranking starts from 0, ends at NFeatures-1
ALGLIB: Copyright 18.04.2013 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> rankdatacentered(real_2d_array xy, ae_int_t npoints, ae_int_t nfeatures);
<b>void</b> rankdatacentered(real_2d_array xy);
</pre>
<a name=sub_sampleadev></a><h6 class=pageheader>sampleadev Function</h6>
<hr width=600 align=left>
<pre class=narration>
ADev

Inputs:
    X   -   sample
    N   -   N &ge; 0, sample size:
            * if given, only leading N elements of X are processed
            * if not given, automatically determined from size of X

Outputs:
    ADev-   ADev
ALGLIB: Copyright 06.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> sampleadev(real_1d_array x, ae_int_t n, <b>double</b> &amp;adev);
<b>void</b> sampleadev(real_1d_array x, <b>double</b> &amp;adev);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_base class=nav>basestat_d_base</a> ]</p>
<a name=sub_samplekurtosis></a><h6 class=pageheader>samplekurtosis Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the kurtosis.

Inputs:
    X       -   sample
    N       -   N &ge; 0, sample size:
                * if given, only leading N elements of X are processed
                * if not given, automatically determined from size of X

NOTE:

This function return result  which calculated by 'SampleMoments' function
and stored at 'Kurtosis' variable.
ALGLIB: Copyright 06.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> samplekurtosis(real_1d_array x, ae_int_t n);
<b>double</b> samplekurtosis(real_1d_array x);
</pre>
<a name=sub_samplemean></a><h6 class=pageheader>samplemean Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the mean.

Inputs:
    X       -   sample
    N       -   N &ge; 0, sample size:
                * if given, only leading N elements of X are processed
                * if not given, automatically determined from size of X

NOTE:

This function return result  which calculated by 'SampleMoments' function
and stored at 'Mean' variable.
ALGLIB: Copyright 06.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> samplemean(real_1d_array x, ae_int_t n);
<b>double</b> samplemean(real_1d_array x);
</pre>
<a name=sub_samplemedian></a><h6 class=pageheader>samplemedian Function</h6>
<hr width=600 align=left>
<pre class=narration>
Median calculation.

Inputs:
    X   -   sample (array indexes: [0..N-1])
    N   -   N &ge; 0, sample size:
            * if given, only leading N elements of X are processed
            * if not given, automatically determined from size of X

Outputs:
    Median
ALGLIB: Copyright 06.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> samplemedian(real_1d_array x, ae_int_t n, <b>double</b> &amp;median);
<b>void</b> samplemedian(real_1d_array x, <b>double</b> &amp;median);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_base class=nav>basestat_d_base</a> ]</p>
<a name=sub_samplemoments></a><h6 class=pageheader>samplemoments Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the distribution moments: mean, variance, skewness, kurtosis.

Inputs:
    X       -   sample
    N       -   N &ge; 0, sample size:
                * if given, only leading N elements of X are processed
                * if not given, automatically determined from size of X

Outputs:
    Mean    -   mean.
    Variance-   variance.
    Skewness-   skewness (if variance &ne; 0; zero otherwise).
    Kurtosis-   kurtosis (if variance &ne; 0; zero otherwise).

NOTE: variance is calculated by dividing sum of squares by N-1, not N.
ALGLIB: Copyright 06.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> samplemoments(real_1d_array x, ae_int_t n, <b>double</b> &amp;mean, <b>double</b> &amp;variance, <b>double</b> &amp;skewness, <b>double</b> &amp;kurtosis);
<b>void</b> samplemoments(real_1d_array x, <b>double</b> &amp;mean, <b>double</b> &amp;variance, <b>double</b> &amp;skewness, <b>double</b> &amp;kurtosis);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_base class=nav>basestat_d_base</a> ]</p>
<a name=sub_samplepercentile></a><h6 class=pageheader>samplepercentile Function</h6>
<hr width=600 align=left>
<pre class=narration>
Percentile calculation.

Inputs:
    X   -   sample (array indexes: [0..N-1])
    N   -   N &ge; 0, sample size:
            * if given, only leading N elements of X are processed
            * if not given, automatically determined from size of X
    P   -   percentile (0 &le; P &le; 1)

Outputs:
    V   -   percentile
ALGLIB: Copyright 01.03.2008 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> samplepercentile(real_1d_array x, ae_int_t n, <b>double</b> p, <b>double</b> &amp;v);
<b>void</b> samplepercentile(real_1d_array x, <b>double</b> p, <b>double</b> &amp;v);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_base class=nav>basestat_d_base</a> ]</p>
<a name=sub_sampleskewness></a><h6 class=pageheader>sampleskewness Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the skewness.

Inputs:
    X       -   sample
    N       -   N &ge; 0, sample size:
                * if given, only leading N elements of X are processed
                * if not given, automatically determined from size of X

NOTE:

This function return result  which calculated by 'SampleMoments' function
and stored at 'Skewness' variable.
ALGLIB: Copyright 06.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> sampleskewness(real_1d_array x, ae_int_t n);
<b>double</b> sampleskewness(real_1d_array x);
</pre>
<a name=sub_samplevariance></a><h6 class=pageheader>samplevariance Function</h6>
<hr width=600 align=left>
<pre class=narration>
Calculation of the variance.

Inputs:
    X       -   sample
    N       -   N &ge; 0, sample size:
                * if given, only leading N elements of X are processed
                * if not given, automatically determined from size of X

NOTE:

This function return result  which calculated by 'SampleMoments' function
and stored at 'Variance' variable.
ALGLIB: Copyright 06.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> samplevariance(real_1d_array x, ae_int_t n);
<b>double</b> samplevariance(real_1d_array x);
</pre>
<a name=sub_spearmancorr2></a><h6 class=pageheader>spearmancorr2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Spearman's rank correlation coefficient

Inputs:
    X       -   sample 1 (array indexes: [0..N-1])
    Y       -   sample 2 (array indexes: [0..N-1])
    N       -   N &ge; 0, sample size:
                * if given, only N leading elements of X/Y are processed
                * if not given, automatically determined from input sizes

Result:
    Spearman's rank correlation coefficient
    (zero for N=0 or N=1)
ALGLIB: Copyright 09.04.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spearmancorr2(real_1d_array x, real_1d_array y, ae_int_t n);
<b>double</b> spearmancorr2(real_1d_array x, real_1d_array y);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_c2 class=nav>basestat_d_c2</a> ]</p>
<a name=sub_spearmancorrm></a><h6 class=pageheader>spearmancorrm Function</h6>
<hr width=600 align=left>
<pre class=narration>
Spearman's rank correlation matrix

Inputs:
    X   -   array[N,M], sample matrix:
            * J-th column corresponds to J-th variable
            * I-th row corresponds to I-th observation
    N   -   N &ge; 0, number of observations:
            * if given, only leading N rows of X are used
            * if not given, automatically determined from input size
    M   -   M &gt; 0, number of variables:
            * if given, only leading M columns of X are used
            * if not given, automatically determined from input size

Outputs:
    C   -   array[M,M], correlation matrix (zero if N=0 or N=1)
ALGLIB: Copyright 28.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spearmancorrm(real_2d_array x, ae_int_t n, ae_int_t m, real_2d_array &amp;c);
<b>void</b> spearmancorrm(real_2d_array x, real_2d_array &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_cm class=nav>basestat_d_cm</a> ]</p>
<a name=sub_spearmancorrm2></a><h6 class=pageheader>spearmancorrm2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Spearman's rank cross-correlation matrix

Inputs:
    X   -   array[N,M1], sample matrix:
            * J-th column corresponds to J-th variable
            * I-th row corresponds to I-th observation
    Y   -   array[N,M2], sample matrix:
            * J-th column corresponds to J-th variable
            * I-th row corresponds to I-th observation
    N   -   N &ge; 0, number of observations:
            * if given, only leading N rows of X/Y are used
            * if not given, automatically determined from input sizes
    M1  -   M1 &gt; 0, number of variables in X:
            * if given, only leading M1 columns of X are used
            * if not given, automatically determined from input size
    M2  -   M2 &gt; 0, number of variables in Y:
            * if given, only leading M1 columns of X are used
            * if not given, automatically determined from input size

Outputs:
    C   -   array[M1,M2], cross-correlation matrix (zero if N=0 or N=1)
ALGLIB: Copyright 28.10.2010 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spearmancorrm2(real_2d_array x, real_2d_array y, ae_int_t n, ae_int_t m1, ae_int_t m2, real_2d_array &amp;c);
<b>void</b> spearmancorrm2(real_2d_array x, real_2d_array y, real_2d_array &amp;c);
</pre>
<p align=left class=pagecontent><span class=inlineheader>Examples:</span> [ <a href=#example_basestat_d_cm2 class=nav>basestat_d_cm2</a> ]</p>
<a name=sub_spearmanrankcorrelation></a><h6 class=pageheader>spearmanrankcorrelation Function</h6>
<hr width=600 align=left>
<pre class=narration>
Obsolete function, we recommend to use SpearmanCorr2().
ALGLIB: Copyright 09.04.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>double</b> spearmanrankcorrelation(real_1d_array x, real_1d_array y, ae_int_t n);
</pre>
<a name=example_basestat_d_base></a><h6 class=pageheader>basestat_d_base Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Statistics.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
   real_1d_array x = <font color=blue><b>&quot;[0,1,4,9,16,25,36,49,64,81]&quot;</b></font>;
   <b>double</b> mean;
   <b>double</b> variance;
   <b>double</b> skewness;
   <b>double</b> kurtosis;
   <b>double</b> adev;
   <b>double</b> p;
   <b>double</b> v;
<font color=navy>// Here we demonstrate calculation of sample moments</font>
<font color=navy>// (mean, variance, skewness, kurtosis)</font>
   samplemoments(x, mean, variance, skewness, kurtosis);
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(mean)); <font color=navy>// EXPECTED: 28.5</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(variance)); <font color=navy>// EXPECTED: 801.1667</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(skewness)); <font color=navy>// EXPECTED: 0.5751</font>
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(kurtosis)); <font color=navy>// EXPECTED: -1.2666</font>
<font color=navy>//</font>
<font color=navy>// Average deviation</font>
   sampleadev(x, adev);
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(adev)); <font color=navy>// EXPECTED: 23.2</font>
<font color=navy>//</font>
<font color=navy>// Median and percentile</font>
   samplemedian(x, v);
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 20.5</font>
   p = 0.5;
   samplepercentile(x, p, v);
   printf(<font color=blue><b>&quot;%.1f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 20.5</font>
   <b>return</b> 0;
}
</pre>
<a name=example_basestat_d_c2></a><h6 class=pageheader>basestat_d_c2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Statistics.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// We have two samples - x and y, and want to measure dependency between them</font>
   real_1d_array x = <font color=blue><b>&quot;[0,1,4,9,16,25,36,49,64,81]&quot;</b></font>;
   real_1d_array y = <font color=blue><b>&quot;[0,1,2,3,4,5,6,7,8,9]&quot;</b></font>;
   <b>double</b> v;
<font color=navy>// Three dependency measures are calculated:</font>
<font color=navy>// * covariation</font>
<font color=navy>// * Pearson correlation</font>
<font color=navy>// * Spearman rank correlation</font>
   v = cov2(x, y);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 82.5</font>
   v = pearsoncorr2(x, y);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 0.9627</font>
   v = spearmancorr2(x, y);
   printf(<font color=blue><b>&quot;%.2f\n&quot;</b></font>, <b>double</b>(v)); <font color=navy>// EXPECTED: 1.000</font>
   <b>return</b> 0;
}
</pre>
<a name=example_basestat_d_cm></a><h6 class=pageheader>basestat_d_cm Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Statistics.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// X is a sample matrix:</font>
<font color=navy>// * I-th row corresponds to I-th observation</font>
<font color=navy>// * J-th column corresponds to J-th variable</font>
   real_2d_array x = <font color=blue><b>&quot;[[1,0,1],[1,1,0],[-1,1,0],[-2,-1,1],[-1,0,9]]&quot;</b></font>;
   real_2d_array c;
<font color=navy>// Three dependency measures are calculated:</font>
<font color=navy>// * covariation</font>
<font color=navy>// * Pearson correlation</font>
<font color=navy>// * Spearman rank correlation</font>
<font color=navy>//</font>
<font color=navy>// Result is stored into C, with C[i,j] equal to correlation</font>
<font color=navy>// (covariance) between I-th and J-th variables of X.</font>
   covm(x, c);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [[1.80,0.60,-1.40],[0.60,0.70,-0.80],[-1.40,-0.80,14.70]]</font>
   pearsoncorrm(x, c);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [[1.000,0.535,-0.272],[0.535,1.000,-0.249],[-0.272,-0.249,1.000]]</font>
   spearmancorrm(x, c);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [[1.000,0.556,-0.306],[0.556,1.000,-0.750],[-0.306,-0.750,1.000]]</font>
   <b>return</b> 0;
}
</pre>
<a name=example_basestat_d_cm2></a><h6 class=pageheader>basestat_d_cm2 Example</h6>
<pre class=source>
#include <font color=blue><b>&quot;Statistics.h&quot;</b></font>

using namespace alglib;

<b>int</b> main(<b>int</b> argc, char **argv) {
<font color=navy>// X and Y are sample matrices:</font>
<font color=navy>// * I-th row corresponds to I-th observation</font>
<font color=navy>// * J-th column corresponds to J-th variable</font>
   real_2d_array x = <font color=blue><b>&quot;[[1,0,1],[1,1,0],[-1,1,0],[-2,-1,1],[-1,0,9]]&quot;</b></font>;
   real_2d_array y = <font color=blue><b>&quot;[[2,3],[2,1],[-1,6],[-9,9],[7,1]]&quot;</b></font>;
   real_2d_array c;
<font color=navy>// Three dependency measures are calculated:</font>
<font color=navy>// * covariation</font>
<font color=navy>// * Pearson correlation</font>
<font color=navy>// * Spearman rank correlation</font>
<font color=navy>//</font>
<font color=navy>// Result is stored into C, with C[i,j] equal to correlation</font>
<font color=navy>// (covariance) between I-th variable of X and J-th variable of Y.</font>
   covm2(x, y, c);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [[4.100,-3.250],[2.450,-1.500],[13.450,-5.750]]</font>
   pearsoncorrm2(x, y, c);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [[0.519,-0.699],[0.497,-0.518],[0.596,-0.433]]</font>
   spearmancorrm2(x, y, c);
   printf(<font color=blue><b>&quot;%s\n&quot;</b></font>, c.tostring(1).c_str()); <font color=navy>// EXPECTED: [[0.541,-0.649],[0.216,-0.433],[0.433,-0.135]]</font>
   <b>return</b> 0;
}
</pre>
<a name=unit_correlationtests></a><h4 class=pageheader>8.11.2. correlationtests Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_pearsoncorrelationsignificance class=toc>pearsoncorrelationsignificance</a> |
<a href=#sub_spearmanrankcorrelationsignificance class=toc>spearmanrankcorrelationsignificance</a>
]</font>
</div>
<a name=sub_pearsoncorrelationsignificance></a><h6 class=pageheader>pearsoncorrelationsignificance Function</h6>
<hr width=600 align=left>
<pre class=narration>
Pearson's correlation coefficient significance test

This test checks hypotheses about whether X  and  Y  are  samples  of  two
continuous  distributions  having  zero  correlation  or   whether   their
correlation is non-zero.

The following tests are performed:
    * two-tailed test (null hypothesis - X and Y have zero correlation)
    * left-tailed test (null hypothesis - the correlation  coefficient  is
      greater than or equal to 0)
    * right-tailed test (null hypothesis - the correlation coefficient  is
      less than or equal to 0).

Requirements:
    * the number of elements in each sample is not less than 5
    * normality of distributions of X and Y.

Inputs:
    R   -   Pearson's correlation coefficient for X and Y
    N   -   number of elements in samples, N &ge; 5.

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.
ALGLIB: Copyright 09.04.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> pearsoncorrelationsignificance(<b>double</b> r, ae_int_t n, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
<a name=sub_spearmanrankcorrelationsignificance></a><h6 class=pageheader>spearmanrankcorrelationsignificance Function</h6>
<hr width=600 align=left>
<pre class=narration>
Spearman's rank correlation coefficient significance test

This test checks hypotheses about whether X  and  Y  are  samples  of  two
continuous  distributions  having  zero  correlation  or   whether   their
correlation is non-zero.

The following tests are performed:
    * two-tailed test (null hypothesis - X and Y have zero correlation)
    * left-tailed test (null hypothesis - the correlation  coefficient  is
      greater than or equal to 0)
    * right-tailed test (null hypothesis - the correlation coefficient  is
      less than or equal to 0).

Requirements:
    * the number of elements in each sample is not less than 5.

The test is non-parametric and doesn't require distributions X and Y to be
normal.

Inputs:
    R   -   Spearman's rank correlation coefficient for X and Y
    N   -   number of elements in samples, N &ge; 5.

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.
ALGLIB: Copyright 09.04.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> spearmanrankcorrelationsignificance(<b>double</b> r, ae_int_t n, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
<a name=unit_jarquebera></a><h4 class=pageheader>8.11.3. jarquebera Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_jarqueberatest class=toc>jarqueberatest</a>
]</font>
</div>
<a name=sub_jarqueberatest></a><h6 class=pageheader>jarqueberatest Function</h6>
<hr width=600 align=left>
<pre class=narration>
Jarque-Bera test

This test checks hypotheses about the fact that a  given  sample  X  is  a
sample of normal random variable.

Requirements:
    * the number of elements in the sample is not less than 5.

Inputs:
    X   -   sample. Array whose index goes from 0 to N-1.
    N   -   size of the sample. N &ge; 5

Outputs:
    P           -   p-value for the test

Accuracy of the approximation used (5 &le; N &le; 1951):

p-value          relative error (5 &le; N &le; 1951)
[1, 0.1]            &lt; 1%
[0.1, 0.01]         &lt; 2%
[0.01, 0.001]       &lt; 6%
[0.001, 0]          wasn't measured

For N &gt; 1951 accuracy wasn't measured but it shouldn't be sharply  different
from table values.
ALGLIB: Copyright 09.04.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> jarqueberatest(real_1d_array x, ae_int_t n, <b>double</b> &amp;p);
</pre>
<a name=unit_mannwhitneyu></a><h4 class=pageheader>8.11.4. mannwhitneyu Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_mannwhitneyutest class=toc>mannwhitneyutest</a>
]</font>
</div>
<a name=sub_mannwhitneyutest></a><h6 class=pageheader>mannwhitneyutest Function</h6>
<hr width=600 align=left>
<pre class=narration>
Mann-Whitney U-test

This test checks hypotheses about whether X  and  Y  are  samples  of  two
continuous distributions of the same shape  and  same  median  or  whether
their medians are different.

The following tests are performed:
    * two-tailed test (null hypothesis - the medians are equal)
    * left-tailed test (null hypothesis - the median of the  first  sample
      is greater than or equal to the median of the second sample)
    * right-tailed test (null hypothesis - the median of the first  sample
      is less than or equal to the median of the second sample).

Requirements:
    * the samples are independent
    * X and Y are continuous distributions (or discrete distributions well-
      approximating continuous distributions)
    * distributions of X and Y have the  same  shape.  The  only  possible
      difference is their position (i.e. the value of the median)
    * the number of elements in each sample is not less than 5
    * the scale of measurement should be ordinal, interval or ratio  (i.e.
      the test could not be applied to nominal variables).

The test is non-parametric and doesn't require distributions to be normal.

Inputs:
    X   -   sample 1. Array whose index goes from 0 to N-1.
    N   -   size of the sample. N &ge; 5
    Y   -   sample 2. Array whose index goes from 0 to M-1.
    M   -   size of the sample. M &ge; 5

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.

To calculate p-values, special approximation is used. This method lets  us
calculate p-values with satisfactory  accuracy  in  interval  [0.0001, 1].
There is no approximation outside the [0.0001, 1] interval. Therefore,  if
the significance level outlies this interval, the test returns 0.0001.

Relative precision of approximation of p-value:

N          M          Max.err.   Rms.err.
5..10      N..10      1.4e-02    6.0e-04
5..10      N..100     2.2e-02    5.3e-06
10..15     N..15      1.0e-02    3.2e-04
10..15     N..100     1.0e-02    2.2e-05
15..100    N..100     6.1e-03    2.7e-06

For N, M &gt; 100 accuracy checks weren't put into  practice,  but  taking  into
account characteristics of asymptotic approximation used, precision should
not be sharply different from the values for interval [5, 100].

NOTE: P-value approximation was  optimized  for  0.0001 &le; p &le; 0.2500.  Thus,
      P's outside of this interval are enforced to these bounds. Say,  you
      may quite often get P equal to exactly 0.25 or 0.0001.
ALGLIB: Copyright 09.04.2007 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> mannwhitneyutest(real_1d_array x, ae_int_t n, real_1d_array y, ae_int_t m, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
<a name=unit_stest></a><h4 class=pageheader>8.11.5. stest Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_onesamplesigntest class=toc>onesamplesigntest</a>
]</font>
</div>
<a name=sub_onesamplesigntest></a><h6 class=pageheader>onesamplesigntest Function</h6>
<hr width=600 align=left>
<pre class=narration>
Sign test

This test checks three hypotheses about the median of  the  given  sample.
The following tests are performed:
    * two-tailed test (null hypothesis - the median is equal to the  given
      value)
    * left-tailed test (null hypothesis - the median is  greater  than  or
      equal to the given value)
    * right-tailed test (null hypothesis - the  median  is  less  than  or
      equal to the given value)

Requirements:
    * the scale of measurement should be ordinal, interval or ratio  (i.e.
      the test could not be applied to nominal variables).

The test is non-parametric and doesn't require distribution X to be normal

Inputs:
    X       -   sample. Array whose index goes from 0 to N-1.
    N       -   size of the sample.
    Median  -   assumed median value.

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.

While   calculating   p-values   high-precision   binomial    distribution
approximation is used, so significance levels have about 15 exact digits.
ALGLIB: Copyright 08.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> onesamplesigntest(real_1d_array x, ae_int_t n, <b>double</b> median, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
<a name=unit_studentttests></a><h4 class=pageheader>8.11.6. studentttests Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_studentttest1 class=toc>studentttest1</a> |
<a href=#sub_studentttest2 class=toc>studentttest2</a> |
<a href=#sub_unequalvariancettest class=toc>unequalvariancettest</a>
]</font>
</div>
<a name=sub_studentttest1></a><h6 class=pageheader>studentttest1 Function</h6>
<hr width=600 align=left>
<pre class=narration>
One-sample t-test

This test checks three hypotheses about the mean of the given sample.  The
following tests are performed:
    * two-tailed test (null hypothesis - the mean is equal  to  the  given
      value)
    * left-tailed test (null hypothesis - the  mean  is  greater  than  or
      equal to the given value)
    * right-tailed test (null hypothesis - the mean is less than or  equal
      to the given value).

The test is based on the assumption that  a  given  sample  has  a  normal
distribution and  an  unknown  dispersion.  If  the  distribution  sharply
differs from normal, the test will work incorrectly.

Inputs:
    X       -   sample. Array whose index goes from 0 to N-1.
    N       -   size of sample, N &ge; 0
    Mean    -   assumed value of the mean.

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.

NOTE: this function correctly handles degenerate cases:
      * when N=0, all p-values are set to 1.0
      * when variance of X[] is exactly zero, p-values are set
        to 1.0 or 0.0, depending on difference between sample mean and
        value of mean being tested.
ALGLIB: Copyright 08.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> studentttest1(real_1d_array x, ae_int_t n, <b>double</b> mean, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
<a name=sub_studentttest2></a><h6 class=pageheader>studentttest2 Function</h6>
<hr width=600 align=left>
<pre class=narration>
Two-sample pooled test

This test checks three hypotheses about the mean of the given samples. The
following tests are performed:
    * two-tailed test (null hypothesis - the means are equal)
    * left-tailed test (null hypothesis - the mean of the first sample  is
      greater than or equal to the mean of the second sample)
    * right-tailed test (null hypothesis - the mean of the first sample is
      less than or equal to the mean of the second sample).

Test is based on the following assumptions:
    * given samples have normal distributions
    * dispersions are equal
    * samples are independent.

Inputs:
    X       -   sample 1. Array whose index goes from 0 to N-1.
    N       -   size of sample.
    Y       -   sample 2. Array whose index goes from 0 to M-1.
    M       -   size of sample.

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.

NOTE: this function correctly handles degenerate cases:
      * when N=0 or M=0, all p-values are set to 1.0
      * when both samples has exactly zero variance, p-values are set
        to 1.0 or 0.0, depending on difference between means.
ALGLIB: Copyright 18.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> studentttest2(real_1d_array x, ae_int_t n, real_1d_array y, ae_int_t m, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
<a name=sub_unequalvariancettest></a><h6 class=pageheader>unequalvariancettest Function</h6>
<hr width=600 align=left>
<pre class=narration>
Two-sample unpooled test

This test checks three hypotheses about the mean of the given samples. The
following tests are performed:
    * two-tailed test (null hypothesis - the means are equal)
    * left-tailed test (null hypothesis - the mean of the first sample  is
      greater than or equal to the mean of the second sample)
    * right-tailed test (null hypothesis - the mean of the first sample is
      less than or equal to the mean of the second sample).

Test is based on the following assumptions:
    * given samples have normal distributions
    * samples are independent.
Equality of variances is NOT required.

Inputs:
    X - sample 1. Array whose index goes from 0 to N-1.
    N - size of the sample.
    Y - sample 2. Array whose index goes from 0 to M-1.
    M - size of the sample.

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.

NOTE: this function correctly handles degenerate cases:
      * when N=0 or M=0, all p-values are set to 1.0
      * when both samples has zero variance, p-values are set
        to 1.0 or 0.0, depending on difference between means.
      * when only one sample has zero variance, test reduces to 1-sample
        version.
ALGLIB: Copyright 18.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> unequalvariancettest(real_1d_array x, ae_int_t n, real_1d_array y, ae_int_t m, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
<a name=unit_variancetests></a><h4 class=pageheader>8.11.7. variancetests Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_ftest class=toc>ftest</a> |
<a href=#sub_onesamplevariancetest class=toc>onesamplevariancetest</a>
]</font>
</div>
<a name=sub_ftest></a><h6 class=pageheader>ftest Function</h6>
<hr width=600 align=left>
<pre class=narration>
Two-sample F-test

This test checks three hypotheses about dispersions of the given  samples.
The following tests are performed:
    * two-tailed test (null hypothesis - the dispersions are equal)
    * left-tailed test (null hypothesis  -  the  dispersion  of  the first
      sample is greater than or equal to  the  dispersion  of  the  second
      sample).
    * right-tailed test (null hypothesis - the  dispersion  of  the  first
      sample is less than or equal to the dispersion of the second sample)

The test is based on the following assumptions:
    * the given samples have normal distributions
    * the samples are independent.

Inputs:
    X   -   sample 1. Array whose index goes from 0 to N-1.
    N   -   sample size.
    Y   -   sample 2. Array whose index goes from 0 to M-1.
    M   -   sample size.

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.
ALGLIB: Copyright 19.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> ftest(real_1d_array x, ae_int_t n, real_1d_array y, ae_int_t m, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
<a name=sub_onesamplevariancetest></a><h6 class=pageheader>onesamplevariancetest Function</h6>
<hr width=600 align=left>
<pre class=narration>
One-sample chi-square test

This test checks three hypotheses about the dispersion of the given sample
The following tests are performed:
    * two-tailed test (null hypothesis - the dispersion equals  the  given
      number)
    * left-tailed test (null hypothesis - the dispersion is  greater  than
      or equal to the given number)
    * right-tailed test (null hypothesis  -  dispersion is  less  than  or
      equal to the given number).

Test is based on the following assumptions:
    * the given sample has a normal distribution.

Inputs:
    X           -   sample 1. Array whose index goes from 0 to N-1.
    N           -   size of the sample.
    Variance    -   dispersion value to compare with.

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.
ALGLIB: Copyright 19.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> onesamplevariancetest(real_1d_array x, ae_int_t n, <b>double</b> variance, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
<a name=unit_wsr></a><h4 class=pageheader>8.11.8. wsr Subpackage</h4>
<div class=pagecontent>
<h5>Functions</h5><font size=2>[
<a href=#sub_wilcoxonsignedranktest class=toc>wilcoxonsignedranktest</a>
]</font>
</div>
<a name=sub_wilcoxonsignedranktest></a><h6 class=pageheader>wilcoxonsignedranktest Function</h6>
<hr width=600 align=left>
<pre class=narration>
Wilcoxon signed-rank test

This test checks three hypotheses about the median  of  the  given sample.
The following tests are performed:
    * two-tailed test (null hypothesis - the median is equal to the  given
      value)
    * left-tailed test (null hypothesis - the median is  greater  than  or
      equal to the given value)
    * right-tailed test (null hypothesis  -  the  median  is  less than or
      equal to the given value)

Requirements:
    * the scale of measurement should be ordinal, interval or  ratio (i.e.
      the test could not be applied to nominal variables).
    * the distribution should be continuous and symmetric relative to  its
      median.
    * number of distinct values in the X array should be greater than 4

The test is non-parametric and doesn't require distribution X to be normal

Inputs:
    X       -   sample. Array whose index goes from 0 to N-1.
    N       -   size of the sample.
    Median  -   assumed median value.

Outputs:
    BothTails   -   p-value for two-tailed test.
                    If BothTails is less than the given significance level
                    the null hypothesis is rejected.
    LeftTail    -   p-value for left-tailed test.
                    If LeftTail is less than the given significance level,
                    the null hypothesis is rejected.
    RightTail   -   p-value for right-tailed test.
                    If RightTail is less than the given significance level
                    the null hypothesis is rejected.

To calculate p-values, special approximation is used. This method lets  us
calculate p-values with two decimal places in interval [0.0001, 1].

&quot;Two decimal places&quot; does not sound very impressive, but in  practice  the
relative error of less than 1% is enough to make a decision.

There is no approximation outside the [0.0001, 1] interval. Therefore,  if
the significance level outlies this interval, the test returns 0.0001.
ALGLIB: Copyright 08.09.2006 by Sergey Bochkanov
</pre>
<hr width=600 align=left>
<pre class=declaration>
<b>void</b> wilcoxonsignedranktest(real_1d_array x, ae_int_t n, <b>double</b> e, <b>double</b> &amp;bothtails, <b>double</b> &amp;lefttail, <b>double</b> &amp;righttail);
</pre>
</p>
</div>
</body>
</html>
