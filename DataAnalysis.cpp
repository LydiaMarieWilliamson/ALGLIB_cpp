// ALGLIB++
// Based on ALGLIB: Copyright (c) Sergey Bochkanov (ALGLIB project).
// Revisions Copyright (c) Lydia Marie Williamson, Mark Hopkins Consulting
// Source License:
//	This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License
//	as published by the Free Software Foundation (www.fsf.org);
//	either version 2 of the License, or (at your option) any later version.
//
//	This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
//	without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
//	See the GNU General Public License for more details.
//
//	A copy of the GNU General Public License is available at http://www.fsf.org/licensing/licenses
#define InAlgLib
#include "DataAnalysis.h"

// === PCA Package ===
// Depends on: (LinAlg) SVD, EVD
// Depends on: (Statistics) BASESTAT
namespace alglib_impl {
// Principal components analysis
//
// This function builds orthogonal basis  where  first  axis  corresponds  to
// direction with maximum variance, second axis  maximizes  variance  in  the
// subspace orthogonal to first axis and so on.
//
// This function builds FULL basis, i.e. returns N vectors  corresponding  to
// ALL directions, no matter how informative. If you need  just a  few  (say,
// 10 or 50) of the most important directions, you may find it faster to  use
// one of the reduced versions:
// * pcatruncatedsubspace() - for subspace iteration based method
//
// It should be noted that, unlike LDA, PCA does not use class labels.
//
// Inputs:
//     X           -   dataset, array[0..NPoints-1,0..NVars-1].
//                     matrix contains ONLY INDEPENDENT VARIABLES.
//     NPoints     -   dataset size, NPoints >= 0
//     NVars       -   number of independent variables, NVars >= 1
//
// Outputs:
//     Info        -   return code:
//                     * -4, if SVD subroutine haven't converged
//                     * -1, if wrong parameters has been passed (NPoints < 0,
//                           NVars < 1)
//                     *  1, if task is solved
//     S2          -   array[0..NVars-1]. variance values corresponding
//                     to basis vectors.
//     V           -   array[0..NVars-1,0..NVars-1]
//                     matrix, whose columns store basis vectors.
// ALGLIB: Copyright 25.08.2008 by Sergey Bochkanov
// API: void pcabuildbasis(const real_2d_array &x, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, real_1d_array &s2, real_2d_array &v);
void pcabuildbasis(RMatrix *x, ae_int_t npoints, ae_int_t nvars, ae_int_t *info, RVector *s2, RMatrix *v) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   double mean;
   double variance;
   double skewness;
   double kurtosis;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetVector(s2);
   SetMatrix(v);
   NewMatrix(a, 0, 0, DT_REAL);
   NewMatrix(u, 0, 0, DT_REAL);
   NewMatrix(vt, 0, 0, DT_REAL);
   NewVector(m, 0, DT_REAL);
   NewVector(t, 0, DT_REAL);
// Check input data
   if (npoints < 0 || nvars < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   *info = 1;
// Special case: NPoints=0
   if (npoints == 0) {
      ae_vector_set_length(s2, nvars);
      ae_matrix_set_length(v, nvars, nvars);
      for (i = 0; i < nvars; i++) {
         s2->xR[i] = 0.0;
      }
      for (i = 0; i < nvars; i++) {
         for (j = 0; j < nvars; j++) {
            if (i == j) {
               v->xyR[i][j] = 1.0;
            } else {
               v->xyR[i][j] = 0.0;
            }
         }
      }
      ae_frame_leave();
      return;
   }
// Calculate means
   ae_vector_set_length(&m, nvars);
   ae_vector_set_length(&t, npoints);
   for (j = 0; j < nvars; j++) {
      ae_v_move(t.xR, 1, &x->xyR[0][j], x->stride, npoints);
      samplemoments(&t, npoints, &mean, &variance, &skewness, &kurtosis);
      m.xR[j] = mean;
   }
// Center, apply SVD, prepare output
   ae_matrix_set_length(&a, imax2(npoints, nvars), nvars);
   for (i = 0; i < npoints; i++) {
      ae_v_move(a.xyR[i], 1, x->xyR[i], 1, nvars);
      ae_v_sub(a.xyR[i], 1, m.xR, 1, nvars);
   }
   for (i = npoints; i < nvars; i++) {
      for (j = 0; j < nvars; j++) {
         a.xyR[i][j] = 0.0;
      }
   }
   if (!rmatrixsvd(&a, imax2(npoints, nvars), nvars, 0, 1, 2, s2, &u, &vt)) {
      *info = -4;
      ae_frame_leave();
      return;
   }
   if (npoints != 1) {
      for (i = 0; i < nvars; i++) {
         s2->xR[i] = ae_sqr(s2->xR[i]) / (npoints - 1);
      }
   }
   ae_matrix_set_length(v, nvars, nvars);
   copyandtranspose(&vt, 0, nvars - 1, 0, nvars - 1, v, 0, nvars - 1, 0, nvars - 1);
   ae_frame_leave();
}

// Principal components analysis
//
// This function performs truncated PCA, i.e. returns just a few most important
// directions.
//
// Internally it uses iterative eigensolver which is very efficient when only
// a minor fraction of full basis is required. Thus, if you need full  basis,
// it is better to use pcabuildbasis() function.
//
// It should be noted that, unlike LDA, PCA does not use class labels.
//
// Inputs:
//     X           -   dataset, array[0..NPoints-1,0..NVars-1].
//                     matrix contains ONLY INDEPENDENT VARIABLES.
//     NPoints     -   dataset size, NPoints >= 0
//     NVars       -   number of independent variables, NVars >= 1
//     NNeeded     -   number of requested components, in [1,NVars] range;
//                     this function is efficient only for NNeeded << NVars.
//     Eps         -   desired  precision  of  vectors  returned;  underlying
//                     solver will stop iterations as soon as absolute  error
//                     in corresponding singular values  reduces  to  roughly
//                     eps*MAX(lambda[]), with lambda[] being array of  eigen
//                     values.
//                     Zero value means that  algorithm  performs  number  of
//                     iterations  specified  by  maxits  parameter,  without
//                     paying attention to precision.
//     MaxIts      -   number of iterations performed by  subspace  iteration
//                     method. Zero value means that no  limit  on  iteration
//                     count is placed (eps-based stopping condition is used).
//
// Outputs:
//     S2          -   array[NNeeded]. Variance values corresponding
//                     to basis vectors.
//     V           -   array[NVars,NNeeded]
//                     matrix, whose columns store basis vectors.
//
// NOTE: passing eps=0 and maxits=0 results in small eps  being  selected  as
// stopping condition. Exact value of automatically selected eps is  version-
// -dependent.
// ALGLIB: Copyright 10.01.2017 by Sergey Bochkanov
// API: void pcatruncatedsubspace(const real_2d_array &x, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nneeded, const double eps, const ae_int_t maxits, real_1d_array &s2, real_2d_array &v);
void pcatruncatedsubspace(RMatrix *x, ae_int_t npoints, ae_int_t nvars, ae_int_t nneeded, double eps, ae_int_t maxits, RVector *s2, RMatrix *v) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   double vv;
   ae_frame_make(&_frame_block);
   SetVector(s2);
   SetMatrix(v);
   NewMatrix(a, 0, 0, DT_REAL);
   NewMatrix(b, 0, 0, DT_REAL);
   NewVector(means, 0, DT_REAL);
   NewObj(eigsubspacestate, solver);
   NewObj(eigsubspacereport, rep);
   ae_assert(npoints >= 0, "PCATruncatedSubspace: npoints<0");
   ae_assert(nvars >= 1, "PCATruncatedSubspace: nvars<1");
   ae_assert(nneeded > 0, "PCATruncatedSubspace: nneeded<1");
   ae_assert(nneeded <= nvars, "PCATruncatedSubspace: nneeded>nvars");
   ae_assert(maxits >= 0, "PCATruncatedSubspace: maxits<0");
   ae_assert(isfinite(eps) && eps >= 0.0, "PCATruncatedSubspace: eps<0 or is not finite");
   ae_assert(x->rows >= npoints, "PCATruncatedSubspace: rows(x)<npoints");
   ae_assert(x->cols >= nvars || npoints == 0, "PCATruncatedSubspace: cols(x)<nvars");
// Special case: NPoints=0
   if (npoints == 0) {
      ae_vector_set_length(s2, nneeded);
      ae_matrix_set_length(v, nvars, nneeded);
      for (i = 0; i < nvars; i++) {
         s2->xR[i] = 0.0;
      }
      for (i = 0; i < nvars; i++) {
         for (j = 0; j < nneeded; j++) {
            if (i == j) {
               v->xyR[i][j] = 1.0;
            } else {
               v->xyR[i][j] = 0.0;
            }
         }
      }
      ae_frame_leave();
      return;
   }
// Center matrix
   ae_vector_set_length(&means, nvars);
   for (i = 0; i < nvars; i++) {
      means.xR[i] = 0.0;
   }
   vv = 1.0 / (double)npoints;
   for (i = 0; i < npoints; i++) {
      ae_v_addd(means.xR, 1, x->xyR[i], 1, nvars, vv);
   }
   ae_matrix_set_length(&a, npoints, nvars);
   for (i = 0; i < npoints; i++) {
      ae_v_move(a.xyR[i], 1, x->xyR[i], 1, nvars);
      ae_v_sub(a.xyR[i], 1, means.xR, 1, nvars);
   }
// Find eigenvalues with subspace iteration solver
   eigsubspacecreate(nvars, nneeded, &solver);
   eigsubspacesetcond(&solver, eps, maxits);
   for (eigsubspaceoocstart(&solver, 0); eigsubspaceooccontinue(&solver); ) {
      ae_assert(solver.requesttype == 0, "PCATruncatedSubspace: integrity check failed");
      k = solver.requestsize;
      matrixsetlengthatleast(&b, npoints, k);
      rmatrixgemm(npoints, k, nvars, 1.0, &a, 0, 0, 0, &solver.x, 0, 0, 0, 0.0, &b, 0, 0);
      rmatrixgemm(nvars, k, npoints, 1.0, &a, 0, 0, 1, &b, 0, 0, 0, 0.0, &solver.ax, 0, 0);
   }
   eigsubspaceoocstop(&solver, s2, v, &rep);
   if (npoints != 1) {
      for (i = 0; i < nneeded; i++) {
         s2->xR[i] /= npoints - 1;
      }
   }
   ae_frame_leave();
}

// Sparse truncated principal components analysis
//
// This function performs sparse truncated PCA, i.e. returns just a few  most
// important principal components for a sparse input X.
//
// Internally it uses iterative eigensolver which is very efficient when only
// a minor fraction of full basis is required.
//
// It should be noted that, unlike LDA, PCA does not use class labels.
//
// Inputs:
//     X           -   sparse dataset, sparse  npoints*nvars  matrix.  It  is
//                     recommended to use CRS sparse storage format;  non-CRS
//                     input will be internally converted to CRS.
//                     Matrix contains ONLY INDEPENDENT VARIABLES,  and  must
//                     be EXACTLY npoints*nvars.
//     NPoints     -   dataset size, NPoints >= 0
//     NVars       -   number of independent variables, NVars >= 1
//     NNeeded     -   number of requested components, in [1,NVars] range;
//                     this function is efficient only for NNeeded << NVars.
//     Eps         -   desired  precision  of  vectors  returned;  underlying
//                     solver will stop iterations as soon as absolute  error
//                     in corresponding singular values  reduces  to  roughly
//                     eps*MAX(lambda[]), with lambda[] being array of  eigen
//                     values.
//                     Zero value means that  algorithm  performs  number  of
//                     iterations  specified  by  maxits  parameter,  without
//                     paying attention to precision.
//     MaxIts      -   number of iterations performed by  subspace  iteration
//                     method. Zero value means that no  limit  on  iteration
//                     count is placed (eps-based stopping condition is used).
//
// Outputs:
//     S2          -   array[NNeeded]. Variance values corresponding
//                     to basis vectors.
//     V           -   array[NVars,NNeeded]
//                     matrix, whose columns store basis vectors.
//
// NOTE: passing eps=0 and maxits=0 results in small eps  being  selected  as
//       a stopping condition. Exact value of automatically selected  eps  is
//       version-dependent.
//
// NOTE: zero  MaxIts  is  silently  replaced  by some reasonable value which
//       prevents eternal loops (possible when inputs are degenerate and  too
//       stringent stopping criteria are specified). In  current  version  it
//       is 50+2*NVars.
// ALGLIB: Copyright 10.01.2017 by Sergey Bochkanov
// API: void pcatruncatedsubspacesparse(const sparsematrix &x, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nneeded, const double eps, const ae_int_t maxits, real_1d_array &s2, real_2d_array &v);
void pcatruncatedsubspacesparse(sparsematrix *x, ae_int_t npoints, ae_int_t nvars, ae_int_t nneeded, double eps, ae_int_t maxits, RVector *s2, RMatrix *v) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   double vv;
   ae_frame_make(&_frame_block);
   SetVector(s2);
   SetMatrix(v);
   NewObj(sparsematrix, xcrs);
   NewVector(b1, 0, DT_REAL);
   NewVector(c1, 0, DT_REAL);
   NewVector(z1, 0, DT_REAL);
   NewVector(means, 0, DT_REAL);
   NewObj(eigsubspacestate, solver);
   NewObj(eigsubspacereport, rep);
   ae_assert(npoints >= 0, "PCATruncatedSubspaceSparse: npoints<0");
   ae_assert(nvars >= 1, "PCATruncatedSubspaceSparse: nvars<1");
   ae_assert(nneeded > 0, "PCATruncatedSubspaceSparse: nneeded<1");
   ae_assert(nneeded <= nvars, "PCATruncatedSubspaceSparse: nneeded>nvars");
   ae_assert(maxits >= 0, "PCATruncatedSubspaceSparse: maxits<0");
   ae_assert(isfinite(eps) && eps >= 0.0, "PCATruncatedSubspaceSparse: eps<0 or is not finite");
   if (npoints > 0) {
      ae_assert(sparsegetnrows(x) == npoints, "PCATruncatedSubspaceSparse: rows(x) != npoints");
      ae_assert(sparsegetncols(x) == nvars, "PCATruncatedSubspaceSparse: cols(x) != nvars");
   }
// Special case: NPoints=0
   if (npoints == 0) {
      ae_vector_set_length(s2, nneeded);
      ae_matrix_set_length(v, nvars, nneeded);
      for (i = 0; i < nvars; i++) {
         s2->xR[i] = 0.0;
      }
      for (i = 0; i < nvars; i++) {
         for (j = 0; j < nneeded; j++) {
            if (i == j) {
               v->xyR[i][j] = 1.0;
            } else {
               v->xyR[i][j] = 0.0;
            }
         }
      }
      ae_frame_leave();
      return;
   }
// If input data are not in CRS format, perform conversion to CRS
   if (!sparseiscrs(x)) {
      sparsecopytocrs(x, &xcrs);
      pcatruncatedsubspacesparse(&xcrs, npoints, nvars, nneeded, eps, maxits, s2, v);
      ae_frame_leave();
      return;
   }
// Initialize parameters, prepare buffers
   ae_vector_set_length(&b1, npoints);
   ae_vector_set_length(&z1, nvars);
   if (eps == 0.0 && maxits == 0) {
      eps = 1.0E-6;
   }
   if (maxits == 0) {
      maxits = 50 + 2 * nvars;
   }
// Calculate mean values
   vv = 1.0 / (double)npoints;
   for (i = 0; i < npoints; i++) {
      b1.xR[i] = vv;
   }
   sparsemtv(x, &b1, &means);
// Find eigenvalues with subspace iteration solver
   eigsubspacecreate(nvars, nneeded, &solver);
   eigsubspacesetcond(&solver, eps, maxits);
   for (eigsubspaceoocstart(&solver, 0); eigsubspaceooccontinue(&solver); ) {
      ae_assert(solver.requesttype == 0, "PCATruncatedSubspace: integrity check failed");
      for (k = 0; k < solver.requestsize; k++) {
      // Calculate B1=(X-meansX)*Zk
         ae_v_move(z1.xR, 1, &solver.x.xyR[0][k], solver.x.stride, nvars);
         sparsemv(x, &z1, &b1);
         vv = ae_v_dotproduct(&solver.x.xyR[0][k], solver.x.stride, means.xR, 1, nvars);
         for (i = 0; i < npoints; i++) {
            b1.xR[i] -= vv;
         }
      // Calculate (X-meansX)^T*B1
         sparsemtv(x, &b1, &c1);
         vv = 0.0;
         for (i = 0; i < npoints; i++) {
            vv += b1.xR[i];
         }
         for (j = 0; j < nvars; j++) {
            solver.ax.xyR[j][k] = c1.xR[j] - vv * means.xR[j];
         }
      }
   }
   eigsubspaceoocstop(&solver, s2, v, &rep);
   if (npoints != 1) {
      for (i = 0; i < nneeded; i++) {
         s2->xR[i] /= npoints - 1;
      }
   }
   ae_frame_leave();
}
} // end of namespace alglib_impl

namespace alglib {
void pcabuildbasis(const real_2d_array &x, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, real_1d_array &s2, real_2d_array &v) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::pcabuildbasis(ConstT(ae_matrix, x), npoints, nvars, &info, ConstT(ae_vector, s2), ConstT(ae_matrix, v));
   alglib_impl::ae_state_clear();
}

void pcatruncatedsubspace(const real_2d_array &x, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nneeded, const double eps, const ae_int_t maxits, real_1d_array &s2, real_2d_array &v) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::pcatruncatedsubspace(ConstT(ae_matrix, x), npoints, nvars, nneeded, eps, maxits, ConstT(ae_vector, s2), ConstT(ae_matrix, v));
   alglib_impl::ae_state_clear();
}

void pcatruncatedsubspacesparse(const sparsematrix &x, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nneeded, const double eps, const ae_int_t maxits, real_1d_array &s2, real_2d_array &v) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::pcatruncatedsubspacesparse(ConstT(sparsematrix, x), npoints, nvars, nneeded, eps, maxits, ConstT(ae_vector, s2), ConstT(ae_matrix, v));
   alglib_impl::ae_state_clear();
}
} // end of namespace alglib

// === BDSS Package ===
// Depends on: (Statistics) BASESTAT
namespace alglib_impl {
// This set of routines (DSErrAllocate, DSErrAccumulate, DSErrFinish)
// calculates different error functions (classification error, cross-entropy,
// rms, avg, avg.rel errors).
//
// 1. DSErrAllocate prepares buffer.
// 2. DSErrAccumulate accumulates individual errors:
//     * Y contains predicted output (posterior probabilities for classification)
//     * DesiredY contains desired output (class number for classification)
// 3. DSErrFinish outputs results:
//    * Buf[0] contains relative classification error (zero for regression tasks)
//    * Buf[1] contains avg. cross-entropy (zero for regression tasks)
//    * Buf[2] contains rms error (regression, classification)
//    * Buf[3] contains average error (regression, classification)
//    * Buf[4] contains average relative error (regression, classification)
//
// NOTES(1):
//     "NClasses>0" means that we have classification task.
//     "NClasses<0" means regression task with -NClasses real outputs.
//
// NOTES(2):
//     rms. avg, avg.rel errors for classification tasks are interpreted as
//     errors in posterior probabilities with respect to probabilities given
//     by training/test set.
// ALGLIB: Copyright 11.01.2009 by Sergey Bochkanov
void dserrallocate(ae_int_t nclasses, RVector *buf) {
   SetVector(buf);
   ae_vector_set_length(buf, 7 + 1);
   buf->xR[0] = 0.0;
   buf->xR[1] = 0.0;
   buf->xR[2] = 0.0;
   buf->xR[3] = 0.0;
   buf->xR[4] = 0.0;
   buf->xR[5] = (double)nclasses;
   buf->xR[6] = 0.0;
   buf->xR[7] = 0.0;
}

// See DSErrAllocate for comments on this routine.
// ALGLIB: Copyright 11.01.2009 by Sergey Bochkanov
void dserraccumulate(RVector *buf, RVector *y, RVector *desiredy) {
   ae_int_t nclasses;
   ae_int_t nout;
   ae_int_t offs;
   ae_int_t mmax;
   ae_int_t rmax;
   ae_int_t j;
   double v;
   double ev;
   offs = 5;
   nclasses = RoundZ(buf->xR[offs]);
   if (nclasses > 0) {
   // Classification
      rmax = RoundZ(desiredy->xR[0]);
      mmax = 0;
      for (j = 1; j < nclasses; j++) {
         if (y->xR[j] > y->xR[mmax]) {
            mmax = j;
         }
      }
      if (mmax != rmax) {
         buf->xR[0]++;
      }
      if (y->xR[rmax] > 0.0) {
         buf->xR[1] -= log(y->xR[rmax]);
      } else {
         buf->xR[1] += log(ae_maxrealnumber);
      }
      for (j = 0; j < nclasses; j++) {
         v = y->xR[j];
         if (j == rmax) {
            ev = 1.0;
         } else {
            ev = 0.0;
         }
         buf->xR[2] += ae_sqr(v - ev);
         buf->xR[3] += fabs(v - ev);
         if (ev != 0.0) {
            buf->xR[4] += fabs((v - ev) / ev);
            buf->xR[offs + 2]++;
         }
      }
      buf->xR[offs + 1]++;
   } else {
   // Regression
      nout = -nclasses;
      rmax = 0;
      for (j = 1; j < nout; j++) {
         if (desiredy->xR[j] > desiredy->xR[rmax]) {
            rmax = j;
         }
      }
      mmax = 0;
      for (j = 1; j < nout; j++) {
         if (y->xR[j] > y->xR[mmax]) {
            mmax = j;
         }
      }
      if (mmax != rmax) {
         buf->xR[0]++;
      }
      for (j = 0; j < nout; j++) {
         v = y->xR[j];
         ev = desiredy->xR[j];
         buf->xR[2] += ae_sqr(v - ev);
         buf->xR[3] += fabs(v - ev);
         if (ev != 0.0) {
            buf->xR[4] += fabs((v - ev) / ev);
            buf->xR[offs + 2]++;
         }
      }
      buf->xR[offs + 1]++;
   }
}

// See DSErrAllocate for comments on this routine.
// ALGLIB: Copyright 11.01.2009 by Sergey Bochkanov
void dserrfinish(RVector *buf) {
   ae_int_t nout;
   ae_int_t offs;
   offs = 5;
   nout = ae_iabs(RoundZ(buf->xR[offs]));
   if (buf->xR[offs + 1] != 0.0) {
      buf->xR[0] /= buf->xR[offs + 1];
      buf->xR[1] /= buf->xR[offs + 1];
      buf->xR[2] = sqrt(buf->xR[2] / (nout * buf->xR[offs + 1]));
      buf->xR[3] /= nout * buf->xR[offs + 1];
   }
   if (buf->xR[offs + 2] != 0.0) {
      buf->xR[4] /= buf->xR[offs + 2];
   }
}

// ALGLIB: Copyright 19.05.2008 by Sergey Bochkanov
void dsnormalize(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t *info, RVector *means, RVector *sigmas) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   double mean;
   double variance;
   double skewness;
   double kurtosis;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetVector(means);
   SetVector(sigmas);
   NewVector(tmp, 0, DT_REAL);
// Test parameters
   if (npoints <= 0 || nvars < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   *info = 1;
// Standartization
   ae_vector_set_length(means, nvars);
   ae_vector_set_length(sigmas, nvars);
   ae_vector_set_length(&tmp, npoints);
   for (j = 0; j < nvars; j++) {
      ae_v_move(tmp.xR, 1, &xy->xyR[0][j], xy->stride, npoints);
      samplemoments(&tmp, npoints, &mean, &variance, &skewness, &kurtosis);
      means->xR[j] = mean;
      sigmas->xR[j] = sqrt(variance);
      if (sigmas->xR[j] == 0.0) {
         sigmas->xR[j] = 1.0;
      }
      for (i = 0; i < npoints; i++) {
         xy->xyR[i][j] = (xy->xyR[i][j] - means->xR[j]) / sigmas->xR[j];
      }
   }
   ae_frame_leave();
}

// ALGLIB: Copyright 19.05.2008 by Sergey Bochkanov
void dsnormalizec(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t *info, RVector *means, RVector *sigmas) {
   ae_frame _frame_block;
   ae_int_t j;
   double mean;
   double variance;
   double skewness;
   double kurtosis;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetVector(means);
   SetVector(sigmas);
   NewVector(tmp, 0, DT_REAL);
// Test parameters
   if (npoints <= 0 || nvars < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   *info = 1;
// Standartization
   ae_vector_set_length(means, nvars);
   ae_vector_set_length(sigmas, nvars);
   ae_vector_set_length(&tmp, npoints);
   for (j = 0; j < nvars; j++) {
      ae_v_move(tmp.xR, 1, &xy->xyR[0][j], xy->stride, npoints);
      samplemoments(&tmp, npoints, &mean, &variance, &skewness, &kurtosis);
      means->xR[j] = mean;
      sigmas->xR[j] = sqrt(variance);
      if (sigmas->xR[j] == 0.0) {
         sigmas->xR[j] = 1.0;
      }
   }
   ae_frame_leave();
}

// ALGLIB: Copyright 19.05.2008 by Sergey Bochkanov
double dsgetmeanmindistance(RMatrix *xy, ae_int_t npoints, ae_int_t nvars) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   double v;
   double result;
   ae_frame_make(&_frame_block);
   NewVector(tmp, 0, DT_REAL);
   NewVector(tmp2, 0, DT_REAL);
// Test parameters
   if (npoints <= 0 || nvars < 1) {
      result = 0.0;
      ae_frame_leave();
      return result;
   }
// Process
   ae_vector_set_length(&tmp, npoints);
   for (i = 0; i < npoints; i++) {
      tmp.xR[i] = ae_maxrealnumber;
   }
   ae_vector_set_length(&tmp2, nvars);
   for (i = 0; i < npoints; i++) {
      for (j = i + 1; j < npoints; j++) {
         ae_v_move(tmp2.xR, 1, xy->xyR[i], 1, nvars);
         ae_v_sub(tmp2.xR, 1, xy->xyR[j], 1, nvars);
         v = ae_v_dotproduct(tmp2.xR, 1, tmp2.xR, 1, nvars);
         v = sqrt(v);
         tmp.xR[i] = rmin2(tmp.xR[i], v);
         tmp.xR[j] = rmin2(tmp.xR[j], v);
      }
   }
   result = 0.0;
   for (i = 0; i < npoints; i++) {
      result += tmp.xR[i] / npoints;
   }
   ae_frame_leave();
   return result;
}

// ALGLIB: Copyright 19.05.2008 by Sergey Bochkanov
void dstie(RVector *a, ae_int_t n, ZVector *ties, ae_int_t *tiecount, ZVector *p1, ZVector *p2) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t k;
   ae_frame_make(&_frame_block);
   SetVector(ties);
   *tiecount = 0;
   SetVector(p1);
   SetVector(p2);
   NewVector(tmp, 0, DT_INT);
// Special case
   if (n <= 0) {
      *tiecount = 0;
      ae_frame_leave();
      return;
   }
// Sort A
   tagsort(a, n, p1, p2);
// Process ties
   *tiecount = 1;
   for (i = 1; i < n; i++) {
      if (a->xR[i] != a->xR[i - 1]) {
         ++*tiecount;
      }
   }
   ae_vector_set_length(ties, *tiecount + 1);
   ties->xZ[0] = 0;
   k = 1;
   for (i = 1; i < n; i++) {
      if (a->xR[i] != a->xR[i - 1]) {
         ties->xZ[k] = i;
         k++;
      }
   }
   ties->xZ[*tiecount] = n;
   ae_frame_leave();
}

// ALGLIB: Copyright 11.12.2008 by Sergey Bochkanov
void dstiefasti(RVector *a, ZVector *b, ae_int_t n, ZVector *ties, ae_int_t *tiecount, RVector *bufr, ZVector *bufi) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t k;
   ae_frame_make(&_frame_block);
   *tiecount = 0;
   NewVector(tmp, 0, DT_INT);
// Special case
   if (n <= 0) {
      *tiecount = 0;
      ae_frame_leave();
      return;
   }
// Sort A
   tagsortfasti(a, b, bufr, bufi, n);
// Process ties
   ties->xZ[0] = 0;
   k = 1;
   for (i = 1; i < n; i++) {
      if (a->xR[i] != a->xR[i - 1]) {
         ties->xZ[k] = i;
         k++;
      }
   }
   ties->xZ[k] = n;
   *tiecount = k;
   ae_frame_leave();
}

// Internal function
static double bdss_xlny(double x, double y) {
   double result;
   if (x == 0.0) {
      result = 0.0;
   } else {
      result = x * log(y);
   }
   return result;
}

// Optimal binary classification
//
// Algorithms finds optimal (=with minimal cross-entropy) binary partition.
// Internal subroutine.
//
// Inputs:
//     A       -   array[0..N-1], variable
//     C       -   array[0..N-1], class numbers (0 or 1).
//     N       -   array size
//
// Outputs:
//     Info    -   completetion code:
//                 * -3, all values of A[] are same (partition is impossible)
//                 * -2, one of C[] is incorrect (< 0, > 1)
//                 * -1, incorrect pararemets were passed (N <= 0).
//                 *  1, OK
//     Threshold-  partiton boundary. Left part contains values which are
//                 strictly less than Threshold. Right part contains values
//                 which are greater than or equal to Threshold.
//     PAL, PBL-   probabilities P(0|v < Threshold) and P(1|v < Threshold)
//     PAR, PBR-   probabilities P(0|v >= Threshold) and P(1|v >= Threshold)
//     CVE     -   cross-validation estimate of cross-entropy
// ALGLIB: Copyright 22.05.2008 by Sergey Bochkanov
// API: void dsoptimalsplit2(const real_1d_array &a, const integer_1d_array &c, const ae_int_t n, ae_int_t &info, double &threshold, double &pal, double &pbl, double &par, double &pbr, double &cve);
void dsoptimalsplit2(RVector *a, ZVector *c, ae_int_t n, ae_int_t *info, double *threshold, double *pal, double *pbl, double *par, double *pbr, double *cve) {
   ae_frame _frame_block;
   ae_int_t i;
   double s;
   ae_int_t tiecount;
   ae_int_t k;
   ae_int_t koptimal;
   double pak;
   double pbk;
   double cvoptimal;
   double cv;
   ae_frame_make(&_frame_block);
   DupVector(a);
   DupVector(c);
   *info = 0;
   *threshold = 0;
   *pal = 0;
   *pbl = 0;
   *par = 0;
   *pbr = 0;
   *cve = 0;
   NewVector(ties, 0, DT_INT);
   NewVector(p1, 0, DT_INT);
   NewVector(p2, 0, DT_INT);
// Test for errors in inputs
   if (n <= 0) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   for (i = 0; i < n; i++) {
      if (c->xZ[i] != 0 && c->xZ[i] != 1) {
         *info = -2;
         ae_frame_leave();
         return;
      }
   }
   *info = 1;
// Tie
   dstie(a, n, &ties, &tiecount, &p1, &p2);
   for (i = 0; i < n; i++) {
      if (p2.xZ[i] != i) {
         swapi(&c->xZ[i], &c->xZ[p2.xZ[i]]);
      }
   }
// Special case: number of ties is 1.
//
// NOTE: we assume that P[i,j] equals to 0 or 1,
//       intermediate values are not allowed.
   if (tiecount == 1) {
      *info = -3;
      ae_frame_leave();
      return;
   }
// General case, number of ties > 1
//
// NOTE: we assume that P[i,j] equals to 0 or 1,
//       intermediate values are not allowed.
   *pal = 0.0;
   *pbl = 0.0;
   *par = 0.0;
   *pbr = 0.0;
   for (i = 0; i < n; i++) {
      if (c->xZ[i] == 0) {
         ++*par;
      }
      if (c->xZ[i] == 1) {
         ++*pbr;
      }
   }
   koptimal = -1;
   cvoptimal = ae_maxrealnumber;
   for (k = 0; k < tiecount - 1; k++) {
   // first, obtain information about K-th tie which is
   // moved from R-part to L-part
      pak = 0.0;
      pbk = 0.0;
      for (i = ties.xZ[k]; i < ties.xZ[k + 1]; i++) {
         if (c->xZ[i] == 0) {
            pak++;
         }
         if (c->xZ[i] == 1) {
            pbk++;
         }
      }
   // Calculate cross-validation CE
      cv = 0.0;
      cv -= bdss_xlny(*pal + pak, (*pal + pak) / (*pal + pak + (*pbl) + pbk + 1));
      cv -= bdss_xlny(*pbl + pbk, (*pbl + pbk) / (*pal + pak + 1 + (*pbl) + pbk));
      cv -= bdss_xlny(*par - pak, (*par - pak) / (*par - pak + (*pbr) - pbk + 1));
      cv -= bdss_xlny(*pbr - pbk, (*pbr - pbk) / (*par - pak + 1 + (*pbr) - pbk));
   // Compare with best
      if (cv < cvoptimal) {
         cvoptimal = cv;
         koptimal = k;
      }
   // update
      *pal += pak;
      *pbl += pbk;
      *par -= pak;
      *pbr -= pbk;
   }
   *cve = cvoptimal;
   *threshold = 0.5 * (a->xR[ties.xZ[koptimal]] + a->xR[ties.xZ[koptimal + 1]]);
   *pal = 0.0;
   *pbl = 0.0;
   *par = 0.0;
   *pbr = 0.0;
   for (i = 0; i < n; i++) {
      if (a->xR[i] < *threshold) {
         if (c->xZ[i] == 0) {
            ++*pal;
         } else {
            ++*pbl;
         }
      } else {
         if (c->xZ[i] == 0) {
            ++*par;
         } else {
            ++*pbr;
         }
      }
   }
   s = *pal + (*pbl);
   *pal /= s;
   *pbl /= s;
   s = *par + (*pbr);
   *par /= s;
   *pbr /= s;
   ae_frame_leave();
}

// Optimal partition, internal subroutine. Fast version.
//
// Accepts:
//     A       array[0..N-1]       array of attributes     array[0..N-1]
//     C       array[0..N-1]       array of class labels
//     TiesBuf array[0..N]         temporaries (ties)
//     CntBuf  array[0..2*NC-1]    temporaries (counts)
//     Alpha                       centering factor (0 <= alpha <= 1, recommended value - 0.05)
//     BufR    array[0..N-1]       temporaries
//     BufI    array[0..N-1]       temporaries
//
// Outputs:
//     Info    error code ("> 0"=OK, "< 0"=bad)
//     RMS     training set RMS error
//     CVRMS   leave-one-out RMS error
//
// Note:
//     content of all arrays is changed by subroutine;
//     it doesn't allocate temporaries.
// ALGLIB: Copyright 11.12.2008 by Sergey Bochkanov
// API: void dsoptimalsplit2fast(real_1d_array &a, integer_1d_array &c, integer_1d_array &tiesbuf, integer_1d_array &cntbuf, real_1d_array &bufr, integer_1d_array &bufi, const ae_int_t n, const ae_int_t nc, const double alpha, ae_int_t &info, double &threshold, double &rms, double &cvrms);
void dsoptimalsplit2fast(RVector *a, ZVector *c, ZVector *tiesbuf, ZVector *cntbuf, RVector *bufr, ZVector *bufi, ae_int_t n, ae_int_t nc, double alpha, ae_int_t *info, double *threshold, double *rms, double *cvrms) {
   ae_int_t i;
   ae_int_t k;
   ae_int_t cl;
   ae_int_t tiecount;
   double cbest;
   double cc;
   ae_int_t koptimal;
   ae_int_t sl;
   ae_int_t sr;
   double v;
   double w;
   double x;
   *info = 0;
   *threshold = 0;
   *rms = 0;
   *cvrms = 0;
// Test for errors in inputs
   if (n <= 0 || nc < 2) {
      *info = -1;
      return;
   }
   for (i = 0; i < n; i++) {
      if (c->xZ[i] < 0 || c->xZ[i] >= nc) {
         *info = -2;
         return;
      }
   }
   *info = 1;
// Tie
   dstiefasti(a, c, n, tiesbuf, &tiecount, bufr, bufi);
// Special case: number of ties is 1.
   if (tiecount == 1) {
      *info = -3;
      return;
   }
// General case, number of ties > 1
   for (i = 0; i < 2 * nc; i++) {
      cntbuf->xZ[i] = 0;
   }
   for (i = 0; i < n; i++) {
      cntbuf->xZ[nc + c->xZ[i]]++;
   }
   koptimal = -1;
   *threshold = a->xR[n - 1];
   cbest = ae_maxrealnumber;
   sl = 0;
   sr = n;
   for (k = 0; k < tiecount - 1; k++) {
   // first, move Kth tie from right to left
      for (i = tiesbuf->xZ[k]; i < tiesbuf->xZ[k + 1]; i++) {
         cl = c->xZ[i];
         cntbuf->xZ[cl]++;
         cntbuf->xZ[nc + cl]--;
      }
      sl += tiesbuf->xZ[k + 1] - tiesbuf->xZ[k];
      sr -= tiesbuf->xZ[k + 1] - tiesbuf->xZ[k];
   // Calculate RMS error
      v = 0.0;
      for (i = 0; i < nc; i++) {
         w = (double)(cntbuf->xZ[i]);
         v += w * ae_sqr(w / sl - 1);
         v += (sl - w) * ae_sqr(w / sl);
         w = (double)(cntbuf->xZ[nc + i]);
         v += w * ae_sqr(w / sr - 1);
         v += (sr - w) * ae_sqr(w / sr);
      }
      v = sqrt(v / (nc * n));
   // Compare with best
      x = (double)(2 * sl) / (double)(sl + sr) - 1;
      cc = v * (1 - alpha + alpha * ae_sqr(x));
      if (cc < cbest) {
      // store split
         *rms = v;
         koptimal = k;
         cbest = cc;
      // calculate CVRMS error
         *cvrms = 0.0;
         for (i = 0; i < nc; i++) {
            if (sl > 1) {
               w = (double)(cntbuf->xZ[i]);
               *cvrms += w * ae_sqr((w - 1) / (sl - 1) - 1);
               *cvrms += (sl - w) * ae_sqr(w / (sl - 1));
            } else {
               w = (double)(cntbuf->xZ[i]);
               *cvrms += w * ae_sqr(1.0 / (double)nc - 1);
               *cvrms += (sl - w) * ae_sqr(1.0 / (double)nc);
            }
            if (sr > 1) {
               w = (double)(cntbuf->xZ[nc + i]);
               *cvrms += w * ae_sqr((w - 1) / (sr - 1) - 1);
               *cvrms += (sr - w) * ae_sqr(w / (sr - 1));
            } else {
               w = (double)(cntbuf->xZ[nc + i]);
               *cvrms += w * ae_sqr(1.0 / (double)nc - 1);
               *cvrms += (sr - w) * ae_sqr(1.0 / (double)nc);
            }
         }
         *cvrms = sqrt(*cvrms / (nc * n));
      }
   }
// Calculate threshold.
// Code is a bit complicated because there can be such
// numbers that 0.5(A+B) equals to A or B (if A-B=epsilon)
   *threshold = 0.5 * (a->xR[tiesbuf->xZ[koptimal]] + a->xR[tiesbuf->xZ[koptimal + 1]]);
   if (*threshold <= a->xR[tiesbuf->xZ[koptimal]]) {
      *threshold = a->xR[tiesbuf->xZ[koptimal + 1]];
   }
}

// Internal function,
// returns number of samples of class I in Cnt[I]
static double bdss_getcv(ZVector *cnt, ae_int_t nc) {
   ae_int_t i;
   double s;
   double result;
   s = 0.0;
   for (i = 0; i < nc; i++) {
      s += cnt->xZ[i];
   }
   result = 0.0;
   for (i = 0; i < nc; i++) {
      result -= bdss_xlny((double)(cnt->xZ[i]), cnt->xZ[i] / (s + nc - 1));
   }
   return result;
}

// Internal function, adds number of samples of class I in tie NTie to Cnt[I]
static void bdss_tieaddc(ZVector *c, ZVector *ties, ae_int_t ntie, ae_int_t nc, ZVector *cnt) {
   ae_int_t i;
   for (i = ties->xZ[ntie]; i < ties->xZ[ntie + 1]; i++) {
      cnt->xZ[c->xZ[i]]++;
   }
}

// Automatic non-optimal discretization, internal subroutine.
// ALGLIB: Copyright 22.05.2008 by Sergey Bochkanov
void dssplitk(RVector *a, ZVector *c, ae_int_t n, ae_int_t nc, ae_int_t kmax, ae_int_t *info, RVector *thresholds, ae_int_t *ni, double *cve) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t j1;
   ae_int_t k;
   ae_int_t tiecount;
   double v2;
   ae_int_t bestk;
   double bestcve;
   double curcve;
   ae_frame_make(&_frame_block);
   DupVector(a);
   DupVector(c);
   *info = 0;
   SetVector(thresholds);
   *ni = 0;
   *cve = 0;
   NewVector(ties, 0, DT_INT);
   NewVector(p1, 0, DT_INT);
   NewVector(p2, 0, DT_INT);
   NewVector(cnt, 0, DT_INT);
   NewVector(bestsizes, 0, DT_INT);
   NewVector(cursizes, 0, DT_INT);
// Test for errors in inputs
   if (n <= 0 || nc < 2 || kmax < 2) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   for (i = 0; i < n; i++) {
      if (c->xZ[i] < 0 || c->xZ[i] >= nc) {
         *info = -2;
         ae_frame_leave();
         return;
      }
   }
   *info = 1;
// Tie
   dstie(a, n, &ties, &tiecount, &p1, &p2);
   for (i = 0; i < n; i++) {
      if (p2.xZ[i] != i) {
         k = c->xZ[i];
         c->xZ[i] = c->xZ[p2.xZ[i]];
         c->xZ[p2.xZ[i]] = k;
      }
   }
// Special cases
   if (tiecount == 1) {
      *info = -3;
      ae_frame_leave();
      return;
   }
// General case:
// 0. allocate arrays
   kmax = imin2(kmax, tiecount);
   ae_vector_set_length(&bestsizes, kmax);
   ae_vector_set_length(&cursizes, kmax);
   ae_vector_set_length(&cnt, nc);
// General case:
// 1. prepare "weak" solution (two subintervals, divided at median)
   v2 = ae_maxrealnumber;
   j = -1;
   for (i = 1; i < tiecount; i++) {
      if (NearR(ties.xZ[i], 0.5 * (n - 1), v2)) {
         v2 = fabs(ties.xZ[i] - 0.5 * n);
         j = i;
      }
   }
   ae_assert(j > 0, "DSSplitK: internal error #1!");
   bestk = 2;
   bestsizes.xZ[0] = ties.xZ[j];
   bestsizes.xZ[1] = n - j;
   bestcve = 0.0;
   for (i = 0; i < nc; i++) {
      cnt.xZ[i] = 0;
   }
   for (i = 0; i < j; i++) {
      bdss_tieaddc(c, &ties, i, nc, &cnt);
   }
   bestcve += bdss_getcv(&cnt, nc);
   for (i = 0; i < nc; i++) {
      cnt.xZ[i] = 0;
   }
   for (i = j; i < tiecount; i++) {
      bdss_tieaddc(c, &ties, i, nc, &cnt);
   }
   bestcve += bdss_getcv(&cnt, nc);
// General case:
// 2. Use greedy algorithm to find sub-optimal split in O(KMax*N) time
   for (k = 2; k <= kmax; k++) {
   // Prepare greedy K-interval split
      for (i = 0; i < k; i++) {
         cursizes.xZ[i] = 0;
      }
      i = 0;
      j = 0;
      while (j < tiecount && i < k) {
      // Rule: I-th bin is empty, fill it
         if (cursizes.xZ[i] == 0) {
            cursizes.xZ[i] = ties.xZ[j + 1] - ties.xZ[j];
            j++;
            continue;
         }
      // Rule: (K-1-I) bins left, (K-1-I) ties left (1 tie per bin); next bin
         if (tiecount - j == k - 1 - i) {
            i++;
            continue;
         }
      // Rule: last bin, always place in current
         if (i == k - 1) {
            cursizes.xZ[i] += ties.xZ[j + 1] - ties.xZ[j];
            j++;
            continue;
         }
      // Place J-th tie in I-th bin, or leave for I+1-th bin.
         if (fabs(cursizes.xZ[i] + ties.xZ[j + 1] - ties.xZ[j] - (double)n / (double)k) < fabs(cursizes.xZ[i] - (double)n / (double)k)) {
            cursizes.xZ[i] += ties.xZ[j + 1] - ties.xZ[j];
            j++;
         } else {
            i++;
         }
      }
      ae_assert(cursizes.xZ[k - 1] != 0 && j == tiecount, "DSSplitK: internal error #1");
   // Calculate CVE
      curcve = 0.0;
      j = 0;
      for (i = 0; i < k; i++) {
         for (j1 = 0; j1 < nc; j1++) {
            cnt.xZ[j1] = 0;
         }
         for (j1 = j; j1 < j + cursizes.xZ[i]; j1++) {
            cnt.xZ[c->xZ[j1]]++;
         }
         curcve += bdss_getcv(&cnt, nc);
         j += cursizes.xZ[i];
      }
   // Choose best variant
      if (curcve < bestcve) {
         for (i = 0; i < k; i++) {
            bestsizes.xZ[i] = cursizes.xZ[i];
         }
         bestcve = curcve;
         bestk = k;
      }
   }
// Transform from sizes to thresholds
   *cve = bestcve;
   *ni = bestk;
   ae_vector_set_length(thresholds, *ni - 1);
   j = bestsizes.xZ[0];
   for (i = 1; i < bestk; i++) {
      thresholds->xR[i - 1] = 0.5 * (a->xR[j - 1] + a->xR[j]);
      j += bestsizes.xZ[i];
   }
   ae_frame_leave();
}

// Internal function, subtracts number of samples of class I in tie NTie to Cnt[I]
static void bdss_tiesubc(ZVector *c, ZVector *ties, ae_int_t ntie, ae_int_t nc, ZVector *cnt) {
   ae_int_t i;
   for (i = ties->xZ[ntie]; i < ties->xZ[ntie + 1]; i++) {
      cnt->xZ[c->xZ[i]]--;
   }
}

// Automatic optimal discretization, internal subroutine.
// ALGLIB: Copyright 22.05.2008 by Sergey Bochkanov
void dsoptimalsplitk(RVector *a, ZVector *c, ae_int_t n, ae_int_t nc, ae_int_t kmax, ae_int_t *info, RVector *thresholds, ae_int_t *ni, double *cve) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t s;
   ae_int_t jl;
   ae_int_t jr;
   double v2;
   ae_int_t tiecount;
   double cvtemp;
   ae_int_t k;
   ae_int_t koptimal;
   double cvoptimal;
   ae_frame_make(&_frame_block);
   DupVector(a);
   DupVector(c);
   *info = 0;
   SetVector(thresholds);
   *ni = 0;
   *cve = 0;
   NewVector(ties, 0, DT_INT);
   NewVector(p1, 0, DT_INT);
   NewVector(p2, 0, DT_INT);
   NewVector(cnt, 0, DT_INT);
   NewVector(cnt2, 0, DT_INT);
   NewMatrix(cv, 0, 0, DT_REAL);
   NewMatrix(splits, 0, 0, DT_INT);
// Test for errors in inputs
   if (n <= 0 || nc < 2 || kmax < 2) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   for (i = 0; i < n; i++) {
      if (c->xZ[i] < 0 || c->xZ[i] >= nc) {
         *info = -2;
         ae_frame_leave();
         return;
      }
   }
   *info = 1;
// Tie
   dstie(a, n, &ties, &tiecount, &p1, &p2);
   for (i = 0; i < n; i++) {
      if (p2.xZ[i] != i) {
         k = c->xZ[i];
         c->xZ[i] = c->xZ[p2.xZ[i]];
         c->xZ[p2.xZ[i]] = k;
      }
   }
// Special cases
   if (tiecount == 1) {
      *info = -3;
      ae_frame_leave();
      return;
   }
// General case
// Use dynamic programming to find best split in O(KMax*NC*TieCount^2) time
   kmax = imin2(kmax, tiecount);
   ae_matrix_set_length(&cv, kmax, tiecount);
   ae_matrix_set_length(&splits, kmax, tiecount);
   ae_vector_set_length(&cnt, nc);
   ae_vector_set_length(&cnt2, nc);
   for (j = 0; j < nc; j++) {
      cnt.xZ[j] = 0;
   }
   for (j = 0; j < tiecount; j++) {
      bdss_tieaddc(c, &ties, j, nc, &cnt);
      splits.xyZ[0][j] = 0;
      cv.xyR[0][j] = bdss_getcv(&cnt, nc);
   }
   for (k = 1; k < kmax; k++) {
      for (j = 0; j < nc; j++) {
         cnt.xZ[j] = 0;
      }
   // Subtask size J in [K..TieCount-1]:
   // optimal K-splitting on ties from 0-th to J-th.
      for (j = k; j < tiecount; j++) {
      // Update Cnt - let it contain classes of ties from K-th to J-th
         bdss_tieaddc(c, &ties, j, nc, &cnt);
      // Search for optimal split point S in [K..J]
         for (i = 0; i < nc; i++) {
            cnt2.xZ[i] = cnt.xZ[i];
         }
         cv.xyR[k][j] = cv.xyR[k - 1][j - 1] + bdss_getcv(&cnt2, nc);
         splits.xyZ[k][j] = j;
         for (s = k + 1; s <= j; s++) {
         // Update Cnt2 - let it contain classes of ties from S-th to J-th
            bdss_tiesubc(c, &ties, s - 1, nc, &cnt2);
         // Calculate CVE
            cvtemp = cv.xyR[k - 1][s - 1] + bdss_getcv(&cnt2, nc);
            if (cvtemp < cv.xyR[k][j]) {
               cv.xyR[k][j] = cvtemp;
               splits.xyZ[k][j] = s;
            }
         }
      }
   }
// Choose best partition, output result
   koptimal = -1;
   cvoptimal = ae_maxrealnumber;
   for (k = 0; k < kmax; k++) {
      if (cv.xyR[k][tiecount - 1] < cvoptimal) {
         cvoptimal = cv.xyR[k][tiecount - 1];
         koptimal = k;
      }
   }
   ae_assert(koptimal >= 0, "DSOptimalSplitK: internal error #1!");
   if (koptimal == 0) {
   // Special case: best partition is one big interval.
   // Even 2-partition is not better.
   // This is possible when dealing with "weak" predictor variables.
   //
   // Make binary split as close to the median as possible.
      v2 = ae_maxrealnumber;
      j = -1;
      for (i = 1; i < tiecount; i++) {
         if (NearR(ties.xZ[i], 0.5 * (n - 1), v2)) {
            v2 = fabs(ties.xZ[i] - 0.5 * (n - 1));
            j = i;
         }
      }
      ae_assert(j > 0, "DSOptimalSplitK: internal error #2!");
      ae_vector_set_length(thresholds, 0 + 1);
      thresholds->xR[0] = 0.5 * (a->xR[ties.xZ[j - 1]] + a->xR[ties.xZ[j]]);
      *ni = 2;
      *cve = 0.0;
      for (i = 0; i < nc; i++) {
         cnt.xZ[i] = 0;
      }
      for (i = 0; i < j; i++) {
         bdss_tieaddc(c, &ties, i, nc, &cnt);
      }
      *cve += bdss_getcv(&cnt, nc);
      for (i = 0; i < nc; i++) {
         cnt.xZ[i] = 0;
      }
      for (i = j; i < tiecount; i++) {
         bdss_tieaddc(c, &ties, i, nc, &cnt);
      }
      *cve += bdss_getcv(&cnt, nc);
   } else {
   // General case: 2 or more intervals
   //
   // NOTE: we initialize both JL and JR (left and right bounds),
   //       altough algorithm needs only JL.
      ae_vector_set_length(thresholds, koptimal);
      *ni = koptimal + 1;
      jr = tiecount - 1;
      *cve = cv.xyR[koptimal][jr];
      jl = splits.xyZ[koptimal][jr];
      for (k = koptimal; k >= 1; k--) {
         thresholds->xR[k - 1] = 0.5 * (a->xR[ties.xZ[jl - 1]] + a->xR[ties.xZ[jl]]);
         jr = jl - 1;
         jl = splits.xyZ[k - 1][jr];
      }
   }
   ae_frame_leave();
}

void cvreport_init(void *_p, bool make_automatic) {
}

void cvreport_copy(void *_dst, void *_src, bool make_automatic) {
   cvreport *dst = (cvreport *)_dst;
   cvreport *src = (cvreport *)_src;
   dst->relclserror = src->relclserror;
   dst->avgce = src->avgce;
   dst->rmserror = src->rmserror;
   dst->avgerror = src->avgerror;
   dst->avgrelerror = src->avgrelerror;
}

void cvreport_free(void *_p, bool make_automatic) {
}
} // end of namespace alglib_impl

namespace alglib {
void dsoptimalsplit2(const real_1d_array &a, const integer_1d_array &c, const ae_int_t n, ae_int_t &info, double &threshold, double &pal, double &pbl, double &par, double &pbr, double &cve) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dsoptimalsplit2(ConstT(ae_vector, a), ConstT(ae_vector, c), n, &info, &threshold, &pal, &pbl, &par, &pbr, &cve);
   alglib_impl::ae_state_clear();
}

void dsoptimalsplit2fast(real_1d_array &a, integer_1d_array &c, integer_1d_array &tiesbuf, integer_1d_array &cntbuf, real_1d_array &bufr, integer_1d_array &bufi, const ae_int_t n, const ae_int_t nc, const double alpha, ae_int_t &info, double &threshold, double &rms, double &cvrms) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dsoptimalsplit2fast(ConstT(ae_vector, a), ConstT(ae_vector, c), ConstT(ae_vector, tiesbuf), ConstT(ae_vector, cntbuf), ConstT(ae_vector, bufr), ConstT(ae_vector, bufi), n, nc, alpha, &info, &threshold, &rms, &cvrms);
   alglib_impl::ae_state_clear();
}
} // end of namespace alglib

// === MLPBASE Package ===
// Depends on: (AlgLibInternal) SCODES, HPCCORES
// Depends on: (LinAlg) SPARSE
// Depends on: BDSS
namespace alglib_impl {
static const ae_int_t mlpbase_mlpvnum = 7;
static const ae_int_t mlpbase_mlpfirstversion = 0;
static const ae_int_t mlpbase_nfieldwidth = 4;
static const ae_int_t mlpbase_hlconnfieldwidth = 5;
static const ae_int_t mlpbase_hlnfieldwidth = 4;
static const ae_int_t mlpbase_gradbasecasecost = 50000;
static const ae_int_t mlpbase_microbatchsize = 64;

// This function returns number of weights  updates  which  is  required  for
// gradient calculation problem to be splitted.
ae_int_t mlpgradsplitcost() {
   ae_int_t result;
   result = mlpbase_gradbasecasecost;
   return result;
}

// This  function  returns  number  of elements in subset of dataset which is
// required for gradient calculation problem to be splitted.
ae_int_t mlpgradsplitsize() {
   ae_int_t result;
   result = mlpbase_microbatchsize;
   return result;
}

// This routine adds input layer to the high-level description of the network.
//
// It modifies Network.HLConnections and Network.HLNeurons  and  assumes that
// these  arrays  have  enough  place  to  store  data.  It accepts following
// parameters:
//     Network     -   network
//     ConnIdx     -   index of the first free entry in the HLConnections
//     NeuroIdx    -   index of the first free entry in the HLNeurons
//     StructInfoIdx-  index of the first entry in the low level description
//                     of the current layer (in the StructInfo array)
//     NIn         -   number of inputs
//
// It modified Network and indices.
static void mlpbase_hladdinputlayer(multilayerperceptron *network, ae_int_t *connidx, ae_int_t *neuroidx, ae_int_t *structinfoidx, ae_int_t nin) {
   ae_int_t i;
   ae_int_t offs;
   offs = mlpbase_hlnfieldwidth * (*neuroidx);
   for (i = 0; i < nin; i++) {
      network->hlneurons.xZ[offs] = 0;
      network->hlneurons.xZ[offs + 1] = i;
      network->hlneurons.xZ[offs + 2] = -1;
      network->hlneurons.xZ[offs + 3] = -1;
      offs += mlpbase_hlnfieldwidth;
   }
   *neuroidx += nin;
   *structinfoidx += nin;
}

// This routine adds output layer to the high-level description of
// the network.
//
// It modifies Network.HLConnections and Network.HLNeurons  and  assumes that
// these  arrays  have  enough  place  to  store  data.  It accepts following
// parameters:
//     Network     -   network
//     ConnIdx     -   index of the first free entry in the HLConnections
//     NeuroIdx    -   index of the first free entry in the HLNeurons
//     StructInfoIdx-  index of the first entry in the low level description
//                     of the current layer (in the StructInfo array)
//     WeightsIdx  -   index of the first entry in the Weights array which
//                     corresponds to the current layer
//     K           -   current layer index
//     NPrev       -   number of neurons in the previous layer
//     NOut        -   number of outputs
//     IsCls       -   is it classifier network?
//     IsLinear    -   is it network with linear output?
//
// It modified Network and ConnIdx/NeuroIdx/StructInfoIdx/WeightsIdx.
static void mlpbase_hladdoutputlayer(multilayerperceptron *network, ae_int_t *connidx, ae_int_t *neuroidx, ae_int_t *structinfoidx, ae_int_t *weightsidx, ae_int_t k, ae_int_t nprev, ae_int_t nout, bool iscls, bool islinearout) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t neurooffs;
   ae_int_t connoffs;
   ae_assert(islinearout || !iscls, "HLAddOutputLayer: internal error");
   neurooffs = mlpbase_hlnfieldwidth * (*neuroidx);
   connoffs = mlpbase_hlconnfieldwidth * (*connidx);
   if (!iscls) {
   // Regression network
      for (i = 0; i < nout; i++) {
         network->hlneurons.xZ[neurooffs] = k;
         network->hlneurons.xZ[neurooffs + 1] = i;
         network->hlneurons.xZ[neurooffs + 2] = *structinfoidx + 1 + nout + i;
         network->hlneurons.xZ[neurooffs + 3] = *weightsidx + nprev + (nprev + 1) * i;
         neurooffs += mlpbase_hlnfieldwidth;
      }
      for (i = 0; i < nprev; i++) {
         for (j = 0; j < nout; j++) {
            network->hlconnections.xZ[connoffs] = k - 1;
            network->hlconnections.xZ[connoffs + 1] = i;
            network->hlconnections.xZ[connoffs + 2] = k;
            network->hlconnections.xZ[connoffs + 3] = j;
            network->hlconnections.xZ[connoffs + 4] = *weightsidx + i + j * (nprev + 1);
            connoffs += mlpbase_hlconnfieldwidth;
         }
      }
      *connidx += nprev * nout;
      *neuroidx += nout;
      *structinfoidx += 2 * nout + 1;
      *weightsidx += nout * (nprev + 1);
   } else {
   // Classification network
      for (i = 0; i < nout - 1; i++) {
         network->hlneurons.xZ[neurooffs] = k;
         network->hlneurons.xZ[neurooffs + 1] = i;
         network->hlneurons.xZ[neurooffs + 2] = -1;
         network->hlneurons.xZ[neurooffs + 3] = *weightsidx + nprev + (nprev + 1) * i;
         neurooffs += mlpbase_hlnfieldwidth;
      }
      network->hlneurons.xZ[neurooffs] = k;
      network->hlneurons.xZ[neurooffs + 1] = i;
      network->hlneurons.xZ[neurooffs + 2] = -1;
      network->hlneurons.xZ[neurooffs + 3] = -1;
      for (i = 0; i < nprev; i++) {
         for (j = 0; j < nout - 1; j++) {
            network->hlconnections.xZ[connoffs] = k - 1;
            network->hlconnections.xZ[connoffs + 1] = i;
            network->hlconnections.xZ[connoffs + 2] = k;
            network->hlconnections.xZ[connoffs + 3] = j;
            network->hlconnections.xZ[connoffs + 4] = *weightsidx + i + j * (nprev + 1);
            connoffs += mlpbase_hlconnfieldwidth;
         }
      }
      *connidx += nprev * (nout - 1);
      *neuroidx += nout;
      *structinfoidx += nout + 2;
      *weightsidx += (nout - 1) * (nprev + 1);
   }
}

// This routine adds hidden layer to the high-level description of
// the network.
//
// It modifies Network.HLConnections and Network.HLNeurons  and  assumes that
// these  arrays  have  enough  place  to  store  data.  It accepts following
// parameters:
//     Network     -   network
//     ConnIdx     -   index of the first free entry in the HLConnections
//     NeuroIdx    -   index of the first free entry in the HLNeurons
//     StructInfoIdx-  index of the first entry in the low level description
//                     of the current layer (in the StructInfo array)
//     WeightsIdx  -   index of the first entry in the Weights array which
//                     corresponds to the current layer
//     K           -   current layer index
//     NPrev       -   number of neurons in the previous layer
//     NCur        -   number of neurons in the current layer
//
// It modified Network and ConnIdx/NeuroIdx/StructInfoIdx/WeightsIdx.
static void mlpbase_hladdhiddenlayer(multilayerperceptron *network, ae_int_t *connidx, ae_int_t *neuroidx, ae_int_t *structinfoidx, ae_int_t *weightsidx, ae_int_t k, ae_int_t nprev, ae_int_t ncur) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t neurooffs;
   ae_int_t connoffs;
   neurooffs = mlpbase_hlnfieldwidth * (*neuroidx);
   connoffs = mlpbase_hlconnfieldwidth * (*connidx);
   for (i = 0; i < ncur; i++) {
      network->hlneurons.xZ[neurooffs] = k;
      network->hlneurons.xZ[neurooffs + 1] = i;
      network->hlneurons.xZ[neurooffs + 2] = *structinfoidx + 1 + ncur + i;
      network->hlneurons.xZ[neurooffs + 3] = *weightsidx + nprev + (nprev + 1) * i;
      neurooffs += mlpbase_hlnfieldwidth;
   }
   for (i = 0; i < nprev; i++) {
      for (j = 0; j < ncur; j++) {
         network->hlconnections.xZ[connoffs] = k - 1;
         network->hlconnections.xZ[connoffs + 1] = i;
         network->hlconnections.xZ[connoffs + 2] = k;
         network->hlconnections.xZ[connoffs + 3] = j;
         network->hlconnections.xZ[connoffs + 4] = *weightsidx + i + j * (nprev + 1);
         connoffs += mlpbase_hlconnfieldwidth;
      }
   }
   *connidx += nprev * ncur;
   *neuroidx += ncur;
   *structinfoidx += 2 * ncur + 1;
   *weightsidx += ncur * (nprev + 1);
}

// This function fills high level information about network created using
// internal MLPCreate() function.
//
// This function does NOT examine StructInfo for low level information, it
// just expects that network has following structure:
//
//     input neuron            \
//     ...                      | input layer
//     input neuron            /
//
//     "-1" neuron             \
//     biased summator          |
//     ...                      |
//     biased summator          | hidden layer(s), if there are exists any
//     activation function      |
//     ...                      |
//     activation function     /
//
//     "-1" neuron            \
//     biased summator         | output layer:
//     ...                     |
//     biased summator         | * we have NOut summators/activators for regression networks
//     activation function     | * we have only NOut-1 summators and no activators for classifiers
//     ...                     | * we have "0" neuron only when we have classifier
//     activation function     |
//     "0" neuron              /
// ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
static void mlpbase_fillhighlevelinformation(multilayerperceptron *network, ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, bool iscls, bool islinearout) {
   ae_int_t idxweights;
   ae_int_t idxstruct;
   ae_int_t idxneuro;
   ae_int_t idxconn;
   ae_assert(islinearout || !iscls, "FillHighLevelInformation: internal error");
// Preparations common to all types of networks
   idxweights = 0;
   idxneuro = 0;
   idxstruct = 0;
   idxconn = 0;
   network->hlnetworktype = 0;
// network without hidden layers
   if (nhid1 == 0) {
      ae_vector_set_length(&network->hllayersizes, 2);
      network->hllayersizes.xZ[0] = nin;
      network->hllayersizes.xZ[1] = nout;
      if (!iscls) {
         ae_vector_set_length(&network->hlconnections, mlpbase_hlconnfieldwidth * nin * nout);
         ae_vector_set_length(&network->hlneurons, mlpbase_hlnfieldwidth * (nin + nout));
         network->hlnormtype = 0;
      } else {
         ae_vector_set_length(&network->hlconnections, mlpbase_hlconnfieldwidth * nin * (nout - 1));
         ae_vector_set_length(&network->hlneurons, mlpbase_hlnfieldwidth * (nin + nout));
         network->hlnormtype = 1;
      }
      mlpbase_hladdinputlayer(network, &idxconn, &idxneuro, &idxstruct, nin);
      mlpbase_hladdoutputlayer(network, &idxconn, &idxneuro, &idxstruct, &idxweights, 1, nin, nout, iscls, islinearout);
      return;
   }
// network with one hidden layers
   if (nhid2 == 0) {
      ae_vector_set_length(&network->hllayersizes, 3);
      network->hllayersizes.xZ[0] = nin;
      network->hllayersizes.xZ[1] = nhid1;
      network->hllayersizes.xZ[2] = nout;
      if (!iscls) {
         ae_vector_set_length(&network->hlconnections, mlpbase_hlconnfieldwidth * (nin * nhid1 + nhid1 * nout));
         ae_vector_set_length(&network->hlneurons, mlpbase_hlnfieldwidth * (nin + nhid1 + nout));
         network->hlnormtype = 0;
      } else {
         ae_vector_set_length(&network->hlconnections, mlpbase_hlconnfieldwidth * (nin * nhid1 + nhid1 * (nout - 1)));
         ae_vector_set_length(&network->hlneurons, mlpbase_hlnfieldwidth * (nin + nhid1 + nout));
         network->hlnormtype = 1;
      }
      mlpbase_hladdinputlayer(network, &idxconn, &idxneuro, &idxstruct, nin);
      mlpbase_hladdhiddenlayer(network, &idxconn, &idxneuro, &idxstruct, &idxweights, 1, nin, nhid1);
      mlpbase_hladdoutputlayer(network, &idxconn, &idxneuro, &idxstruct, &idxweights, 2, nhid1, nout, iscls, islinearout);
      return;
   }
// Two hidden layers
   ae_vector_set_length(&network->hllayersizes, 4);
   network->hllayersizes.xZ[0] = nin;
   network->hllayersizes.xZ[1] = nhid1;
   network->hllayersizes.xZ[2] = nhid2;
   network->hllayersizes.xZ[3] = nout;
   if (!iscls) {
      ae_vector_set_length(&network->hlconnections, mlpbase_hlconnfieldwidth * (nin * nhid1 + nhid1 * nhid2 + nhid2 * nout));
      ae_vector_set_length(&network->hlneurons, mlpbase_hlnfieldwidth * (nin + nhid1 + nhid2 + nout));
      network->hlnormtype = 0;
   } else {
      ae_vector_set_length(&network->hlconnections, mlpbase_hlconnfieldwidth * (nin * nhid1 + nhid1 * nhid2 + nhid2 * (nout - 1)));
      ae_vector_set_length(&network->hlneurons, mlpbase_hlnfieldwidth * (nin + nhid1 + nhid2 + nout));
      network->hlnormtype = 1;
   }
   mlpbase_hladdinputlayer(network, &idxconn, &idxneuro, &idxstruct, nin);
   mlpbase_hladdhiddenlayer(network, &idxconn, &idxneuro, &idxstruct, &idxweights, 1, nin, nhid1);
   mlpbase_hladdhiddenlayer(network, &idxconn, &idxneuro, &idxstruct, &idxweights, 2, nhid1, nhid2);
   mlpbase_hladdoutputlayer(network, &idxconn, &idxneuro, &idxstruct, &idxweights, 3, nhid2, nout, iscls, islinearout);
}

// Internal subroutine: adding new input layer to network
static void mlpbase_addinputlayer(ae_int_t ncount, ZVector *lsizes, ZVector *ltypes, ZVector *lconnfirst, ZVector *lconnlast, ae_int_t *lastproc) {
   lsizes->xZ[0] = ncount;
   ltypes->xZ[0] = -2;
   lconnfirst->xZ[0] = 0;
   lconnlast->xZ[0] = 0;
   *lastproc = 0;
}

// Internal subroutine: adding new summator layer to network
static void mlpbase_addbiasedsummatorlayer(ae_int_t ncount, ZVector *lsizes, ZVector *ltypes, ZVector *lconnfirst, ZVector *lconnlast, ae_int_t *lastproc) {
   lsizes->xZ[*lastproc + 1] = 1;
   ltypes->xZ[*lastproc + 1] = -3;
   lconnfirst->xZ[*lastproc + 1] = 0;
   lconnlast->xZ[*lastproc + 1] = 0;
   lsizes->xZ[*lastproc + 2] = ncount;
   ltypes->xZ[*lastproc + 2] = 0;
   lconnfirst->xZ[*lastproc + 2] = *lastproc;
   lconnlast->xZ[*lastproc + 2] = *lastproc + 1;
   *lastproc += 2;
}

// Internal subroutine: adding new summator layer to network
static void mlpbase_addactivationlayer(ae_int_t functype, ZVector *lsizes, ZVector *ltypes, ZVector *lconnfirst, ZVector *lconnlast, ae_int_t *lastproc) {
   ae_assert(functype > 0 || functype == -5, "AddActivationLayer: incorrect function type");
   lsizes->xZ[*lastproc + 1] = lsizes->xZ[*lastproc];
   ltypes->xZ[*lastproc + 1] = functype;
   lconnfirst->xZ[*lastproc + 1] = *lastproc;
   lconnlast->xZ[*lastproc + 1] = *lastproc;
   ++*lastproc;
}

// Internal subroutine: adding new zero layer to network
static void mlpbase_addzerolayer(ZVector *lsizes, ZVector *ltypes, ZVector *lconnfirst, ZVector *lconnlast, ae_int_t *lastproc) {
   lsizes->xZ[*lastproc + 1] = 1;
   ltypes->xZ[*lastproc + 1] = -4;
   lconnfirst->xZ[*lastproc + 1] = 0;
   lconnlast->xZ[*lastproc + 1] = 0;
   ++*lastproc;
}

// Internal subroutine.
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
static void mlpbase_mlpcreate(ae_int_t nin, ae_int_t nout, ZVector *lsizes, ZVector *ltypes, ZVector *lconnfirst, ZVector *lconnlast, ae_int_t layerscount, bool isclsnet, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t ssize;
   ae_int_t ntotal;
   ae_int_t wcount;
   ae_int_t offs;
   ae_int_t nprocessed;
   ae_int_t wallocated;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(localtemp, 0, DT_INT);
   NewVector(lnfirst, 0, DT_INT);
   NewVector(lnsyn, 0, DT_INT);
   NewObj(mlpbuffers, buf);
   NewObj(smlpgrad, sgrad);
// Check
   ae_assert(layerscount > 0, "MLPCreate: wrong parameters!");
   ae_assert(ltypes->xZ[0] == -2, "MLPCreate: wrong LTypes[0] (must be -2)!");
   for (i = 0; i < layerscount; i++) {
      ae_assert(lsizes->xZ[i] > 0, "MLPCreate: wrong LSizes!");
      ae_assert(lconnfirst->xZ[i] >= 0 && (lconnfirst->xZ[i] < i || i == 0), "MLPCreate: wrong LConnFirst!");
      ae_assert(lconnlast->xZ[i] >= lconnfirst->xZ[i] && (lconnlast->xZ[i] < i || i == 0), "MLPCreate: wrong LConnLast!");
   }
// Build network geometry
   ae_vector_set_length(&lnfirst, layerscount);
   ae_vector_set_length(&lnsyn, layerscount);
   ntotal = 0;
   wcount = 0;
   for (i = 0; i < layerscount; i++) {
   // Analyze connections.
   // This code must throw an assertion in case of unknown LTypes[I]
      lnsyn.xZ[i] = -1;
      if (ltypes->xZ[i] >= 0 || ltypes->xZ[i] == -5) {
         lnsyn.xZ[i] = 0;
         for (j = lconnfirst->xZ[i]; j <= lconnlast->xZ[i]; j++) {
            lnsyn.xZ[i] += lsizes->xZ[j];
         }
      } else {
         if (ltypes->xZ[i] == -2 || ltypes->xZ[i] == -3 || ltypes->xZ[i] == -4) {
            lnsyn.xZ[i] = 0;
         }
      }
      ae_assert(lnsyn.xZ[i] >= 0, "MLPCreate: internal error #0!");
   // Other info
      lnfirst.xZ[i] = ntotal;
      ntotal += lsizes->xZ[i];
      if (ltypes->xZ[i] == 0) {
         wcount += lnsyn.xZ[i] * lsizes->xZ[i];
      }
   }
   ssize = 7 + ntotal * mlpbase_nfieldwidth;
// Allocate
   ae_vector_set_length(&network->structinfo, ssize);
   ae_vector_set_length(&network->weights, wcount);
   if (isclsnet) {
      ae_vector_set_length(&network->columnmeans, nin);
      ae_vector_set_length(&network->columnsigmas, nin);
   } else {
      ae_vector_set_length(&network->columnmeans, nin + nout);
      ae_vector_set_length(&network->columnsigmas, nin + nout);
   }
   ae_vector_set_length(&network->neurons, ntotal);
   ae_vector_set_length(&network->nwbuf, imax2(wcount, 2 * nout));
   ae_vector_set_length(&network->integerbuf, 3 + 1);
   ae_vector_set_length(&network->dfdnet, ntotal);
   ae_vector_set_length(&network->x, nin);
   ae_vector_set_length(&network->y, nout);
   ae_vector_set_length(&network->derror, ntotal);
// Fill structure:
// * first, fill by dummy values to avoid spurious reports by Valgrind
// * then fill global info header
   for (i = 0; i < ssize; i++) {
      network->structinfo.xZ[i] = -999999;
   }
   network->structinfo.xZ[0] = ssize;
   network->structinfo.xZ[1] = nin;
   network->structinfo.xZ[2] = nout;
   network->structinfo.xZ[3] = ntotal;
   network->structinfo.xZ[4] = wcount;
   network->structinfo.xZ[5] = 7;
   if (isclsnet) {
      network->structinfo.xZ[6] = 1;
   } else {
      network->structinfo.xZ[6] = 0;
   }
// Fill structure: neuron connections
   nprocessed = 0;
   wallocated = 0;
   for (i = 0; i < layerscount; i++) {
      for (j = 0; j < lsizes->xZ[i]; j++) {
         offs = network->structinfo.xZ[5] + nprocessed * mlpbase_nfieldwidth;
         network->structinfo.xZ[offs] = ltypes->xZ[i];
         if (ltypes->xZ[i] == 0) {
         // Adaptive summator:
         // * connections with weights to previous neurons
            network->structinfo.xZ[offs + 1] = lnsyn.xZ[i];
            network->structinfo.xZ[offs + 2] = lnfirst.xZ[lconnfirst->xZ[i]];
            network->structinfo.xZ[offs + 3] = wallocated;
            wallocated += lnsyn.xZ[i];
            nprocessed++;
         }
         if (ltypes->xZ[i] > 0 || ltypes->xZ[i] == -5) {
         // Activation layer:
         // * each neuron connected to one (only one) of previous neurons.
         // * no weights
            network->structinfo.xZ[offs + 1] = 1;
            network->structinfo.xZ[offs + 2] = lnfirst.xZ[lconnfirst->xZ[i]] + j;
            network->structinfo.xZ[offs + 3] = -1;
            nprocessed++;
         }
         if (ltypes->xZ[i] == -2 || ltypes->xZ[i] == -3 || ltypes->xZ[i] == -4) {
            nprocessed++;
         }
      }
   }
   ae_assert(wallocated == wcount, "MLPCreate: internal error #1!");
   ae_assert(nprocessed == ntotal, "MLPCreate: internal error #2!");
// Fill weights by small random values
// Initialize means and sigmas
   for (i = 0; i < nin; i++) {
      network->columnmeans.xR[i] = 0.0;
      network->columnsigmas.xR[i] = 1.0;
   }
   if (!isclsnet) {
      for (i = 0; i < nout; i++) {
         network->columnmeans.xR[nin + i] = 0.0;
         network->columnsigmas.xR[nin + i] = 1.0;
      }
   }
   mlprandomize(network);
// Seed buffers
   ae_shared_pool_set_seed(&network->buf, &buf, sizeof(buf), mlpbuffers_init, mlpbuffers_copy, mlpbuffers_free);
   ae_vector_set_length(&sgrad.g, wcount);
   sgrad.f = 0.0;
   for (i = 0; i < wcount; i++) {
      sgrad.g.xR[i] = 0.0;
   }
   ae_shared_pool_set_seed(&network->gradbuf, &sgrad, sizeof(sgrad), smlpgrad_init, smlpgrad_copy, smlpgrad_free);
   ae_frame_leave();
}

// Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
// layers, with linear output layer. Network weights are  filled  with  small
// random values.
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpcreate0(const ae_int_t nin, const ae_int_t nout, multilayerperceptron &network);
void mlpcreate0(ae_int_t nin, ae_int_t nout, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   layerscount = 1 + 3;
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(-5, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, false, network);
   mlpbase_fillhighlevelinformation(network, nin, 0, 0, nout, false, true);
   ae_frame_leave();
}

// Same  as  MLPCreate0,  but  with  one  hidden  layer  (NHid  neurons) with
// non-linear activation function. Output layer is linear.
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpcreate1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, multilayerperceptron &network);
void mlpcreate1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   layerscount = 1 + 3 + 3;
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(-5, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, false, network);
   mlpbase_fillhighlevelinformation(network, nin, nhid, 0, nout, false, true);
   ae_frame_leave();
}

// Same as MLPCreate0, but with two hidden layers (NHid1 and  NHid2  neurons)
// with non-linear activation function. Output layer is linear.
//  $ALL
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpcreate2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, multilayerperceptron &network);
void mlpcreate2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   layerscount = 1 + 3 + 3 + 3;
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid2, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(-5, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, false, network);
   mlpbase_fillhighlevelinformation(network, nin, nhid1, nhid2, nout, false, true);
   ae_frame_leave();
}

// Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
// layers with non-linear output layer. Network weights are filled with small
// random values.
//
// Activation function of the output layer takes values:
//
//     (B, +INF), if D >= 0
//
// or
//
//     (-INF, B), if D < 0.
// ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
// API: void mlpcreateb0(const ae_int_t nin, const ae_int_t nout, const double b, const double d, multilayerperceptron &network);
void mlpcreateb0(ae_int_t nin, ae_int_t nout, double b, double d, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   layerscount = 1 + 3;
   if (d >= 0.0) {
      d = 1.0;
   } else {
      d = -1.0;
   }
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(3, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, false, network);
   mlpbase_fillhighlevelinformation(network, nin, 0, 0, nout, false, false);
// Turn on outputs shift/scaling.
   for (i = nin; i < nin + nout; i++) {
      network->columnmeans.xR[i] = b;
      network->columnsigmas.xR[i] = d;
   }
   ae_frame_leave();
}

// Same as MLPCreateB0 but with non-linear hidden layer.
// ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
// API: void mlpcreateb1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const double b, const double d, multilayerperceptron &network);
void mlpcreateb1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, double b, double d, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   layerscount = 1 + 3 + 3;
   if (d >= 0.0) {
      d = 1.0;
   } else {
      d = -1.0;
   }
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(3, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, false, network);
   mlpbase_fillhighlevelinformation(network, nin, nhid, 0, nout, false, false);
// Turn on outputs shift/scaling.
   for (i = nin; i < nin + nout; i++) {
      network->columnmeans.xR[i] = b;
      network->columnsigmas.xR[i] = d;
   }
   ae_frame_leave();
}

// Same as MLPCreateB0 but with two non-linear hidden layers.
// ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
// API: void mlpcreateb2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const double b, const double d, multilayerperceptron &network);
void mlpcreateb2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, double b, double d, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   layerscount = 1 + 3 + 3 + 3;
   if (d >= 0.0) {
      d = 1.0;
   } else {
      d = -1.0;
   }
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid2, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(3, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, false, network);
   mlpbase_fillhighlevelinformation(network, nin, nhid1, nhid2, nout, false, false);
// Turn on outputs shift/scaling.
   for (i = nin; i < nin + nout; i++) {
      network->columnmeans.xR[i] = b;
      network->columnsigmas.xR[i] = d;
   }
   ae_frame_leave();
}

// Creates  neural  network  with  NIn  inputs,  NOut outputs, without hidden
// layers with non-linear output layer. Network weights are filled with small
// random values. Activation function of the output layer takes values [A,B].
// ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
// API: void mlpcreater0(const ae_int_t nin, const ae_int_t nout, const double a, const double b, multilayerperceptron &network);
void mlpcreater0(ae_int_t nin, ae_int_t nout, double a, double b, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   layerscount = 1 + 3;
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, false, network);
   mlpbase_fillhighlevelinformation(network, nin, 0, 0, nout, false, false);
// Turn on outputs shift/scaling.
   for (i = nin; i < nin + nout; i++) {
      network->columnmeans.xR[i] = 0.5 * (a + b);
      network->columnsigmas.xR[i] = 0.5 * (a - b);
   }
   ae_frame_leave();
}

// Same as MLPCreateR0, but with non-linear hidden layer.
// ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
// API: void mlpcreater1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const double a, const double b, multilayerperceptron &network);
void mlpcreater1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, double a, double b, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   layerscount = 1 + 3 + 3;
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, false, network);
   mlpbase_fillhighlevelinformation(network, nin, nhid, 0, nout, false, false);
// Turn on outputs shift/scaling.
   for (i = nin; i < nin + nout; i++) {
      network->columnmeans.xR[i] = 0.5 * (a + b);
      network->columnsigmas.xR[i] = 0.5 * (a - b);
   }
   ae_frame_leave();
}

// Same as MLPCreateR0, but with two non-linear hidden layers.
// ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
// API: void mlpcreater2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const double a, const double b, multilayerperceptron &network);
void mlpcreater2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, double a, double b, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   layerscount = 1 + 3 + 3 + 3;
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid2, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, false, network);
   mlpbase_fillhighlevelinformation(network, nin, nhid1, nhid2, nout, false, false);
// Turn on outputs shift/scaling.
   for (i = nin; i < nin + nout; i++) {
      network->columnmeans.xR[i] = 0.5 * (a + b);
      network->columnsigmas.xR[i] = 0.5 * (a - b);
   }
   ae_frame_leave();
}

// Creates classifier network with NIn  inputs  and  NOut  possible  classes.
// Network contains no hidden layers and linear output  layer  with  SOFTMAX-
// normalization  (so  outputs  sums  up  to  1.0  and  converge to posterior
// probabilities).
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpcreatec0(const ae_int_t nin, const ae_int_t nout, multilayerperceptron &network);
void mlpcreatec0(ae_int_t nin, ae_int_t nout, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   ae_assert(nout >= 2, "MLPCreateC0: NOut<2!");
   layerscount = 1 + 2 + 1;
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout - 1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addzerolayer(&lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, true, network);
   mlpbase_fillhighlevelinformation(network, nin, 0, 0, nout, true, true);
   ae_frame_leave();
}

// Same as MLPCreateC0, but with one non-linear hidden layer.
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpcreatec1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, multilayerperceptron &network);
void mlpcreatec1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   ae_assert(nout >= 2, "MLPCreateC1: NOut<2!");
   layerscount = 1 + 3 + 2 + 1;
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout - 1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addzerolayer(&lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, true, network);
   mlpbase_fillhighlevelinformation(network, nin, nhid, 0, nout, true, true);
   ae_frame_leave();
}

// Same as MLPCreateC0, but with two non-linear hidden layers.
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpcreatec2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, multilayerperceptron &network);
void mlpcreatec2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t layerscount;
   ae_int_t lastproc;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(lsizes, 0, DT_INT);
   NewVector(ltypes, 0, DT_INT);
   NewVector(lconnfirst, 0, DT_INT);
   NewVector(lconnlast, 0, DT_INT);
   ae_assert(nout >= 2, "MLPCreateC2: NOut<2!");
   layerscount = 1 + 3 + 3 + 2 + 1;
// Allocate arrays
   ae_vector_set_length(&lsizes, layerscount);
   ae_vector_set_length(&ltypes, layerscount);
   ae_vector_set_length(&lconnfirst, layerscount);
   ae_vector_set_length(&lconnlast, layerscount);
// Layers
   mlpbase_addinputlayer(nin, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nhid2, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addactivationlayer(1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addbiasedsummatorlayer(nout - 1, &lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
   mlpbase_addzerolayer(&lsizes, &ltypes, &lconnfirst, &lconnlast, &lastproc);
// Create
   mlpbase_mlpcreate(nin, nout, &lsizes, &ltypes, &lconnfirst, &lconnlast, layerscount, true, network);
   mlpbase_fillhighlevelinformation(network, nin, nhid1, nhid2, nout, true, true);
   ae_frame_leave();
}

// Copying of neural network
//
// Inputs:
//     Network1 -   original
//
// Outputs:
//     Network2 -   copy
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpcopy(const multilayerperceptron &network1, multilayerperceptron &network2);
void mlpcopy(multilayerperceptron *network1, multilayerperceptron *network2) {
   SetObj(multilayerperceptron, network2);
   mlpcopyshared(network1, network2);
}

// Copying of neural network (second parameter is passed as shared object).
//
// Inputs:
//     Network1 -   original
//
// Outputs:
//     Network2 -   copy
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
void mlpcopyshared(multilayerperceptron *network1, multilayerperceptron *network2) {
   ae_frame _frame_block;
   ae_int_t wcount;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   NewObj(mlpbuffers, buf);
   NewObj(smlpgrad, sgrad);
// Copy scalar and array fields
   network2->hlnetworktype = network1->hlnetworktype;
   network2->hlnormtype = network1->hlnormtype;
   copyintegerarray(&network1->hllayersizes, &network2->hllayersizes);
   copyintegerarray(&network1->hlconnections, &network2->hlconnections);
   copyintegerarray(&network1->hlneurons, &network2->hlneurons);
   copyintegerarray(&network1->structinfo, &network2->structinfo);
   copyrealarray(&network1->weights, &network2->weights);
   copyrealarray(&network1->columnmeans, &network2->columnmeans);
   copyrealarray(&network1->columnsigmas, &network2->columnsigmas);
   copyrealarray(&network1->neurons, &network2->neurons);
   copyrealarray(&network1->dfdnet, &network2->dfdnet);
   copyrealarray(&network1->derror, &network2->derror);
   copyrealarray(&network1->x, &network2->x);
   copyrealarray(&network1->y, &network2->y);
   copyrealarray(&network1->nwbuf, &network2->nwbuf);
   copyintegerarray(&network1->integerbuf, &network2->integerbuf);
// copy buffers
   wcount = mlpgetweightscount(network1);
   ae_shared_pool_set_seed(&network2->buf, &buf, sizeof(buf), mlpbuffers_init, mlpbuffers_copy, mlpbuffers_free);
   ae_vector_set_length(&sgrad.g, wcount);
   sgrad.f = 0.0;
   for (i = 0; i < wcount; i++) {
      sgrad.g.xR[i] = 0.0;
   }
   ae_shared_pool_set_seed(&network2->gradbuf, &sgrad, sizeof(sgrad), smlpgrad_init, smlpgrad_copy, smlpgrad_free);
   ae_frame_leave();
}

// This function compares architectures of neural networks.  Only  geometries
// are compared, weights and other parameters are not tested.
// ALGLIB: Copyright 20.06.2013 by Sergey Bochkanov
bool mlpsamearchitecture(multilayerperceptron *network1, multilayerperceptron *network2) {
   ae_int_t i;
   ae_int_t ninfo;
   bool result;
   ae_assert(network1->structinfo.cnt > 0 && network1->structinfo.cnt >= network1->structinfo.xZ[0], "MLPSameArchitecture: Network1 is uninitialized");
   ae_assert(network2->structinfo.cnt > 0 && network2->structinfo.cnt >= network2->structinfo.xZ[0], "MLPSameArchitecture: Network2 is uninitialized");
   result = false;
   if (network1->structinfo.xZ[0] != network2->structinfo.xZ[0]) {
      return result;
   }
   ninfo = network1->structinfo.xZ[0];
   for (i = 0; i < ninfo; i++) {
      if (network1->structinfo.xZ[i] != network2->structinfo.xZ[i]) {
         return result;
      }
   }
   result = true;
   return result;
}

// This function copies tunable  parameters (weights/means/sigmas)  from  one
// network to another with same architecture. It  performs  some  rudimentary
// checks that architectures are same, and throws exception if check fails.
//
// It is intended for fast copying of states between two  network  which  are
// known to have same geometry.
//
// Inputs:
//     Network1 -   source, must be correctly initialized
//     Network2 -   target, must have same architecture
//
// Outputs:
//     Network2 -   network state is copied from source to target
// ALGLIB: Copyright 20.06.2013 by Sergey Bochkanov
// API: void mlpcopytunableparameters(const multilayerperceptron &network1, const multilayerperceptron &network2);
void mlpcopytunableparameters(multilayerperceptron *network1, multilayerperceptron *network2) {
   ae_int_t i;
   ae_int_t ninfo;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_assert(network1->structinfo.cnt > 0 && network1->structinfo.cnt >= network1->structinfo.xZ[0], "MLPCopyTunableParameters: Network1 is uninitialized");
   ae_assert(network2->structinfo.cnt > 0 && network2->structinfo.cnt >= network2->structinfo.xZ[0], "MLPCopyTunableParameters: Network2 is uninitialized");
   ae_assert(network1->structinfo.xZ[0] == network2->structinfo.xZ[0], "MLPCopyTunableParameters: Network1 geometry differs from that of Network2");
   ninfo = network1->structinfo.xZ[0];
   for (i = 0; i < ninfo; i++) {
      ae_assert(network1->structinfo.xZ[i] == network2->structinfo.xZ[i], "MLPCopyTunableParameters: Network1 geometry differs from that of Network2");
   }
   mlpproperties(network1, &nin, &nout, &wcount);
   for (i = 0; i < wcount; i++) {
      network2->weights.xR[i] = network1->weights.xR[i];
   }
   if (mlpissoftmax(network1)) {
      for (i = 0; i < nin; i++) {
         network2->columnmeans.xR[i] = network1->columnmeans.xR[i];
         network2->columnsigmas.xR[i] = network1->columnsigmas.xR[i];
      }
   } else {
      for (i = 0; i < nin + nout; i++) {
         network2->columnmeans.xR[i] = network1->columnmeans.xR[i];
         network2->columnsigmas.xR[i] = network1->columnsigmas.xR[i];
      }
   }
}

// This  function  exports  tunable   parameters  (weights/means/sigmas) from
// network to contiguous array. Nothing is guaranteed about array format, the
// only thing you can count for is that MLPImportTunableParameters() will  be
// able to parse it.
//
// It is intended for fast copying of states between network and backup array
//
// Inputs:
//     Network     -   source, must be correctly initialized
//     P           -   array to use. If its size is enough to store data,  it
//                     is reused.
//
// Outputs:
//     P           -   array which stores network parameters, resized if needed
//     PCount      -   number of parameters stored in array.
// ALGLIB: Copyright 20.06.2013 by Sergey Bochkanov
void mlpexporttunableparameters(multilayerperceptron *network, RVector *p, ae_int_t *pcount) {
   ae_int_t i;
   ae_int_t k;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   *pcount = 0;
   ae_assert(network->structinfo.cnt > 0 && network->structinfo.cnt >= network->structinfo.xZ[0], "MLPExportTunableParameters: Network is uninitialized");
   mlpproperties(network, &nin, &nout, &wcount);
   if (mlpissoftmax(network)) {
      *pcount = wcount + 2 * nin;
      vectorsetlengthatleast(p, *pcount);
      k = 0;
      for (i = 0; i < wcount; i++) {
         p->xR[k] = network->weights.xR[i];
         k++;
      }
      for (i = 0; i < nin; i++) {
         p->xR[k] = network->columnmeans.xR[i];
         k++;
         p->xR[k] = network->columnsigmas.xR[i];
         k++;
      }
   } else {
      *pcount = wcount + 2 * (nin + nout);
      vectorsetlengthatleast(p, *pcount);
      k = 0;
      for (i = 0; i < wcount; i++) {
         p->xR[k] = network->weights.xR[i];
         k++;
      }
      for (i = 0; i < nin + nout; i++) {
         p->xR[k] = network->columnmeans.xR[i];
         k++;
         p->xR[k] = network->columnsigmas.xR[i];
         k++;
      }
   }
}

// This  function imports  tunable   parameters  (weights/means/sigmas) which
// were exported by MLPExportTunableParameters().
//
// It is intended for fast copying of states between network and backup array
//
// Inputs:
//     Network     -   target:
//                     * must be correctly initialized
//                     * must have same geometry as network used to export params
//     P           -   array with parameters
// ALGLIB: Copyright 20.06.2013 by Sergey Bochkanov
void mlpimporttunableparameters(multilayerperceptron *network, RVector *p) {
   ae_int_t i;
   ae_int_t k;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_assert(network->structinfo.cnt > 0 && network->structinfo.cnt >= network->structinfo.xZ[0], "MLPImportTunableParameters: Network is uninitialized");
   mlpproperties(network, &nin, &nout, &wcount);
   if (mlpissoftmax(network)) {
      k = 0;
      for (i = 0; i < wcount; i++) {
         network->weights.xR[i] = p->xR[k];
         k++;
      }
      for (i = 0; i < nin; i++) {
         network->columnmeans.xR[i] = p->xR[k];
         k++;
         network->columnsigmas.xR[i] = p->xR[k];
         k++;
      }
   } else {
      k = 0;
      for (i = 0; i < wcount; i++) {
         network->weights.xR[i] = p->xR[k];
         k++;
      }
      for (i = 0; i < nin + nout; i++) {
         network->columnmeans.xR[i] = p->xR[k];
         k++;
         network->columnsigmas.xR[i] = p->xR[k];
         k++;
      }
   }
}

// Serialization of MultiLayerPerceptron structure
//
// Inputs:
//     Network -   original
//
// Outputs:
//     RA      -   array of real numbers which stores network,
//                 array[0..RLen-1]
//     RLen    -   RA lenght
// ALGLIB: Copyright 29.03.2008 by Sergey Bochkanov
void mlpserializeold(multilayerperceptron *network, RVector *ra, ae_int_t *rlen) {
   ae_int_t i;
   ae_int_t ssize;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t sigmalen;
   ae_int_t offs;
   SetVector(ra);
   *rlen = 0;
// Unload info
   ssize = network->structinfo.xZ[0];
   nin = network->structinfo.xZ[1];
   nout = network->structinfo.xZ[2];
   wcount = network->structinfo.xZ[4];
   if (mlpissoftmax(network)) {
      sigmalen = nin;
   } else {
      sigmalen = nin + nout;
   }
//  RA format:
//      LEN         DESRC.
//      1           RLen
//      1           version (MLPVNum)
//      1           StructInfo size
//      SSize       StructInfo
//      WCount      Weights
//      SigmaLen    ColumnMeans
//      SigmaLen    ColumnSigmas
   *rlen = 3 + ssize + wcount + 2 * sigmalen;
   ae_vector_set_length(ra, *rlen);
   ra->xR[0] = (double)(*rlen);
   ra->xR[1] = (double)mlpbase_mlpvnum;
   ra->xR[2] = (double)ssize;
   offs = 3;
   for (i = 0; i < ssize; i++) {
      ra->xR[offs + i] = (double)(network->structinfo.xZ[i]);
   }
   offs += ssize;
   ae_v_move(&ra->xR[offs], 1, network->weights.xR, 1, wcount);
   offs += wcount;
   ae_v_move(&ra->xR[offs], 1, network->columnmeans.xR, 1, sigmalen);
   offs += sigmalen;
   ae_v_move(&ra->xR[offs], 1, network->columnsigmas.xR, 1, sigmalen);
   offs += sigmalen;
}

// Unserialization of MultiLayerPerceptron structure
//
// Inputs:
//     RA      -   real array which stores network
//
// Outputs:
//     Network -   restored network
// ALGLIB: Copyright 29.03.2008 by Sergey Bochkanov
void mlpunserializeold(RVector *ra, multilayerperceptron *network) {
   ae_int_t i;
   ae_int_t ssize;
   ae_int_t ntotal;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t sigmalen;
   ae_int_t offs;
   SetObj(multilayerperceptron, network);
   ae_assert(RoundZ(ra->xR[1]) == mlpbase_mlpvnum, "MLPUnserialize: incorrect array!");
// Unload StructInfo from IA
   offs = 3;
   ssize = RoundZ(ra->xR[2]);
   ae_vector_set_length(&network->structinfo, ssize);
   for (i = 0; i < ssize; i++) {
      network->structinfo.xZ[i] = RoundZ(ra->xR[offs + i]);
   }
   offs += ssize;
// Unload info from StructInfo
   ssize = network->structinfo.xZ[0];
   nin = network->structinfo.xZ[1];
   nout = network->structinfo.xZ[2];
   ntotal = network->structinfo.xZ[3];
   wcount = network->structinfo.xZ[4];
   if (network->structinfo.xZ[6] == 0) {
      sigmalen = nin + nout;
   } else {
      sigmalen = nin;
   }
// Allocate space for other fields
   ae_vector_set_length(&network->weights, wcount);
   ae_vector_set_length(&network->columnmeans, sigmalen);
   ae_vector_set_length(&network->columnsigmas, sigmalen);
   ae_vector_set_length(&network->neurons, ntotal);
   ae_vector_set_length(&network->nwbuf, imax2(wcount, 2 * nout));
   ae_vector_set_length(&network->dfdnet, ntotal);
   ae_vector_set_length(&network->x, nin);
   ae_vector_set_length(&network->y, nout);
   ae_vector_set_length(&network->derror, ntotal);
// Copy parameters from RA
   ae_v_move(network->weights.xR, 1, &ra->xR[offs], 1, wcount);
   offs += wcount;
   ae_v_move(network->columnmeans.xR, 1, &ra->xR[offs], 1, sigmalen);
   offs += sigmalen;
   ae_v_move(network->columnsigmas.xR, 1, &ra->xR[offs], 1, sigmalen);
   offs += sigmalen;
}

// This function performs backward pass of neural network randimization:
// * it assumes that Network.Weights stores standard deviation of weights
//   (weights are not generated yet, only their deviations are present)
// * it sets deviations of weights which feed NeuronIdx-th neuron to specified value
// * it recursively passes to deeper neuron and modifies their weights
// * it stops after encountering nonlinear neurons, linear activation function,
//   input neurons, "0" and "-1" neurons
// ALGLIB: Copyright 27.06.2013 by Sergey Bochkanov
static void mlpbase_randomizebackwardpass(multilayerperceptron *network, ae_int_t neuronidx, double v) {
   ae_int_t istart;
   ae_int_t neurontype;
   ae_int_t n1;
   ae_int_t n2;
   ae_int_t w1;
   ae_int_t w2;
   ae_int_t offs;
   ae_int_t i;
   istart = network->structinfo.xZ[5];
   neurontype = network->structinfo.xZ[istart + neuronidx * mlpbase_nfieldwidth];
   if (neurontype == -2) {
   // Input neuron - stop
      return;
   }
   if (neurontype == -3) {
   // "-1" neuron: stop
      return;
   }
   if (neurontype == -4) {
   // "0" neuron: stop
      return;
   }
   if (neurontype == 0) {
   // Adaptive summator neuron:
   // * modify deviations of its weights
   // * recursively call this function for its inputs
      offs = istart + neuronidx * mlpbase_nfieldwidth;
      n1 = network->structinfo.xZ[offs + 2];
      n2 = n1 + network->structinfo.xZ[offs + 1] - 1;
      w1 = network->structinfo.xZ[offs + 3];
      w2 = w1 + network->structinfo.xZ[offs + 1] - 1;
      for (i = w1; i <= w2; i++) {
         network->weights.xR[i] = v;
      }
      for (i = n1; i <= n2; i++) {
         mlpbase_randomizebackwardpass(network, i, v);
      }
      return;
   }
   if (neurontype == -5) {
   // Linear activation function: stop
      return;
   }
   if (neurontype > 0) {
   // Nonlinear activation function: stop
      return;
   }
   ae_assert(false, "RandomizeBackwardPass: unexpected neuron type");
}

// Randomization of neural network weights
// ALGLIB: Copyright 06.11.2007 by Sergey Bochkanov
// API: void mlprandomize(const multilayerperceptron &network);
void mlprandomize(multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntotal;
   ae_int_t istart;
   ae_int_t entrysize;
   ae_int_t entryoffs;
   ae_int_t neuronidx;
   ae_int_t neurontype;
   double vmean;
   double vvar;
   ae_int_t i;
   ae_int_t n1;
   ae_int_t n2;
   double desiredsigma;
   ae_int_t montecarlocnt;
   double ef;
   double ef2;
   double v;
   double wscale;
   ae_frame_make(&_frame_block);
   NewObj(hqrndstate, r);
   hqrndrandomize(&r);
   mlpproperties(network, &nin, &nout, &wcount);
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
   desiredsigma = 0.5;
   montecarlocnt = 20;
// Stage 1:
// * Network.Weights is filled by standard deviation of weights
// * default values: sigma=1
   for (i = 0; i < wcount; i++) {
      network->weights.xR[i] = 1.0;
   }
// Stage 2:
// * assume that input neurons have zero mean and unit standard deviation
// * assume that constant neurons have zero standard deviation
// * perform forward pass along neurons
// * for each non-input non-constant neuron:
//   * calculate mean and standard deviation of neuron's output
//     assuming that we know means/deviations of neurons which feed it
//     and assuming that weights has unit variance and zero mean.
// * for each nonlinear neuron additionally we perform backward pass:
//   * scale variances of weights which feed it in such way that neuron's
//     input has unit standard deviation
//
// NOTE: this algorithm assumes that each connection feeds at most one
//       non-linear neuron. This assumption can be incorrect in upcoming
//       architectures with strong neurons. However, algorithm should
//       work smoothly even in this case.
//
// During this stage we use Network.RndBuf, which is grouped into NTotal
// entries, each of them having following format:
//
// Buf[Offset+0]        mean value of neuron's output
// Buf[Offset+1]        standard deviation of neuron's output
//
   entrysize = 2;
   vectorsetlengthatleast(&network->rndbuf, entrysize * ntotal);
   for (neuronidx = 0; neuronidx < ntotal; neuronidx++) {
      neurontype = network->structinfo.xZ[istart + neuronidx * mlpbase_nfieldwidth];
      entryoffs = entrysize * neuronidx;
      if (neurontype == -2) {
      // Input neuron: zero mean, unit variance.
         network->rndbuf.xR[entryoffs] = 0.0;
         network->rndbuf.xR[entryoffs + 1] = 1.0;
         continue;
      }
      if (neurontype == -3) {
      // "-1" neuron: mean=-1, zero variance.
         network->rndbuf.xR[entryoffs] = -1.0;
         network->rndbuf.xR[entryoffs + 1] = 0.0;
         continue;
      }
      if (neurontype == -4) {
      // "0" neuron: mean=0, zero variance.
         network->rndbuf.xR[entryoffs] = 0.0;
         network->rndbuf.xR[entryoffs + 1] = 0.0;
         continue;
      }
      if (neurontype == 0) {
      // Adaptive summator neuron:
      // * calculate its mean and variance.
      // * we assume that weights of this neuron have unit variance and zero mean.
      // * thus, neuron's output is always have zero mean
      // * as for variance, it is a bit more interesting:
      //   * let n[i] is i-th input neuron
      //   * let w[i] is i-th weight
      //   * we assume that n[i] and w[i] are independently distributed
      //   * Var(n0*w0+n1*w1+...) = Var(n0*w0)+Var(n1*w1)+...
      //   * Var(X*Y) = mean(X)^2*Var(Y) + mean(Y)^2*Var(X) + Var(X)*Var(Y)
      //   * mean(w[i])=0, var(w[i])=1
      //   * Var(n[i]*w[i]) = mean(n[i])^2 + Var(n[i])
         n1 = network->structinfo.xZ[istart + neuronidx * mlpbase_nfieldwidth + 2];
         n2 = n1 + network->structinfo.xZ[istart + neuronidx * mlpbase_nfieldwidth + 1] - 1;
         vmean = 0.0;
         vvar = 0.0;
         for (i = n1; i <= n2; i++) {
            vvar += ae_sqr(network->rndbuf.xR[entrysize * i]) + ae_sqr(network->rndbuf.xR[entrysize * i + 1]);
         }
         network->rndbuf.xR[entryoffs] = vmean;
         network->rndbuf.xR[entryoffs + 1] = sqrt(vvar);
         continue;
      }
      if (neurontype == -5) {
      // Linear activation function
         i = network->structinfo.xZ[istart + neuronidx * mlpbase_nfieldwidth + 2];
         vmean = network->rndbuf.xR[entrysize * i];
         vvar = ae_sqr(network->rndbuf.xR[entrysize * i + 1]);
         if (vvar > 0.0) {
            wscale = desiredsigma / sqrt(vvar);
         } else {
            wscale = 1.0;
         }
         mlpbase_randomizebackwardpass(network, i, wscale);
         network->rndbuf.xR[entryoffs] = vmean * wscale;
         network->rndbuf.xR[entryoffs + 1] = desiredsigma;
         continue;
      }
      if (neurontype > 0) {
      // Nonlinear activation function:
      // * scale its inputs
      // * estimate mean/sigma of its output using Monte-Carlo method
      //   (we simulate different inputs with unit deviation and
      //   sample activation function output on such inputs)
         i = network->structinfo.xZ[istart + neuronidx * mlpbase_nfieldwidth + 2];
         vmean = network->rndbuf.xR[entrysize * i];
         vvar = ae_sqr(network->rndbuf.xR[entrysize * i + 1]);
         if (vvar > 0.0) {
            wscale = desiredsigma / sqrt(vvar);
         } else {
            wscale = 1.0;
         }
         mlpbase_randomizebackwardpass(network, i, wscale);
         ef = 0.0;
         ef2 = 0.0;
         vmean *= wscale;
         for (i = 0; i < montecarlocnt; i++) {
            v = vmean + desiredsigma * hqrndnormal(&r);
            ef += v;
            ef2 += v * v;
         }
         ef /= montecarlocnt;
         ef2 /= montecarlocnt;
         network->rndbuf.xR[entryoffs] = ef;
         network->rndbuf.xR[entryoffs + 1] = rmax2(ef2 - ef * ef, 0.0);
         continue;
      }
      ae_assert(false, "MLPRandomize: unexpected neuron type");
   }
// Stage 3: generate weights.
   for (i = 0; i < wcount; i++) {
      network->weights.xR[i] *= hqrndnormal(&r);
   }
   ae_frame_leave();
}

// Randomization of neural network weights and standartisator
// ALGLIB: Copyright 10.03.2008 by Sergey Bochkanov
// API: void mlprandomizefull(const multilayerperceptron &network);
void mlprandomizefull(multilayerperceptron *network) {
   ae_int_t i;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntotal;
   ae_int_t istart;
   ae_int_t offs;
   ae_int_t ntype;
   mlpproperties(network, &nin, &nout, &wcount);
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
// Process network
   mlprandomize(network);
   for (i = 0; i < nin; i++) {
      network->columnmeans.xR[i] = ae_randomreal() - 0.5;
      network->columnsigmas.xR[i] = ae_randomreal() + 0.5;
   }
   if (!mlpissoftmax(network)) {
      for (i = 0; i < nout; i++) {
         offs = istart + (ntotal - nout + i) * mlpbase_nfieldwidth;
         ntype = network->structinfo.xZ[offs];
         if (ntype == 0) {
         // Shifts are changed only for linear outputs neurons
            network->columnmeans.xR[nin + i] = ae_randommid();
         }
         if (ntype == 0 || ntype == 3) {
         // Scales are changed only for linear or bounded outputs neurons.
         // Note that scale randomization preserves sign.
            network->columnsigmas.xR[nin + i] = ae_sign(network->columnsigmas.xR[nin + i]) * (1.5 * ae_randomreal() + 0.5);
         }
      }
   }
}

// Internal subroutine.
// ALGLIB: Copyright 30.03.2008 by Sergey Bochkanov
// API: void mlpinitpreprocessor(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize);
void mlpinitpreprocessor(multilayerperceptron *network, RMatrix *xy, ae_int_t ssize) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t jmax;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntotal;
   ae_int_t istart;
   ae_int_t offs;
   ae_int_t ntype;
   double s;
   ae_frame_make(&_frame_block);
   NewVector(means, 0, DT_REAL);
   NewVector(sigmas, 0, DT_REAL);
   mlpproperties(network, &nin, &nout, &wcount);
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
// Means/Sigmas
   if (mlpissoftmax(network)) {
      jmax = nin - 1;
   } else {
      jmax = nin + nout - 1;
   }
   ae_vector_set_length(&means, jmax + 1);
   ae_vector_set_length(&sigmas, jmax + 1);
   for (i = 0; i <= jmax; i++) {
      means.xR[i] = 0.0;
      sigmas.xR[i] = 0.0;
   }
   for (i = 0; i < ssize; i++) {
      for (j = 0; j <= jmax; j++) {
         means.xR[j] += xy->xyR[i][j];
      }
   }
   for (i = 0; i <= jmax; i++) {
      means.xR[i] /= ssize;
   }
   for (i = 0; i < ssize; i++) {
      for (j = 0; j <= jmax; j++) {
         sigmas.xR[j] += ae_sqr(xy->xyR[i][j] - means.xR[j]);
      }
   }
   for (i = 0; i <= jmax; i++) {
      sigmas.xR[i] = sqrt(sigmas.xR[i] / ssize);
   }
// Inputs
   for (i = 0; i < nin; i++) {
      network->columnmeans.xR[i] = means.xR[i];
      network->columnsigmas.xR[i] = sigmas.xR[i];
      if (network->columnsigmas.xR[i] == 0.0) {
         network->columnsigmas.xR[i] = 1.0;
      }
   }
// Outputs
   if (!mlpissoftmax(network)) {
      for (i = 0; i < nout; i++) {
         offs = istart + (ntotal - nout + i) * mlpbase_nfieldwidth;
         ntype = network->structinfo.xZ[offs];
      // Linear outputs
         if (ntype == 0) {
            network->columnmeans.xR[nin + i] = means.xR[nin + i];
            network->columnsigmas.xR[nin + i] = sigmas.xR[nin + i];
            if (network->columnsigmas.xR[nin + i] == 0.0) {
               network->columnsigmas.xR[nin + i] = 1.0;
            }
         }
      // Bounded outputs (half-interval)
         if (ntype == 3) {
            s = means.xR[nin + i] - network->columnmeans.xR[nin + i];
            if (s == 0.0) {
               s = (double)(ae_sign(network->columnsigmas.xR[nin + i]));
            }
            if (s == 0.0) {
               s = 1.0;
            }
            network->columnsigmas.xR[nin + i] = ae_sign(network->columnsigmas.xR[nin + i]) * fabs(s);
            if (network->columnsigmas.xR[nin + i] == 0.0) {
               network->columnsigmas.xR[nin + i] = 1.0;
            }
         }
      }
   }
   ae_frame_leave();
}

// Internal subroutine.
// Initialization for preprocessor based on a sample.
//
// Inputs:
//     Network -   initialized neural network;
//     XY      -   sample, given by sparse matrix;
//     SSize   -   sample size.
//
// Outputs:
//     Network -   neural network with initialised preprocessor.
// ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
void mlpinitpreprocessorsparse(multilayerperceptron *network, sparsematrix *xy, ae_int_t ssize) {
   ae_frame _frame_block;
   ae_int_t jmax;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntotal;
   ae_int_t istart;
   ae_int_t offs;
   ae_int_t ntype;
   double s;
   ae_int_t i;
   ae_int_t j;
   ae_frame_make(&_frame_block);
   NewVector(means, 0, DT_REAL);
   NewVector(sigmas, 0, DT_REAL);
   mlpproperties(network, &nin, &nout, &wcount);
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
// Means/Sigmas
   if (mlpissoftmax(network)) {
      jmax = nin - 1;
   } else {
      jmax = nin + nout - 1;
   }
   ae_vector_set_length(&means, jmax + 1);
   ae_vector_set_length(&sigmas, jmax + 1);
   for (i = 0; i <= jmax; i++) {
      means.xR[i] = 0.0;
      sigmas.xR[i] = 0.0;
   }
   for (i = 0; i < ssize; i++) {
      sparsegetrow(xy, i, &network->xyrow);
      for (j = 0; j <= jmax; j++) {
         means.xR[j] += network->xyrow.xR[j];
      }
   }
   for (i = 0; i <= jmax; i++) {
      means.xR[i] /= ssize;
   }
   for (i = 0; i < ssize; i++) {
      sparsegetrow(xy, i, &network->xyrow);
      for (j = 0; j <= jmax; j++) {
         sigmas.xR[j] += ae_sqr(network->xyrow.xR[j] - means.xR[j]);
      }
   }
   for (i = 0; i <= jmax; i++) {
      sigmas.xR[i] = sqrt(sigmas.xR[i] / ssize);
   }
// Inputs
   for (i = 0; i < nin; i++) {
      network->columnmeans.xR[i] = means.xR[i];
      network->columnsigmas.xR[i] = sigmas.xR[i];
      if (network->columnsigmas.xR[i] == 0.0) {
         network->columnsigmas.xR[i] = 1.0;
      }
   }
// Outputs
   if (!mlpissoftmax(network)) {
      for (i = 0; i < nout; i++) {
         offs = istart + (ntotal - nout + i) * mlpbase_nfieldwidth;
         ntype = network->structinfo.xZ[offs];
      // Linear outputs
         if (ntype == 0) {
            network->columnmeans.xR[nin + i] = means.xR[nin + i];
            network->columnsigmas.xR[nin + i] = sigmas.xR[nin + i];
            if (network->columnsigmas.xR[nin + i] == 0.0) {
               network->columnsigmas.xR[nin + i] = 1.0;
            }
         }
      // Bounded outputs (half-interval)
         if (ntype == 3) {
            s = means.xR[nin + i] - network->columnmeans.xR[nin + i];
            if (s == 0.0) {
               s = (double)(ae_sign(network->columnsigmas.xR[nin + i]));
            }
            if (s == 0.0) {
               s = 1.0;
            }
            network->columnsigmas.xR[nin + i] = ae_sign(network->columnsigmas.xR[nin + i]) * fabs(s);
            if (network->columnsigmas.xR[nin + i] == 0.0) {
               network->columnsigmas.xR[nin + i] = 1.0;
            }
         }
      }
   }
   ae_frame_leave();
}

// Internal subroutine.
// Initialization for preprocessor based on a subsample.
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     XY      -   original dataset; one sample = one row;
//                 first NIn columns contain inputs,
//                 next NOut columns - desired outputs.
//     SetSize -   real size of XY, SetSize >= 0;
//     Idx     -   subset of SubsetSize elements, array[SubsetSize]:
//                 * Idx[I] stores row index in the original dataset which is
//                   given by XY. Gradient is calculated with respect to rows
//                   whose indexes are stored in Idx[].
//                 * Idx[]  must store correct indexes; this function  throws
//                   an  exception  in  case  incorrect index (less than 0 or
//                   larger than rows(XY)) is given
//                 * Idx[]  may  store  indexes  in  any  order and even with
//                   repetitions.
//     SubsetSize- number of elements in Idx[] array.
//
// Outputs:
//     Network -   neural network with initialised preprocessor.
//
// NOTE: when  SubsetSize < 0 is used full dataset by call MLPInitPreprocessor
//       function.
// ALGLIB: Copyright 23.08.2012 by Sergey Bochkanov
void mlpinitpreprocessorsubset(multilayerperceptron *network, RMatrix *xy, ae_int_t setsize, ZVector *idx, ae_int_t subsetsize) {
   ae_frame _frame_block;
   ae_int_t jmax;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntotal;
   ae_int_t istart;
   ae_int_t offs;
   ae_int_t ntype;
   double s;
   ae_int_t npoints;
   ae_int_t i;
   ae_int_t j;
   ae_frame_make(&_frame_block);
   NewVector(means, 0, DT_REAL);
   NewVector(sigmas, 0, DT_REAL);
   ae_assert(setsize >= 0, "MLPInitPreprocessorSubset: SetSize<0");
   if (subsetsize < 0) {
      mlpinitpreprocessor(network, xy, setsize);
      ae_frame_leave();
      return;
   }
   ae_assert(subsetsize <= idx->cnt, "MLPInitPreprocessorSubset: SubsetSize>Length(Idx)");
   npoints = setsize;
   for (i = 0; i < subsetsize; i++) {
      ae_assert(idx->xZ[i] >= 0, "MLPInitPreprocessorSubset: incorrect index of XY row(Idx[I] < 0)");
      ae_assert(idx->xZ[i] < npoints, "MLPInitPreprocessorSubset: incorrect index of XY row(Idx[I]>Rows(XY)-1)");
   }
   mlpproperties(network, &nin, &nout, &wcount);
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
// Means/Sigmas
   if (mlpissoftmax(network)) {
      jmax = nin - 1;
   } else {
      jmax = nin + nout - 1;
   }
   ae_vector_set_length(&means, jmax + 1);
   ae_vector_set_length(&sigmas, jmax + 1);
   for (i = 0; i <= jmax; i++) {
      means.xR[i] = 0.0;
      sigmas.xR[i] = 0.0;
   }
   for (i = 0; i < subsetsize; i++) {
      for (j = 0; j <= jmax; j++) {
         means.xR[j] += xy->xyR[idx->xZ[i]][j];
      }
   }
   for (i = 0; i <= jmax; i++) {
      means.xR[i] /= subsetsize;
   }
   for (i = 0; i < subsetsize; i++) {
      for (j = 0; j <= jmax; j++) {
         sigmas.xR[j] += ae_sqr(xy->xyR[idx->xZ[i]][j] - means.xR[j]);
      }
   }
   for (i = 0; i <= jmax; i++) {
      sigmas.xR[i] = sqrt(sigmas.xR[i] / subsetsize);
   }
// Inputs
   for (i = 0; i < nin; i++) {
      network->columnmeans.xR[i] = means.xR[i];
      network->columnsigmas.xR[i] = sigmas.xR[i];
      if (network->columnsigmas.xR[i] == 0.0) {
         network->columnsigmas.xR[i] = 1.0;
      }
   }
// Outputs
   if (!mlpissoftmax(network)) {
      for (i = 0; i < nout; i++) {
         offs = istart + (ntotal - nout + i) * mlpbase_nfieldwidth;
         ntype = network->structinfo.xZ[offs];
      // Linear outputs
         if (ntype == 0) {
            network->columnmeans.xR[nin + i] = means.xR[nin + i];
            network->columnsigmas.xR[nin + i] = sigmas.xR[nin + i];
            if (network->columnsigmas.xR[nin + i] == 0.0) {
               network->columnsigmas.xR[nin + i] = 1.0;
            }
         }
      // Bounded outputs (half-interval)
         if (ntype == 3) {
            s = means.xR[nin + i] - network->columnmeans.xR[nin + i];
            if (s == 0.0) {
               s = (double)(ae_sign(network->columnsigmas.xR[nin + i]));
            }
            if (s == 0.0) {
               s = 1.0;
            }
            network->columnsigmas.xR[nin + i] = ae_sign(network->columnsigmas.xR[nin + i]) * fabs(s);
            if (network->columnsigmas.xR[nin + i] == 0.0) {
               network->columnsigmas.xR[nin + i] = 1.0;
            }
         }
      }
   }
   ae_frame_leave();
}

// Internal subroutine.
// Initialization for preprocessor based on a subsample.
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     XY      -   original dataset, given by sparse matrix;
//                 one sample = one row;
//                 first NIn columns contain inputs,
//                 next NOut columns - desired outputs.
//     SetSize -   real size of XY, SetSize >= 0;
//     Idx     -   subset of SubsetSize elements, array[SubsetSize]:
//                 * Idx[I] stores row index in the original dataset which is
//                   given by XY. Gradient is calculated with respect to rows
//                   whose indexes are stored in Idx[].
//                 * Idx[]  must store correct indexes; this function  throws
//                   an  exception  in  case  incorrect index (less than 0 or
//                   larger than rows(XY)) is given
//                 * Idx[]  may  store  indexes  in  any  order and even with
//                   repetitions.
//     SubsetSize- number of elements in Idx[] array.
//
// Outputs:
//     Network -   neural network with initialised preprocessor.
//
// NOTE: when SubsetSize < 0 is used full dataset by call
//       MLPInitPreprocessorSparse function.
// ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
void mlpinitpreprocessorsparsesubset(multilayerperceptron *network, sparsematrix *xy, ae_int_t setsize, ZVector *idx, ae_int_t subsetsize) {
   ae_frame _frame_block;
   ae_int_t jmax;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntotal;
   ae_int_t istart;
   ae_int_t offs;
   ae_int_t ntype;
   double s;
   ae_int_t npoints;
   ae_int_t i;
   ae_int_t j;
   ae_frame_make(&_frame_block);
   NewVector(means, 0, DT_REAL);
   NewVector(sigmas, 0, DT_REAL);
   ae_assert(setsize >= 0, "MLPInitPreprocessorSparseSubset: SetSize<0");
   if (subsetsize < 0) {
      mlpinitpreprocessorsparse(network, xy, setsize);
      ae_frame_leave();
      return;
   }
   ae_assert(subsetsize <= idx->cnt, "MLPInitPreprocessorSparseSubset: SubsetSize>Length(Idx)");
   npoints = setsize;
   for (i = 0; i < subsetsize; i++) {
      ae_assert(idx->xZ[i] >= 0, "MLPInitPreprocessorSparseSubset: incorrect index of XY row(Idx[I] < 0)");
      ae_assert(idx->xZ[i] < npoints, "MLPInitPreprocessorSparseSubset: incorrect index of XY row(Idx[I]>Rows(XY)-1)");
   }
   mlpproperties(network, &nin, &nout, &wcount);
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
// Means/Sigmas
   if (mlpissoftmax(network)) {
      jmax = nin - 1;
   } else {
      jmax = nin + nout - 1;
   }
   ae_vector_set_length(&means, jmax + 1);
   ae_vector_set_length(&sigmas, jmax + 1);
   for (i = 0; i <= jmax; i++) {
      means.xR[i] = 0.0;
      sigmas.xR[i] = 0.0;
   }
   for (i = 0; i < subsetsize; i++) {
      sparsegetrow(xy, idx->xZ[i], &network->xyrow);
      for (j = 0; j <= jmax; j++) {
         means.xR[j] += network->xyrow.xR[j];
      }
   }
   for (i = 0; i <= jmax; i++) {
      means.xR[i] /= subsetsize;
   }
   for (i = 0; i < subsetsize; i++) {
      sparsegetrow(xy, idx->xZ[i], &network->xyrow);
      for (j = 0; j <= jmax; j++) {
         sigmas.xR[j] += ae_sqr(network->xyrow.xR[j] - means.xR[j]);
      }
   }
   for (i = 0; i <= jmax; i++) {
      sigmas.xR[i] = sqrt(sigmas.xR[i] / subsetsize);
   }
// Inputs
   for (i = 0; i < nin; i++) {
      network->columnmeans.xR[i] = means.xR[i];
      network->columnsigmas.xR[i] = sigmas.xR[i];
      if (network->columnsigmas.xR[i] == 0.0) {
         network->columnsigmas.xR[i] = 1.0;
      }
   }
// Outputs
   if (!mlpissoftmax(network)) {
      for (i = 0; i < nout; i++) {
         offs = istart + (ntotal - nout + i) * mlpbase_nfieldwidth;
         ntype = network->structinfo.xZ[offs];
      // Linear outputs
         if (ntype == 0) {
            network->columnmeans.xR[nin + i] = means.xR[nin + i];
            network->columnsigmas.xR[nin + i] = sigmas.xR[nin + i];
            if (network->columnsigmas.xR[nin + i] == 0.0) {
               network->columnsigmas.xR[nin + i] = 1.0;
            }
         }
      // Bounded outputs (half-interval)
         if (ntype == 3) {
            s = means.xR[nin + i] - network->columnmeans.xR[nin + i];
            if (s == 0.0) {
               s = (double)(ae_sign(network->columnsigmas.xR[nin + i]));
            }
            if (s == 0.0) {
               s = 1.0;
            }
            network->columnsigmas.xR[nin + i] = ae_sign(network->columnsigmas.xR[nin + i]) * fabs(s);
            if (network->columnsigmas.xR[nin + i] == 0.0) {
               network->columnsigmas.xR[nin + i] = 1.0;
            }
         }
      }
   }
   ae_frame_leave();
}

// Returns information about initialized network: number of inputs, outputs,
// weights.
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpproperties(const multilayerperceptron &network, ae_int_t &nin, ae_int_t &nout, ae_int_t &wcount);
void mlpproperties(multilayerperceptron *network, ae_int_t *nin, ae_int_t *nout, ae_int_t *wcount) {
   *nin = 0;
   *nout = 0;
   *wcount = 0;
   *nin = network->structinfo.xZ[1];
   *nout = network->structinfo.xZ[2];
   *wcount = network->structinfo.xZ[4];
}

// Returns number of "internal", low-level neurons in the network (one  which
// is stored in StructInfo).
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
ae_int_t mlpntotal(multilayerperceptron *network) {
   ae_int_t result;
   result = network->structinfo.xZ[3];
   return result;
}

// Returns number of inputs.
// ALGLIB: Copyright 19.10.2011 by Sergey Bochkanov
// API: ae_int_t mlpgetinputscount(const multilayerperceptron &network);
ae_int_t mlpgetinputscount(multilayerperceptron *network) {
   ae_int_t result;
   result = network->structinfo.xZ[1];
   return result;
}

// Returns number of outputs.
// ALGLIB: Copyright 19.10.2011 by Sergey Bochkanov
// API: ae_int_t mlpgetoutputscount(const multilayerperceptron &network);
ae_int_t mlpgetoutputscount(multilayerperceptron *network) {
   ae_int_t result;
   result = network->structinfo.xZ[2];
   return result;
}

// Returns number of weights.
// ALGLIB: Copyright 19.10.2011 by Sergey Bochkanov
// API: ae_int_t mlpgetweightscount(const multilayerperceptron &network);
ae_int_t mlpgetweightscount(multilayerperceptron *network) {
   ae_int_t result;
   result = network->structinfo.xZ[4];
   return result;
}

// Tells whether network is SOFTMAX-normalized (i.e. classifier) or not.
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: bool mlpissoftmax(const multilayerperceptron &network);
bool mlpissoftmax(multilayerperceptron *network) {
   bool result;
   result = network->structinfo.xZ[6] == 1;
   return result;
}

// This function returns total number of layers (including input, hidden and
// output layers).
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: ae_int_t mlpgetlayerscount(const multilayerperceptron &network);
ae_int_t mlpgetlayerscount(multilayerperceptron *network) {
   ae_int_t result;
   result = network->hllayersizes.cnt;
   return result;
}

// This function returns size of K-th layer.
//
// K=0 corresponds to input layer, K=CNT-1 corresponds to output layer.
//
// Size of the output layer is always equal to the number of outputs, although
// when we have softmax-normalized network, last neuron doesn't have any
// connections - it is just zero.
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: ae_int_t mlpgetlayersize(const multilayerperceptron &network, const ae_int_t k);
ae_int_t mlpgetlayersize(multilayerperceptron *network, ae_int_t k) {
   ae_int_t result;
   ae_assert(k >= 0 && k < network->hllayersizes.cnt, "MLPGetLayerSize: incorrect layer index");
   result = network->hllayersizes.xZ[k];
   return result;
}

// This function returns offset/scaling coefficients for I-th input of the
// network.
//
// Inputs:
//     Network     -   network
//     I           -   input index
//
// Outputs:
//     Mean        -   mean term
//     Sigma       -   sigma term, guaranteed to be nonzero.
//
// I-th input is passed through linear transformation
//     IN[i] = (IN[i]-Mean)/Sigma
// before feeding to the network
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: void mlpgetinputscaling(const multilayerperceptron &network, const ae_int_t i, double &mean, double &sigma);
void mlpgetinputscaling(multilayerperceptron *network, ae_int_t i, double *mean, double *sigma) {
   *mean = 0;
   *sigma = 0;
   ae_assert(i >= 0 && i < network->hllayersizes.xZ[0], "MLPGetInputScaling: incorrect (nonexistent) I");
   *mean = network->columnmeans.xR[i];
   *sigma = network->columnsigmas.xR[i];
   if (*sigma == 0.0) {
      *sigma = 1.0;
   }
}

// This function returns offset/scaling coefficients for I-th output of the
// network.
//
// Inputs:
//     Network     -   network
//     I           -   input index
//
// Outputs:
//     Mean        -   mean term
//     Sigma       -   sigma term, guaranteed to be nonzero.
//
// I-th output is passed through linear transformation
//     OUT[i] = OUT[i]*Sigma+Mean
// before returning it to user. In case we have SOFTMAX-normalized network,
// we return (Mean,Sigma)=(0.0,1.0).
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: void mlpgetoutputscaling(const multilayerperceptron &network, const ae_int_t i, double &mean, double &sigma);
void mlpgetoutputscaling(multilayerperceptron *network, ae_int_t i, double *mean, double *sigma) {
   *mean = 0;
   *sigma = 0;
   ae_assert(i >= 0 && i < network->hllayersizes.xZ[network->hllayersizes.cnt - 1], "MLPGetOutputScaling: incorrect (nonexistent) I");
   if (network->structinfo.xZ[6] == 1) {
      *mean = 0.0;
      *sigma = 1.0;
   } else {
      *mean = network->columnmeans.xR[network->hllayersizes.xZ[0] + i];
      *sigma = network->columnsigmas.xR[network->hllayersizes.xZ[0] + i];
   }
}

// This function returns information about Ith neuron of Kth layer
//
// Inputs:
//     Network     -   network
//     K           -   layer index
//     I           -   neuron index (within layer)
//
// Outputs:
//     FKind       -   activation function type (used by MLPActivationFunction())
//                     this value is zero for input or linear neurons
//     Threshold   -   also called offset, bias
//                     zero for input neurons
//
// NOTE: this function throws exception if layer or neuron with  given  index
// do not exists.
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: void mlpgetneuroninfo(const multilayerperceptron &network, const ae_int_t k, const ae_int_t i, ae_int_t &fkind, double &threshold);
void mlpgetneuroninfo(multilayerperceptron *network, ae_int_t k, ae_int_t i, ae_int_t *fkind, double *threshold) {
   ae_int_t ncnt;
   ae_int_t istart;
   ae_int_t highlevelidx;
   ae_int_t activationoffset;
   *fkind = 0;
   *threshold = 0;
   ncnt = network->hlneurons.cnt / mlpbase_hlnfieldwidth;
   istart = network->structinfo.xZ[5];
// search
   network->integerbuf.xZ[0] = k;
   network->integerbuf.xZ[1] = i;
   highlevelidx = recsearch(&network->hlneurons, mlpbase_hlnfieldwidth, 2, 0, ncnt, &network->integerbuf);
   ae_assert(highlevelidx >= 0, "MLPGetNeuronInfo: incorrect (nonexistent) layer or neuron index");
// 1. find offset of the activation function record in the
   if (network->hlneurons.xZ[highlevelidx * mlpbase_hlnfieldwidth + 2] >= 0) {
      activationoffset = istart + network->hlneurons.xZ[highlevelidx * mlpbase_hlnfieldwidth + 2] * mlpbase_nfieldwidth;
      *fkind = network->structinfo.xZ[activationoffset];
   } else {
      *fkind = 0;
   }
   if (network->hlneurons.xZ[highlevelidx * mlpbase_hlnfieldwidth + 3] >= 0) {
      *threshold = network->weights.xR[network->hlneurons.xZ[highlevelidx * mlpbase_hlnfieldwidth + 3]];
   } else {
      *threshold = 0.0;
   }
}

// This function returns information about connection from I0-th neuron of
// K0-th layer to I1-th neuron of K1-th layer.
//
// Inputs:
//     Network     -   network
//     K0          -   layer index
//     I0          -   neuron index (within layer)
//     K1          -   layer index
//     I1          -   neuron index (within layer)
//
// Result:
//     connection weight (zero for non-existent connections)
//
// This function:
// 1. throws exception if layer or neuron with given index do not exists.
// 2. returns zero if neurons exist, but there is no connection between them
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: double mlpgetweight(const multilayerperceptron &network, const ae_int_t k0, const ae_int_t i0, const ae_int_t k1, const ae_int_t i1);
double mlpgetweight(multilayerperceptron *network, ae_int_t k0, ae_int_t i0, ae_int_t k1, ae_int_t i1) {
   ae_int_t ccnt;
   ae_int_t highlevelidx;
   double result;
   ccnt = network->hlconnections.cnt / mlpbase_hlconnfieldwidth;
// check params
   ae_assert(k0 >= 0 && k0 < network->hllayersizes.cnt, "MLPGetWeight: incorrect (nonexistent) K0");
   ae_assert(i0 >= 0 && i0 < network->hllayersizes.xZ[k0], "MLPGetWeight: incorrect (nonexistent) I0");
   ae_assert(k1 >= 0 && k1 < network->hllayersizes.cnt, "MLPGetWeight: incorrect (nonexistent) K1");
   ae_assert(i1 >= 0 && i1 < network->hllayersizes.xZ[k1], "MLPGetWeight: incorrect (nonexistent) I1");
// search
   network->integerbuf.xZ[0] = k0;
   network->integerbuf.xZ[1] = i0;
   network->integerbuf.xZ[2] = k1;
   network->integerbuf.xZ[3] = i1;
   highlevelidx = recsearch(&network->hlconnections, mlpbase_hlconnfieldwidth, 4, 0, ccnt, &network->integerbuf);
   if (highlevelidx >= 0) {
      result = network->weights.xR[network->hlconnections.xZ[highlevelidx * mlpbase_hlconnfieldwidth + 4]];
   } else {
      result = 0.0;
   }
   return result;
}

// This function sets offset/scaling coefficients for I-th input of the
// network.
//
// Inputs:
//     Network     -   network
//     I           -   input index
//     Mean        -   mean term
//     Sigma       -   sigma term (if zero, will be replaced by 1.0)
//
// NTE: I-th input is passed through linear transformation
//     IN[i] = (IN[i]-Mean)/Sigma
// before feeding to the network. This function sets Mean and Sigma.
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: void mlpsetinputscaling(const multilayerperceptron &network, const ae_int_t i, const double mean, const double sigma);
void mlpsetinputscaling(multilayerperceptron *network, ae_int_t i, double mean, double sigma) {
   ae_assert(i >= 0 && i < network->hllayersizes.xZ[0], "MLPSetInputScaling: incorrect (nonexistent) I");
   ae_assert(isfinite(mean), "MLPSetInputScaling: infinite or NAN Mean");
   ae_assert(isfinite(sigma), "MLPSetInputScaling: infinite or NAN Sigma");
   if (sigma == 0.0) {
      sigma = 1.0;
   }
   network->columnmeans.xR[i] = mean;
   network->columnsigmas.xR[i] = sigma;
}

// This function sets offset/scaling coefficients for I-th output of the
// network.
//
// Inputs:
//     Network     -   network
//     I           -   input index
//     Mean        -   mean term
//     Sigma       -   sigma term (if zero, will be replaced by 1.0)
//
// Outputs:
//
// NOTE: I-th output is passed through linear transformation
//     OUT[i] = OUT[i]*Sigma+Mean
// before returning it to user. This function sets Sigma/Mean. In case we
// have SOFTMAX-normalized network, you can not set (Sigma,Mean) to anything
// other than(0.0,1.0) - this function will throw exception.
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: void mlpsetoutputscaling(const multilayerperceptron &network, const ae_int_t i, const double mean, const double sigma);
void mlpsetoutputscaling(multilayerperceptron *network, ae_int_t i, double mean, double sigma) {
   ae_assert(i >= 0 && i < network->hllayersizes.xZ[network->hllayersizes.cnt - 1], "MLPSetOutputScaling: incorrect (nonexistent) I");
   ae_assert(isfinite(mean), "MLPSetOutputScaling: infinite or NAN Mean");
   ae_assert(isfinite(sigma), "MLPSetOutputScaling: infinite or NAN Sigma");
   if (network->structinfo.xZ[6] == 1) {
      ae_assert(mean == 0.0, "MLPSetOutputScaling: you can not set non-zero Mean term for classifier network");
      ae_assert(sigma == 1.0, "MLPSetOutputScaling: you can not set non-unit Sigma term for classifier network");
   } else {
      if (sigma == 0.0) {
         sigma = 1.0;
      }
      network->columnmeans.xR[network->hllayersizes.xZ[0] + i] = mean;
      network->columnsigmas.xR[network->hllayersizes.xZ[0] + i] = sigma;
   }
}

// This function modifies information about Ith neuron of Kth layer
//
// Inputs:
//     Network     -   network
//     K           -   layer index
//     I           -   neuron index (within layer)
//     FKind       -   activation function type (used by MLPActivationFunction())
//                     this value must be zero for input neurons
//                     (you can not set activation function for input neurons)
//     Threshold   -   also called offset, bias
//                     this value must be zero for input neurons
//                     (you can not set threshold for input neurons)
//
// NOTES:
// 1. this function throws exception if layer or neuron with given index do
//    not exists.
// 2. this function also throws exception when you try to set non-linear
//    activation function for input neurons (any kind of network) or for output
//    neurons of classifier network.
// 3. this function throws exception when you try to set non-zero threshold for
//    input neurons (any kind of network).
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: void mlpsetneuroninfo(const multilayerperceptron &network, const ae_int_t k, const ae_int_t i, const ae_int_t fkind, const double threshold);
void mlpsetneuroninfo(multilayerperceptron *network, ae_int_t k, ae_int_t i, ae_int_t fkind, double threshold) {
   ae_int_t ncnt;
   ae_int_t istart;
   ae_int_t highlevelidx;
   ae_int_t activationoffset;
   ae_assert(isfinite(threshold), "MLPSetNeuronInfo: infinite or NAN Threshold");
// convenience vars
   ncnt = network->hlneurons.cnt / mlpbase_hlnfieldwidth;
   istart = network->structinfo.xZ[5];
// search
   network->integerbuf.xZ[0] = k;
   network->integerbuf.xZ[1] = i;
   highlevelidx = recsearch(&network->hlneurons, mlpbase_hlnfieldwidth, 2, 0, ncnt, &network->integerbuf);
   ae_assert(highlevelidx >= 0, "MLPSetNeuronInfo: incorrect (nonexistent) layer or neuron index");
// activation function
   if (network->hlneurons.xZ[highlevelidx * mlpbase_hlnfieldwidth + 2] >= 0) {
      activationoffset = istart + network->hlneurons.xZ[highlevelidx * mlpbase_hlnfieldwidth + 2] * mlpbase_nfieldwidth;
      network->structinfo.xZ[activationoffset] = fkind;
   } else {
      ae_assert(fkind == 0, "MLPSetNeuronInfo: you try to set activation function for neuron which can not have one");
   }
// Threshold
   if (network->hlneurons.xZ[highlevelidx * mlpbase_hlnfieldwidth + 3] >= 0) {
      network->weights.xR[network->hlneurons.xZ[highlevelidx * mlpbase_hlnfieldwidth + 3]] = threshold;
   } else {
      ae_assert(threshold == 0.0, "MLPSetNeuronInfo: you try to set non-zero threshold for neuron which can not have one");
   }
}

// This function modifies information about connection from I0-th neuron of
// K0-th layer to I1-th neuron of K1-th layer.
//
// Inputs:
//     Network     -   network
//     K0          -   layer index
//     I0          -   neuron index (within layer)
//     K1          -   layer index
//     I1          -   neuron index (within layer)
//     W           -   connection weight (must be zero for non-existent
//                     connections)
//
// This function:
// 1. throws exception if layer or neuron with given index do not exists.
// 2. throws exception if you try to set non-zero weight for non-existent
//    connection
// ALGLIB: Copyright 25.03.2011 by Sergey Bochkanov
// API: void mlpsetweight(const multilayerperceptron &network, const ae_int_t k0, const ae_int_t i0, const ae_int_t k1, const ae_int_t i1, const double w);
void mlpsetweight(multilayerperceptron *network, ae_int_t k0, ae_int_t i0, ae_int_t k1, ae_int_t i1, double w) {
   ae_int_t ccnt;
   ae_int_t highlevelidx;
   ccnt = network->hlconnections.cnt / mlpbase_hlconnfieldwidth;
// check params
   ae_assert(k0 >= 0 && k0 < network->hllayersizes.cnt, "MLPSetWeight: incorrect (nonexistent) K0");
   ae_assert(i0 >= 0 && i0 < network->hllayersizes.xZ[k0], "MLPSetWeight: incorrect (nonexistent) I0");
   ae_assert(k1 >= 0 && k1 < network->hllayersizes.cnt, "MLPSetWeight: incorrect (nonexistent) K1");
   ae_assert(i1 >= 0 && i1 < network->hllayersizes.xZ[k1], "MLPSetWeight: incorrect (nonexistent) I1");
   ae_assert(isfinite(w), "MLPSetWeight: infinite or NAN weight");
// search
   network->integerbuf.xZ[0] = k0;
   network->integerbuf.xZ[1] = i0;
   network->integerbuf.xZ[2] = k1;
   network->integerbuf.xZ[3] = i1;
   highlevelidx = recsearch(&network->hlconnections, mlpbase_hlconnfieldwidth, 4, 0, ccnt, &network->integerbuf);
   if (highlevelidx >= 0) {
      network->weights.xR[network->hlconnections.xZ[highlevelidx * mlpbase_hlconnfieldwidth + 4]] = w;
   } else {
      ae_assert(w == 0.0, "MLPSetWeight: you try to set non-zero weight for non-existent connection");
   }
}

// Neural network activation function
//
// Inputs:
//     NET         -   neuron input
//     K           -   function index (zero for linear function)
//
// Outputs:
//     F           -   function
//     DF          -   its derivative
//     D2F         -   its second derivative
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpactivationfunction(const double net, const ae_int_t k, double &f, double &df, double &d2f);
void mlpactivationfunction(double net, ae_int_t k, double *f, double *df, double *d2f) {
   double net2;
   double arg;
   double root;
   double r;
   *f = 0;
   *df = 0;
   *d2f = 0;
   if (k == 0 || k == -5) {
      *f = net;
      *df = 1.0;
      *d2f = 0.0;
      return;
   }
   if (k == 1) {
   // tanh activation function
      if (SmallR(net, 100.0)) {
         *f = tanh(net);
      } else {
         *f = (double)(ae_sign(net));
      }
      *df = 1 - *f * (*f);
      *d2f = -2 * (*f) * (*df);
      return;
   }
   if (k == 3) {
   // EX activation function
      if (net >= 0.0) {
         net2 = net * net;
         arg = net2 + 1;
         root = sqrt(arg);
         *f = net + root;
         r = net / root;
         *df = 1 + r;
         *d2f = (root - net * r) / arg;
      } else {
         *f = exp(net);
         *df = *f;
         *d2f = *f;
      }
      return;
   }
   if (k == 2) {
      *f = exp(-ae_sqr(net));
      *df = -2 * net * (*f);
      *d2f = -2 * (*f + *df * net);
      return;
   }
   *f = 0.0;
   *df = 0.0;
   *d2f = 0.0;
}

// Procesing
//
// Inputs:
//     Network -   neural network
//     X       -   input vector,  array[0..NIn-1].
//
// Outputs:
//     Y       -   result. Regression estimate when solving regression  task,
//                 vector of posterior probabilities for classification task.
//
// See also MLPProcessI
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpprocess(const multilayerperceptron &network, const real_1d_array &x, real_1d_array &y);
void mlpprocess(multilayerperceptron *network, RVector *x, RVector *y) {
   if (y->cnt < network->structinfo.xZ[2]) {
      ae_vector_set_length(y, network->structinfo.xZ[2]);
   }
   mlpinternalprocessvector(&network->structinfo, &network->weights, &network->columnmeans, &network->columnsigmas, &network->neurons, &network->dfdnet, x, y);
}

// 'interactive'  variant  of  MLPProcess  for  languages  like  Python which
// support constructs like "Y = MLPProcess(NN,X)" and interactive mode of the
// interpreter
//
// This function allocates new array on each call,  so  it  is  significantly
// slower than its 'non-interactive' counterpart, but it is  more  convenient
// when you call it from command line.
// ALGLIB: Copyright 21.09.2010 by Sergey Bochkanov
// API: void mlpprocessi(const multilayerperceptron &network, const real_1d_array &x, real_1d_array &y);
void mlpprocessi(multilayerperceptron *network, RVector *x, RVector *y) {
   SetVector(y);
   mlpprocess(network, x, y);
}

// Error of the neural network on dataset.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format;
//     NPoints     -   points count.
//
// Result:
//     sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: double mlperror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints);
double mlperror(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(xy->rows >= npoints, "MLPError: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(xy->cols >= mlpgetinputscount(network) + 1, "MLPError: XY has less than NIn+1 columns");
      } else {
         ae_assert(xy->cols >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPError: XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, xy, &network->dummysxy, npoints, 0, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = ae_sqr(network->err.rmserror) * npoints * mlpgetoutputscount(network) / 2;
   return result;
}

// Error of the neural network on dataset given by sparse matrix.
//
// Inputs:
//     Network     -   neural network
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format. This function checks  correctness
//                     of  the  dataset  (no  NANs/INFs,  class  numbers  are
//                     correct) and throws exception when  incorrect  dataset
//                     is passed.  Sparse  matrix  must  use  CRS  format for
//                     storage.
//     NPoints     -   points count, >= 0
//
// Result:
//     sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: double mlperrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints);
double mlperrorsparse(multilayerperceptron *network, sparsematrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(sparseiscrs(xy), "MLPErrorSparse: XY is not in CRS format.");
   ae_assert(sparsegetnrows(xy) >= npoints, "MLPErrorSparse: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPErrorSparse: XY has less than NIn+1 columns");
      } else {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPErrorSparse: XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, &network->dummydxy, xy, npoints, 1, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = ae_sqr(network->err.rmserror) * npoints * mlpgetoutputscount(network) / 2;
   return result;
}

// Returns T*Ln(T/Z), guarded against overflow/underflow.
// Internal subroutine.
static double mlpbase_safecrossentropy(double t, double z) {
   double r;
   double result;
   if (t == 0.0) {
      result = 0.0;
   } else {
      if (!SmallAtR(z, 1.0)) {
      // Shouldn't be the case with softmax,
      // but we just want to be sure.
         if (t / z == 0.0) {
            r = ae_minrealnumber;
         } else {
            r = t / z;
         }
      } else {
      // Normal case
         if (z == 0.0 || !SmallR(t, ae_maxrealnumber * fabs(z))) {
            r = ae_maxrealnumber;
         } else {
            r = t / z;
         }
      }
      result = t * log(r);
   }
   return result;
}

// Natural error function for neural network, internal subroutine.
//
// NOTE: this function is single-threaded. Unlike other  error  function,  it
// receives no speed-up from being executed in SMP mode.
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: double mlperrorn(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize);
double mlperrorn(multilayerperceptron *network, RMatrix *xy, ae_int_t ssize) {
   ae_int_t i;
   ae_int_t k;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   double e;
   double result;
   mlpproperties(network, &nin, &nout, &wcount);
   result = 0.0;
   for (i = 0; i < ssize; i++) {
   // Process vector
      ae_v_move(network->x.xR, 1, xy->xyR[i], 1, nin);
      mlpprocess(network, &network->x, &network->y);
   // Update error function
      if (network->structinfo.xZ[6] == 0) {
      // Least squares error function
         ae_v_sub(network->y.xR, 1, &xy->xyR[i][nin], 1, nout);
         e = ae_v_dotproduct(network->y.xR, 1, network->y.xR, 1, nout);
         result += e / 2;
      } else {
      // Cross-entropy error function
         k = RoundZ(xy->xyR[i][nin]);
         if (k >= 0 && k < nout) {
            result += mlpbase_safecrossentropy(1.0, network->y.xR[k]);
         }
      }
   }
   return result;
}

// Classification error of the neural network on dataset.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format;
//     NPoints     -   points count.
//
// Result:
//     classification error (number of misclassified cases)
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: ae_int_t mlpclserror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints);
ae_int_t mlpclserror(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints) {
   ae_int_t result;
   ae_assert(xy->rows >= npoints, "MLPClsError: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(xy->cols >= mlpgetinputscount(network) + 1, "MLPClsError: XY has less than NIn+1 columns");
      } else {
         ae_assert(xy->cols >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPClsError: XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, xy, &network->dummysxy, npoints, 0, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = RoundZ(npoints * network->err.relclserror);
   return result;
}

// Relative classification error on the test set.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format;
//     NPoints     -   points count.
//
// Result:
// Percent   of incorrectly   classified  cases.  Works  both  for classifier
// networks and general purpose networks used as classifiers.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 25.12.2008 by Sergey Bochkanov
// API: double mlprelclserror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints);
double mlprelclserror(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(xy->rows >= npoints, "MLPRelClsError: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(xy->cols >= mlpgetinputscount(network) + 1, "MLPRelClsError: XY has less than NIn+1 columns");
      } else {
         ae_assert(xy->cols >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPRelClsError: XY has less than NIn+NOut columns");
      }
   }
   if (npoints > 0) {
      result = (double)mlpclserror(network, xy, npoints) / (double)npoints;
   } else {
      result = 0.0;
   }
   return result;
}

// Relative classification error on the test set given by sparse matrix.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format. Sparse matrix must use CRS format
//                     for storage.
//     NPoints     -   points count, >= 0.
//
// Result:
// Percent   of incorrectly   classified  cases.  Works  both  for classifier
// networks and general purpose networks used as classifiers.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
// API: double mlprelclserrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints);
double mlprelclserrorsparse(multilayerperceptron *network, sparsematrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(sparseiscrs(xy), "MLPRelClsErrorSparse: sparse matrix XY is not in CRS format.");
   ae_assert(sparsegetnrows(xy) >= npoints, "MLPRelClsErrorSparse: sparse matrix XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPRelClsErrorSparse: sparse matrix XY has less than NIn+1 columns");
      } else {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPRelClsErrorSparse: sparse matrix XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, &network->dummydxy, xy, npoints, 1, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = network->err.relclserror;
   return result;
}

// Average cross-entropy  (in bits  per element) on the test set.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format;
//     NPoints     -   points count.
//
// Result:
// CrossEntropy/(NPoints*LN(2)).
// Zero if network solves regression task.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 08.01.2009 by Sergey Bochkanov
// API: double mlpavgce(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints);
double mlpavgce(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(xy->rows >= npoints, "MLPAvgCE: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(xy->cols >= mlpgetinputscount(network) + 1, "MLPAvgCE: XY has less than NIn+1 columns");
      } else {
         ae_assert(xy->cols >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgCE: XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, xy, &network->dummysxy, npoints, 0, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = network->err.avgce;
   return result;
}

// Average  cross-entropy  (in bits  per element)  on the  test set  given by
// sparse matrix.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format. This function checks  correctness
//                     of  the  dataset  (no  NANs/INFs,  class  numbers  are
//                     correct) and throws exception when  incorrect  dataset
//                     is passed.  Sparse  matrix  must  use  CRS  format for
//                     storage.
//     NPoints     -   points count, >= 0.
//
// Result:
// CrossEntropy/(NPoints*LN(2)).
// Zero if network solves regression task.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
// API: double mlpavgcesparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints);
double mlpavgcesparse(multilayerperceptron *network, sparsematrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(sparseiscrs(xy), "MLPAvgCESparse: sparse matrix XY is not in CRS format.");
   ae_assert(sparsegetnrows(xy) >= npoints, "MLPAvgCESparse: sparse matrix XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPAvgCESparse: sparse matrix XY has less than NIn+1 columns");
      } else {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgCESparse: sparse matrix XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, &network->dummydxy, xy, npoints, 1, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = network->err.avgce;
   return result;
}

// RMS error on the test set given.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format;
//     NPoints     -   points count.
//
// Result:
// Root mean  square error. Its meaning for regression task is obvious. As for
// classification  task,  RMS  error  means  error  when estimating  posterior
// probabilities.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: double mlprmserror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints);
double mlprmserror(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(xy->rows >= npoints, "MLPRMSError: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(xy->cols >= mlpgetinputscount(network) + 1, "MLPRMSError: XY has less than NIn+1 columns");
      } else {
         ae_assert(xy->cols >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPRMSError: XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, xy, &network->dummysxy, npoints, 0, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = network->err.rmserror;
   return result;
}

// RMS error on the test set given by sparse matrix.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format. This function checks  correctness
//                     of  the  dataset  (no  NANs/INFs,  class  numbers  are
//                     correct) and throws exception when  incorrect  dataset
//                     is passed.  Sparse  matrix  must  use  CRS  format for
//                     storage.
//     NPoints     -   points count, >= 0.
//
// Result:
// Root mean  square error. Its meaning for regression task is obvious. As for
// classification  task,  RMS  error  means  error  when estimating  posterior
// probabilities.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
// API: double mlprmserrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints);
double mlprmserrorsparse(multilayerperceptron *network, sparsematrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(sparseiscrs(xy), "MLPRMSErrorSparse: sparse matrix XY is not in CRS format.");
   ae_assert(sparsegetnrows(xy) >= npoints, "MLPRMSErrorSparse: sparse matrix XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPRMSErrorSparse: sparse matrix XY has less than NIn+1 columns");
      } else {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPRMSErrorSparse: sparse matrix XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, &network->dummydxy, xy, npoints, 1, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = network->err.rmserror;
   return result;
}

// Average absolute error on the test set.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format;
//     NPoints     -   points count.
//
// Result:
// Its meaning for regression task is obvious. As for classification task, it
// means average error when estimating posterior probabilities.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 11.03.2008 by Sergey Bochkanov
// API: double mlpavgerror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints);
double mlpavgerror(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(xy->rows >= npoints, "MLPAvgError: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(xy->cols >= mlpgetinputscount(network) + 1, "MLPAvgError: XY has less than NIn+1 columns");
      } else {
         ae_assert(xy->cols >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgError: XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, xy, &network->dummysxy, npoints, 0, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = network->err.avgerror;
   return result;
}

// Average absolute error on the test set given by sparse matrix.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format. This function checks  correctness
//                     of  the  dataset  (no  NANs/INFs,  class  numbers  are
//                     correct) and throws exception when  incorrect  dataset
//                     is passed.  Sparse  matrix  must  use  CRS  format for
//                     storage.
//     NPoints     -   points count, >= 0.
//
// Result:
// Its meaning for regression task is obvious. As for classification task, it
// means average error when estimating posterior probabilities.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
// API: double mlpavgerrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints);
double mlpavgerrorsparse(multilayerperceptron *network, sparsematrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(sparseiscrs(xy), "MLPAvgErrorSparse: XY is not in CRS format.");
   ae_assert(sparsegetnrows(xy) >= npoints, "MLPAvgErrorSparse: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPAvgErrorSparse: XY has less than NIn+1 columns");
      } else {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgErrorSparse: XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, &network->dummydxy, xy, npoints, 1, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = network->err.avgerror;
   return result;
}

// Average relative error on the test set.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format;
//     NPoints     -   points count.
//
// Result:
// Its meaning for regression task is obvious. As for classification task, it
// means  average  relative  error  when  estimating posterior probability of
// belonging to the correct class.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 11.03.2008 by Sergey Bochkanov
// API: double mlpavgrelerror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints);
double mlpavgrelerror(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(xy->rows >= npoints, "MLPAvgRelError: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(xy->cols >= mlpgetinputscount(network) + 1, "MLPAvgRelError: XY has less than NIn+1 columns");
      } else {
         ae_assert(xy->cols >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgRelError: XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, xy, &network->dummysxy, npoints, 0, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = network->err.avgrelerror;
   return result;
}

// Average relative error on the test set given by sparse matrix.
//
// Inputs:
//     Network     -   neural network;
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format. This function checks  correctness
//                     of  the  dataset  (no  NANs/INFs,  class  numbers  are
//                     correct) and throws exception when  incorrect  dataset
//                     is passed.  Sparse  matrix  must  use  CRS  format for
//                     storage.
//     NPoints     -   points count, >= 0.
//
// Result:
// Its meaning for regression task is obvious. As for classification task, it
// means  average  relative  error  when  estimating posterior probability of
// belonging to the correct class.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 09.08.2012 by Sergey Bochkanov
// API: double mlpavgrelerrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints);
double mlpavgrelerrorsparse(multilayerperceptron *network, sparsematrix *xy, ae_int_t npoints) {
   double result;
   ae_assert(sparseiscrs(xy), "MLPAvgRelErrorSparse: XY is not in CRS format.");
   ae_assert(sparsegetnrows(xy) >= npoints, "MLPAvgRelErrorSparse: XY has less than NPoints rows");
   if (npoints > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPAvgRelErrorSparse: XY has less than NIn+1 columns");
      } else {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAvgRelErrorSparse: XY has less than NIn+NOut columns");
      }
   }
   mlpallerrorsx(network, &network->dummydxy, xy, npoints, 1, &network->dummyidx, 0, npoints, 0, &network->buf, &network->err);
   result = network->err.avgrelerror;
   return result;
}

// Internal subroutine
//
// Network must be processed by MLPProcess on X
static void mlpbase_mlpinternalcalculategradient(multilayerperceptron *network, RVector *neurons, RVector *weights, RVector *derror, RVector *grad, bool naturalerrorfunc) {
   ae_int_t i;
   ae_int_t n1;
   ae_int_t n2;
   ae_int_t w1;
   ae_int_t w2;
   ae_int_t ntotal;
   ae_int_t istart;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t offs;
   double dedf;
   double dfdnet;
   double v;
   double fown;
   double deown;
   double net;
   double mx;
   bool bflag;
// Read network geometry
   nin = network->structinfo.xZ[1];
   nout = network->structinfo.xZ[2];
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
// Pre-processing of dError/dOut:
// from dError/dOut(normalized) to dError/dOut(non-normalized)
   ae_assert(network->structinfo.xZ[6] == 0 || network->structinfo.xZ[6] == 1, "MLPInternalCalculateGradient: unknown normalization type!");
   if (network->structinfo.xZ[6] == 1) {
   // Softmax
      if (!naturalerrorfunc) {
         mx = network->neurons.xR[ntotal - nout];
         for (i = 0; i < nout; i++) {
            mx = rmax2(mx, network->neurons.xR[ntotal - nout + i]);
         }
         net = 0.0;
         for (i = 0; i < nout; i++) {
            network->nwbuf.xR[i] = exp(network->neurons.xR[ntotal - nout + i] - mx);
            net += network->nwbuf.xR[i];
         }
         v = ae_v_dotproduct(&network->derror.xR[ntotal - nout], 1, network->nwbuf.xR, 1, nout);
         for (i = 0; i < nout; i++) {
            fown = network->nwbuf.xR[i];
            deown = network->derror.xR[ntotal - nout + i];
            network->nwbuf.xR[nout + i] = (-v + deown * fown + deown * (net - fown)) * fown / ae_sqr(net);
         }
         for (i = 0; i < nout; i++) {
            network->derror.xR[ntotal - nout + i] = network->nwbuf.xR[nout + i];
         }
      }
   } else {
   // Un-standardisation
      for (i = 0; i < nout; i++) {
         network->derror.xR[ntotal - nout + i] *= network->columnsigmas.xR[nin + i];
      }
   }
// Backpropagation
   for (i = ntotal - 1; i >= 0; i--) {
   // Extract info
      offs = istart + i * mlpbase_nfieldwidth;
      if (network->structinfo.xZ[offs] > 0 || network->structinfo.xZ[offs] == -5) {
      // Activation function
         dedf = network->derror.xR[i];
         dfdnet = network->dfdnet.xR[i];
         derror->xR[network->structinfo.xZ[offs + 2]] += dedf * dfdnet;
         continue;
      }
      if (network->structinfo.xZ[offs] == 0) {
      // Adaptive summator
         n1 = network->structinfo.xZ[offs + 2];
         n2 = n1 + network->structinfo.xZ[offs + 1] - 1;
         w1 = network->structinfo.xZ[offs + 3];
         w2 = w1 + network->structinfo.xZ[offs + 1] - 1;
         dedf = network->derror.xR[i];
         dfdnet = 1.0;
         v = dedf * dfdnet;
         ae_v_moved(&grad->xR[w1], 1, &neurons->xR[n1], 1, w2 - w1 + 1, v);
         ae_v_addd(&derror->xR[n1], 1, &weights->xR[w1], 1, n2 - n1 + 1, v);
         continue;
      }
      if (network->structinfo.xZ[offs] < 0) {
         bflag = false;
         if (network->structinfo.xZ[offs] == -2 || network->structinfo.xZ[offs] == -3 || network->structinfo.xZ[offs] == -4) {
         // Special neuron type, no back-propagation required
            bflag = true;
         }
         ae_assert(bflag, "MLPInternalCalculateGradient: unknown neuron type!");
         continue;
      }
   }
}

// Gradient calculation
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     X       -   input vector, length of array must be at least NIn
//     DesiredY-   desired outputs, length of array must be at least NOut
//     Grad    -   possibly preallocated array. If size of array is smaller
//                 than WCount, it will be reallocated. It is recommended to
//                 reuse previously allocated array to reduce allocation
//                 overhead.
//
// Outputs:
//     E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
//     Grad    -   gradient of E with respect to weights of network, array[WCount]
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpgrad(const multilayerperceptron &network, const real_1d_array &x, const real_1d_array &desiredy, double &e, real_1d_array &grad);
void mlpgrad(multilayerperceptron *network, RVector *x, RVector *desiredy, double *e, RVector *grad) {
   ae_int_t i;
   ae_int_t nout;
   ae_int_t ntotal;
   *e = 0;
// Alloc
   vectorsetlengthatleast(grad, network->structinfo.xZ[4]);
// Prepare dError/dOut, internal structures
   mlpprocess(network, x, &network->y);
   nout = network->structinfo.xZ[2];
   ntotal = network->structinfo.xZ[3];
   *e = 0.0;
   for (i = 0; i < ntotal; i++) {
      network->derror.xR[i] = 0.0;
   }
   for (i = 0; i < nout; i++) {
      network->derror.xR[ntotal - nout + i] = network->y.xR[i] - desiredy->xR[i];
      *e += ae_sqr(network->y.xR[i] - desiredy->xR[i]) / 2;
   }
// gradient
   mlpbase_mlpinternalcalculategradient(network, &network->neurons, &network->weights, &network->derror, grad, false);
}

// Gradient calculation (natural error function is used)
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     X       -   input vector, length of array must be at least NIn
//     DesiredY-   desired outputs, length of array must be at least NOut
//     Grad    -   possibly preallocated array. If size of array is smaller
//                 than WCount, it will be reallocated. It is recommended to
//                 reuse previously allocated array to reduce allocation
//                 overhead.
//
// Outputs:
//     E       -   error function, sum-of-squares for regression networks,
//                 cross-entropy for classification networks.
//     Grad    -   gradient of E with respect to weights of network, array[WCount]
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpgradn(const multilayerperceptron &network, const real_1d_array &x, const real_1d_array &desiredy, double &e, real_1d_array &grad);
void mlpgradn(multilayerperceptron *network, RVector *x, RVector *desiredy, double *e, RVector *grad) {
   double s;
   ae_int_t i;
   ae_int_t nout;
   ae_int_t ntotal;
   *e = 0;
// Alloc
   vectorsetlengthatleast(grad, network->structinfo.xZ[4]);
// Prepare dError/dOut, internal structures
   mlpprocess(network, x, &network->y);
   nout = network->structinfo.xZ[2];
   ntotal = network->structinfo.xZ[3];
   for (i = 0; i < ntotal; i++) {
      network->derror.xR[i] = 0.0;
   }
   *e = 0.0;
   if (network->structinfo.xZ[6] == 0) {
   // Regression network, least squares
      for (i = 0; i < nout; i++) {
         network->derror.xR[ntotal - nout + i] = network->y.xR[i] - desiredy->xR[i];
         *e += ae_sqr(network->y.xR[i] - desiredy->xR[i]) / 2;
      }
   } else {
   // Classification network, cross-entropy
      s = 0.0;
      for (i = 0; i < nout; i++) {
         s += desiredy->xR[i];
      }
      for (i = 0; i < nout; i++) {
         network->derror.xR[ntotal - nout + i] = s * network->y.xR[i] - desiredy->xR[i];
         *e += mlpbase_safecrossentropy(desiredy->xR[i], network->y.xR[i]);
      }
   }
// gradient
   mlpbase_mlpinternalcalculategradient(network, &network->neurons, &network->weights, &network->derror, grad, true);
}

// Batch gradient calculation for a set of inputs/outputs
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     XY      -   original dataset in dense format; one sample = one row:
//                 * first NIn columns contain inputs,
//                 * for regression problem, next NOut columns store
//                   desired outputs.
//                 * for classification problem, next column (just one!)
//                   stores class number.
//     SSize   -   number of elements in XY
//     Grad    -   possibly preallocated array. If size of array is smaller
//                 than WCount, it will be reallocated. It is recommended to
//                 reuse previously allocated array to reduce allocation
//                 overhead.
//
// Outputs:
//     E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
//     Grad    -   gradient of E with respect to weights of network, array[WCount]
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpgradbatch(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize, double &e, real_1d_array &grad);
void mlpgradbatch(multilayerperceptron *network, RMatrix *xy, ae_int_t ssize, double *e, RVector *grad) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t subset0;
   ae_int_t subset1;
   ae_int_t subsettype;
   ae_frame_make(&_frame_block);
   *e = 0;
   RefObj(smlpgrad, sgrad);
   ae_assert(ssize >= 0, "MLPGradBatchSparse: SSize<0");
   subset0 = 0;
   subset1 = ssize;
   subsettype = 0;
   mlpproperties(network, &nin, &nout, &wcount);
   vectorsetlengthatleast(grad, wcount);
   for (ae_shared_pool_first_recycled(&network->gradbuf, &_sgrad); sgrad != NULL; ae_shared_pool_next_recycled(&network->gradbuf, &_sgrad)) {
      sgrad->f = 0.0;
      for (i = 0; i < wcount; i++) {
         sgrad->g.xR[i] = 0.0;
      }
   }
   mlpgradbatchx(network, xy, &network->dummysxy, ssize, 0, &network->dummyidx, subset0, subset1, subsettype, &network->buf, &network->gradbuf);
   *e = 0.0;
   for (i = 0; i < wcount; i++) {
      grad->xR[i] = 0.0;
   }
   for (ae_shared_pool_first_recycled(&network->gradbuf, &_sgrad); sgrad != NULL; ae_shared_pool_next_recycled(&network->gradbuf, &_sgrad)) {
      *e += sgrad->f;
      for (i = 0; i < wcount; i++) {
         grad->xR[i] += sgrad->g.xR[i];
      }
   }
   ae_frame_leave();
}

// Batch gradient calculation for a set  of inputs/outputs  given  by  sparse
// matrices
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     XY      -   original dataset in sparse format; one sample = one row:
//                 * MATRIX MUST BE STORED IN CRS FORMAT
//                 * first NIn columns contain inputs.
//                 * for regression problem, next NOut columns store
//                   desired outputs.
//                 * for classification problem, next column (just one!)
//                   stores class number.
//     SSize   -   number of elements in XY
//     Grad    -   possibly preallocated array. If size of array is smaller
//                 than WCount, it will be reallocated. It is recommended to
//                 reuse previously allocated array to reduce allocation
//                 overhead.
//
// Outputs:
//     E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
//     Grad    -   gradient of E with respect to weights of network, array[WCount]
// ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
// API: void mlpgradbatchsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t ssize, double &e, real_1d_array &grad);
void mlpgradbatchsparse(multilayerperceptron *network, sparsematrix *xy, ae_int_t ssize, double *e, RVector *grad) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t subset0;
   ae_int_t subset1;
   ae_int_t subsettype;
   ae_frame_make(&_frame_block);
   *e = 0;
   RefObj(smlpgrad, sgrad);
   ae_assert(ssize >= 0, "MLPGradBatchSparse: SSize<0");
   ae_assert(sparseiscrs(xy), "MLPGradBatchSparse: sparse matrix XY must be in CRS format.");
   subset0 = 0;
   subset1 = ssize;
   subsettype = 0;
   mlpproperties(network, &nin, &nout, &wcount);
   vectorsetlengthatleast(grad, wcount);
   for (ae_shared_pool_first_recycled(&network->gradbuf, &_sgrad); sgrad != NULL; ae_shared_pool_next_recycled(&network->gradbuf, &_sgrad)) {
      sgrad->f = 0.0;
      for (i = 0; i < wcount; i++) {
         sgrad->g.xR[i] = 0.0;
      }
   }
   mlpgradbatchx(network, &network->dummydxy, xy, ssize, 1, &network->dummyidx, subset0, subset1, subsettype, &network->buf, &network->gradbuf);
   *e = 0.0;
   for (i = 0; i < wcount; i++) {
      grad->xR[i] = 0.0;
   }
   for (ae_shared_pool_first_recycled(&network->gradbuf, &_sgrad); sgrad != NULL; ae_shared_pool_next_recycled(&network->gradbuf, &_sgrad)) {
      *e += sgrad->f;
      for (i = 0; i < wcount; i++) {
         grad->xR[i] += sgrad->g.xR[i];
      }
   }
   ae_frame_leave();
}

// Batch gradient calculation for a subset of dataset
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     XY      -   original dataset in dense format; one sample = one row:
//                 * first NIn columns contain inputs,
//                 * for regression problem, next NOut columns store
//                   desired outputs.
//                 * for classification problem, next column (just one!)
//                   stores class number.
//     SetSize -   real size of XY, SetSize >= 0;
//     Idx     -   subset of SubsetSize elements, array[SubsetSize]:
//                 * Idx[I] stores row index in the original dataset which is
//                   given by XY. Gradient is calculated with respect to rows
//                   whose indexes are stored in Idx[].
//                 * Idx[]  must store correct indexes; this function  throws
//                   an  exception  in  case  incorrect index (less than 0 or
//                   larger than rows(XY)) is given
//                 * Idx[]  may  store  indexes  in  any  order and even with
//                   repetitions.
//     SubsetSize- number of elements in Idx[] array:
//                 * positive value means that subset given by Idx[] is processed
//                 * zero value results in zero gradient
//                 * negative value means that full dataset is processed
//     Grad      - possibly  preallocated array. If size of array is  smaller
//                 than WCount, it will be reallocated. It is  recommended to
//                 reuse  previously  allocated  array  to  reduce allocation
//                 overhead.
//
// Outputs:
//     E         - error function, SUM(sqr(y[i]-desiredy[i])/2,i)
//     Grad      - gradient  of  E  with  respect   to  weights  of  network,
//                 array[WCount]
// ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
// API: void mlpgradbatchsubset(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t setsize, const integer_1d_array &idx, const ae_int_t subsetsize, double &e, real_1d_array &grad);
void mlpgradbatchsubset(multilayerperceptron *network, RMatrix *xy, ae_int_t setsize, ZVector *idx, ae_int_t subsetsize, double *e, RVector *grad) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t npoints;
   ae_int_t subset0;
   ae_int_t subset1;
   ae_int_t subsettype;
   ae_frame_make(&_frame_block);
   *e = 0;
   RefObj(smlpgrad, sgrad);
   ae_assert(setsize >= 0, "MLPGradBatchSubset: SetSize<0");
   ae_assert(subsetsize <= idx->cnt, "MLPGradBatchSubset: SubsetSize>Length(Idx)");
   npoints = setsize;
   if (subsetsize < 0) {
      subset0 = 0;
      subset1 = setsize;
      subsettype = 0;
   } else {
      subset0 = 0;
      subset1 = subsetsize;
      subsettype = 1;
      for (i = 0; i < subsetsize; i++) {
         ae_assert(idx->xZ[i] >= 0, "MLPGradBatchSubset: incorrect index of XY row(Idx[I] < 0)");
         ae_assert(idx->xZ[i] < npoints, "MLPGradBatchSubset: incorrect index of XY row(Idx[I]>Rows(XY)-1)");
      }
   }
   mlpproperties(network, &nin, &nout, &wcount);
   vectorsetlengthatleast(grad, wcount);
   for (ae_shared_pool_first_recycled(&network->gradbuf, &_sgrad); sgrad != NULL; ae_shared_pool_next_recycled(&network->gradbuf, &_sgrad)) {
      sgrad->f = 0.0;
      for (i = 0; i < wcount; i++) {
         sgrad->g.xR[i] = 0.0;
      }
   }
   mlpgradbatchx(network, xy, &network->dummysxy, setsize, 0, idx, subset0, subset1, subsettype, &network->buf, &network->gradbuf);
   *e = 0.0;
   for (i = 0; i < wcount; i++) {
      grad->xR[i] = 0.0;
   }
   for (ae_shared_pool_first_recycled(&network->gradbuf, &_sgrad); sgrad != NULL; ae_shared_pool_next_recycled(&network->gradbuf, &_sgrad)) {
      *e += sgrad->f;
      for (i = 0; i < wcount; i++) {
         grad->xR[i] += sgrad->g.xR[i];
      }
   }
   ae_frame_leave();
}

// Batch gradient calculation for a set of inputs/outputs  for  a  subset  of
// dataset given by set of indexes.
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     XY      -   original dataset in sparse format; one sample = one row:
//                 * MATRIX MUST BE STORED IN CRS FORMAT
//                 * first NIn columns contain inputs,
//                 * for regression problem, next NOut columns store
//                   desired outputs.
//                 * for classification problem, next column (just one!)
//                   stores class number.
//     SetSize -   real size of XY, SetSize >= 0;
//     Idx     -   subset of SubsetSize elements, array[SubsetSize]:
//                 * Idx[I] stores row index in the original dataset which is
//                   given by XY. Gradient is calculated with respect to rows
//                   whose indexes are stored in Idx[].
//                 * Idx[]  must store correct indexes; this function  throws
//                   an  exception  in  case  incorrect index (less than 0 or
//                   larger than rows(XY)) is given
//                 * Idx[]  may  store  indexes  in  any  order and even with
//                   repetitions.
//     SubsetSize- number of elements in Idx[] array:
//                 * positive value means that subset given by Idx[] is processed
//                 * zero value results in zero gradient
//                 * negative value means that full dataset is processed
//     Grad      - possibly  preallocated array. If size of array is  smaller
//                 than WCount, it will be reallocated. It is  recommended to
//                 reuse  previously  allocated  array  to  reduce allocation
//                 overhead.
//
// Outputs:
//     E       -   error function, SUM(sqr(y[i]-desiredy[i])/2,i)
//     Grad    -   gradient  of  E  with  respect   to  weights  of  network,
//                 array[WCount]
//
// NOTE: when  SubsetSize < 0 is used full dataset by call MLPGradBatchSparse
//       function.
// ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
// API: void mlpgradbatchsparsesubset(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t setsize, const integer_1d_array &idx, const ae_int_t subsetsize, double &e, real_1d_array &grad);
void mlpgradbatchsparsesubset(multilayerperceptron *network, sparsematrix *xy, ae_int_t setsize, ZVector *idx, ae_int_t subsetsize, double *e, RVector *grad) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t npoints;
   ae_int_t subset0;
   ae_int_t subset1;
   ae_int_t subsettype;
   ae_frame_make(&_frame_block);
   *e = 0;
   RefObj(smlpgrad, sgrad);
   ae_assert(setsize >= 0, "MLPGradBatchSparseSubset: SetSize<0");
   ae_assert(subsetsize <= idx->cnt, "MLPGradBatchSparseSubset: SubsetSize>Length(Idx)");
   ae_assert(sparseiscrs(xy), "MLPGradBatchSparseSubset: sparse matrix XY must be in CRS format.");
   npoints = setsize;
   if (subsetsize < 0) {
      subset0 = 0;
      subset1 = setsize;
      subsettype = 0;
   } else {
      subset0 = 0;
      subset1 = subsetsize;
      subsettype = 1;
      for (i = 0; i < subsetsize; i++) {
         ae_assert(idx->xZ[i] >= 0, "MLPGradBatchSparseSubset: incorrect index of XY row(Idx[I] < 0)");
         ae_assert(idx->xZ[i] < npoints, "MLPGradBatchSparseSubset: incorrect index of XY row(Idx[I]>Rows(XY)-1)");
      }
   }
   mlpproperties(network, &nin, &nout, &wcount);
   vectorsetlengthatleast(grad, wcount);
   for (ae_shared_pool_first_recycled(&network->gradbuf, &_sgrad); sgrad != NULL; ae_shared_pool_next_recycled(&network->gradbuf, &_sgrad)) {
      sgrad->f = 0.0;
      for (i = 0; i < wcount; i++) {
         sgrad->g.xR[i] = 0.0;
      }
   }
   mlpgradbatchx(network, &network->dummydxy, xy, setsize, 1, idx, subset0, subset1, subsettype, &network->buf, &network->gradbuf);
   *e = 0.0;
   for (i = 0; i < wcount; i++) {
      grad->xR[i] = 0.0;
   }
   for (ae_shared_pool_first_recycled(&network->gradbuf, &_sgrad); sgrad != NULL; ae_shared_pool_next_recycled(&network->gradbuf, &_sgrad)) {
      *e += sgrad->f;
      for (i = 0; i < wcount; i++) {
         grad->xR[i] += sgrad->g.xR[i];
      }
   }
   ae_frame_leave();
}

static void mlpbase_mlpchunkedgradient(multilayerperceptron *network, RMatrix *xy, ae_int_t cstart, ae_int_t csize, RVector *batch4buf, RVector *hpcbuf, double *e, bool naturalerrorfunc) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t kl;
   ae_int_t ntotal;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t offs;
   double f;
   double df;
   double d2f;
   double v;
   double vv;
   double s;
   double fown;
   double deown;
   bool bflag;
   ae_int_t istart;
   ae_int_t entrysize;
   ae_int_t dfoffs;
   ae_int_t derroroffs;
   ae_int_t entryoffs;
   ae_int_t neuronidx;
   ae_int_t srcentryoffs;
   ae_int_t srcneuronidx;
   ae_int_t srcweightidx;
   ae_int_t neurontype;
   ae_int_t nweights;
   ae_int_t offs0;
   ae_int_t offs1;
   ae_int_t offs2;
   double v0;
   double v1;
   double v2;
   double v3;
   double s0;
   double s1;
   double s2;
   double s3;
   ae_int_t chunksize;
   chunksize = 4;
   ae_assert(csize <= chunksize, "MLPChunkedGradient: internal error (CSize>ChunkSize)");
// Try to use HPC core, if possible
   if (hpcchunkedgradient(&network->weights, &network->structinfo, &network->columnmeans, &network->columnsigmas, xy, cstart, csize, batch4buf, hpcbuf, e, naturalerrorfunc)) {
      return;
   }
// Read network geometry, prepare data
   nin = network->structinfo.xZ[1];
   nout = network->structinfo.xZ[2];
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
   entrysize = 12;
   dfoffs = 4;
   derroroffs = 8;
// Fill Batch4Buf by zeros.
//
// THIS STAGE IS VERY IMPORTANT!
//
// We fill all components of entry - neuron values, dF/dNET, dError/dF.
// It allows us to easily handle  situations  when  CSize<ChunkSize  by
// simply  working  with  ALL  components  of  Batch4Buf,  without ever
// looking at CSize. The idea is that dError/dF for  absent  components
// will be initialized by zeros - and won't be  rewritten  by  non-zero
// values during backpropagation.
   for (i = 0; i < entrysize * ntotal; i++) {
      batch4buf->xR[i] = 0.0;
   }
// Forward pass:
// 1. Load data into Batch4Buf. If CSize<ChunkSize, data are padded by zeros.
// 2. Perform forward pass through network
   for (i = 0; i < nin; i++) {
      entryoffs = entrysize * i;
      for (j = 0; j < csize; j++) {
         if (network->columnsigmas.xR[i] != 0.0) {
            batch4buf->xR[entryoffs + j] = (xy->xyR[cstart + j][i] - network->columnmeans.xR[i]) / network->columnsigmas.xR[i];
         } else {
            batch4buf->xR[entryoffs + j] = xy->xyR[cstart + j][i] - network->columnmeans.xR[i];
         }
      }
   }
   for (neuronidx = 0; neuronidx < ntotal; neuronidx++) {
      entryoffs = entrysize * neuronidx;
      offs = istart + neuronidx * mlpbase_nfieldwidth;
      neurontype = network->structinfo.xZ[offs];
      if (neurontype > 0 || neurontype == -5) {
      // "activation function" neuron, which takes value of neuron SrcNeuronIdx
      // and applies activation function to it.
      //
      // This neuron has no weights and no tunable parameters.
         srcneuronidx = network->structinfo.xZ[offs + 2];
         srcentryoffs = entrysize * srcneuronidx;
         mlpactivationfunction(batch4buf->xR[srcentryoffs], neurontype, &f, &df, &d2f);
         batch4buf->xR[entryoffs] = f;
         batch4buf->xR[entryoffs + 0 + dfoffs] = df;
         mlpactivationfunction(batch4buf->xR[srcentryoffs + 1], neurontype, &f, &df, &d2f);
         batch4buf->xR[entryoffs + 1] = f;
         batch4buf->xR[entryoffs + 1 + dfoffs] = df;
         mlpactivationfunction(batch4buf->xR[srcentryoffs + 2], neurontype, &f, &df, &d2f);
         batch4buf->xR[entryoffs + 2] = f;
         batch4buf->xR[entryoffs + 2 + dfoffs] = df;
         mlpactivationfunction(batch4buf->xR[srcentryoffs + 3], neurontype, &f, &df, &d2f);
         batch4buf->xR[entryoffs + 3] = f;
         batch4buf->xR[entryoffs + 3 + dfoffs] = df;
         continue;
      }
      if (neurontype == 0) {
      // "adaptive summator" neuron, whose output is a weighted sum of inputs.
      // It has weights, but has no activation function.
         nweights = network->structinfo.xZ[offs + 1];
         srcneuronidx = network->structinfo.xZ[offs + 2];
         srcentryoffs = entrysize * srcneuronidx;
         srcweightidx = network->structinfo.xZ[offs + 3];
         v0 = 0.0;
         v1 = 0.0;
         v2 = 0.0;
         v3 = 0.0;
         for (j = 0; j < nweights; j++) {
            v = network->weights.xR[srcweightidx];
            srcweightidx++;
            v0 += v * batch4buf->xR[srcentryoffs];
            v1 += v * batch4buf->xR[srcentryoffs + 1];
            v2 += v * batch4buf->xR[srcentryoffs + 2];
            v3 += v * batch4buf->xR[srcentryoffs + 3];
            srcentryoffs += entrysize;
         }
         batch4buf->xR[entryoffs] = v0;
         batch4buf->xR[entryoffs + 1] = v1;
         batch4buf->xR[entryoffs + 2] = v2;
         batch4buf->xR[entryoffs + 3] = v3;
         batch4buf->xR[entryoffs + 0 + dfoffs] = 1.0;
         batch4buf->xR[entryoffs + 1 + dfoffs] = 1.0;
         batch4buf->xR[entryoffs + 2 + dfoffs] = 1.0;
         batch4buf->xR[entryoffs + 3 + dfoffs] = 1.0;
         continue;
      }
      if (neurontype < 0) {
         bflag = false;
         if (neurontype == -2) {
         // Input neuron, left unchanged
            bflag = true;
         }
         if (neurontype == -3) {
         // "-1" neuron
            batch4buf->xR[entryoffs] = -1.0;
            batch4buf->xR[entryoffs + 1] = -1.0;
            batch4buf->xR[entryoffs + 2] = -1.0;
            batch4buf->xR[entryoffs + 3] = -1.0;
            batch4buf->xR[entryoffs + 0 + dfoffs] = 0.0;
            batch4buf->xR[entryoffs + 1 + dfoffs] = 0.0;
            batch4buf->xR[entryoffs + 2 + dfoffs] = 0.0;
            batch4buf->xR[entryoffs + 3 + dfoffs] = 0.0;
            bflag = true;
         }
         if (neurontype == -4) {
         // "0" neuron
            batch4buf->xR[entryoffs] = 0.0;
            batch4buf->xR[entryoffs + 1] = 0.0;
            batch4buf->xR[entryoffs + 2] = 0.0;
            batch4buf->xR[entryoffs + 3] = 0.0;
            batch4buf->xR[entryoffs + 0 + dfoffs] = 0.0;
            batch4buf->xR[entryoffs + 1 + dfoffs] = 0.0;
            batch4buf->xR[entryoffs + 2 + dfoffs] = 0.0;
            batch4buf->xR[entryoffs + 3 + dfoffs] = 0.0;
            bflag = true;
         }
         ae_assert(bflag, "MLPChunkedGradient: internal error - unknown neuron type!");
         continue;
      }
   }
// Intermediate phase between forward and backward passes.
//
// For regression networks:
// * forward pass is completely done (no additional post-processing is
//   needed).
// * before starting backward pass, we have to  calculate  dError/dOut
//   for output neurons. We also update error at this phase.
//
// For classification networks:
// * in addition to forward pass we  apply  SOFTMAX  normalization  to
//   output neurons.
// * after applying normalization, we have to  calculate  dError/dOut,
//   which is calculated in two steps:
//   * first, we calculate derivative of error with respect to SOFTMAX
//     normalized outputs (normalized dError)
//   * then,  we calculate derivative of error with respect to  values
//     of outputs BEFORE normalization was applied to them
   ae_assert(network->structinfo.xZ[6] == 0 || network->structinfo.xZ[6] == 1, "MLPChunkedGradient: unknown normalization type!");
   if (network->structinfo.xZ[6] == 1) {
   // SOFTMAX-normalized network.
   //
   // First,  calculate (V0,V1,V2,V3)  -  component-wise  maximum
   // of output neurons. This vector of maximum  values  will  be
   // used for normalization  of  outputs  prior  to  calculating
   // exponentials.
   //
   // NOTE: the only purpose of this stage is to prevent overflow
   //       during calculation of exponentials.  With  this stage
   //       we  make  sure  that  all exponentials are calculated
   //       with non-positive argument. If you load (0,0,0,0)  to
   //       (V0,V1,V2,V3), your program will continue  working  -
   //       although with less robustness.
      entryoffs = entrysize * (ntotal - nout);
      v0 = batch4buf->xR[entryoffs];
      v1 = batch4buf->xR[entryoffs + 1];
      v2 = batch4buf->xR[entryoffs + 2];
      v3 = batch4buf->xR[entryoffs + 3];
      entryoffs += entrysize;
      for (i = 1; i < nout; i++) {
         v = batch4buf->xR[entryoffs];
         if (v > v0) {
            v0 = v;
         }
         v = batch4buf->xR[entryoffs + 1];
         if (v > v1) {
            v1 = v;
         }
         v = batch4buf->xR[entryoffs + 2];
         if (v > v2) {
            v2 = v;
         }
         v = batch4buf->xR[entryoffs + 3];
         if (v > v3) {
            v3 = v;
         }
         entryoffs += entrysize;
      }
   // Then,  calculate exponentials and place them to part of the
   // array which  is  located  past  the  last  entry.  We  also
   // calculate sum of exponentials which will be stored past the
   // exponentials.
      entryoffs = entrysize * (ntotal - nout);
      offs0 = entrysize * ntotal;
      s0 = 0.0;
      s1 = 0.0;
      s2 = 0.0;
      s3 = 0.0;
      for (i = 0; i < nout; i++) {
         v = exp(batch4buf->xR[entryoffs] - v0);
         s0 += v;
         batch4buf->xR[offs0] = v;
         v = exp(batch4buf->xR[entryoffs + 1] - v1);
         s1 += v;
         batch4buf->xR[offs0 + 1] = v;
         v = exp(batch4buf->xR[entryoffs + 2] - v2);
         s2 += v;
         batch4buf->xR[offs0 + 2] = v;
         v = exp(batch4buf->xR[entryoffs + 3] - v3);
         s3 += v;
         batch4buf->xR[offs0 + 3] = v;
         entryoffs += entrysize;
         offs0 += chunksize;
      }
      offs0 = entrysize * ntotal + 2 * nout * chunksize;
      batch4buf->xR[offs0] = s0;
      batch4buf->xR[offs0 + 1] = s1;
      batch4buf->xR[offs0 + 2] = s2;
      batch4buf->xR[offs0 + 3] = s3;
   // Now we have:
   // * Batch4Buf[0...EntrySize*NTotal-1] stores:
   //   * NTotal*ChunkSize neuron output values (SOFTMAX normalization
   //     was not applied to these values),
   //   * NTotal*ChunkSize values of dF/dNET (derivative of neuron
   //     output with respect to its input)
   //   * NTotal*ChunkSize zeros in the elements which correspond to
   //     dError/dOut (derivative of error with respect to neuron output).
   // * Batch4Buf[EntrySize*NTotal...EntrySize*NTotal+ChunkSize*NOut-1] -
   //   stores exponentials of last NOut neurons.
   // * Batch4Buf[EntrySize*NTotal+ChunkSize*NOut-1...EntrySize*NTotal+ChunkSize*2*NOut-1]
   //   - can be used for temporary calculations
   // * Batch4Buf[EntrySize*NTotal+ChunkSize*2*NOut...EntrySize*NTotal+ChunkSize*2*NOut+ChunkSize-1]
   //   - stores sum-of-exponentials
   //
   // Block below calculates derivatives of error function with respect
   // to non-SOFTMAX-normalized output values of last NOut neurons.
   //
   // It is quite complicated; we do not describe algebra behind it,
   // but if you want you may check it yourself :)
      if (naturalerrorfunc) {
      // Calculate  derivative  of  error  with respect to values of
      // output  neurons  PRIOR TO SOFTMAX NORMALIZATION. Because we
      // use natural error function (cross-entropy), we  can  do  so
      // very easy.
         offs0 = entrysize * ntotal + 2 * nout * chunksize;
         for (k = 0; k < csize; k++) {
            s = batch4buf->xR[offs0 + k];
            kl = RoundZ(xy->xyR[cstart + k][nin]);
            offs1 = (ntotal - nout) * entrysize + derroroffs + k;
            offs2 = entrysize * ntotal + k;
            for (i = 0; i < nout; i++) {
               if (i == kl) {
                  v = 1.0;
               } else {
                  v = 0.0;
               }
               vv = batch4buf->xR[offs2];
               batch4buf->xR[offs1] = vv / s - v;
               *e += mlpbase_safecrossentropy(v, vv / s);
               offs1 += entrysize;
               offs2 += chunksize;
            }
         }
      } else {
      // SOFTMAX normalization makes things very difficult.
      // Sorry, we do not dare to describe this esoteric math
      // in details.
         offs0 = entrysize * ntotal + chunksize * 2 * nout;
         for (k = 0; k < csize; k++) {
            s = batch4buf->xR[offs0 + k];
            kl = RoundZ(xy->xyR[cstart + k][nin]);
            vv = 0.0;
            offs1 = entrysize * ntotal + k;
            offs2 = entrysize * ntotal + nout * chunksize + k;
            for (i = 0; i < nout; i++) {
               fown = batch4buf->xR[offs1];
               if (i == kl) {
                  deown = fown / s - 1;
               } else {
                  deown = fown / s;
               }
               batch4buf->xR[offs2] = deown;
               vv += deown * fown;
               *e += deown * deown / 2;
               offs1 += chunksize;
               offs2 += chunksize;
            }
            offs1 = entrysize * ntotal + k;
            offs2 = entrysize * ntotal + nout * chunksize + k;
            for (i = 0; i < nout; i++) {
               fown = batch4buf->xR[offs1];
               deown = batch4buf->xR[offs2];
               batch4buf->xR[(ntotal - nout + i) * entrysize + derroroffs + k] = (-vv + deown * fown + deown * (s - fown)) * fown / ae_sqr(s);
               offs1 += chunksize;
               offs2 += chunksize;
            }
         }
      }
   } else {
   // Regression network with sum-of-squares function.
   //
   // For each NOut of last neurons:
   // * calculate difference between actual and desired output
   // * calculate dError/dOut for this neuron (proportional to difference)
   // * store in in last 4 components of entry (these values are used
   //   to start backpropagation)
   // * update error
      for (i = 0; i < nout; i++) {
         v0 = network->columnsigmas.xR[nin + i];
         v1 = network->columnmeans.xR[nin + i];
         entryoffs = entrysize * (ntotal - nout + i);
         offs0 = entryoffs;
         offs1 = entryoffs + derroroffs;
         for (j = 0; j < csize; j++) {
            v = batch4buf->xR[offs0 + j] * v0 + v1 - xy->xyR[cstart + j][nin + i];
            batch4buf->xR[offs1 + j] = v * v0;
            *e += v * v / 2;
         }
      }
   }
// Backpropagation
   for (neuronidx = ntotal - 1; neuronidx >= 0; neuronidx--) {
      entryoffs = entrysize * neuronidx;
      offs = istart + neuronidx * mlpbase_nfieldwidth;
      neurontype = network->structinfo.xZ[offs];
      if (neurontype > 0 || neurontype == -5) {
      // Activation function
         srcneuronidx = network->structinfo.xZ[offs + 2];
         srcentryoffs = entrysize * srcneuronidx;
         offs0 = srcentryoffs + derroroffs;
         offs1 = entryoffs + derroroffs;
         offs2 = entryoffs + dfoffs;
         batch4buf->xR[offs0] += batch4buf->xR[offs1] * batch4buf->xR[offs2];
         batch4buf->xR[offs0 + 1] += batch4buf->xR[offs1 + 1] * batch4buf->xR[offs2 + 1];
         batch4buf->xR[offs0 + 2] += batch4buf->xR[offs1 + 2] * batch4buf->xR[offs2 + 2];
         batch4buf->xR[offs0 + 3] += batch4buf->xR[offs1 + 3] * batch4buf->xR[offs2 + 3];
         continue;
      }
      if (neurontype == 0) {
      // Adaptive summator
         nweights = network->structinfo.xZ[offs + 1];
         srcneuronidx = network->structinfo.xZ[offs + 2];
         srcentryoffs = entrysize * srcneuronidx;
         srcweightidx = network->structinfo.xZ[offs + 3];
         v0 = batch4buf->xR[entryoffs + derroroffs];
         v1 = batch4buf->xR[entryoffs + derroroffs + 1];
         v2 = batch4buf->xR[entryoffs + derroroffs + 2];
         v3 = batch4buf->xR[entryoffs + derroroffs + 3];
         for (j = 0; j < nweights; j++) {
            offs0 = srcentryoffs;
            offs1 = srcentryoffs + derroroffs;
            v = network->weights.xR[srcweightidx];
            hpcbuf->xR[srcweightidx] += batch4buf->xR[offs0] * v0 + batch4buf->xR[offs0 + 1] * v1 + batch4buf->xR[offs0 + 2] * v2 + batch4buf->xR[offs0 + 3] * v3;
            batch4buf->xR[offs1] += v * v0;
            batch4buf->xR[offs1 + 1] += v * v1;
            batch4buf->xR[offs1 + 2] += v * v2;
            batch4buf->xR[offs1 + 3] += v * v3;
            srcentryoffs += entrysize;
            srcweightidx++;
         }
         continue;
      }
      if (neurontype < 0) {
         bflag = false;
         if (neurontype == -2 || neurontype == -3 || neurontype == -4) {
         // Special neuron type, no back-propagation required
            bflag = true;
         }
         ae_assert(bflag, "MLPInternalCalculateGradient: unknown neuron type!");
         continue;
      }
   }
}

// Internal function which actually calculates batch gradient for a subset or
// full dataset, which can be represented in different formats.
//
// THIS FUNCTION IS NOT INTENDED TO BE USED BY ALGLIB USERS!
// ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
void mlpgradbatchx(multilayerperceptron *network, RMatrix *densexy, sparsematrix *sparsexy, ae_int_t datasetsize, ae_int_t datasettype, ZVector *idx, ae_int_t subset0, ae_int_t subset1, ae_int_t subsettype, ae_shared_pool *buf, ae_shared_pool *gradbuf) {
   ae_frame _frame_block;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t rowsize;
   ae_int_t srcidx;
   ae_int_t cstart;
   ae_int_t csize;
   ae_int_t j;
   double problemcost;
   ae_int_t len0;
   ae_int_t len1;
   ae_frame_make(&_frame_block);
   RefObj(mlpbuffers, buf2);
   RefObj(mlpbuffers, pbuf);
   RefObj(smlpgrad, sgrad);
   ae_assert(datasetsize >= 0, "MLPGradBatchX: SetSize<0");
   ae_assert(datasettype == 0 || datasettype == 1, "MLPGradBatchX: DatasetType is incorrect");
   ae_assert(subsettype == 0 || subsettype == 1, "MLPGradBatchX: SubsetType is incorrect");
// Determine network and dataset properties
   mlpproperties(network, &nin, &nout, &wcount);
   if (mlpissoftmax(network)) {
      rowsize = nin + 1;
   } else {
      rowsize = nin + nout;
   }
// Split problem.
//
// Splitting problem allows us to reduce  effect  of  single-precision
// arithmetics (SSE-optimized version of MLPChunkedGradient uses single
// precision  internally, but converts them to  double precision after
// results are exported from HPC buffer to network). Small batches are
// calculated in single precision, results are  aggregated  in  double
// precision, and it allows us to avoid accumulation  of  errors  when
// we process very large batches (tens of thousands of items).
//
// NOTE: it is important to use real arithmetics for ProblemCost
//       because ProblemCost may be larger than MAXINT.
   problemcost = (double)(subset1 - subset0);
   problemcost *= wcount * 2;
// Parallelism was activated if: problemcost >= smpactivationlevel() && subset1 - subset0 >= 2 * mlpbase_microbatchsize
   if (subset1 - subset0 >= 2 * mlpbase_microbatchsize && problemcost > spawnlevel()) {
      len0 = splitlength(subset1 - subset0, mlpbase_microbatchsize), len1 = subset1 - subset0 - len0;
      mlpgradbatchx(network, densexy, sparsexy, datasetsize, datasettype, idx, subset0, subset0 + len0, subsettype, buf, gradbuf);
      mlpgradbatchx(network, densexy, sparsexy, datasetsize, datasettype, idx, subset0 + len0, subset1, subsettype, buf, gradbuf);
      ae_frame_leave();
      return;
   }
// Chunked processing
   ae_shared_pool_retrieve(gradbuf, &_sgrad);
   ae_shared_pool_retrieve(buf, &_pbuf);
   hpcpreparechunkedgradient(&network->weights, wcount, mlpntotal(network), nin, nout, pbuf);
   cstart = subset0;
   while (cstart < subset1) {
   // Determine size of current chunk and copy it to PBuf.XY
      csize = imin2(subset1, cstart + pbuf->chunksize) - cstart;
      for (j = 0; j < csize; j++) {
         srcidx = -1;
         if (subsettype == 0) {
            srcidx = cstart + j;
         }
         if (subsettype == 1) {
            srcidx = idx->xZ[cstart + j];
         }
         ae_assert(srcidx >= 0, "MLPGradBatchX: internal error");
         if (datasettype == 0) {
            ae_v_move(pbuf->xy.xyR[j], 1, densexy->xyR[srcidx], 1, rowsize);
         }
         if (datasettype == 1) {
            sparsegetrow(sparsexy, srcidx, &pbuf->xyrow);
            ae_v_move(pbuf->xy.xyR[j], 1, pbuf->xyrow.xR, 1, rowsize);
         }
      }
   // Process chunk and advance line pointer
      mlpbase_mlpchunkedgradient(network, &pbuf->xy, 0, csize, &pbuf->batch4buf, &pbuf->hpcbuf, &sgrad->f, false);
      cstart += pbuf->chunksize;
   }
   hpcfinalizechunkedgradient(pbuf, &sgrad->g);
   ae_shared_pool_recycle(buf, &_pbuf);
   ae_shared_pool_recycle(gradbuf, &_sgrad);
   ae_frame_leave();
}

// Batch gradient calculation for a set of inputs/outputs
// (natural error function is used)
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     XY      -   set of inputs/outputs; one sample = one row;
//                 first NIn columns contain inputs,
//                 next NOut columns - desired outputs.
//     SSize   -   number of elements in XY
//     Grad    -   possibly preallocated array. If size of array is smaller
//                 than WCount, it will be reallocated. It is recommended to
//                 reuse previously allocated array to reduce allocation
//                 overhead.
//
// Outputs:
//     E       -   error function, sum-of-squares for regression networks,
//                 cross-entropy for classification networks.
//     Grad    -   gradient of E with respect to weights of network, array[WCount]
// ALGLIB: Copyright 04.11.2007 by Sergey Bochkanov
// API: void mlpgradnbatch(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize, double &e, real_1d_array &grad);
void mlpgradnbatch(multilayerperceptron *network, RMatrix *xy, ae_int_t ssize, double *e, RVector *grad) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_frame_make(&_frame_block);
   *e = 0;
   RefObj(mlpbuffers, pbuf);
// Alloc
   mlpproperties(network, &nin, &nout, &wcount);
   ae_shared_pool_retrieve(&network->buf, &_pbuf);
   hpcpreparechunkedgradient(&network->weights, wcount, mlpntotal(network), nin, nout, pbuf);
   vectorsetlengthatleast(grad, wcount);
   for (i = 0; i < wcount; i++) {
      grad->xR[i] = 0.0;
   }
   *e = 0.0;
   i = 0;
   while (i < ssize) {
      mlpbase_mlpchunkedgradient(network, xy, i, imin2(ssize, i + pbuf->chunksize) - i, &pbuf->batch4buf, &pbuf->hpcbuf, e, true);
      i += pbuf->chunksize;
   }
   hpcfinalizechunkedgradient(pbuf, grad);
   ae_shared_pool_recycle(&network->buf, &_pbuf);
   ae_frame_leave();
}

// Internal subroutine for Hessian calculation.
//
// WARNING! Unspeakable math far beyong human capabilities :)
static void mlpbase_mlphessianbatchinternal(multilayerperceptron *network, RMatrix *xy, ae_int_t ssize, bool naturalerr, double *e, RVector *grad, RMatrix *h) {
   ae_frame _frame_block;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntotal;
   ae_int_t istart;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t kl;
   ae_int_t offs;
   ae_int_t n1;
   ae_int_t n2;
   ae_int_t w1;
   ae_int_t w2;
   double s;
   double t;
   double v;
   double et;
   bool bflag;
   double f;
   double df;
   double d2f;
   double deidyj;
   double mx;
   double q;
   double z;
   double s2;
   double expi;
   double expj;
   ae_frame_make(&_frame_block);
   *e = 0;
   NewVector(x, 0, DT_REAL);
   NewVector(desiredy, 0, DT_REAL);
   NewVector(gt, 0, DT_REAL);
   NewVector(zeros, 0, DT_REAL);
   NewMatrix(rx, 0, 0, DT_REAL);
   NewMatrix(ry, 0, 0, DT_REAL);
   NewMatrix(rdx, 0, 0, DT_REAL);
   NewMatrix(rdy, 0, 0, DT_REAL);
   mlpproperties(network, &nin, &nout, &wcount);
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
// Prepare
   ae_vector_set_length(&x, nin);
   ae_vector_set_length(&desiredy, nout);
   ae_vector_set_length(&zeros, wcount);
   ae_vector_set_length(&gt, wcount);
   ae_matrix_set_length(&rx, ntotal + nout, wcount);
   ae_matrix_set_length(&ry, ntotal + nout, wcount);
   ae_matrix_set_length(&rdx, ntotal + nout, wcount);
   ae_matrix_set_length(&rdy, ntotal + nout, wcount);
   *e = 0.0;
   for (i = 0; i < wcount; i++) {
      zeros.xR[i] = 0.0;
   }
   ae_v_move(grad->xR, 1, zeros.xR, 1, wcount);
   for (i = 0; i < wcount; i++) {
      ae_v_move(h->xyR[i], 1, zeros.xR, 1, wcount);
   }
// Process
   for (k = 0; k < ssize; k++) {
   // Process vector with MLPGradN.
   // Now Neurons, DFDNET and DError contains results of the last run.
      ae_v_move(x.xR, 1, xy->xyR[k], 1, nin);
      if (mlpissoftmax(network)) {
      // class labels outputs
         kl = RoundZ(xy->xyR[k][nin]);
         for (i = 0; i < nout; i++) {
            if (i == kl) {
               desiredy.xR[i] = 1.0;
            } else {
               desiredy.xR[i] = 0.0;
            }
         }
      } else {
      // real outputs
         ae_v_move(desiredy.xR, 1, &xy->xyR[k][nin], 1, nout);
      }
      if (naturalerr) {
         mlpgradn(network, &x, &desiredy, &et, &gt);
      } else {
         mlpgrad(network, &x, &desiredy, &et, &gt);
      }
   // grad, error
      *e += et;
      ae_v_add(grad->xR, 1, gt.xR, 1, wcount);
   // Hessian.
   // Forward pass of the R-algorithm
      for (i = 0; i < ntotal; i++) {
         offs = istart + i * mlpbase_nfieldwidth;
         ae_v_move(rx.xyR[i], 1, zeros.xR, 1, wcount);
         ae_v_move(ry.xyR[i], 1, zeros.xR, 1, wcount);
         if (network->structinfo.xZ[offs] > 0 || network->structinfo.xZ[offs] == -5) {
         // Activation function
            n1 = network->structinfo.xZ[offs + 2];
            ae_v_move(rx.xyR[i], 1, ry.xyR[n1], 1, wcount);
            v = network->dfdnet.xR[i];
            ae_v_moved(ry.xyR[i], 1, rx.xyR[i], 1, wcount, v);
            continue;
         }
         if (network->structinfo.xZ[offs] == 0) {
         // Adaptive summator
            n1 = network->structinfo.xZ[offs + 2];
            n2 = n1 + network->structinfo.xZ[offs + 1] - 1;
            w1 = network->structinfo.xZ[offs + 3];
            w2 = w1 + network->structinfo.xZ[offs + 1] - 1;
            for (j = n1; j <= n2; j++) {
               v = network->weights.xR[w1 + j - n1];
               ae_v_addd(rx.xyR[i], 1, ry.xyR[j], 1, wcount, v);
               rx.xyR[i][w1 + j - n1] += network->neurons.xR[j];
            }
            ae_v_move(ry.xyR[i], 1, rx.xyR[i], 1, wcount);
            continue;
         }
         if (network->structinfo.xZ[offs] < 0) {
            bflag = true;
            if (network->structinfo.xZ[offs] == -2) {
            // input neuron, left unchanged
               bflag = false;
            }
            if (network->structinfo.xZ[offs] == -3) {
            // "-1" neuron, left unchanged
               bflag = false;
            }
            if (network->structinfo.xZ[offs] == -4) {
            // "0" neuron, left unchanged
               bflag = false;
            }
            ae_assert(!bflag, "MLPHessianNBatch: internal error - unknown neuron type!");
            continue;
         }
      }
   // Hessian. Backward pass of the R-algorithm.
   //
   // Stage 1. Initialize RDY
      for (i = 0; i < ntotal + nout; i++) {
         ae_v_move(rdy.xyR[i], 1, zeros.xR, 1, wcount);
      }
      if (network->structinfo.xZ[6] == 0) {
      // Standardisation.
      //
      // In context of the Hessian calculation standardisation
      // is considered as additional layer with weightless
      // activation function:
      //
      // F(NET) := Sigma*NET
      //
      // So we add one more layer to forward pass, and
      // make forward/backward pass through this layer.
         for (i = 0; i < nout; i++) {
            n1 = ntotal - nout + i;
            n2 = ntotal + i;
         // Forward pass from N1 to N2
            ae_v_move(rx.xyR[n2], 1, ry.xyR[n1], 1, wcount);
            v = network->columnsigmas.xR[nin + i];
            ae_v_moved(ry.xyR[n2], 1, rx.xyR[n2], 1, wcount, v);
         // Initialization of RDY
            ae_v_move(rdy.xyR[n2], 1, ry.xyR[n2], 1, wcount);
         // Backward pass from N2 to N1:
         // 1. Calculate R(dE/dX).
         // 2. No R(dE/dWij) is needed since weight of activation neuron
         //    is fixed to 1. So we can update R(dE/dY) for
         //    the connected neuron (note that Vij=0, Wij=1)
            df = network->columnsigmas.xR[nin + i];
            ae_v_moved(rdx.xyR[n2], 1, rdy.xyR[n2], 1, wcount, df);
            ae_v_add(rdy.xyR[n1], 1, rdx.xyR[n2], 1, wcount);
         }
      } else {
      // Softmax.
      //
      // Initialize RDY using generalized expression for ei'(yi)
      // (see expression (9) from p. 5 of "Fast Exact Multiplication by the Hessian").
      //
      // When we are working with softmax network, generalized
      // expression for ei'(yi) is used because softmax
      // normalization leads to ei, which depends on all y's
         if (naturalerr) {
         // softmax + cross-entropy.
         // We have:
         //
         // S = sum(exp(yk)),
         // ei = sum(trn)*exp(yi)/S-trn_i
         //
         // j=i:   d(ei)/d(yj) = T*exp(yi)*(S-exp(yi))/S^2
         // j != i:  d(ei)/d(yj) = -T*exp(yi)*exp(yj)/S^2
            t = 0.0;
            for (i = 0; i < nout; i++) {
               t += desiredy.xR[i];
            }
            mx = network->neurons.xR[ntotal - nout];
            for (i = 0; i < nout; i++) {
               mx = rmax2(mx, network->neurons.xR[ntotal - nout + i]);
            }
            s = 0.0;
            for (i = 0; i < nout; i++) {
               network->nwbuf.xR[i] = exp(network->neurons.xR[ntotal - nout + i] - mx);
               s += network->nwbuf.xR[i];
            }
            for (i = 0; i < nout; i++) {
               for (j = 0; j < nout; j++) {
                  if (j == i) {
                     deidyj = t * network->nwbuf.xR[i] * (s - network->nwbuf.xR[i]) / ae_sqr(s);
                     ae_v_addd(rdy.xyR[ntotal - nout + i], 1, ry.xyR[ntotal - nout + i], 1, wcount, deidyj);
                  } else {
                     deidyj = -t * network->nwbuf.xR[i] * network->nwbuf.xR[j] / ae_sqr(s);
                     ae_v_addd(rdy.xyR[ntotal - nout + i], 1, ry.xyR[ntotal - nout + j], 1, wcount, deidyj);
                  }
               }
            }
         } else {
         // For a softmax + squared error we have expression
         // far beyond human imagination so we dont even try
         // to comment on it. Just enjoy the code...
         //
         // P.S. That's why "natural error" is called "natural" -
         // compact beatiful expressions, fast code....
            mx = network->neurons.xR[ntotal - nout];
            for (i = 0; i < nout; i++) {
               mx = rmax2(mx, network->neurons.xR[ntotal - nout + i]);
            }
            s = 0.0;
            s2 = 0.0;
            for (i = 0; i < nout; i++) {
               network->nwbuf.xR[i] = exp(network->neurons.xR[ntotal - nout + i] - mx);
               s += network->nwbuf.xR[i];
               s2 += ae_sqr(network->nwbuf.xR[i]);
            }
            q = 0.0;
            for (i = 0; i < nout; i++) {
               q += (network->y.xR[i] - desiredy.xR[i]) * network->nwbuf.xR[i];
            }
            for (i = 0; i < nout; i++) {
               z = -q + (network->y.xR[i] - desiredy.xR[i]) * s;
               expi = network->nwbuf.xR[i];
               for (j = 0; j < nout; j++) {
                  expj = network->nwbuf.xR[j];
                  if (j == i) {
                     deidyj = expi / ae_sqr(s) * ((z + expi) * (s - 2 * expi) / s + expi * s2 / ae_sqr(s));
                  } else {
                     deidyj = expi * expj / ae_sqr(s) * (s2 / ae_sqr(s) - 2 * z / s - (expi + expj) / s + (network->y.xR[i] - desiredy.xR[i]) - (network->y.xR[j] - desiredy.xR[j]));
                  }
                  ae_v_addd(rdy.xyR[ntotal - nout + i], 1, ry.xyR[ntotal - nout + j], 1, wcount, deidyj);
               }
            }
         }
      }
   // Hessian. Backward pass of the R-algorithm
   //
   // Stage 2. Process.
      for (i = ntotal - 1; i >= 0; i--) {
      // Possible variants:
      // 1. Activation function
      // 2. Adaptive summator
      // 3. Special neuron
         offs = istart + i * mlpbase_nfieldwidth;
         if (network->structinfo.xZ[offs] > 0 || network->structinfo.xZ[offs] == -5) {
            n1 = network->structinfo.xZ[offs + 2];
         // First, calculate R(dE/dX).
            mlpactivationfunction(network->neurons.xR[n1], network->structinfo.xZ[offs], &f, &df, &d2f);
            v = d2f * network->derror.xR[i];
            ae_v_moved(rdx.xyR[i], 1, rdy.xyR[i], 1, wcount, df);
            ae_v_addd(rdx.xyR[i], 1, rx.xyR[i], 1, wcount, v);
         // No R(dE/dWij) is needed since weight of activation neuron
         // is fixed to 1.
         //
         // So we can update R(dE/dY) for the connected neuron.
         // (note that Vij=0, Wij=1)
            ae_v_add(rdy.xyR[n1], 1, rdx.xyR[i], 1, wcount);
            continue;
         }
         if (network->structinfo.xZ[offs] == 0) {
         // Adaptive summator
            n1 = network->structinfo.xZ[offs + 2];
            n2 = n1 + network->structinfo.xZ[offs + 1] - 1;
            w1 = network->structinfo.xZ[offs + 3];
            w2 = w1 + network->structinfo.xZ[offs + 1] - 1;
         // First, calculate R(dE/dX).
            ae_v_move(rdx.xyR[i], 1, rdy.xyR[i], 1, wcount);
         // Then, calculate R(dE/dWij)
            for (j = w1; j <= w2; j++) {
               v = network->neurons.xR[n1 + j - w1];
               ae_v_addd(h->xyR[j], 1, rdx.xyR[i], 1, wcount, v);
               v = network->derror.xR[i];
               ae_v_addd(h->xyR[j], 1, ry.xyR[n1 + j - w1], 1, wcount, v);
            }
         // And finally, update R(dE/dY) for connected neurons.
            for (j = w1; j <= w2; j++) {
               v = network->weights.xR[j];
               ae_v_addd(rdy.xyR[n1 + j - w1], 1, rdx.xyR[i], 1, wcount, v);
               rdy.xyR[n1 + j - w1][j] += network->derror.xR[i];
            }
            continue;
         }
         if (network->structinfo.xZ[offs] < 0) {
            bflag = false;
            if (network->structinfo.xZ[offs] == -2 || network->structinfo.xZ[offs] == -3 || network->structinfo.xZ[offs] == -4) {
            // Special neuron type, no back-propagation required
               bflag = true;
            }
            ae_assert(bflag, "MLPHessianNBatch: unknown neuron type!");
            continue;
         }
      }
   }
   ae_frame_leave();
}

// Batch Hessian calculation (natural error function) using R-algorithm.
// Internal subroutine.
// Hessian calculation based on R-algorithm described in
//      "Fast Exact Multiplication by the Hessian",
//      B. A. Pearlmutter,
//      Neural Computation, 1994.
// ALGLIB: Copyright 26.01.2008 by Sergey Bochkanov
// API: void mlphessiannbatch(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize, double &e, real_1d_array &grad, real_2d_array &h);
void mlphessiannbatch(multilayerperceptron *network, RMatrix *xy, ae_int_t ssize, double *e, RVector *grad, RMatrix *h) {
   *e = 0;
   mlpbase_mlphessianbatchinternal(network, xy, ssize, true, e, grad, h);
}

// Batch Hessian calculation using R-algorithm.
// Internal subroutine.
// Hessian calculation based on R-algorithm described in
//      "Fast Exact Multiplication by the Hessian",
//      B. A. Pearlmutter,
//      Neural Computation, 1994.
// ALGLIB: Copyright 26.01.2008 by Sergey Bochkanov
// API: void mlphessianbatch(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize, double &e, real_1d_array &grad, real_2d_array &h);
void mlphessianbatch(multilayerperceptron *network, RMatrix *xy, ae_int_t ssize, double *e, RVector *grad, RMatrix *h) {
   *e = 0;
   mlpbase_mlphessianbatchinternal(network, xy, ssize, false, e, grad, h);
}

// Internal subroutine, shouldn't be called by user.
void mlpinternalprocessvector(ZVector *structinfo, RVector *weights, RVector *columnmeans, RVector *columnsigmas, RVector *neurons, RVector *dfdnet, RVector *x, RVector *y) {
   ae_int_t i;
   ae_int_t n1;
   ae_int_t w1;
   ae_int_t w2;
   ae_int_t ntotal;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t istart;
   ae_int_t offs;
   double net;
   double f;
   double df;
   double d2f;
   double mx;
   bool perr;
// Read network geometry
   nin = structinfo->xZ[1];
   nout = structinfo->xZ[2];
   ntotal = structinfo->xZ[3];
   istart = structinfo->xZ[5];
// Inputs standartisation and putting in the network
   for (i = 0; i < nin; i++) {
      if (columnsigmas->xR[i] != 0.0) {
         neurons->xR[i] = (x->xR[i] - columnmeans->xR[i]) / columnsigmas->xR[i];
      } else {
         neurons->xR[i] = x->xR[i] - columnmeans->xR[i];
      }
   }
// Process network
   for (i = 0; i < ntotal; i++) {
      offs = istart + i * mlpbase_nfieldwidth;
      if (structinfo->xZ[offs] > 0 || structinfo->xZ[offs] == -5) {
      // Activation function
         mlpactivationfunction(neurons->xR[structinfo->xZ[offs + 2]], structinfo->xZ[offs], &f, &df, &d2f);
         neurons->xR[i] = f;
         dfdnet->xR[i] = df;
         continue;
      }
      if (structinfo->xZ[offs] == 0) {
      // Adaptive summator
         n1 = structinfo->xZ[offs + 2];
      // ae_int_t n2 = n1 + structinfo->xZ[offs + 1] - 1; //(@) Unused.
         w1 = structinfo->xZ[offs + 3];
         w2 = w1 + structinfo->xZ[offs + 1] - 1;
         net = ae_v_dotproduct(&weights->xR[w1], 1, &neurons->xR[n1], 1, w2 - w1 + 1);
         neurons->xR[i] = net;
         dfdnet->xR[i] = 1.0;
         continue;
      }
      if (structinfo->xZ[offs] < 0) {
         perr = true;
         if (structinfo->xZ[offs] == -2) {
         // input neuron, left unchanged
            perr = false;
         }
         if (structinfo->xZ[offs] == -3) {
         // "-1" neuron
            neurons->xR[i] = -1.0;
            perr = false;
         }
         if (structinfo->xZ[offs] == -4) {
         // "0" neuron
            neurons->xR[i] = 0.0;
            perr = false;
         }
         ae_assert(!perr, "MLPInternalProcessVector: internal error - unknown neuron type!");
         continue;
      }
   }
// Extract result
   ae_v_move(y->xR, 1, &neurons->xR[ntotal - nout], 1, nout);
// Softmax post-processing or standardisation if needed
   ae_assert(structinfo->xZ[6] == 0 || structinfo->xZ[6] == 1, "MLPInternalProcessVector: unknown normalization type!");
   if (structinfo->xZ[6] == 1) {
   // Softmax
      mx = y->xR[0];
      for (i = 1; i < nout; i++) {
         mx = rmax2(mx, y->xR[i]);
      }
      net = 0.0;
      for (i = 0; i < nout; i++) {
         y->xR[i] = exp(y->xR[i] - mx);
         net += y->xR[i];
      }
      for (i = 0; i < nout; i++) {
         y->xR[i] /= net;
      }
   } else {
   // Standardisation
      for (i = 0; i < nout; i++) {
         y->xR[i] = y->xR[i] * columnsigmas->xR[nin + i] + columnmeans->xR[nin + i];
      }
   }
}

// Calculation of all types of errors on subset of dataset.
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     XY      -   original dataset; one sample = one row;
//                 first NIn columns contain inputs,
//                 next NOut columns - desired outputs.
//     SetSize -   real size of XY, SetSize >= 0;
//     Subset  -   subset of SubsetSize elements, array[SubsetSize];
//     SubsetSize- number of elements in Subset[] array:
//                 * if SubsetSize > 0, rows of XY with indices Subset[0]...
//                   ...Subset[SubsetSize-1] are processed
//                 * if SubsetSize=0, zeros are returned
//                 * if SubsetSize < 0, entire dataset is  processed;  Subset[]
//                   array is ignored in this case.
//
// Outputs:
//     Rep     -   it contains all type of errors.
// ALGLIB: Copyright 04.09.2012 by Sergey Bochkanov
// API: void mlpallerrorssubset(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t setsize, const integer_1d_array &subset, const ae_int_t subsetsize, modelerrors &rep);
void mlpallerrorssubset(multilayerperceptron *network, RMatrix *xy, ae_int_t setsize, ZVector *subset, ae_int_t subsetsize, modelerrors *rep) {
   ae_int_t idx0;
   ae_int_t idx1;
   ae_int_t idxtype;
   SetObj(modelerrors, rep);
   ae_assert(xy->rows >= setsize, "MLPAllErrorsSubset: XY has less than SetSize rows");
   if (setsize > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(xy->cols >= mlpgetinputscount(network) + 1, "MLPAllErrorsSubset: XY has less than NIn+1 columns");
      } else {
         ae_assert(xy->cols >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAllErrorsSubset: XY has less than NIn+NOut columns");
      }
   }
   if (subsetsize >= 0) {
      idx0 = 0;
      idx1 = subsetsize;
      idxtype = 1;
   } else {
      idx0 = 0;
      idx1 = setsize;
      idxtype = 0;
   }
   mlpallerrorsx(network, xy, &network->dummysxy, setsize, 0, subset, idx0, idx1, idxtype, &network->buf, rep);
}

// Calculation of all types of errors on subset of dataset.
//
// Inputs:
//     Network -   network initialized with one of the network creation funcs
//     XY      -   original dataset given by sparse matrix;
//                 one sample = one row;
//                 first NIn columns contain inputs,
//                 next NOut columns - desired outputs.
//     SetSize -   real size of XY, SetSize >= 0;
//     Subset  -   subset of SubsetSize elements, array[SubsetSize];
//     SubsetSize- number of elements in Subset[] array:
//                 * if SubsetSize > 0, rows of XY with indices Subset[0]...
//                   ...Subset[SubsetSize-1] are processed
//                 * if SubsetSize=0, zeros are returned
//                 * if SubsetSize < 0, entire dataset is  processed;  Subset[]
//                   array is ignored in this case.
//
// Outputs:
//     Rep     -   it contains all type of errors.
// ALGLIB: Copyright 04.09.2012 by Sergey Bochkanov
// API: void mlpallerrorssparsesubset(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t setsize, const integer_1d_array &subset, const ae_int_t subsetsize, modelerrors &rep);
void mlpallerrorssparsesubset(multilayerperceptron *network, sparsematrix *xy, ae_int_t setsize, ZVector *subset, ae_int_t subsetsize, modelerrors *rep) {
   ae_int_t idx0;
   ae_int_t idx1;
   ae_int_t idxtype;
   SetObj(modelerrors, rep);
   ae_assert(sparseiscrs(xy), "MLPAllErrorsSparseSubset: XY is not in CRS format.");
   ae_assert(sparsegetnrows(xy) >= setsize, "MLPAllErrorsSparseSubset: XY has less than SetSize rows");
   if (setsize > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPAllErrorsSparseSubset: XY has less than NIn+1 columns");
      } else {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPAllErrorsSparseSubset: XY has less than NIn+NOut columns");
      }
   }
   if (subsetsize >= 0) {
      idx0 = 0;
      idx1 = subsetsize;
      idxtype = 1;
   } else {
      idx0 = 0;
      idx1 = setsize;
      idxtype = 0;
   }
   mlpallerrorsx(network, &network->dummydxy, xy, setsize, 1, subset, idx0, idx1, idxtype, &network->buf, rep);
}

// Error of the neural network on subset of dataset.
//
// Inputs:
//     Network   -     neural network;
//     XY        -     training  set,  see  below  for  information  on   the
//                     training set format;
//     SetSize   -     real size of XY, SetSize >= 0;
//     Subset    -     subset of SubsetSize elements, array[SubsetSize];
//     SubsetSize-     number of elements in Subset[] array:
//                     * if SubsetSize > 0, rows of XY with indices Subset[0]...
//                       ...Subset[SubsetSize-1] are processed
//                     * if SubsetSize=0, zeros are returned
//                     * if SubsetSize < 0, entire dataset is  processed;  Subset[]
//                       array is ignored in this case.
//
// Result:
//     sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 04.09.2012 by Sergey Bochkanov
// API: double mlperrorsubset(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t setsize, const integer_1d_array &subset, const ae_int_t subsetsize);
double mlperrorsubset(multilayerperceptron *network, RMatrix *xy, ae_int_t setsize, ZVector *subset, ae_int_t subsetsize) {
   ae_int_t idx0;
   ae_int_t idx1;
   ae_int_t idxtype;
   double result;
   ae_assert(xy->rows >= setsize, "MLPErrorSubset: XY has less than SetSize rows");
   if (setsize > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(xy->cols >= mlpgetinputscount(network) + 1, "MLPErrorSubset: XY has less than NIn+1 columns");
      } else {
         ae_assert(xy->cols >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPErrorSubset: XY has less than NIn+NOut columns");
      }
   }
   if (subsetsize >= 0) {
      idx0 = 0;
      idx1 = subsetsize;
      idxtype = 1;
   } else {
      idx0 = 0;
      idx1 = setsize;
      idxtype = 0;
   }
   mlpallerrorsx(network, xy, &network->dummysxy, setsize, 0, subset, idx0, idx1, idxtype, &network->buf, &network->err);
   result = ae_sqr(network->err.rmserror) * (idx1 - idx0) * mlpgetoutputscount(network) / 2;
   return result;
}

// Error of the neural network on subset of sparse dataset.
//
// Inputs:
//     Network   -     neural network;
//     XY        -     training  set,  see  below  for  information  on   the
//                     training set format. This function checks  correctness
//                     of  the  dataset  (no  NANs/INFs,  class  numbers  are
//                     correct) and throws exception when  incorrect  dataset
//                     is passed.  Sparse  matrix  must  use  CRS  format for
//                     storage.
//     SetSize   -     real size of XY, SetSize >= 0;
//                     it is used when SubsetSize < 0;
//     Subset    -     subset of SubsetSize elements, array[SubsetSize];
//     SubsetSize-     number of elements in Subset[] array:
//                     * if SubsetSize > 0, rows of XY with indices Subset[0]...
//                       ...Subset[SubsetSize-1] are processed
//                     * if SubsetSize=0, zeros are returned
//                     * if SubsetSize < 0, entire dataset is  processed;  Subset[]
//                       array is ignored in this case.
//
// Result:
//     sum-of-squares error, SUM(sqr(y[i]-desired_y[i])/2)
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// dataset format is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 04.09.2012 by Sergey Bochkanov
// API: double mlperrorsparsesubset(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t setsize, const integer_1d_array &subset, const ae_int_t subsetsize);
double mlperrorsparsesubset(multilayerperceptron *network, sparsematrix *xy, ae_int_t setsize, ZVector *subset, ae_int_t subsetsize) {
   ae_int_t idx0;
   ae_int_t idx1;
   ae_int_t idxtype;
   double result;
   ae_assert(sparseiscrs(xy), "MLPErrorSparseSubset: XY is not in CRS format.");
   ae_assert(sparsegetnrows(xy) >= setsize, "MLPErrorSparseSubset: XY has less than SetSize rows");
   if (setsize > 0) {
      if (mlpissoftmax(network)) {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + 1, "MLPErrorSparseSubset: XY has less than NIn+1 columns");
      } else {
         ae_assert(sparsegetncols(xy) >= mlpgetinputscount(network) + mlpgetoutputscount(network), "MLPErrorSparseSubset: XY has less than NIn+NOut columns");
      }
   }
   if (subsetsize >= 0) {
      idx0 = 0;
      idx1 = subsetsize;
      idxtype = 1;
   } else {
      idx0 = 0;
      idx1 = setsize;
      idxtype = 0;
   }
   mlpallerrorsx(network, &network->dummydxy, xy, setsize, 1, subset, idx0, idx1, idxtype, &network->buf, &network->err);
   result = ae_sqr(network->err.rmserror) * (idx1 - idx0) * mlpgetoutputscount(network) / 2;
   return result;
}

static void mlpbase_mlpchunkedprocess(multilayerperceptron *network, RMatrix *xy, ae_int_t cstart, ae_int_t csize, RVector *batch4buf, RVector *hpcbuf) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t ntotal;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t offs;
   double f;
   double df;
   double d2f;
   double v;
   bool bflag;
   ae_int_t istart;
   ae_int_t entrysize;
   ae_int_t entryoffs;
   ae_int_t neuronidx;
   ae_int_t srcentryoffs;
   ae_int_t srcneuronidx;
   ae_int_t srcweightidx;
   ae_int_t neurontype;
   ae_int_t nweights;
   ae_int_t offs0;
   double v0;
   double v1;
   double v2;
   double v3;
   double s0;
   double s1;
   double s2;
   double s3;
   ae_int_t chunksize;
   chunksize = 4;
   ae_assert(csize <= chunksize, "MLPChunkedProcess: internal error (CSize>ChunkSize)");
// Try to use HPC core, if possible
   if (hpcchunkedprocess(&network->weights, &network->structinfo, &network->columnmeans, &network->columnsigmas, xy, cstart, csize, batch4buf, hpcbuf)) {
      return;
   }
// Read network geometry, prepare data
   nin = network->structinfo.xZ[1];
   nout = network->structinfo.xZ[2];
   ntotal = network->structinfo.xZ[3];
   istart = network->structinfo.xZ[5];
   entrysize = 4;
// Fill Batch4Buf by zeros.
//
// THIS STAGE IS VERY IMPORTANT!
//
// We fill all components of entry - neuron values, dF/dNET, dError/dF.
// It allows us to easily handle  situations  when  CSize<ChunkSize  by
// simply  working  with  ALL  components  of  Batch4Buf,  without ever
// looking at CSize.
   for (i = 0; i < entrysize * ntotal; i++) {
      batch4buf->xR[i] = 0.0;
   }
// Forward pass:
// 1. Load data into Batch4Buf. If CSize<ChunkSize, data are padded by zeros.
// 2. Perform forward pass through network
   for (i = 0; i < nin; i++) {
      entryoffs = entrysize * i;
      for (j = 0; j < csize; j++) {
         if (network->columnsigmas.xR[i] != 0.0) {
            batch4buf->xR[entryoffs + j] = (xy->xyR[cstart + j][i] - network->columnmeans.xR[i]) / network->columnsigmas.xR[i];
         } else {
            batch4buf->xR[entryoffs + j] = xy->xyR[cstart + j][i] - network->columnmeans.xR[i];
         }
      }
   }
   for (neuronidx = 0; neuronidx < ntotal; neuronidx++) {
      entryoffs = entrysize * neuronidx;
      offs = istart + neuronidx * mlpbase_nfieldwidth;
      neurontype = network->structinfo.xZ[offs];
      if (neurontype > 0 || neurontype == -5) {
      // "activation function" neuron, which takes value of neuron SrcNeuronIdx
      // and applies activation function to it.
      //
      // This neuron has no weights and no tunable parameters.
         srcneuronidx = network->structinfo.xZ[offs + 2];
         srcentryoffs = entrysize * srcneuronidx;
         mlpactivationfunction(batch4buf->xR[srcentryoffs], neurontype, &f, &df, &d2f);
         batch4buf->xR[entryoffs] = f;
         mlpactivationfunction(batch4buf->xR[srcentryoffs + 1], neurontype, &f, &df, &d2f);
         batch4buf->xR[entryoffs + 1] = f;
         mlpactivationfunction(batch4buf->xR[srcentryoffs + 2], neurontype, &f, &df, &d2f);
         batch4buf->xR[entryoffs + 2] = f;
         mlpactivationfunction(batch4buf->xR[srcentryoffs + 3], neurontype, &f, &df, &d2f);
         batch4buf->xR[entryoffs + 3] = f;
         continue;
      }
      if (neurontype == 0) {
      // "adaptive summator" neuron, whose output is a weighted sum of inputs.
      // It has weights, but has no activation function.
         nweights = network->structinfo.xZ[offs + 1];
         srcneuronidx = network->structinfo.xZ[offs + 2];
         srcentryoffs = entrysize * srcneuronidx;
         srcweightidx = network->structinfo.xZ[offs + 3];
         v0 = 0.0;
         v1 = 0.0;
         v2 = 0.0;
         v3 = 0.0;
         for (j = 0; j < nweights; j++) {
            v = network->weights.xR[srcweightidx];
            srcweightidx++;
            v0 += v * batch4buf->xR[srcentryoffs];
            v1 += v * batch4buf->xR[srcentryoffs + 1];
            v2 += v * batch4buf->xR[srcentryoffs + 2];
            v3 += v * batch4buf->xR[srcentryoffs + 3];
            srcentryoffs += entrysize;
         }
         batch4buf->xR[entryoffs] = v0;
         batch4buf->xR[entryoffs + 1] = v1;
         batch4buf->xR[entryoffs + 2] = v2;
         batch4buf->xR[entryoffs + 3] = v3;
         continue;
      }
      if (neurontype < 0) {
         bflag = false;
         if (neurontype == -2) {
         // Input neuron, left unchanged
            bflag = true;
         }
         if (neurontype == -3) {
         // "-1" neuron
            batch4buf->xR[entryoffs] = -1.0;
            batch4buf->xR[entryoffs + 1] = -1.0;
            batch4buf->xR[entryoffs + 2] = -1.0;
            batch4buf->xR[entryoffs + 3] = -1.0;
            bflag = true;
         }
         if (neurontype == -4) {
         // "0" neuron
            batch4buf->xR[entryoffs] = 0.0;
            batch4buf->xR[entryoffs + 1] = 0.0;
            batch4buf->xR[entryoffs + 2] = 0.0;
            batch4buf->xR[entryoffs + 3] = 0.0;
            bflag = true;
         }
         ae_assert(bflag, "MLPChunkedProcess: internal error - unknown neuron type!");
         continue;
      }
   }
// SOFTMAX normalization or scaling.
   ae_assert(network->structinfo.xZ[6] == 0 || network->structinfo.xZ[6] == 1, "MLPChunkedProcess: unknown normalization type!");
   if (network->structinfo.xZ[6] == 1) {
   // SOFTMAX-normalized network.
   //
   // First,  calculate (V0,V1,V2,V3)  -  component-wise  maximum
   // of output neurons. This vector of maximum  values  will  be
   // used for normalization  of  outputs  prior  to  calculating
   // exponentials.
   //
   // NOTE: the only purpose of this stage is to prevent overflow
   //       during calculation of exponentials.  With  this stage
   //       we  make  sure  that  all exponentials are calculated
   //       with non-positive argument. If you load (0,0,0,0)  to
   //       (V0,V1,V2,V3), your program will continue  working  -
   //       although with less robustness.
      entryoffs = entrysize * (ntotal - nout);
      v0 = batch4buf->xR[entryoffs];
      v1 = batch4buf->xR[entryoffs + 1];
      v2 = batch4buf->xR[entryoffs + 2];
      v3 = batch4buf->xR[entryoffs + 3];
      entryoffs += entrysize;
      for (i = 1; i < nout; i++) {
         v = batch4buf->xR[entryoffs];
         if (v > v0) {
            v0 = v;
         }
         v = batch4buf->xR[entryoffs + 1];
         if (v > v1) {
            v1 = v;
         }
         v = batch4buf->xR[entryoffs + 2];
         if (v > v2) {
            v2 = v;
         }
         v = batch4buf->xR[entryoffs + 3];
         if (v > v3) {
            v3 = v;
         }
         entryoffs += entrysize;
      }
   // Then,  calculate exponentials and place them to part of the
   // array which  is  located  past  the  last  entry.  We  also
   // calculate sum of exponentials.
      entryoffs = entrysize * (ntotal - nout);
      offs0 = entrysize * ntotal;
      s0 = 0.0;
      s1 = 0.0;
      s2 = 0.0;
      s3 = 0.0;
      for (i = 0; i < nout; i++) {
         v = exp(batch4buf->xR[entryoffs] - v0);
         s0 += v;
         batch4buf->xR[offs0] = v;
         v = exp(batch4buf->xR[entryoffs + 1] - v1);
         s1 += v;
         batch4buf->xR[offs0 + 1] = v;
         v = exp(batch4buf->xR[entryoffs + 2] - v2);
         s2 += v;
         batch4buf->xR[offs0 + 2] = v;
         v = exp(batch4buf->xR[entryoffs + 3] - v3);
         s3 += v;
         batch4buf->xR[offs0 + 3] = v;
         entryoffs += entrysize;
         offs0 += chunksize;
      }
   // Write SOFTMAX-normalized values to the output array.
      offs0 = entrysize * ntotal;
      for (i = 0; i < nout; i++) {
         if (csize > 0) {
            xy->xyR[cstart][nin + i] = batch4buf->xR[offs0] / s0;
         }
         if (csize > 1) {
            xy->xyR[cstart + 1][nin + i] = batch4buf->xR[offs0 + 1] / s1;
         }
         if (csize > 2) {
            xy->xyR[cstart + 2][nin + i] = batch4buf->xR[offs0 + 2] / s2;
         }
         if (csize > 3) {
            xy->xyR[cstart + 3][nin + i] = batch4buf->xR[offs0 + 3] / s3;
         }
         offs0 += chunksize;
      }
   } else {
   // Regression network with sum-of-squares function.
   //
   // For each NOut of last neurons:
   // * calculate difference between actual and desired output
   // * calculate dError/dOut for this neuron (proportional to difference)
   // * store in in last 4 components of entry (these values are used
   //   to start backpropagation)
   // * update error
      for (i = 0; i < nout; i++) {
         v0 = network->columnsigmas.xR[nin + i];
         v1 = network->columnmeans.xR[nin + i];
         entryoffs = entrysize * (ntotal - nout + i);
         for (j = 0; j < csize; j++) {
            xy->xyR[cstart + j][nin + i] = batch4buf->xR[entryoffs + j] * v0 + v1;
         }
      }
   }
}

// Calculation of all types of errors at once for a subset or  full  dataset,
// which can be represented in different formats.
//
// THIS INTERNAL FUNCTION IS NOT INTENDED TO BE USED BY ALGLIB USERS!
// ALGLIB: Copyright 26.07.2012 by Sergey Bochkanov
void mlpallerrorsx(multilayerperceptron *network, RMatrix *densexy, sparsematrix *sparsexy, ae_int_t datasetsize, ae_int_t datasettype, ZVector *idx, ae_int_t subset0, ae_int_t subset1, ae_int_t subsettype, ae_shared_pool *buf, modelerrors *rep) {
   ae_frame _frame_block;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t rowsize;
   bool iscls;
   ae_int_t srcidx;
   ae_int_t cstart;
   ae_int_t csize;
   ae_int_t j;
   ae_int_t len0;
   ae_int_t len1;
   double problemcost;
   ae_frame_make(&_frame_block);
   RefObj(mlpbuffers, pbuf);
   NewObj(modelerrors, rep0);
   NewObj(modelerrors, rep1);
   ae_assert(datasetsize >= 0, "MLPAllErrorsX: SetSize<0");
   ae_assert(datasettype == 0 || datasettype == 1, "MLPAllErrorsX: DatasetType is incorrect");
   ae_assert(subsettype == 0 || subsettype == 1, "MLPAllErrorsX: SubsetType is incorrect");
// Determine network properties
   mlpproperties(network, &nin, &nout, &wcount);
   iscls = mlpissoftmax(network);
// Split problem.
//
// Splitting problem allows us to reduce  effect  of  single-precision
// arithmetics (SSE-optimized version of MLPChunkedProcess uses single
// precision  internally, but converts them to  double precision after
// results are exported from HPC buffer to network). Small batches are
// calculated in single precision, results are  aggregated  in  double
// precision, and it allows us to avoid accumulation  of  errors  when
// we process very large batches (tens of thousands of items).
//
// NOTE: it is important to use real arithmetics for ProblemCost
//       because ProblemCost may be larger than MAXINT.
   problemcost = (double)(subset1 - subset0);
   problemcost *= wcount * 2;
// Parallelism was activated if: problemcost >= smpactivationlevel() && subset1 - subset0 >= 2 * mlpbase_microbatchsize
   if (subset1 - subset0 >= 2 * mlpbase_microbatchsize && problemcost > spawnlevel()) {
      len0 = splitlength(subset1 - subset0, mlpbase_microbatchsize), len1 = subset1 - subset0 - len0;
      mlpallerrorsx(network, densexy, sparsexy, datasetsize, datasettype, idx, subset0, subset0 + len0, subsettype, buf, &rep0);
      mlpallerrorsx(network, densexy, sparsexy, datasetsize, datasettype, idx, subset0 + len0, subset1, subsettype, buf, &rep1);
      rep->relclserror = (len0 * rep0.relclserror + len1 * rep1.relclserror) / (len0 + len1);
      rep->avgce = (len0 * rep0.avgce + len1 * rep1.avgce) / (len0 + len1);
      rep->rmserror = sqrt((len0 * ae_sqr(rep0.rmserror) + len1 * ae_sqr(rep1.rmserror)) / (len0 + len1));
      rep->avgerror = (len0 * rep0.avgerror + len1 * rep1.avgerror) / (len0 + len1);
      rep->avgrelerror = (len0 * rep0.avgrelerror + len1 * rep1.avgrelerror) / (len0 + len1);
      ae_frame_leave();
      return;
   }
// Retrieve and prepare
   ae_shared_pool_retrieve(buf, &_pbuf);
   if (iscls) {
      rowsize = nin + 1;
      dserrallocate(nout, &pbuf->tmp0);
   } else {
      rowsize = nin + nout;
      dserrallocate(-nout, &pbuf->tmp0);
   }
// Processing
   hpcpreparechunkedgradient(&network->weights, wcount, mlpntotal(network), nin, nout, pbuf);
   cstart = subset0;
   while (cstart < subset1) {
   // Determine size of current chunk and copy it to PBuf.XY
      csize = imin2(subset1, cstart + pbuf->chunksize) - cstart;
      for (j = 0; j < csize; j++) {
         srcidx = -1;
         if (subsettype == 0) {
            srcidx = cstart + j;
         }
         if (subsettype == 1) {
            srcidx = idx->xZ[cstart + j];
         }
         ae_assert(srcidx >= 0, "MLPAllErrorsX: internal error");
         if (datasettype == 0) {
            ae_v_move(pbuf->xy.xyR[j], 1, densexy->xyR[srcidx], 1, rowsize);
         }
         if (datasettype == 1) {
            sparsegetrow(sparsexy, srcidx, &pbuf->xyrow);
            ae_v_move(pbuf->xy.xyR[j], 1, pbuf->xyrow.xR, 1, rowsize);
         }
      }
   // Unpack XY and process (temporary code, to be replaced by chunked processing)
      for (j = 0; j < csize; j++) {
         ae_v_move(pbuf->xy2.xyR[j], 1, pbuf->xy.xyR[j], 1, rowsize);
      }
      mlpbase_mlpchunkedprocess(network, &pbuf->xy2, 0, csize, &pbuf->batch4buf, &pbuf->hpcbuf);
      for (j = 0; j < csize; j++) {
         ae_v_move(pbuf->x.xR, 1, pbuf->xy2.xyR[j], 1, nin);
         ae_v_move(pbuf->y.xR, 1, &pbuf->xy2.xyR[j][nin], 1, nout);
         if (iscls) {
            pbuf->desiredy.xR[0] = pbuf->xy.xyR[j][nin];
         } else {
            ae_v_move(pbuf->desiredy.xR, 1, &pbuf->xy.xyR[j][nin], 1, nout);
         }
         dserraccumulate(&pbuf->tmp0, &pbuf->y, &pbuf->desiredy);
      }
   // Process chunk and advance line pointer
      cstart += pbuf->chunksize;
   }
   dserrfinish(&pbuf->tmp0);
   rep->relclserror = pbuf->tmp0.xR[0];
   rep->avgce = pbuf->tmp0.xR[1] / log(2.0);
   rep->rmserror = pbuf->tmp0.xR[2];
   rep->avgerror = pbuf->tmp0.xR[3];
   rep->avgrelerror = pbuf->tmp0.xR[4];
// Recycle
   ae_shared_pool_recycle(buf, &_pbuf);
   ae_frame_leave();
}

// Serializer: allocation
// ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
void mlpalloc(ae_serializer *s, multilayerperceptron *network) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t fkind;
   double threshold;
   double v0;
   double v1;
   ae_int_t nin;
   ae_int_t nout;
   nin = network->hllayersizes.xZ[0];
   nout = network->hllayersizes.xZ[network->hllayersizes.cnt - 1];
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   allocintegerarray(s, &network->hllayersizes, -1);
   for (i = 1; i < network->hllayersizes.cnt; i++) {
      for (j = 0; j < network->hllayersizes.xZ[i]; j++) {
         mlpgetneuroninfo(network, i, j, &fkind, &threshold);
         ae_serializer_alloc_entry(s);
         ae_serializer_alloc_entry(s);
         for (k = 0; k < network->hllayersizes.xZ[i - 1]; k++) {
            ae_serializer_alloc_entry(s);
         }
      }
   }
   for (j = 0; j < nin; j++) {
      mlpgetinputscaling(network, j, &v0, &v1);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
   }
   for (j = 0; j < nout; j++) {
      mlpgetoutputscaling(network, j, &v0, &v1);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
   }
}

// Serializer: serialization
// These functions serialize a data structure to a C++ string or stream.
// * serialization can be freely moved across 32-bit and 64-bit systems,
//   and different byte orders. For example, you can serialize a string
//   on a SPARC and unserialize it on an x86.
// * ALGLIB++ serialization is compatible with serialization in ALGLIB,
//   in both directions.
// Important properties of s_out:
// * it contains alphanumeric characters, dots, underscores, minus signs
// * these symbols are grouped into words, which are separated by spaces
//   and Windows-style (CR+LF) newlines
// ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
// API: void mlpserialize(multilayerperceptron &obj, std::string &s_out);
// API: void mlpserialize(multilayerperceptron &obj, std::ostream &s_out);
void mlpserialize(ae_serializer *s, multilayerperceptron *network) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t fkind;
   double threshold;
   double v0;
   double v1;
   ae_int_t nin;
   ae_int_t nout;
   nin = network->hllayersizes.xZ[0];
   nout = network->hllayersizes.xZ[network->hllayersizes.cnt - 1];
   ae_serializer_serialize_int(s, getmlpserializationcode());
   ae_serializer_serialize_int(s, mlpbase_mlpfirstversion);
   ae_serializer_serialize_bool(s, mlpissoftmax(network));
   serializeintegerarray(s, &network->hllayersizes, -1);
   for (i = 1; i < network->hllayersizes.cnt; i++) {
      for (j = 0; j < network->hllayersizes.xZ[i]; j++) {
         mlpgetneuroninfo(network, i, j, &fkind, &threshold);
         ae_serializer_serialize_int(s, fkind);
         ae_serializer_serialize_double(s, threshold);
         for (k = 0; k < network->hllayersizes.xZ[i - 1]; k++) {
            ae_serializer_serialize_double(s, mlpgetweight(network, i - 1, k, i, j));
         }
      }
   }
   for (j = 0; j < nin; j++) {
      mlpgetinputscaling(network, j, &v0, &v1);
      ae_serializer_serialize_double(s, v0);
      ae_serializer_serialize_double(s, v1);
   }
   for (j = 0; j < nout; j++) {
      mlpgetoutputscaling(network, j, &v0, &v1);
      ae_serializer_serialize_double(s, v0);
      ae_serializer_serialize_double(s, v1);
   }
}

// Serializer: unserialization
// These functions unserialize a data structure from a C++ string or stream.
// Important properties of s_in:
// * any combination of spaces, tabs, Windows or Unix stype newlines can
//   be used as separators, so as to allow flexible reformatting of the
//   stream or string from text or XML files.
// * But you should not insert separators into the middle of the "words"
//   nor you should change case of letters.
// ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
// API: void mlpunserialize(const std::string &s_in, multilayerperceptron &obj);
// API: void mlpunserialize(const std::istream &s_in, multilayerperceptron &obj);
void mlpunserialize(ae_serializer *s, multilayerperceptron *network) {
   ae_frame _frame_block;
   ae_int_t i0;
   ae_int_t i1;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t fkind;
   double threshold;
   double v0;
   double v1;
   ae_int_t nin;
   ae_int_t nout;
   bool issoftmax;
   ae_frame_make(&_frame_block);
   SetObj(multilayerperceptron, network);
   NewVector(layersizes, 0, DT_INT);
// check correctness of header
   i0 = ae_serializer_unserialize_int(s);
   ae_assert(i0 == getmlpserializationcode(), "mlpunserialize: stream header corrupted");
   i1 = ae_serializer_unserialize_int(s);
   ae_assert(i1 == mlpbase_mlpfirstversion, "mlpunserialize: stream header corrupted");
// Create network
   issoftmax = ae_serializer_unserialize_bool(s);
   unserializeintegerarray(s, &layersizes);
   ae_assert(layersizes.cnt == 2 || layersizes.cnt == 3 || layersizes.cnt == 4, "mlpunserialize: too many hidden layers!");
   nin = layersizes.xZ[0];
   nout = layersizes.xZ[layersizes.cnt - 1];
   if (layersizes.cnt == 2) {
      if (issoftmax) {
         mlpcreatec0(layersizes.xZ[0], layersizes.xZ[1], network);
      } else {
         mlpcreate0(layersizes.xZ[0], layersizes.xZ[1], network);
      }
   }
   if (layersizes.cnt == 3) {
      if (issoftmax) {
         mlpcreatec1(layersizes.xZ[0], layersizes.xZ[1], layersizes.xZ[2], network);
      } else {
         mlpcreate1(layersizes.xZ[0], layersizes.xZ[1], layersizes.xZ[2], network);
      }
   }
   if (layersizes.cnt == 4) {
      if (issoftmax) {
         mlpcreatec2(layersizes.xZ[0], layersizes.xZ[1], layersizes.xZ[2], layersizes.xZ[3], network);
      } else {
         mlpcreate2(layersizes.xZ[0], layersizes.xZ[1], layersizes.xZ[2], layersizes.xZ[3], network);
      }
   }
// Load neurons and weights
   for (i = 1; i < layersizes.cnt; i++) {
      for (j = 0; j < layersizes.xZ[i]; j++) {
         fkind = ae_serializer_unserialize_int(s);
         threshold = ae_serializer_unserialize_double(s);
         mlpsetneuroninfo(network, i, j, fkind, threshold);
         for (k = 0; k < layersizes.xZ[i - 1]; k++) {
            v0 = ae_serializer_unserialize_double(s);
            mlpsetweight(network, i - 1, k, i, j, v0);
         }
      }
   }
// Load standartizator
   for (j = 0; j < nin; j++) {
      v0 = ae_serializer_unserialize_double(s);
      v1 = ae_serializer_unserialize_double(s);
      mlpsetinputscaling(network, j, v0, v1);
   }
   for (j = 0; j < nout; j++) {
      v0 = ae_serializer_unserialize_double(s);
      v1 = ae_serializer_unserialize_double(s);
      mlpsetoutputscaling(network, j, v0, v1);
   }
   ae_frame_leave();
}

void modelerrors_init(void *_p, bool make_automatic) {
}

void modelerrors_copy(void *_dst, void *_src, bool make_automatic) {
   modelerrors *dst = (modelerrors *)_dst;
   modelerrors *src = (modelerrors *)_src;
   dst->relclserror = src->relclserror;
   dst->avgce = src->avgce;
   dst->rmserror = src->rmserror;
   dst->avgerror = src->avgerror;
   dst->avgrelerror = src->avgrelerror;
}

void modelerrors_free(void *_p, bool make_automatic) {
}

void smlpgrad_init(void *_p, bool make_automatic) {
   smlpgrad *p = (smlpgrad *)_p;
   ae_vector_init(&p->g, 0, DT_REAL, make_automatic);
}

void smlpgrad_copy(void *_dst, void *_src, bool make_automatic) {
   smlpgrad *dst = (smlpgrad *)_dst;
   smlpgrad *src = (smlpgrad *)_src;
   dst->f = src->f;
   ae_vector_copy(&dst->g, &src->g, make_automatic);
}

void smlpgrad_free(void *_p, bool make_automatic) {
   smlpgrad *p = (smlpgrad *)_p;
   ae_vector_free(&p->g, make_automatic);
}

void multilayerperceptron_init(void *_p, bool make_automatic) {
   multilayerperceptron *p = (multilayerperceptron *)_p;
   ae_vector_init(&p->hllayersizes, 0, DT_INT, make_automatic);
   ae_vector_init(&p->hlconnections, 0, DT_INT, make_automatic);
   ae_vector_init(&p->hlneurons, 0, DT_INT, make_automatic);
   ae_vector_init(&p->structinfo, 0, DT_INT, make_automatic);
   ae_vector_init(&p->weights, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->columnmeans, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->columnsigmas, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->neurons, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->dfdnet, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->derror, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->x, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->y, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->xy, 0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->xyrow, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->nwbuf, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->integerbuf, 0, DT_INT, make_automatic);
   modelerrors_init(&p->err, make_automatic);
   ae_vector_init(&p->rndbuf, 0, DT_REAL, make_automatic);
   ae_shared_pool_init(&p->buf, make_automatic);
   ae_shared_pool_init(&p->gradbuf, make_automatic);
   ae_matrix_init(&p->dummydxy, 0, 0, DT_REAL, make_automatic);
   sparsematrix_init(&p->dummysxy, make_automatic);
   ae_vector_init(&p->dummyidx, 0, DT_INT, make_automatic);
   ae_shared_pool_init(&p->dummypool, make_automatic);
}

void multilayerperceptron_copy(void *_dst, void *_src, bool make_automatic) {
   multilayerperceptron *dst = (multilayerperceptron *)_dst;
   multilayerperceptron *src = (multilayerperceptron *)_src;
   dst->hlnetworktype = src->hlnetworktype;
   dst->hlnormtype = src->hlnormtype;
   ae_vector_copy(&dst->hllayersizes, &src->hllayersizes, make_automatic);
   ae_vector_copy(&dst->hlconnections, &src->hlconnections, make_automatic);
   ae_vector_copy(&dst->hlneurons, &src->hlneurons, make_automatic);
   ae_vector_copy(&dst->structinfo, &src->structinfo, make_automatic);
   ae_vector_copy(&dst->weights, &src->weights, make_automatic);
   ae_vector_copy(&dst->columnmeans, &src->columnmeans, make_automatic);
   ae_vector_copy(&dst->columnsigmas, &src->columnsigmas, make_automatic);
   ae_vector_copy(&dst->neurons, &src->neurons, make_automatic);
   ae_vector_copy(&dst->dfdnet, &src->dfdnet, make_automatic);
   ae_vector_copy(&dst->derror, &src->derror, make_automatic);
   ae_vector_copy(&dst->x, &src->x, make_automatic);
   ae_vector_copy(&dst->y, &src->y, make_automatic);
   ae_matrix_copy(&dst->xy, &src->xy, make_automatic);
   ae_vector_copy(&dst->xyrow, &src->xyrow, make_automatic);
   ae_vector_copy(&dst->nwbuf, &src->nwbuf, make_automatic);
   ae_vector_copy(&dst->integerbuf, &src->integerbuf, make_automatic);
   modelerrors_copy(&dst->err, &src->err, make_automatic);
   ae_vector_copy(&dst->rndbuf, &src->rndbuf, make_automatic);
   ae_shared_pool_copy(&dst->buf, &src->buf, make_automatic);
   ae_shared_pool_copy(&dst->gradbuf, &src->gradbuf, make_automatic);
   ae_matrix_copy(&dst->dummydxy, &src->dummydxy, make_automatic);
   sparsematrix_copy(&dst->dummysxy, &src->dummysxy, make_automatic);
   ae_vector_copy(&dst->dummyidx, &src->dummyidx, make_automatic);
   ae_shared_pool_copy(&dst->dummypool, &src->dummypool, make_automatic);
}

void multilayerperceptron_free(void *_p, bool make_automatic) {
   multilayerperceptron *p = (multilayerperceptron *)_p;
   ae_vector_free(&p->hllayersizes, make_automatic);
   ae_vector_free(&p->hlconnections, make_automatic);
   ae_vector_free(&p->hlneurons, make_automatic);
   ae_vector_free(&p->structinfo, make_automatic);
   ae_vector_free(&p->weights, make_automatic);
   ae_vector_free(&p->columnmeans, make_automatic);
   ae_vector_free(&p->columnsigmas, make_automatic);
   ae_vector_free(&p->neurons, make_automatic);
   ae_vector_free(&p->dfdnet, make_automatic);
   ae_vector_free(&p->derror, make_automatic);
   ae_vector_free(&p->x, make_automatic);
   ae_vector_free(&p->y, make_automatic);
   ae_matrix_free(&p->xy, make_automatic);
   ae_vector_free(&p->xyrow, make_automatic);
   ae_vector_free(&p->nwbuf, make_automatic);
   ae_vector_free(&p->integerbuf, make_automatic);
   modelerrors_free(&p->err, make_automatic);
   ae_vector_free(&p->rndbuf, make_automatic);
   ae_shared_pool_free(&p->buf, make_automatic);
   ae_shared_pool_free(&p->gradbuf, make_automatic);
   ae_matrix_free(&p->dummydxy, make_automatic);
   sparsematrix_free(&p->dummysxy, make_automatic);
   ae_vector_free(&p->dummyidx, make_automatic);
   ae_shared_pool_free(&p->dummypool, make_automatic);
}
} // end of namespace alglib_impl

namespace alglib {
// Model's errors:
//     * RelCLSError   -   fraction of misclassified cases.
//     * AvgCE         -   acerage cross-entropy
//     * RMSError      -   root-mean-square error
//     * AvgError      -   average error
//     * AvgRelError   -   average relative error
//
// NOTE 1: RelCLSError/AvgCE are zero on regression problems.
//
// NOTE 2: on classification problems  RMSError/AvgError/AvgRelError  contain
//         errors in prediction of posterior probabilities
DefClass(modelerrors, AndD DecVal(relclserror) AndD DecVal(avgce) AndD DecVal(rmserror) AndD DecVal(avgerror) AndD DecVal(avgrelerror))
DefClass(multilayerperceptron, EndD)

void mlpserialize(multilayerperceptron &obj, std::string &s_out) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_alloc_start(&serializer);
   alglib_impl::mlpalloc(&serializer, obj.c_ptr());
   ae_int_t ssize = alglib_impl::ae_serializer_get_alloc_size(&serializer);
   s_out.clear();
   s_out.reserve((size_t)(ssize + 1));
   alglib_impl::ae_serializer_sstart_str(&serializer, &s_out);
   alglib_impl::mlpserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_assert(s_out.length() <= (size_t)ssize, "mlpserialize: serialization integrity error");
   alglib_impl::ae_state_clear();
}
void mlpserialize(multilayerperceptron &obj, std::ostream &s_out) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_alloc_start(&serializer);
   alglib_impl::mlpalloc(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_get_alloc_size(&serializer); // not actually needed, but we have to ask
   alglib_impl::ae_serializer_sstart_stream(&serializer, &s_out);
   alglib_impl::mlpserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}

void mlpunserialize(const std::string &s_in, multilayerperceptron &obj) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_ustart_str(&serializer, &s_in);
   alglib_impl::mlpunserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}
void mlpunserialize(const std::istream &s_in, multilayerperceptron &obj) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_ustart_stream(&serializer, &s_in);
   alglib_impl::mlpunserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}

void mlpcreate0(const ae_int_t nin, const ae_int_t nout, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreate0(nin, nout, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreate1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreate1(nin, nhid, nout, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreate2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreate2(nin, nhid1, nhid2, nout, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreateb0(const ae_int_t nin, const ae_int_t nout, const double b, const double d, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreateb0(nin, nout, b, d, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreateb1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const double b, const double d, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreateb1(nin, nhid, nout, b, d, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreateb2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const double b, const double d, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreateb2(nin, nhid1, nhid2, nout, b, d, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreater0(const ae_int_t nin, const ae_int_t nout, const double a, const double b, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreater0(nin, nout, a, b, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreater1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const double a, const double b, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreater1(nin, nhid, nout, a, b, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreater2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const double a, const double b, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreater2(nin, nhid1, nhid2, nout, a, b, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreatec0(const ae_int_t nin, const ae_int_t nout, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreatec0(nin, nout, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreatec1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreatec1(nin, nhid, nout, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcreatec2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreatec2(nin, nhid1, nhid2, nout, ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpcopy(const multilayerperceptron &network1, multilayerperceptron &network2) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcopy(ConstT(multilayerperceptron, network1), ConstT(multilayerperceptron, network2));
   alglib_impl::ae_state_clear();
}

void mlpcopytunableparameters(const multilayerperceptron &network1, const multilayerperceptron &network2) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcopytunableparameters(ConstT(multilayerperceptron, network1), ConstT(multilayerperceptron, network2));
   alglib_impl::ae_state_clear();
}

void mlprandomize(const multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlprandomize(ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlprandomizefull(const multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlprandomizefull(ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
}

void mlpinitpreprocessor(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpinitpreprocessor(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), ssize);
   alglib_impl::ae_state_clear();
}

void mlpproperties(const multilayerperceptron &network, ae_int_t &nin, ae_int_t &nout, ae_int_t &wcount) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpproperties(ConstT(multilayerperceptron, network), &nin, &nout, &wcount);
   alglib_impl::ae_state_clear();
}

ae_int_t mlpgetinputscount(const multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch(0)
   ae_int_t Z = alglib_impl::mlpgetinputscount(ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
   return Z;
}

ae_int_t mlpgetoutputscount(const multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch(0)
   ae_int_t Z = alglib_impl::mlpgetoutputscount(ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
   return Z;
}

ae_int_t mlpgetweightscount(const multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch(0)
   ae_int_t Z = alglib_impl::mlpgetweightscount(ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
   return Z;
}

bool mlpissoftmax(const multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch(false)
   bool Ok = alglib_impl::mlpissoftmax(ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
   return Ok;
}

ae_int_t mlpgetlayerscount(const multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch(0)
   ae_int_t Z = alglib_impl::mlpgetlayerscount(ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
   return Z;
}

ae_int_t mlpgetlayersize(const multilayerperceptron &network, const ae_int_t k) {
   alglib_impl::ae_state_init();
   TryCatch(0)
   ae_int_t Z = alglib_impl::mlpgetlayersize(ConstT(multilayerperceptron, network), k);
   alglib_impl::ae_state_clear();
   return Z;
}

void mlpgetinputscaling(const multilayerperceptron &network, const ae_int_t i, double &mean, double &sigma) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgetinputscaling(ConstT(multilayerperceptron, network), i, &mean, &sigma);
   alglib_impl::ae_state_clear();
}

void mlpgetoutputscaling(const multilayerperceptron &network, const ae_int_t i, double &mean, double &sigma) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgetoutputscaling(ConstT(multilayerperceptron, network), i, &mean, &sigma);
   alglib_impl::ae_state_clear();
}

void mlpgetneuroninfo(const multilayerperceptron &network, const ae_int_t k, const ae_int_t i, ae_int_t &fkind, double &threshold) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgetneuroninfo(ConstT(multilayerperceptron, network), k, i, &fkind, &threshold);
   alglib_impl::ae_state_clear();
}

double mlpgetweight(const multilayerperceptron &network, const ae_int_t k0, const ae_int_t i0, const ae_int_t k1, const ae_int_t i1) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpgetweight(ConstT(multilayerperceptron, network), k0, i0, k1, i1);
   alglib_impl::ae_state_clear();
   return D;
}

void mlpsetinputscaling(const multilayerperceptron &network, const ae_int_t i, const double mean, const double sigma) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpsetinputscaling(ConstT(multilayerperceptron, network), i, mean, sigma);
   alglib_impl::ae_state_clear();
}

void mlpsetoutputscaling(const multilayerperceptron &network, const ae_int_t i, const double mean, const double sigma) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpsetoutputscaling(ConstT(multilayerperceptron, network), i, mean, sigma);
   alglib_impl::ae_state_clear();
}

void mlpsetneuroninfo(const multilayerperceptron &network, const ae_int_t k, const ae_int_t i, const ae_int_t fkind, const double threshold) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpsetneuroninfo(ConstT(multilayerperceptron, network), k, i, fkind, threshold);
   alglib_impl::ae_state_clear();
}

void mlpsetweight(const multilayerperceptron &network, const ae_int_t k0, const ae_int_t i0, const ae_int_t k1, const ae_int_t i1, const double w) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpsetweight(ConstT(multilayerperceptron, network), k0, i0, k1, i1, w);
   alglib_impl::ae_state_clear();
}

void mlpactivationfunction(const double net, const ae_int_t k, double &f, double &df, double &d2f) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpactivationfunction(net, k, &f, &df, &d2f);
   alglib_impl::ae_state_clear();
}

void mlpprocess(const multilayerperceptron &network, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpprocess(ConstT(multilayerperceptron, network), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

void mlpprocessi(const multilayerperceptron &network, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpprocessi(ConstT(multilayerperceptron, network), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

double mlperror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlperror(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlperrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlperrorsparse(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlperrorn(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlperrorn(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), ssize);
   alglib_impl::ae_state_clear();
   return D;
}

ae_int_t mlpclserror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0)
   ae_int_t Z = alglib_impl::mlpclserror(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return Z;
}

double mlprelclserror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlprelclserror(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlprelclserrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlprelclserrorsparse(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpavgce(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpavgce(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpavgcesparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpavgcesparse(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlprmserror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlprmserror(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlprmserrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlprmserrorsparse(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpavgerror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpavgerror(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpavgerrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpavgerrorsparse(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpavgrelerror(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpavgrelerror(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpavgrelerrorsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpavgrelerrorsparse(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

void mlpgrad(const multilayerperceptron &network, const real_1d_array &x, const real_1d_array &desiredy, double &e, real_1d_array &grad) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgrad(ConstT(multilayerperceptron, network), ConstT(ae_vector, x), ConstT(ae_vector, desiredy), &e, ConstT(ae_vector, grad));
   alglib_impl::ae_state_clear();
}

void mlpgradn(const multilayerperceptron &network, const real_1d_array &x, const real_1d_array &desiredy, double &e, real_1d_array &grad) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgradn(ConstT(multilayerperceptron, network), ConstT(ae_vector, x), ConstT(ae_vector, desiredy), &e, ConstT(ae_vector, grad));
   alglib_impl::ae_state_clear();
}

void mlpgradbatch(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize, double &e, real_1d_array &grad) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgradbatch(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), ssize, &e, ConstT(ae_vector, grad));
   alglib_impl::ae_state_clear();
}

void mlpgradbatchsparse(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t ssize, double &e, real_1d_array &grad) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgradbatchsparse(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), ssize, &e, ConstT(ae_vector, grad));
   alglib_impl::ae_state_clear();
}

void mlpgradbatchsubset(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t setsize, const integer_1d_array &idx, const ae_int_t subsetsize, double &e, real_1d_array &grad) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgradbatchsubset(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), setsize, ConstT(ae_vector, idx), subsetsize, &e, ConstT(ae_vector, grad));
   alglib_impl::ae_state_clear();
}

void mlpgradbatchsparsesubset(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t setsize, const integer_1d_array &idx, const ae_int_t subsetsize, double &e, real_1d_array &grad) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgradbatchsparsesubset(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), setsize, ConstT(ae_vector, idx), subsetsize, &e, ConstT(ae_vector, grad));
   alglib_impl::ae_state_clear();
}

void mlpgradnbatch(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize, double &e, real_1d_array &grad) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpgradnbatch(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), ssize, &e, ConstT(ae_vector, grad));
   alglib_impl::ae_state_clear();
}

void mlphessiannbatch(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize, double &e, real_1d_array &grad, real_2d_array &h) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlphessiannbatch(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), ssize, &e, ConstT(ae_vector, grad), ConstT(ae_matrix, h));
   alglib_impl::ae_state_clear();
}

void mlphessianbatch(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t ssize, double &e, real_1d_array &grad, real_2d_array &h) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlphessianbatch(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), ssize, &e, ConstT(ae_vector, grad), ConstT(ae_matrix, h));
   alglib_impl::ae_state_clear();
}

void mlpallerrorssubset(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t setsize, const integer_1d_array &subset, const ae_int_t subsetsize, modelerrors &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpallerrorssubset(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), setsize, ConstT(ae_vector, subset), subsetsize, ConstT(modelerrors, rep));
   alglib_impl::ae_state_clear();
}

void mlpallerrorssparsesubset(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t setsize, const integer_1d_array &subset, const ae_int_t subsetsize, modelerrors &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpallerrorssparsesubset(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), setsize, ConstT(ae_vector, subset), subsetsize, ConstT(modelerrors, rep));
   alglib_impl::ae_state_clear();
}

double mlperrorsubset(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t setsize, const integer_1d_array &subset, const ae_int_t subsetsize) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlperrorsubset(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), setsize, ConstT(ae_vector, subset), subsetsize);
   alglib_impl::ae_state_clear();
   return D;
}

double mlperrorsparsesubset(const multilayerperceptron &network, const sparsematrix &xy, const ae_int_t setsize, const integer_1d_array &subset, const ae_int_t subsetsize) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlperrorsparsesubset(ConstT(multilayerperceptron, network), ConstT(sparsematrix, xy), setsize, ConstT(ae_vector, subset), subsetsize);
   alglib_impl::ae_state_clear();
   return D;
}
} // end of namespace alglib

// === MLPE Package ===
// Depends on: MLPBASE
namespace alglib_impl {
static const ae_int_t mlpe_mlpefirstversion = 1;

// Like MLPCreate0, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreate0(const ae_int_t nin, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreate0(ae_int_t nin, ae_int_t nout, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreate0(nin, nout, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreate1, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreate1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreate1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreate1(nin, nhid, nout, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreate2, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreate2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreate2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreate2(nin, nhid1, nhid2, nout, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreateB0, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreateb0(const ae_int_t nin, const ae_int_t nout, const double b, const double d, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreateb0(ae_int_t nin, ae_int_t nout, double b, double d, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreateb0(nin, nout, b, d, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreateB1, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreateb1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const double b, const double d, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreateb1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, double b, double d, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreateb1(nin, nhid, nout, b, d, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreateB2, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreateb2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const double b, const double d, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreateb2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, double b, double d, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreateb2(nin, nhid1, nhid2, nout, b, d, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreateR0, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreater0(const ae_int_t nin, const ae_int_t nout, const double a, const double b, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreater0(ae_int_t nin, ae_int_t nout, double a, double b, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreater0(nin, nout, a, b, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreateR1, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreater1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const double a, const double b, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreater1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, double a, double b, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreater1(nin, nhid, nout, a, b, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreateR2, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreater2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const double a, const double b, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreater2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, double a, double b, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreater2(nin, nhid1, nhid2, nout, a, b, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreateC0, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreatec0(const ae_int_t nin, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreatec0(ae_int_t nin, ae_int_t nout, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreatec0(nin, nout, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreateC1, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreatec1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreatec1(ae_int_t nin, ae_int_t nhid, ae_int_t nout, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreatec1(nin, nhid, nout, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Like MLPCreateC2, but for ensembles.
// ALGLIB: Copyright 18.02.2009 by Sergey Bochkanov
// API: void mlpecreatec2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreatec2(ae_int_t nin, ae_int_t nhid1, ae_int_t nhid2, ae_int_t nout, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(mlpensemble, ensemble);
   NewObj(multilayerperceptron, net);
   mlpcreatec2(nin, nhid1, nhid2, nout, &net);
   mlpecreatefromnetwork(&net, ensemblesize, ensemble);
   ae_frame_leave();
}

// Creates ensemble from network. Only network geometry is copied.
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: void mlpecreatefromnetwork(const multilayerperceptron &network, const ae_int_t ensemblesize, mlpensemble &ensemble);
void mlpecreatefromnetwork(multilayerperceptron *network, ae_int_t ensemblesize, mlpensemble *ensemble) {
   ae_int_t i;
   ae_int_t ccount;
   ae_int_t wcount;
   SetObj(mlpensemble, ensemble);
   ae_assert(ensemblesize > 0, "MLPECreate: incorrect ensemble size!");
// Copy network
   mlpcopy(network, &ensemble->network);
// network properties
   if (mlpissoftmax(network)) {
      ccount = mlpgetinputscount(&ensemble->network);
   } else {
      ccount = mlpgetinputscount(&ensemble->network) + mlpgetoutputscount(&ensemble->network);
   }
   wcount = mlpgetweightscount(&ensemble->network);
   ensemble->ensemblesize = ensemblesize;
// weights, means, sigmas
   ae_vector_set_length(&ensemble->weights, ensemblesize * wcount);
   ae_vector_set_length(&ensemble->columnmeans, ensemblesize * ccount);
   ae_vector_set_length(&ensemble->columnsigmas, ensemblesize * ccount);
   for (i = 0; i < ensemblesize * wcount; i++) {
      ensemble->weights.xR[i] = ae_randomreal() - 0.5;
   }
   for (i = 0; i < ensemblesize; i++) {
      ae_v_move(&ensemble->columnmeans.xR[i * ccount], 1, network->columnmeans.xR, 1, ccount);
      ae_v_move(&ensemble->columnsigmas.xR[i * ccount], 1, network->columnsigmas.xR, 1, ccount);
   }
// temporaries, internal buffers
   ae_vector_set_length(&ensemble->y, mlpgetoutputscount(&ensemble->network));
}

// Copying of MLPEnsemble structure
//
// Inputs:
//     Ensemble1 -   original
//
// Outputs:
//     Ensemble2 -   copy
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
void mlpecopy(mlpensemble *ensemble1, mlpensemble *ensemble2) {
   ae_int_t ccount;
   ae_int_t wcount;
   SetObj(mlpensemble, ensemble2);
// Unload info
   if (mlpissoftmax(&ensemble1->network)) {
      ccount = mlpgetinputscount(&ensemble1->network);
   } else {
      ccount = mlpgetinputscount(&ensemble1->network) + mlpgetoutputscount(&ensemble1->network);
   }
   wcount = mlpgetweightscount(&ensemble1->network);
// Allocate space
   ae_vector_set_length(&ensemble2->weights, ensemble1->ensemblesize * wcount);
   ae_vector_set_length(&ensemble2->columnmeans, ensemble1->ensemblesize * ccount);
   ae_vector_set_length(&ensemble2->columnsigmas, ensemble1->ensemblesize * ccount);
   ae_vector_set_length(&ensemble2->y, mlpgetoutputscount(&ensemble1->network));
// Copy
   ensemble2->ensemblesize = ensemble1->ensemblesize;
   ae_v_move(ensemble2->weights.xR, 1, ensemble1->weights.xR, 1, ensemble1->ensemblesize * wcount);
   ae_v_move(ensemble2->columnmeans.xR, 1, ensemble1->columnmeans.xR, 1, ensemble1->ensemblesize * ccount);
   ae_v_move(ensemble2->columnsigmas.xR, 1, ensemble1->columnsigmas.xR, 1, ensemble1->ensemblesize * ccount);
   mlpcopy(&ensemble1->network, &ensemble2->network);
}

// Randomization of MLP ensemble
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: void mlperandomize(const mlpensemble &ensemble);
void mlperandomize(mlpensemble *ensemble) {
   ae_int_t i;
   ae_int_t wcount;
   wcount = mlpgetweightscount(&ensemble->network);
   for (i = 0; i < ensemble->ensemblesize * wcount; i++) {
      ensemble->weights.xR[i] = ae_randomreal() - 0.5;
   }
}

// Return ensemble properties (number of inputs and outputs).
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: void mlpeproperties(const mlpensemble &ensemble, ae_int_t &nin, ae_int_t &nout);
void mlpeproperties(mlpensemble *ensemble, ae_int_t *nin, ae_int_t *nout) {
   *nin = 0;
   *nout = 0;
   *nin = mlpgetinputscount(&ensemble->network);
   *nout = mlpgetoutputscount(&ensemble->network);
}

// Return normalization type (whether ensemble is SOFTMAX-normalized or not).
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: bool mlpeissoftmax(const mlpensemble &ensemble);
bool mlpeissoftmax(mlpensemble *ensemble) {
   bool result;
   result = mlpissoftmax(&ensemble->network);
   return result;
}

// Procesing
//
// Inputs:
//     Ensemble-   neural networks ensemble
//     X       -   input vector,  array[0..NIn-1].
//     Y       -   (possibly) preallocated buffer; if size of Y is less than
//                 NOut, it will be reallocated. If it is large enough, it
//                 is NOT reallocated, so we can save some time on reallocation.
//
// Outputs:
//     Y       -   result. Regression estimate when solving regression  task,
//                 vector of posterior probabilities for classification task.
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: void mlpeprocess(const mlpensemble &ensemble, const real_1d_array &x, real_1d_array &y);
void mlpeprocess(mlpensemble *ensemble, RVector *x, RVector *y) {
   ae_int_t i;
   ae_int_t es;
   ae_int_t wc;
   ae_int_t cc;
   double v;
   ae_int_t nout;
   if (y->cnt < mlpgetoutputscount(&ensemble->network)) {
      ae_vector_set_length(y, mlpgetoutputscount(&ensemble->network));
   }
   es = ensemble->ensemblesize;
   wc = mlpgetweightscount(&ensemble->network);
   if (mlpissoftmax(&ensemble->network)) {
      cc = mlpgetinputscount(&ensemble->network);
   } else {
      cc = mlpgetinputscount(&ensemble->network) + mlpgetoutputscount(&ensemble->network);
   }
   v = 1.0 / (double)es;
   nout = mlpgetoutputscount(&ensemble->network);
   for (i = 0; i < nout; i++) {
      y->xR[i] = 0.0;
   }
   for (i = 0; i < es; i++) {
      ae_v_move(ensemble->network.weights.xR, 1, &ensemble->weights.xR[i * wc], 1, wc);
      ae_v_move(ensemble->network.columnmeans.xR, 1, &ensemble->columnmeans.xR[i * cc], 1, cc);
      ae_v_move(ensemble->network.columnsigmas.xR, 1, &ensemble->columnsigmas.xR[i * cc], 1, cc);
      mlpprocess(&ensemble->network, x, &ensemble->y);
      ae_v_addd(y->xR, 1, ensemble->y.xR, 1, nout, v);
   }
}

// 'interactive'  variant  of  MLPEProcess  for  languages  like Python which
// support constructs like "Y = MLPEProcess(LM,X)" and interactive mode of the
// interpreter
//
// This function allocates new array on each call,  so  it  is  significantly
// slower than its 'non-interactive' counterpart, but it is  more  convenient
// when you call it from command line.
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: void mlpeprocessi(const mlpensemble &ensemble, const real_1d_array &x, real_1d_array &y);
void mlpeprocessi(mlpensemble *ensemble, RVector *x, RVector *y) {
   SetVector(y);
   mlpeprocess(ensemble, x, y);
}

// Calculation of all types of errors
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
void mlpeallerrorsx(mlpensemble *ensemble, RMatrix *densexy, sparsematrix *sparsexy, ae_int_t datasetsize, ae_int_t datasettype, ZVector *idx, ae_int_t subset0, ae_int_t subset1, ae_int_t subsettype, ae_shared_pool *buf, modelerrors *rep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t nin;
   ae_int_t nout;
   bool iscls;
   ae_int_t srcidx;
   ae_frame_make(&_frame_block);
   RefObj(mlpbuffers, pbuf);
   NewObj(modelerrors, rep0);
   NewObj(modelerrors, rep1);
// Get network information
   nin = mlpgetinputscount(&ensemble->network);
   nout = mlpgetoutputscount(&ensemble->network);
   iscls = mlpissoftmax(&ensemble->network);
// Retrieve buffer, prepare, process data, recycle buffer
   ae_shared_pool_retrieve(buf, &_pbuf);
   if (iscls) {
      dserrallocate(nout, &pbuf->tmp0);
   } else {
      dserrallocate(-nout, &pbuf->tmp0);
   }
   vectorsetlengthatleast(&pbuf->x, nin);
   vectorsetlengthatleast(&pbuf->y, nout);
   vectorsetlengthatleast(&pbuf->desiredy, nout);
   for (i = subset0; i < subset1; i++) {
      srcidx = -1;
      if (subsettype == 0) {
         srcidx = i;
      }
      if (subsettype == 1) {
         srcidx = idx->xZ[i];
      }
      ae_assert(srcidx >= 0, "MLPEAllErrorsX: internal error");
      if (datasettype == 0) {
         ae_v_move(pbuf->x.xR, 1, densexy->xyR[srcidx], 1, nin);
      }
      if (datasettype == 1) {
         sparsegetrow(sparsexy, srcidx, &pbuf->x);
      }
      mlpeprocess(ensemble, &pbuf->x, &pbuf->y);
      if (mlpissoftmax(&ensemble->network)) {
         if (datasettype == 0) {
            pbuf->desiredy.xR[0] = densexy->xyR[srcidx][nin];
         }
         if (datasettype == 1) {
            pbuf->desiredy.xR[0] = sparseget(sparsexy, srcidx, nin);
         }
      } else {
         if (datasettype == 0) {
            ae_v_move(pbuf->desiredy.xR, 1, &densexy->xyR[srcidx][nin], 1, nout);
         }
         if (datasettype == 1) {
            for (j = 0; j < nout; j++) {
               pbuf->desiredy.xR[j] = sparseget(sparsexy, srcidx, nin + j);
            }
         }
      }
      dserraccumulate(&pbuf->tmp0, &pbuf->y, &pbuf->desiredy);
   }
   dserrfinish(&pbuf->tmp0);
   rep->relclserror = pbuf->tmp0.xR[0];
   rep->avgce = pbuf->tmp0.xR[1] / log(2.0);
   rep->rmserror = pbuf->tmp0.xR[2];
   rep->avgerror = pbuf->tmp0.xR[3];
   rep->avgrelerror = pbuf->tmp0.xR[4];
   ae_shared_pool_recycle(buf, &_pbuf);
   ae_frame_leave();
}

// Calculation of all types of errors on dataset given by sparse matrix
// ALGLIB: Copyright 10.09.2012 by Sergey Bochkanov
void mlpeallerrorssparse(mlpensemble *ensemble, sparsematrix *xy, ae_int_t npoints, double *relcls, double *avgce, double *rms, double *avg, double *avgrel) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t nin;
   ae_int_t nout;
   ae_frame_make(&_frame_block);
   *relcls = 0;
   *avgce = 0;
   *rms = 0;
   *avg = 0;
   *avgrel = 0;
   NewVector(buf, 0, DT_REAL);
   NewVector(workx, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   NewVector(dy, 0, DT_REAL);
   nin = mlpgetinputscount(&ensemble->network);
   nout = mlpgetoutputscount(&ensemble->network);
   if (mlpissoftmax(&ensemble->network)) {
      ae_vector_set_length(&dy, 1);
      dserrallocate(nout, &buf);
   } else {
      ae_vector_set_length(&dy, nout);
      dserrallocate(-nout, &buf);
   }
   for (i = 0; i < npoints; i++) {
      sparsegetrow(xy, i, &workx);
      mlpeprocess(ensemble, &workx, &y);
      if (mlpissoftmax(&ensemble->network)) {
         dy.xR[0] = workx.xR[nin];
      } else {
         ae_v_move(dy.xR, 1, &workx.xR[nin], 1, nout);
      }
      dserraccumulate(&buf, &y, &dy);
   }
   dserrfinish(&buf);
   *relcls = buf.xR[0];
   *avgce = buf.xR[1];
   *rms = buf.xR[2];
   *avg = buf.xR[3];
   *avgrel = buf.xR[4];
   ae_frame_leave();
}

// Relative classification error on the test set
//
// Inputs:
//     Ensemble-   ensemble
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     percent of incorrectly classified cases.
//     Works both for classifier betwork and for regression networks which
// are used as classifiers.
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: double mlperelclserror(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints);
double mlperelclserror(mlpensemble *ensemble, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(modelerrors, rep);
   mlpeallerrorsx(ensemble, xy, &ensemble->network.dummysxy, npoints, 0, &ensemble->network.dummyidx, 0, npoints, 0, &ensemble->network.buf, &rep);
   result = rep.relclserror;
   ae_frame_leave();
   return result;
}

// Average cross-entropy (in bits per element) on the test set
//
// Inputs:
//     Ensemble-   ensemble
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     CrossEntropy/(NPoints*LN(2)).
//     Zero if ensemble solves regression task.
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: double mlpeavgce(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints);
double mlpeavgce(mlpensemble *ensemble, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(modelerrors, rep);
   mlpeallerrorsx(ensemble, xy, &ensemble->network.dummysxy, npoints, 0, &ensemble->network.dummyidx, 0, npoints, 0, &ensemble->network.buf, &rep);
   result = rep.avgce;
   ae_frame_leave();
   return result;
}

// RMS error on the test set
//
// Inputs:
//     Ensemble-   ensemble
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     root mean square error.
//     Its meaning for regression task is obvious. As for classification task
// RMS error means error when estimating posterior probabilities.
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: double mlpermserror(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints);
double mlpermserror(mlpensemble *ensemble, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(modelerrors, rep);
   mlpeallerrorsx(ensemble, xy, &ensemble->network.dummysxy, npoints, 0, &ensemble->network.dummyidx, 0, npoints, 0, &ensemble->network.buf, &rep);
   result = rep.rmserror;
   ae_frame_leave();
   return result;
}

// Average error on the test set
//
// Inputs:
//     Ensemble-   ensemble
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     Its meaning for regression task is obvious. As for classification task
// it means average error when estimating posterior probabilities.
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: double mlpeavgerror(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints);
double mlpeavgerror(mlpensemble *ensemble, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(modelerrors, rep);
   mlpeallerrorsx(ensemble, xy, &ensemble->network.dummysxy, npoints, 0, &ensemble->network.dummyidx, 0, npoints, 0, &ensemble->network.buf, &rep);
   result = rep.avgerror;
   ae_frame_leave();
   return result;
}

// Average relative error on the test set
//
// Inputs:
//     Ensemble-   ensemble
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     Its meaning for regression task is obvious. As for classification task
// it means average relative error when estimating posterior probabilities.
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: double mlpeavgrelerror(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints);
double mlpeavgrelerror(mlpensemble *ensemble, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(modelerrors, rep);
   mlpeallerrorsx(ensemble, xy, &ensemble->network.dummysxy, npoints, 0, &ensemble->network.dummyidx, 0, npoints, 0, &ensemble->network.buf, &rep);
   result = rep.avgrelerror;
   ae_frame_leave();
   return result;
}

// Serializer: allocation
// ALGLIB: Copyright 19.10.2011 by Sergey Bochkanov
void mlpealloc(ae_serializer *s, mlpensemble *ensemble) {
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   allocrealarray(s, &ensemble->weights, -1);
   allocrealarray(s, &ensemble->columnmeans, -1);
   allocrealarray(s, &ensemble->columnsigmas, -1);
   mlpalloc(s, &ensemble->network);
}

// Serializer: serialization
// These functions serialize a data structure to a C++ string or stream.
// * serialization can be freely moved across 32-bit and 64-bit systems,
//   and different byte orders. For example, you can serialize a string
//   on a SPARC and unserialize it on an x86.
// * ALGLIB++ serialization is compatible with serialization in ALGLIB,
//   in both directions.
// Important properties of s_out:
// * it contains alphanumeric characters, dots, underscores, minus signs
// * these symbols are grouped into words, which are separated by spaces
//   and Windows-style (CR+LF) newlines
// ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
// API: void mlpeserialize(mlpensemble &obj, std::string &s_out);
// API: void mlpeserialize(mlpensemble &obj, std::ostream &s_out);
void mlpeserialize(ae_serializer *s, mlpensemble *ensemble) {
   ae_serializer_serialize_int(s, getmlpeserializationcode());
   ae_serializer_serialize_int(s, mlpe_mlpefirstversion);
   ae_serializer_serialize_int(s, ensemble->ensemblesize);
   serializerealarray(s, &ensemble->weights, -1);
   serializerealarray(s, &ensemble->columnmeans, -1);
   serializerealarray(s, &ensemble->columnsigmas, -1);
   mlpserialize(s, &ensemble->network);
}

// Serializer: unserialization
// These functions unserialize a data structure from a C++ string or stream.
// Important properties of s_in:
// * any combination of spaces, tabs, Windows or Unix stype newlines can
//   be used as separators, so as to allow flexible reformatting of the
//   stream or string from text or XML files.
// * But you should not insert separators into the middle of the "words"
//   nor you should change case of letters.
// ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
// API: void mlpeunserialize(const std::string &s_in, mlpensemble &obj);
// API: void mlpeunserialize(const std::istream &s_in, mlpensemble &obj);
void mlpeunserialize(ae_serializer *s, mlpensemble *ensemble) {
   ae_int_t i0;
   ae_int_t i1;
   SetObj(mlpensemble, ensemble);
// check correctness of header
   i0 = ae_serializer_unserialize_int(s);
   ae_assert(i0 == getmlpeserializationcode(), "mlpeunserialize: stream header corrupted");
   i1 = ae_serializer_unserialize_int(s);
   ae_assert(i1 == mlpe_mlpefirstversion, "mlpeunserialize: stream header corrupted");
// Create network
   ensemble->ensemblesize = ae_serializer_unserialize_int(s);
   unserializerealarray(s, &ensemble->weights);
   unserializerealarray(s, &ensemble->columnmeans);
   unserializerealarray(s, &ensemble->columnsigmas);
   mlpunserialize(s, &ensemble->network);
// Allocate termoraries
   ae_vector_set_length(&ensemble->y, mlpgetoutputscount(&ensemble->network));
}

void mlpensemble_init(void *_p, bool make_automatic) {
   mlpensemble *p = (mlpensemble *)_p;
   ae_vector_init(&p->weights, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->columnmeans, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->columnsigmas, 0, DT_REAL, make_automatic);
   multilayerperceptron_init(&p->network, make_automatic);
   ae_vector_init(&p->y, 0, DT_REAL, make_automatic);
}

void mlpensemble_copy(void *_dst, void *_src, bool make_automatic) {
   mlpensemble *dst = (mlpensemble *)_dst;
   mlpensemble *src = (mlpensemble *)_src;
   dst->ensemblesize = src->ensemblesize;
   ae_vector_copy(&dst->weights, &src->weights, make_automatic);
   ae_vector_copy(&dst->columnmeans, &src->columnmeans, make_automatic);
   ae_vector_copy(&dst->columnsigmas, &src->columnsigmas, make_automatic);
   multilayerperceptron_copy(&dst->network, &src->network, make_automatic);
   ae_vector_copy(&dst->y, &src->y, make_automatic);
}

void mlpensemble_free(void *_p, bool make_automatic) {
   mlpensemble *p = (mlpensemble *)_p;
   ae_vector_free(&p->weights, make_automatic);
   ae_vector_free(&p->columnmeans, make_automatic);
   ae_vector_free(&p->columnsigmas, make_automatic);
   multilayerperceptron_free(&p->network, make_automatic);
   ae_vector_free(&p->y, make_automatic);
}
} // end of namespace alglib_impl

namespace alglib {
// Neural networks ensemble
DefClass(mlpensemble, EndD)

void mlpeserialize(mlpensemble &obj, std::string &s_out) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_alloc_start(&serializer);
   alglib_impl::mlpealloc(&serializer, obj.c_ptr());
   ae_int_t ssize = alglib_impl::ae_serializer_get_alloc_size(&serializer);
   s_out.clear();
   s_out.reserve((size_t)(ssize + 1));
   alglib_impl::ae_serializer_sstart_str(&serializer, &s_out);
   alglib_impl::mlpeserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_assert(s_out.length() <= (size_t)ssize, "mlpeserialize: serialization integrity error");
   alglib_impl::ae_state_clear();
}
void mlpeserialize(mlpensemble &obj, std::ostream &s_out) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_alloc_start(&serializer);
   alglib_impl::mlpealloc(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_get_alloc_size(&serializer); // not actually needed, but we have to ask
   alglib_impl::ae_serializer_sstart_stream(&serializer, &s_out);
   alglib_impl::mlpeserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}

void mlpeunserialize(const std::string &s_in, mlpensemble &obj) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_ustart_str(&serializer, &s_in);
   alglib_impl::mlpeunserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}
void mlpeunserialize(const std::istream &s_in, mlpensemble &obj) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_ustart_stream(&serializer, &s_in);
   alglib_impl::mlpeunserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}

void mlpecreate0(const ae_int_t nin, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreate0(nin, nout, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreate1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreate1(nin, nhid, nout, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreate2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreate2(nin, nhid1, nhid2, nout, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreateb0(const ae_int_t nin, const ae_int_t nout, const double b, const double d, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreateb0(nin, nout, b, d, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreateb1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const double b, const double d, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreateb1(nin, nhid, nout, b, d, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreateb2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const double b, const double d, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreateb2(nin, nhid1, nhid2, nout, b, d, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreater0(const ae_int_t nin, const ae_int_t nout, const double a, const double b, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreater0(nin, nout, a, b, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreater1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const double a, const double b, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreater1(nin, nhid, nout, a, b, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreater2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const double a, const double b, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreater2(nin, nhid1, nhid2, nout, a, b, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreatec0(const ae_int_t nin, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreatec0(nin, nout, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreatec1(const ae_int_t nin, const ae_int_t nhid, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreatec1(nin, nhid, nout, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreatec2(const ae_int_t nin, const ae_int_t nhid1, const ae_int_t nhid2, const ae_int_t nout, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreatec2(nin, nhid1, nhid2, nout, ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpecreatefromnetwork(const multilayerperceptron &network, const ae_int_t ensemblesize, mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpecreatefromnetwork(ConstT(multilayerperceptron, network), ensemblesize, ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlperandomize(const mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlperandomize(ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
}

void mlpeproperties(const mlpensemble &ensemble, ae_int_t &nin, ae_int_t &nout) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpeproperties(ConstT(mlpensemble, ensemble), &nin, &nout);
   alglib_impl::ae_state_clear();
}

bool mlpeissoftmax(const mlpensemble &ensemble) {
   alglib_impl::ae_state_init();
   TryCatch(false)
   bool Ok = alglib_impl::mlpeissoftmax(ConstT(mlpensemble, ensemble));
   alglib_impl::ae_state_clear();
   return Ok;
}

void mlpeprocess(const mlpensemble &ensemble, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpeprocess(ConstT(mlpensemble, ensemble), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

void mlpeprocessi(const mlpensemble &ensemble, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpeprocessi(ConstT(mlpensemble, ensemble), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

double mlperelclserror(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlperelclserror(ConstT(mlpensemble, ensemble), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpeavgce(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpeavgce(ConstT(mlpensemble, ensemble), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpermserror(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpermserror(ConstT(mlpensemble, ensemble), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpeavgerror(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpeavgerror(ConstT(mlpensemble, ensemble), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mlpeavgrelerror(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mlpeavgrelerror(ConstT(mlpensemble, ensemble), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}
} // end of namespace alglib

// === CLUSTERING Package ===
// Depends on: (AlgLibInternal) BLAS
// Depends on: (AlgLibMisc) HQRND
// Depends on: (Statistics) BASESTAT
namespace alglib_impl {
static const ae_int_t clustering_kmeansblocksize = 32;
static const ae_int_t clustering_kmeansparalleldim = 8;
static const ae_int_t clustering_kmeansparallelk = 4;
static const double clustering_complexitymultiplier = 1.0;

// This function initializes clusterizer object. Newly initialized object  is
// empty, i.e. it does not contain dataset. You should use it as follows:
// 1. creation
// 2. dataset is added with ClusterizerSetPoints()
// 3. additional parameters are set
// 3. clusterization is performed with one of the clustering functions
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizercreate(clusterizerstate &s);
void clusterizercreate(clusterizerstate *s) {
   SetObj(clusterizerstate, s);
   s->npoints = 0;
   s->nfeatures = 0;
   s->disttype = 2;
   s->ahcalgo = 0;
   s->kmeansrestarts = 1;
   s->kmeansmaxits = 0;
   s->kmeansinitalgo = 0;
   s->kmeansdbgnoits = false;
   s->seed = 1;
   kmeansinitbuf(&s->kmeanstmp);
}

// This function adds dataset to the clusterizer structure.
//
// This function overrides all previous calls  of  ClusterizerSetPoints()  or
// ClusterizerSetDistances().
//
// Inputs:
//     S       -   clusterizer state, initialized by ClusterizerCreate()
//     XY      -   array[NPoints,NFeatures], dataset
//     NPoints -   number of points, >= 0
//     NFeatures-  number of features, >= 1
//     DistType-   distance function:
//                 *  0    Chebyshev distance  (L-inf norm)
//                 *  1    city block distance (L1 norm)
//                 *  2    Euclidean distance  (L2 norm), non-squared
//                 * 10    Pearson correlation:
//                         dist(a,b) = 1-corr(a,b)
//                 * 11    Absolute Pearson correlation:
//                         dist(a,b) = 1-|corr(a,b)|
//                 * 12    Uncentered Pearson correlation (cosine of the angle):
//                         dist(a,b) = a'*b/(|a|*|b|)
//                 * 13    Absolute uncentered Pearson correlation
//                         dist(a,b) = |a'*b|/(|a|*|b|)
//                 * 20    Spearman rank correlation:
//                         dist(a,b) = 1-rankcorr(a,b)
//                 * 21    Absolute Spearman rank correlation
//                         dist(a,b) = 1-|rankcorr(a,b)|
//
// NOTE 1: different distance functions have different performance penalty:
//         * Euclidean or Pearson correlation distances are the fastest ones
//         * Spearman correlation distance function is a bit slower
//         * city block and Chebyshev distances are order of magnitude slower
//
//         The reason behing difference in performance is that correlation-based
//         distance functions are computed using optimized linear algebra kernels,
//         while Chebyshev and city block distance functions are computed using
//         simple nested loops with two branches at each iteration.
//
// NOTE 2: different clustering algorithms have different limitations:
//         * agglomerative hierarchical clustering algorithms may be used with
//           any kind of distance metric
//         * k-means++ clustering algorithm may be used only  with  Euclidean
//           distance function
//         Thus, list of specific clustering algorithms you may  use  depends
//         on distance function you specify when you set your dataset.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizersetpoints(const clusterizerstate &s, const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nfeatures, const ae_int_t disttype);
// API: void clusterizersetpoints(const clusterizerstate &s, const real_2d_array &xy, const ae_int_t disttype);
void clusterizersetpoints(clusterizerstate *s, RMatrix *xy, ae_int_t npoints, ae_int_t nfeatures, ae_int_t disttype) {
   ae_int_t i;
   ae_assert(disttype == 0 || disttype == 1 || disttype == 2 || disttype == 10 || disttype == 11 || disttype == 12 || disttype == 13 || disttype == 20 || disttype == 21, "ClusterizerSetPoints: incorrect DistType");
   ae_assert(npoints >= 0, "ClusterizerSetPoints: NPoints < 0");
   ae_assert(nfeatures >= 1, "ClusterizerSetPoints: NFeatures<1");
   ae_assert(xy->rows >= npoints, "ClusterizerSetPoints: Rows(XY)<NPoints");
   ae_assert(xy->cols >= nfeatures, "ClusterizerSetPoints: Cols(XY)<NFeatures");
   ae_assert(apservisfinitematrix(xy, npoints, nfeatures), "ClusterizerSetPoints: XY contains NAN/INF");
   s->npoints = npoints;
   s->nfeatures = nfeatures;
   s->disttype = disttype;
   matrixsetlengthatleast(&s->xy, npoints, nfeatures);
   for (i = 0; i < npoints; i++) {
      ae_v_move(s->xy.xyR[i], 1, xy->xyR[i], 1, nfeatures);
   }
}

// This function adds dataset given by distance  matrix  to  the  clusterizer
// structure. It is important that dataset is not  given  explicitly  -  only
// distance matrix is given.
//
// This function overrides all previous calls  of  ClusterizerSetPoints()  or
// ClusterizerSetDistances().
//
// Inputs:
//     S       -   clusterizer state, initialized by ClusterizerCreate()
//     D       -   array[NPoints,NPoints], distance matrix given by its upper
//                 or lower triangle (main diagonal is  ignored  because  its
//                 entries are expected to be zero).
//     NPoints -   number of points
//     IsUpper -   whether upper or lower triangle of D is given.
//
// NOTE 1: different clustering algorithms have different limitations:
//         * agglomerative hierarchical clustering algorithms may be used with
//           any kind of distance metric, including one  which  is  given  by
//           distance matrix
//         * k-means++ clustering algorithm may be used only  with  Euclidean
//           distance function and explicitly given points - it  can  not  be
//           used with dataset given by distance matrix
//         Thus, if you call this function, you will be unable to use k-means
//         clustering algorithm to process your problem.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizersetdistances(const clusterizerstate &s, const real_2d_array &d, const ae_int_t npoints, const bool isupper);
// API: void clusterizersetdistances(const clusterizerstate &s, const real_2d_array &d, const bool isupper);
void clusterizersetdistances(clusterizerstate *s, RMatrix *d, ae_int_t npoints, bool isupper) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t j0;
   ae_int_t j1;
   ae_assert(npoints >= 0, "ClusterizerSetDistances: NPoints < 0");
   ae_assert(d->rows >= npoints, "ClusterizerSetDistances: Rows(D)<NPoints");
   ae_assert(d->cols >= npoints, "ClusterizerSetDistances: Cols(D)<NPoints");
   s->npoints = npoints;
   s->nfeatures = 0;
   s->disttype = -1;
   matrixsetlengthatleast(&s->d, npoints, npoints);
   for (i = 0; i < npoints; i++) {
      if (isupper) {
         j0 = i + 1;
         j1 = npoints - 1;
      } else {
         j0 = 0;
         j1 = i - 1;
      }
      for (j = j0; j <= j1; j++) {
         ae_assert(isfinite(d->xyR[i][j]) && d->xyR[i][j] >= 0.0, "ClusterizerSetDistances: D contains infinite, NAN or negative elements");
         s->d.xyR[i][j] = d->xyR[i][j];
         s->d.xyR[j][i] = d->xyR[i][j];
      }
      s->d.xyR[i][i] = 0.0;
   }
}

// This function sets agglomerative hierarchical clustering algorithm
//
// Inputs:
//     S       -   clusterizer state, initialized by ClusterizerCreate()
//     Algo    -   algorithm type:
//                 * 0     complete linkage (default algorithm)
//                 * 1     single linkage
//                 * 2     unweighted average linkage
//                 * 3     weighted average linkage
//                 * 4     Ward's method
//
// NOTE: Ward's method works correctly only with Euclidean  distance,  that's
//       why algorithm will return negative termination  code  (failure)  for
//       any other distance type.
//
//       It is possible, however,  to  use  this  method  with  user-supplied
//       distance matrix. It  is  your  responsibility  to pass one which was
//       calculated with Euclidean distance function.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizersetahcalgo(const clusterizerstate &s, const ae_int_t algo);
void clusterizersetahcalgo(clusterizerstate *s, ae_int_t algo) {
   ae_assert(algo == 0 || algo == 1 || algo == 2 || algo == 3 || algo == 4, "ClusterizerSetHCAlgo: incorrect algorithm type");
   s->ahcalgo = algo;
}

// This  function  sets k-means properties:  number  of  restarts and maximum
// number of iterations per one run.
//
// Inputs:
//     S       -   clusterizer state, initialized by ClusterizerCreate()
//     Restarts-   restarts count, >= 1.
//                 k-means++ algorithm performs several restarts and  chooses
//                 best set of centers (one with minimum squared distance).
//     MaxIts  -   maximum number of k-means iterations performed during  one
//                 run. >= 0, zero value means that algorithm performs unlimited
//                 number of iterations.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizersetkmeanslimits(const clusterizerstate &s, const ae_int_t restarts, const ae_int_t maxits);
void clusterizersetkmeanslimits(clusterizerstate *s, ae_int_t restarts, ae_int_t maxits) {
   ae_assert(restarts >= 1, "ClusterizerSetKMeansLimits: Restarts <= 0");
   ae_assert(maxits >= 0, "ClusterizerSetKMeansLimits: MaxIts<0");
   s->kmeansrestarts = restarts;
   s->kmeansmaxits = maxits;
}

// This function sets k-means  initialization  algorithm.  Several  different
// algorithms can be chosen, including k-means++.
//
// Inputs:
//     S       -   clusterizer state, initialized by ClusterizerCreate()
//     InitAlgo-   initialization algorithm:
//                 * 0  automatic selection ( different  versions  of  ALGLIB
//                      may select different algorithms)
//                 * 1  random initialization
//                 * 2  k-means++ initialization  (best  quality  of  initial
//                      centers, but long  non-parallelizable  initialization
//                      phase with bad cache locality)
//                 * 3  "fast-greedy"  algorithm  with  efficient,  easy   to
//                      parallelize initialization. Quality of initial centers
//                      is  somewhat  worse  than  that  of  k-means++.  This
//                      algorithm is a default one in the current version  of
//                      ALGLIB.
//                 *-1  "debug" algorithm which always selects first  K  rows
//                      of dataset; this algorithm is used for debug purposes
//                      only. Do not use it in the industrial code!
// ALGLIB: Copyright 21.01.2015 by Sergey Bochkanov
// API: void clusterizersetkmeansinit(const clusterizerstate &s, const ae_int_t initalgo);
void clusterizersetkmeansinit(clusterizerstate *s, ae_int_t initalgo) {
   ae_assert(initalgo >= -1 && initalgo <= 3, "ClusterizerSetKMeansInit: InitAlgo is incorrect");
   s->kmeansinitalgo = initalgo;
}

// This  function  sets  seed  which  is  used to initialize internal RNG. By
// default, deterministic seed is used - same for each run of clusterizer. If
// you specify non-deterministic  seed  value,  then  some  algorithms  which
// depend on random initialization (in current version: k-means)  may  return
// slightly different results after each run.
//
// Inputs:
//     S       -   clusterizer state, initialized by ClusterizerCreate()
//     Seed    -   seed:
//                 * positive values = use deterministic seed for each run of
//                   algorithms which depend on random initialization
//                 * zero or negative values = use non-deterministic seed
// ALGLIB: Copyright 08.06.2017 by Sergey Bochkanov
// API: void clusterizersetseed(const clusterizerstate &s, const ae_int_t seed);
void clusterizersetseed(clusterizerstate *s, ae_int_t seed) {
   s->seed = seed;
}

// This  function  performs  agglomerative  hierarchical  clustering    using
// precomputed  distance  matrix.  Internal  function,  should  not be called
// directly.
//
// Inputs:
//     S       -   clusterizer state, initialized by ClusterizerCreate()
//     D       -   distance matrix, array[S.NFeatures,S.NFeatures]
//                 Contents of the matrix is destroyed during
//                 algorithm operation.
//
// Outputs:
//     Rep     -   clustering results; see description of AHCReport
//                 structure for more information.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
static void clustering_clusterizerrunahcinternal(clusterizerstate *s, RMatrix *d, ahcreport *rep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   double v;
   ae_int_t mergeidx;
   ae_int_t c0;
   ae_int_t c1;
   ae_int_t s0;
   ae_int_t s1;
   ae_int_t ar;
   ae_int_t br;
   ae_int_t npoints;
   ae_int_t n0;
   ae_int_t n1;
   ae_int_t ni;
   double d01;
   ae_frame_make(&_frame_block);
   NewVector(cidx, 0, DT_INT);
   NewVector(csizes, 0, DT_INT);
   NewVector(nnidx, 0, DT_INT);
   NewMatrix(cinfo, 0, 0, DT_INT);
   npoints = s->npoints;
// Fill Rep.NPoints, quick exit when NPoints <= 1
   rep->npoints = npoints;
   if (npoints == 0) {
      ae_vector_set_length(&rep->p, 0);
      ae_matrix_set_length(&rep->z, 0, 0);
      ae_matrix_set_length(&rep->pz, 0, 0);
      ae_matrix_set_length(&rep->pm, 0, 0);
      ae_vector_set_length(&rep->mergedist, 0);
      rep->terminationtype = 1;
      ae_frame_leave();
      return;
   }
   if (npoints == 1) {
      ae_vector_set_length(&rep->p, 1);
      ae_matrix_set_length(&rep->z, 0, 0);
      ae_matrix_set_length(&rep->pz, 0, 0);
      ae_matrix_set_length(&rep->pm, 0, 0);
      ae_vector_set_length(&rep->mergedist, 0);
      rep->p.xZ[0] = 0;
      rep->terminationtype = 1;
      ae_frame_leave();
      return;
   }
   ae_matrix_set_length(&rep->z, npoints - 1, 2);
   ae_vector_set_length(&rep->mergedist, npoints - 1);
   rep->terminationtype = 1;
// Build list of nearest neighbors
   ae_vector_set_length(&nnidx, npoints);
   for (i = 0; i < npoints; i++) {
   // Calculate index of the nearest neighbor
      k = -1;
      v = ae_maxrealnumber;
      for (j = 0; j < npoints; j++) {
         if (j != i && d->xyR[i][j] < v) {
            k = j;
            v = d->xyR[i][j];
         }
      }
      ae_assert(v < ae_maxrealnumber, "ClusterizerRunAHC: internal error");
      nnidx.xZ[i] = k;
   }
// For AHCAlgo=4 (Ward's method) replace distances by their squares times 0.5
   if (s->ahcalgo == 4) {
      for (i = 0; i < npoints; i++) {
         for (j = 0; j < npoints; j++) {
            d->xyR[i][j] *= 0.5 * d->xyR[i][j];
         }
      }
   }
// Distance matrix is built, perform merges.
//
// NOTE 1: CIdx is array[NPoints] which maps rows/columns of the
//         distance matrix D to indexes of clusters. Values of CIdx
//         from [0,NPoints) denote single-point clusters, and values
//         from [NPoints,2*NPoints-1) denote ones obtained by merging
//         smaller clusters. Negative calues correspond to absent clusters.
//
//         Initially it contains [0...NPoints-1], after each merge
//         one element of CIdx (one with index C0) is replaced by
//         NPoints+MergeIdx, and another one with index C1 is
//         rewritten by -1.
//
// NOTE 2: CSizes is array[NPoints] which stores sizes of clusters.
//
   ae_vector_set_length(&cidx, npoints);
   ae_vector_set_length(&csizes, npoints);
   for (i = 0; i < npoints; i++) {
      cidx.xZ[i] = i;
      csizes.xZ[i] = 1;
   }
   for (mergeidx = 0; mergeidx < npoints - 1; mergeidx++) {
   // Select pair of clusters (C0,C1) with CIdx[C0]<CIdx[C1] to merge.
      c0 = -1;
      c1 = -1;
      d01 = ae_maxrealnumber;
      for (i = 0; i < npoints; i++) {
         if (cidx.xZ[i] >= 0) {
            if (d->xyR[i][nnidx.xZ[i]] < d01) {
               c0 = i;
               c1 = nnidx.xZ[i];
               d01 = d->xyR[i][nnidx.xZ[i]];
            }
         }
      }
      ae_assert(d01 < ae_maxrealnumber, "ClusterizerRunAHC: internal error");
      if (cidx.xZ[c0] > cidx.xZ[c1]) {
         i = c1;
         c1 = c0;
         c0 = i;
      }
   // Fill one row of Rep.Z and one element of Rep.MergeDist
      rep->z.xyZ[mergeidx][0] = cidx.xZ[c0];
      rep->z.xyZ[mergeidx][1] = cidx.xZ[c1];
      rep->mergedist.xR[mergeidx] = d01;
   // Update distance matrix:
   // * row/column C0 are updated by distances to the new cluster
   // * row/column C1 are considered empty (we can fill them by zeros,
   //   but do not want to spend time - we just ignore them)
   //
   // NOTE: it is important to update distance matrix BEFORE CIdx/CSizes
   //       are updated.
      ae_assert(s->ahcalgo == 0 || s->ahcalgo == 1 || s->ahcalgo == 2 || s->ahcalgo == 3 || s->ahcalgo == 4, "ClusterizerRunAHC: internal error");
      for (i = 0; i < npoints; i++) {
         if (i != c0 && i != c1) {
            n0 = csizes.xZ[c0];
            n1 = csizes.xZ[c1];
            ni = csizes.xZ[i];
            if (s->ahcalgo == 0) {
               d->xyR[i][c0] = rmax2(d->xyR[i][c0], d->xyR[i][c1]);
            }
            if (s->ahcalgo == 1) {
               d->xyR[i][c0] = rmin2(d->xyR[i][c0], d->xyR[i][c1]);
            }
            if (s->ahcalgo == 2) {
               d->xyR[i][c0] = (csizes.xZ[c0] * d->xyR[i][c0] + csizes.xZ[c1] * d->xyR[i][c1]) / (csizes.xZ[c0] + csizes.xZ[c1]);
            }
            if (s->ahcalgo == 3) {
               d->xyR[i][c0] = (d->xyR[i][c0] + d->xyR[i][c1]) / 2;
            }
            if (s->ahcalgo == 4) {
               d->xyR[i][c0] = ((n0 + ni) * d->xyR[i][c0] + (n1 + ni) * d->xyR[i][c1] - ni * d01) / (n0 + n1 + ni);
            }
            d->xyR[c0][i] = d->xyR[i][c0];
         }
      }
   // Update CIdx and CSizes
      cidx.xZ[c0] = npoints + mergeidx;
      cidx.xZ[c1] = -1;
      csizes.xZ[c0] += csizes.xZ[c1];
      csizes.xZ[c1] = 0;
   // Update nearest neighbors array:
   // * update nearest neighbors of everything except for C0/C1
   // * update neighbors of C0/C1
      for (i = 0; i < npoints; i++) {
         if (cidx.xZ[i] >= 0 && i != c0 && (nnidx.xZ[i] == c0 || nnidx.xZ[i] == c1)) {
         // I-th cluster which is distinct from C0/C1 has former C0/C1 cluster as its nearest
         // neighbor. We handle this issue depending on specific AHC algorithm being used.
            if (s->ahcalgo == 1) {
            // Single linkage. Merging of two clusters together
            // does NOT change distances between new cluster and
            // other clusters.
            //
            // The only thing we have to do is to update nearest neighbor index
               nnidx.xZ[i] = c0;
            } else {
            // Something other than single linkage. We have to re-examine
            // all the row to find nearest neighbor.
               k = -1;
               v = ae_maxrealnumber;
               for (j = 0; j < npoints; j++) {
                  if (cidx.xZ[j] >= 0 && j != i && d->xyR[i][j] < v) {
                     k = j;
                     v = d->xyR[i][j];
                  }
               }
               ae_assert(v < ae_maxrealnumber || mergeidx == npoints - 2, "ClusterizerRunAHC: internal error");
               nnidx.xZ[i] = k;
            }
         }
      }
      k = -1;
      v = ae_maxrealnumber;
      for (j = 0; j < npoints; j++) {
         if (cidx.xZ[j] >= 0 && j != c0 && d->xyR[c0][j] < v) {
            k = j;
            v = d->xyR[c0][j];
         }
      }
      ae_assert(v < ae_maxrealnumber || mergeidx == npoints - 2, "ClusterizerRunAHC: internal error");
      nnidx.xZ[c0] = k;
   }
// Calculate Rep.P and Rep.PM.
//
// In order to do that, we fill CInfo matrix - (2*NPoints-1)*3 matrix,
// with I-th row containing:
// * CInfo[I,0]     -   size of I-th cluster
// * CInfo[I,1]     -   beginning of I-th cluster
// * CInfo[I,2]     -   end of I-th cluster
// * CInfo[I,3]     -   height of I-th cluster
//
// We perform it as follows:
// * first NPoints clusters have unit size (CInfo[I,0]=1) and zero
//   height (CInfo[I,3]=0)
// * we replay NPoints-1 merges from first to last and fill sizes of
//   corresponding clusters (new size is a sum of sizes of clusters
//   being merged) and height (new height is max(heights)+1).
// * now we ready to determine locations of clusters. Last cluster
//   spans entire dataset, we know it. We replay merges from last to
//   first, during each merge we already know location of the merge
//   result, and we can position first cluster to the left part of
//   the result, and second cluster to the right part.
   ae_vector_set_length(&rep->p, npoints);
   ae_matrix_set_length(&rep->pm, npoints - 1, 6);
   ae_matrix_set_length(&cinfo, 2 * npoints - 1, 4);
   for (i = 0; i < npoints; i++) {
      cinfo.xyZ[i][0] = 1;
      cinfo.xyZ[i][3] = 0;
   }
   for (i = 0; i < npoints - 1; i++) {
      cinfo.xyZ[npoints + i][0] = cinfo.xyZ[rep->z.xyZ[i][0]][0] + cinfo.xyZ[rep->z.xyZ[i][1]][0];
      cinfo.xyZ[npoints + i][3] = imax2(cinfo.xyZ[rep->z.xyZ[i][0]][3], cinfo.xyZ[rep->z.xyZ[i][1]][3]) + 1;
   }
   cinfo.xyZ[2 * npoints - 2][1] = 0;
   cinfo.xyZ[2 * npoints - 2][2] = npoints - 1;
   for (i = npoints - 2; i >= 0; i--) {
   // We merge C0 which spans [A0,B0] and C1 (spans [A1,B1]),
   // with unknown A0, B0, A1, B1. However, we know that result
   // is CR, which spans [AR,BR] with known AR/BR, and we know
   // sizes of C0, C1, CR (denotes as S0, S1, SR).
      c0 = rep->z.xyZ[i][0];
      c1 = rep->z.xyZ[i][1];
      s0 = cinfo.xyZ[c0][0];
      s1 = cinfo.xyZ[c1][0];
      ar = cinfo.xyZ[npoints + i][1];
      br = cinfo.xyZ[npoints + i][2];
      cinfo.xyZ[c0][1] = ar;
      cinfo.xyZ[c0][2] = ar + s0 - 1;
      cinfo.xyZ[c1][1] = br - (s1 - 1);
      cinfo.xyZ[c1][2] = br;
      rep->pm.xyZ[i][0] = cinfo.xyZ[c0][1];
      rep->pm.xyZ[i][1] = cinfo.xyZ[c0][2];
      rep->pm.xyZ[i][2] = cinfo.xyZ[c1][1];
      rep->pm.xyZ[i][3] = cinfo.xyZ[c1][2];
      rep->pm.xyZ[i][4] = cinfo.xyZ[c0][3];
      rep->pm.xyZ[i][5] = cinfo.xyZ[c1][3];
   }
   for (i = 0; i < npoints; i++) {
      ae_assert(cinfo.xyZ[i][1] == cinfo.xyZ[i][2], "Assertion failed");
      rep->p.xZ[i] = cinfo.xyZ[i][1];
   }
// Calculate Rep.PZ
   ae_matrix_set_length(&rep->pz, npoints - 1, 2);
   for (i = 0; i < npoints - 1; i++) {
      rep->pz.xyZ[i][0] = rep->z.xyZ[i][0];
      rep->pz.xyZ[i][1] = rep->z.xyZ[i][1];
      if (rep->pz.xyZ[i][0] < npoints) {
         rep->pz.xyZ[i][0] = rep->p.xZ[rep->pz.xyZ[i][0]];
      }
      if (rep->pz.xyZ[i][1] < npoints) {
         rep->pz.xyZ[i][1] = rep->p.xZ[rep->pz.xyZ[i][1]];
      }
   }
   ae_frame_leave();
}

// This function performs agglomerative hierarchical clustering
//
// NOTE: Agglomerative  hierarchical  clustering  algorithm  has two  phases:
//       distance matrix calculation and clustering  itself. Only first phase
//       (distance matrix  calculation)  is  accelerated  by  Intel  MKL  and
//       multithreading. Thus, acceleration is significant only for medium or
//       high-dimensional problems.
//
//       Although activating multithreading gives some speedup  over  single-
//       threaded execution, you  should  not  expect  nearly-linear  scaling
//       with respect to cores count.
//
// Inputs:
//     S       -   clusterizer state, initialized by ClusterizerCreate()
//
// Outputs:
//     Rep     -   clustering results; see description of AHCReport
//                 structure for more information.
//
// NOTE 1: hierarchical clustering algorithms require large amounts of memory.
//         In particular, this implementation needs  sizeof(double)*NPoints^2
//         bytes, which are used to store distance matrix. In  case  we  work
//         with user-supplied matrix, this amount is multiplied by 2 (we have
//         to store original matrix and to work with its copy).
//
//         For example, problem with 10000 points  would require 800M of RAM,
//         even when working in a 1-dimensional space.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizerrunahc(const clusterizerstate &s, ahcreport &rep);
void clusterizerrunahc(clusterizerstate *s, ahcreport *rep) {
   ae_int_t npoints;
   ae_int_t nfeatures;
   SetObj(ahcreport, rep);
   npoints = s->npoints;
   nfeatures = s->nfeatures;
// Fill Rep.NPoints, quick exit when NPoints <= 1
   rep->npoints = npoints;
   if (npoints == 0) {
      ae_vector_set_length(&rep->p, 0);
      ae_matrix_set_length(&rep->z, 0, 0);
      ae_matrix_set_length(&rep->pz, 0, 0);
      ae_matrix_set_length(&rep->pm, 0, 0);
      ae_vector_set_length(&rep->mergedist, 0);
      rep->terminationtype = 1;
      return;
   }
   if (npoints == 1) {
      ae_vector_set_length(&rep->p, 1);
      ae_matrix_set_length(&rep->z, 0, 0);
      ae_matrix_set_length(&rep->pz, 0, 0);
      ae_matrix_set_length(&rep->pm, 0, 0);
      ae_vector_set_length(&rep->mergedist, 0);
      rep->p.xZ[0] = 0;
      rep->terminationtype = 1;
      return;
   }
// More than one point
   if (s->disttype == -1) {
   // Run clusterizer with user-supplied distance matrix
      clustering_clusterizerrunahcinternal(s, &s->d, rep);
      return;
   } else {
   // Check combination of AHC algo and distance type
      if (s->ahcalgo == 4 && s->disttype != 2) {
         rep->terminationtype = -5;
         return;
      }
   // Build distance matrix D.
      clusterizergetdistancesbuf(&s->distbuf, &s->xy, npoints, nfeatures, s->disttype, &s->tmpd);
   // Run clusterizer
      clustering_clusterizerrunahcinternal(s, &s->tmpd, rep);
      return;
   }
}

// This function performs clustering by k-means++ algorithm.
//
// You may change algorithm properties by calling:
// * ClusterizerSetKMeansLimits() to change number of restarts or iterations
// * ClusterizerSetKMeansInit() to change initialization algorithm
//
// By  default,  one  restart  and  unlimited number of iterations are  used.
// Initialization algorithm is chosen automatically.
//
// NOTE: k-means clustering  algorithm has two  phases:  selection of initial
//       centers and clustering  itself.  ALGLIB  parallelizes  both  phases.
//       Parallel version is optimized for the following  scenario: medium or
//       high-dimensional problem (8 or more dimensions) with large number of
//       points and clusters. However, some speed-up  can  be  obtained  even
//       when assumptions above are violated.
//
// Inputs:
//     S       -   clusterizer state, initialized by ClusterizerCreate()
//     K       -   number of clusters, K >= 0.
//                 K  can  be  zero only when algorithm is called  for  empty
//                 dataset,  in   this   case   completion  code  is  set  to
//                 success (+1).
//                 If  K=0  and  dataset  size  is  non-zero,  we   can   not
//                 meaningfully assign points to some center  (there  are  no
//                 centers because K=0) and  return  -3  as  completion  code
//                 (failure).
//
// Outputs:
//     Rep     -   clustering results; see description of KMeansReport
//                 structure for more information.
//
// NOTE 1: k-means  clustering  can  be  performed  only  for  datasets  with
//         Euclidean  distance  function.  Algorithm  will  return   negative
//         completion code in Rep.TerminationType in case dataset  was  added
//         to clusterizer with DistType other than Euclidean (or dataset  was
//         specified by distance matrix instead of explicitly given points).
//
// NOTE 2: by default, k-means uses non-deterministic seed to initialize  RNG
//         which is used to select initial centers. As  result,  each  run of
//         algorithm may return different values. If you  need  deterministic
//         behavior, use ClusterizerSetSeed() function.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizerrunkmeans(const clusterizerstate &s, const ae_int_t k, kmeansreport &rep);
void clusterizerrunkmeans(clusterizerstate *s, ae_int_t k, kmeansreport *rep) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetObj(kmeansreport, rep);
   NewMatrix(dummy, 0, 0, DT_REAL);
   ae_assert(k >= 0, "ClusterizerRunKMeans: K<0");
// Incorrect distance type
   if (s->disttype != 2) {
      rep->npoints = s->npoints;
      rep->terminationtype = -5;
      rep->k = k;
      rep->iterationscount = 0;
      rep->energy = 0.0;
      ae_frame_leave();
      return;
   }
// K>NPoints or (K=0 and NPoints > 0)
   if (k > s->npoints || k == 0 && s->npoints > 0) {
      rep->npoints = s->npoints;
      rep->terminationtype = -3;
      rep->k = k;
      rep->iterationscount = 0;
      rep->energy = 0.0;
      ae_frame_leave();
      return;
   }
// No points
   if (s->npoints == 0) {
      rep->npoints = 0;
      rep->terminationtype = 1;
      rep->k = k;
      rep->iterationscount = 0;
      rep->energy = 0.0;
      ae_frame_leave();
      return;
   }
// Normal case:
// 1 <= K <= NPoints, Euclidean distance
   rep->npoints = s->npoints;
   rep->nfeatures = s->nfeatures;
   rep->k = k;
   rep->npoints = s->npoints;
   rep->nfeatures = s->nfeatures;
   kmeansgenerateinternal(&s->xy, s->npoints, s->nfeatures, k, s->kmeansinitalgo, s->seed, s->kmeansmaxits, s->kmeansrestarts, s->kmeansdbgnoits, &rep->terminationtype, &rep->iterationscount, &dummy, false, &rep->c, true, &rep->cidx, &rep->energy, &s->kmeanstmp);
   ae_frame_leave();
}

// This function returns distance matrix for dataset
//
// Inputs:
//     XY      -   array[NPoints,NFeatures], dataset
//     NPoints -   number of points, >= 0
//     NFeatures-  number of features, >= 1
//     DistType-   distance function:
//                 *  0    Chebyshev distance  (L-inf norm)
//                 *  1    city block distance (L1 norm)
//                 *  2    Euclidean distance  (L2 norm, non-squared)
//                 * 10    Pearson correlation:
//                         dist(a,b) = 1-corr(a,b)
//                 * 11    Absolute Pearson correlation:
//                         dist(a,b) = 1-|corr(a,b)|
//                 * 12    Uncentered Pearson correlation (cosine of the angle):
//                         dist(a,b) = a'*b/(|a|*|b|)
//                 * 13    Absolute uncentered Pearson correlation
//                         dist(a,b) = |a'*b|/(|a|*|b|)
//                 * 20    Spearman rank correlation:
//                         dist(a,b) = 1-rankcorr(a,b)
//                 * 21    Absolute Spearman rank correlation
//                         dist(a,b) = 1-|rankcorr(a,b)|
//
// Outputs:
//     D       -   array[NPoints,NPoints], distance matrix
//                 (full matrix is returned, with lower and upper triangles)
//
// NOTE:  different distance functions have different performance penalty:
//        * Euclidean or Pearson correlation distances are the fastest ones
//        * Spearman correlation distance function is a bit slower
//        * city block and Chebyshev distances are order of magnitude slower
//
//        The reason behing difference in performance is that correlation-based
//        distance functions are computed using optimized linear algebra kernels,
//        while Chebyshev and city block distance functions are computed using
//        simple nested loops with two branches at each iteration.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizergetdistances(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nfeatures, const ae_int_t disttype, real_2d_array &d);
void clusterizergetdistances(RMatrix *xy, ae_int_t npoints, ae_int_t nfeatures, ae_int_t disttype, RMatrix *d) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   SetMatrix(d);
   NewObj(apbuffers, buf);
   ae_assert(nfeatures >= 1, "ClusterizerGetDistances: NFeatures<1");
   ae_assert(npoints >= 0, "ClusterizerGetDistances: NPoints<1");
   ae_assert(disttype == 0 || disttype == 1 || disttype == 2 || disttype == 10 || disttype == 11 || disttype == 12 || disttype == 13 || disttype == 20 || disttype == 21, "ClusterizerGetDistances: incorrect DistType");
   ae_assert(xy->rows >= npoints, "ClusterizerGetDistances: Rows(XY)<NPoints");
   ae_assert(xy->cols >= nfeatures, "ClusterizerGetDistances: Cols(XY)<NFeatures");
   ae_assert(apservisfinitematrix(xy, npoints, nfeatures), "ClusterizerGetDistances: XY contains NAN/INF");
   clusterizergetdistancesbuf(&buf, xy, npoints, nfeatures, disttype, d);
   ae_frame_leave();
}

// This function recursively evaluates distance matrix  for  SOME  (not all!)
// distance types.
//
// Inputs:
//     XY      -   array[?,NFeatures], dataset
//     NFeatures-  number of features, >= 1
//     DistType-   distance function:
//                 *  0    Chebyshev distance  (L-inf norm)
//                 *  1    city block distance (L1 norm)
//     D       -   preallocated output matrix
//     I0,I1   -   half interval of rows to calculate: [I0,I1) is processed
//     J0,J1   -   half interval of cols to calculate: [J0,J1) is processed
//
// Outputs:
//     D       -   array[NPoints,NPoints], distance matrix
//                 upper triangle and main diagonal are initialized with
//                 data.
//
// NOTE: intersection of [I0,I1) and [J0,J1)  may  completely  lie  in  upper
//       triangle, only partially intersect with it, or have zero intersection.
//       In any case, only intersection of submatrix given by [I0,I1)*[J0,J1)
//       with upper triangle of the matrix is evaluated.
//
//       Say, for 4x4 distance matrix A:
//       * [0,2)*[0,2) will result in evaluation of A00, A01, A11
//       * [2,4)*[2,4) will result in evaluation of A22, A23, A32, A33
//       * [2,4)*[0,2) will result in evaluation of empty set of elements
// ALGLIB: Copyright 07.04.2013 by Sergey Bochkanov
static void clustering_evaluatedistancematrixrec(RMatrix *xy, ae_int_t nfeatures, ae_int_t disttype, RMatrix *d, ae_int_t i0, ae_int_t i1, ae_int_t j0, ae_int_t j1) {
   double rcomplexity;
   ae_int_t len0;
   ae_int_t len1;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   double v;
   double vv;
   ae_assert(disttype == 0 || disttype == 1, "EvaluateDistanceMatrixRec: incorrect DistType");
// Normalize J0/J1:
// * J0:=max(J0,I0) - we ignore lower triangle
// * J1:=max(J1,J0) - normalize J1
   j0 = imax2(j0, i0);
   j1 = imax2(j1, j0);
   if (j1 <= j0 || i1 <= i0) {
      return;
   }
   rcomplexity = clustering_complexitymultiplier * (i1 - i0) * (j1 - j0) * nfeatures;
// Parallelism was activated if: (i1 - i0 > 2 || j1 - j0 > 2) && rcomplexity >= smpactivationlevel()
// Try to process in parallel. Two condtions must hold in order to
// activate parallel processing:
// 1. I1-I0>2 or J1-J0>2
// 2. (I1-I0)*(J1-J0)*NFeatures >= ParallelComplexity
//
// NOTE: all quantities are converted to reals in order to avoid
//       integer overflow during multiplication
//
// NOTE: strict inequality in (1) is necessary to reduce task to 2x2
//       basecases. In future versions we will be able to handle such
//       basecases more efficiently than 1x1 cases.
   if (rcomplexity >= spawnlevel() && (i1 - i0 > 2 || j1 - j0 > 2)) {
   // Recursive division along largest of dimensions
      if (i1 - i0 > j1 - j0) {
         len0 = splitlengtheven(i1 - i0), len1 = i1 - i0 - len0;
         clustering_evaluatedistancematrixrec(xy, nfeatures, disttype, d, i0, i0 + len0, j0, j1);
         clustering_evaluatedistancematrixrec(xy, nfeatures, disttype, d, i0 + len0, i1, j0, j1);
      } else {
         len0 = splitlengtheven(j1 - j0), len1 = j1 - j0 - len0;
         clustering_evaluatedistancematrixrec(xy, nfeatures, disttype, d, i0, i1, j0, j0 + len0);
         clustering_evaluatedistancematrixrec(xy, nfeatures, disttype, d, i0, i1, j0 + len0, j1);
      }
      return;
   }
// Sequential processing
   for (i = i0; i < i1; i++) {
      for (j = j0; j < j1; j++) {
         if (j >= i) {
            v = 0.0;
            if (disttype == 0) {
               for (k = 0; k < nfeatures; k++) {
                  vv = xy->xyR[i][k] - xy->xyR[j][k];
                  if (vv < 0.0) {
                     vv = -vv;
                  }
                  if (vv > v) {
                     v = vv;
                  }
               }
            }
            if (disttype == 1) {
               for (k = 0; k < nfeatures; k++) {
                  vv = xy->xyR[i][k] - xy->xyR[j][k];
                  if (vv < 0.0) {
                     vv = -vv;
                  }
                  v += vv;
               }
            }
            d->xyR[i][j] = v;
         }
      }
   }
}

// Buffered version  of  ClusterizerGetDistances()  which  reuses  previously
// allocated space.
// ALGLIB: Copyright 29.05.2015 by Sergey Bochkanov
void clusterizergetdistancesbuf(apbuffers *buf, RMatrix *xy, ae_int_t npoints, ae_int_t nfeatures, ae_int_t disttype, RMatrix *d) {
   ae_int_t i;
   ae_int_t j;
   double v;
   double vv;
   double vr;
   ae_assert(nfeatures >= 1, "ClusterizerGetDistancesBuf: NFeatures<1");
   ae_assert(npoints >= 0, "ClusterizerGetDistancesBuf: NPoints<1");
   ae_assert(disttype == 0 || disttype == 1 || disttype == 2 || disttype == 10 || disttype == 11 || disttype == 12 || disttype == 13 || disttype == 20 || disttype == 21, "ClusterizerGetDistancesBuf: incorrect DistType");
   ae_assert(xy->rows >= npoints, "ClusterizerGetDistancesBuf: Rows(XY)<NPoints");
   ae_assert(xy->cols >= nfeatures, "ClusterizerGetDistancesBuf: Cols(XY)<NFeatures");
   ae_assert(apservisfinitematrix(xy, npoints, nfeatures), "ClusterizerGetDistancesBuf: XY contains NAN/INF");
// Quick exit
   if (npoints == 0) {
      return;
   }
   if (npoints == 1) {
      matrixsetlengthatleast(d, 1, 1);
      d->xyR[0][0] = 0.0;
      return;
   }
// Build distance matrix D.
   if (disttype == 0 || disttype == 1) {
   // Chebyshev or city-block distances:
   // * recursively calculate upper triangle (with main diagonal)
   // * copy it to the bottom part of the matrix
      matrixsetlengthatleast(d, npoints, npoints);
      clustering_evaluatedistancematrixrec(xy, nfeatures, disttype, d, 0, npoints, 0, npoints);
      rmatrixenforcesymmetricity(d, npoints, true);
      return;
   }
   if (disttype == 2) {
   // Euclidean distance
   //
   // NOTE: parallelization is done within RMatrixSYRK
      matrixsetlengthatleast(d, npoints, npoints);
      matrixsetlengthatleast(&buf->rm0, npoints, nfeatures);
      vectorsetlengthatleast(&buf->ra1, nfeatures);
      vectorsetlengthatleast(&buf->ra0, npoints);
      for (j = 0; j < nfeatures; j++) {
         buf->ra1.xR[j] = 0.0;
      }
      v = 1.0 / (double)npoints;
      for (i = 0; i < npoints; i++) {
         ae_v_addd(buf->ra1.xR, 1, xy->xyR[i], 1, nfeatures, v);
      }
      for (i = 0; i < npoints; i++) {
         ae_v_move(buf->rm0.xyR[i], 1, xy->xyR[i], 1, nfeatures);
         ae_v_sub(buf->rm0.xyR[i], 1, buf->ra1.xR, 1, nfeatures);
      }
      rmatrixsyrk(npoints, nfeatures, 1.0, &buf->rm0, 0, 0, 0, 0.0, d, 0, 0, true);
      for (i = 0; i < npoints; i++) {
         buf->ra0.xR[i] = d->xyR[i][i];
      }
      for (i = 0; i < npoints; i++) {
         d->xyR[i][i] = 0.0;
         for (j = i + 1; j < npoints; j++) {
            v = sqrt(rmax2(buf->ra0.xR[i] + buf->ra0.xR[j] - 2 * d->xyR[i][j], 0.0));
            d->xyR[i][j] = v;
         }
      }
      rmatrixenforcesymmetricity(d, npoints, true);
      return;
   }
   if (disttype == 10 || disttype == 11) {
   // Absolute/nonabsolute Pearson correlation distance
   //
   // NOTE: parallelization is done within PearsonCorrM, which calls RMatrixSYRK internally
      matrixsetlengthatleast(d, npoints, npoints);
      vectorsetlengthatleast(&buf->ra0, npoints);
      matrixsetlengthatleast(&buf->rm0, npoints, nfeatures);
      for (i = 0; i < npoints; i++) {
         v = 0.0;
         for (j = 0; j < nfeatures; j++) {
            v += xy->xyR[i][j];
         }
         v /= nfeatures;
         for (j = 0; j < nfeatures; j++) {
            buf->rm0.xyR[i][j] = xy->xyR[i][j] - v;
         }
      }
      rmatrixsyrk(npoints, nfeatures, 1.0, &buf->rm0, 0, 0, 0, 0.0, d, 0, 0, true);
      for (i = 0; i < npoints; i++) {
         buf->ra0.xR[i] = d->xyR[i][i];
      }
      for (i = 0; i < npoints; i++) {
         d->xyR[i][i] = 0.0;
         for (j = i + 1; j < npoints; j++) {
            v = d->xyR[i][j] / sqrt(buf->ra0.xR[i] * buf->ra0.xR[j]);
            if (disttype == 10) {
               v = 1 - v;
            } else {
               v = 1 - fabs(v);
            }
            v = rmax2(v, 0.0);
            d->xyR[i][j] = v;
         }
      }
      rmatrixenforcesymmetricity(d, npoints, true);
      return;
   }
   if (disttype == 12 || disttype == 13) {
   // Absolute/nonabsolute uncentered Pearson correlation distance
   //
   // NOTE: parallelization is done within RMatrixSYRK
      matrixsetlengthatleast(d, npoints, npoints);
      vectorsetlengthatleast(&buf->ra0, npoints);
      rmatrixsyrk(npoints, nfeatures, 1.0, xy, 0, 0, 0, 0.0, d, 0, 0, true);
      for (i = 0; i < npoints; i++) {
         buf->ra0.xR[i] = d->xyR[i][i];
      }
      for (i = 0; i < npoints; i++) {
         d->xyR[i][i] = 0.0;
         for (j = i + 1; j < npoints; j++) {
            v = d->xyR[i][j] / sqrt(buf->ra0.xR[i] * buf->ra0.xR[j]);
            if (disttype == 13) {
               v = fabs(v);
            }
            v = rmin2(v, 1.0);
            d->xyR[i][j] = 1 - v;
         }
      }
      rmatrixenforcesymmetricity(d, npoints, true);
      return;
   }
   if (disttype == 20 || disttype == 21) {
   // Spearman rank correlation
   //
   // NOTE: parallelization of correlation matrix is done within
   //       PearsonCorrM, which calls RMatrixSYRK internally
      matrixsetlengthatleast(d, npoints, npoints);
      vectorsetlengthatleast(&buf->ra0, npoints);
      matrixsetlengthatleast(&buf->rm0, npoints, nfeatures);
      rmatrixcopy(npoints, nfeatures, xy, 0, 0, &buf->rm0, 0, 0);
      rankdatacentered(&buf->rm0, npoints, nfeatures);
      rmatrixsyrk(npoints, nfeatures, 1.0, &buf->rm0, 0, 0, 0, 0.0, d, 0, 0, true);
      for (i = 0; i < npoints; i++) {
         if (d->xyR[i][i] > 0.0) {
            buf->ra0.xR[i] = 1 / sqrt(d->xyR[i][i]);
         } else {
            buf->ra0.xR[i] = 0.0;
         }
      }
      for (i = 0; i < npoints; i++) {
         v = buf->ra0.xR[i];
         d->xyR[i][i] = 0.0;
         for (j = i + 1; j < npoints; j++) {
            vv = d->xyR[i][j] * v * buf->ra0.xR[j];
            if (disttype == 20) {
               vr = 1 - vv;
            } else {
               vr = 1 - fabs(vv);
            }
            if (vr < 0.0) {
               vr = 0.0;
            }
            d->xyR[i][j] = vr;
         }
      }
      rmatrixenforcesymmetricity(d, npoints, true);
      return;
   }
   ae_assert(false, "Assertion failed");
}

// This function takes as input clusterization report Rep,  desired  clusters
// count K, and builds top K clusters from hierarchical clusterization  tree.
// It returns assignment of points to clusters (array of cluster indexes).
//
// Inputs:
//     Rep     -   report from ClusterizerRunAHC() performed on XY
//     K       -   desired number of clusters, 1 <= K <= NPoints.
//                 K can be zero only when NPoints=0.
//
// Outputs:
//     CIdx    -   array[NPoints], I-th element contains cluster index  (from
//                 0 to K-1) for I-th point of the dataset.
//     CZ      -   array[K]. This array allows  to  convert  cluster  indexes
//                 returned by this function to indexes used by  Rep.Z.  J-th
//                 cluster returned by this function corresponds to  CZ[J]-th
//                 cluster stored in Rep.Z/PZ/PM.
//                 It is guaranteed that CZ[I] < CZ[I+1].
//
// NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
//       Although  they  were  obtained  by  manipulation with top K nodes of
//       dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
//       function does not return information about hierarchy.  Each  of  the
//       clusters stand on its own.
//
// NOTE: Cluster indexes returned by this function  does  not  correspond  to
//       indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
//       representation of the dataset (dendrogram), or you work with  "flat"
//       representation returned by this function.  Each  of  representations
//       has its own clusters indexing system (former uses [0, 2*NPoints-2]),
//       while latter uses [0..K-1]), although  it  is  possible  to  perform
//       conversion from one system to another by means of CZ array, returned
//       by this function, which allows you to convert indexes stored in CIdx
//       to the numeration system used by Rep.Z.
//
// NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
//       it will perform many times faster than  for  K=100.  Its  worst-case
//       performance is O(N*K), although in average case  it  perform  better
//       (up to O(N*log(K))).
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizergetkclusters(const ahcreport &rep, const ae_int_t k, integer_1d_array &cidx, integer_1d_array &cz);
void clusterizergetkclusters(ahcreport *rep, ae_int_t k, ZVector *cidx, ZVector *cz) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t mergeidx;
   ae_int_t i0;
   ae_int_t i1;
   ae_int_t t;
   ae_int_t npoints;
   ae_frame_make(&_frame_block);
   SetVector(cidx);
   SetVector(cz);
   NewVector(presentclusters, 0, DT_BOOL);
   NewVector(clusterindexes, 0, DT_INT);
   NewVector(clustersizes, 0, DT_INT);
   NewVector(tmpidx, 0, DT_INT);
   npoints = rep->npoints;
   ae_assert(npoints >= 0, "ClusterizerGetKClusters: internal error in Rep integrity");
   ae_assert(k >= 0, "ClusterizerGetKClusters: K <= 0");
   ae_assert(k <= npoints, "ClusterizerGetKClusters: K>NPoints");
   ae_assert(k > 0 || npoints == 0, "ClusterizerGetKClusters: K <= 0");
   ae_assert(npoints == rep->npoints, "ClusterizerGetKClusters: NPoints != Rep.NPoints");
// Quick exit
   if (npoints == 0) {
      ae_frame_leave();
      return;
   }
   if (npoints == 1) {
      ae_vector_set_length(cz, 1);
      ae_vector_set_length(cidx, 1);
      cz->xZ[0] = 0;
      cidx->xZ[0] = 0;
      ae_frame_leave();
      return;
   }
// Replay merges, from top to bottom,
// keep track of clusters being present at the moment
   ae_vector_set_length(&presentclusters, 2 * npoints - 1);
   ae_vector_set_length(&tmpidx, npoints);
   for (i = 0; i < 2 * npoints - 2; i++) {
      presentclusters.xB[i] = false;
   }
   presentclusters.xB[2 * npoints - 2] = true;
   for (i = 0; i < npoints; i++) {
      tmpidx.xZ[i] = 2 * npoints - 2;
   }
   for (mergeidx = npoints - 2; mergeidx >= npoints - k; mergeidx--) {
   // Update information about clusters being present at the moment
      presentclusters.xB[npoints + mergeidx] = false;
      presentclusters.xB[rep->z.xyZ[mergeidx][0]] = true;
      presentclusters.xB[rep->z.xyZ[mergeidx][1]] = true;
   // Update TmpIdx according to the current state of the dataset
   //
   // NOTE: TmpIdx contains cluster indexes from [0..2*NPoints-2];
   //       we will convert them to [0..K-1] later.
      i0 = rep->pm.xyZ[mergeidx][0];
      i1 = rep->pm.xyZ[mergeidx][1];
      t = rep->z.xyZ[mergeidx][0];
      for (i = i0; i <= i1; i++) {
         tmpidx.xZ[i] = t;
      }
      i0 = rep->pm.xyZ[mergeidx][2];
      i1 = rep->pm.xyZ[mergeidx][3];
      t = rep->z.xyZ[mergeidx][1];
      for (i = i0; i <= i1; i++) {
         tmpidx.xZ[i] = t;
      }
   }
// Fill CZ - array which allows us to convert cluster indexes
// from one system to another.
   ae_vector_set_length(cz, k);
   ae_vector_set_length(&clusterindexes, 2 * npoints - 1);
   t = 0;
   for (i = 0; i < 2 * npoints - 1; i++) {
      if (presentclusters.xB[i]) {
         cz->xZ[t] = i;
         clusterindexes.xZ[i] = t;
         t++;
      }
   }
   ae_assert(t == k, "ClusterizerGetKClusters: internal error");
// Convert indexes stored in CIdx
   ae_vector_set_length(cidx, npoints);
   for (i = 0; i < npoints; i++) {
      cidx->xZ[i] = clusterindexes.xZ[tmpidx.xZ[rep->p.xZ[i]]];
   }
   ae_frame_leave();
}

// This  function  accepts  AHC  report  Rep,  desired  minimum  intercluster
// distance and returns top clusters from  hierarchical  clusterization  tree
// which are separated by distance R or HIGHER.
//
// It returns assignment of points to clusters (array of cluster indexes).
//
// There is one more function with similar name - ClusterizerSeparatedByCorr,
// which returns clusters with intercluster correlation equal to R  or  LOWER
// (note: higher for distance, lower for correlation).
//
// Inputs:
//     Rep     -   report from ClusterizerRunAHC() performed on XY
//     R       -   desired minimum intercluster distance, R >= 0
//
// Outputs:
//     K       -   number of clusters, 1 <= K <= NPoints
//     CIdx    -   array[NPoints], I-th element contains cluster index  (from
//                 0 to K-1) for I-th point of the dataset.
//     CZ      -   array[K]. This array allows  to  convert  cluster  indexes
//                 returned by this function to indexes used by  Rep.Z.  J-th
//                 cluster returned by this function corresponds to  CZ[J]-th
//                 cluster stored in Rep.Z/PZ/PM.
//                 It is guaranteed that CZ[I] < CZ[I+1].
//
// NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
//       Although  they  were  obtained  by  manipulation with top K nodes of
//       dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
//       function does not return information about hierarchy.  Each  of  the
//       clusters stand on its own.
//
// NOTE: Cluster indexes returned by this function  does  not  correspond  to
//       indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
//       representation of the dataset (dendrogram), or you work with  "flat"
//       representation returned by this function.  Each  of  representations
//       has its own clusters indexing system (former uses [0, 2*NPoints-2]),
//       while latter uses [0..K-1]), although  it  is  possible  to  perform
//       conversion from one system to another by means of CZ array, returned
//       by this function, which allows you to convert indexes stored in CIdx
//       to the numeration system used by Rep.Z.
//
// NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
//       it will perform many times faster than  for  K=100.  Its  worst-case
//       performance is O(N*K), although in average case  it  perform  better
//       (up to O(N*log(K))).
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizerseparatedbydist(const ahcreport &rep, const double r, ae_int_t &k, integer_1d_array &cidx, integer_1d_array &cz);
void clusterizerseparatedbydist(ahcreport *rep, double r, ae_int_t *k, ZVector *cidx, ZVector *cz) {
   *k = 0;
   SetVector(cidx);
   SetVector(cz);
   ae_assert(isfinite(r) && r >= 0.0, "ClusterizerSeparatedByDist: R is infinite or less than 0");
   *k = 1;
   while (*k < rep->npoints && rep->mergedist.xR[rep->npoints - 1 - (*k)] >= r) {
      ++*k;
   }
   clusterizergetkclusters(rep, *k, cidx, cz);
}

// This  function  accepts  AHC  report  Rep,  desired  maximum  intercluster
// correlation and returns top clusters from hierarchical clusterization tree
// which are separated by correlation R or LOWER.
//
// It returns assignment of points to clusters (array of cluster indexes).
//
// There is one more function with similar name - ClusterizerSeparatedByDist,
// which returns clusters with intercluster distance equal  to  R  or  HIGHER
// (note: higher for distance, lower for correlation).
//
// Inputs:
//     Rep     -   report from ClusterizerRunAHC() performed on XY
//     R       -   desired maximum intercluster correlation, -1 <= R <= +1
//
// Outputs:
//     K       -   number of clusters, 1 <= K <= NPoints
//     CIdx    -   array[NPoints], I-th element contains cluster index  (from
//                 0 to K-1) for I-th point of the dataset.
//     CZ      -   array[K]. This array allows  to  convert  cluster  indexes
//                 returned by this function to indexes used by  Rep.Z.  J-th
//                 cluster returned by this function corresponds to  CZ[J]-th
//                 cluster stored in Rep.Z/PZ/PM.
//                 It is guaranteed that CZ[I] < CZ[I+1].
//
// NOTE: K clusters built by this subroutine are assumed to have no hierarchy.
//       Although  they  were  obtained  by  manipulation with top K nodes of
//       dendrogram  (i.e.  hierarchical  decomposition  of  dataset),   this
//       function does not return information about hierarchy.  Each  of  the
//       clusters stand on its own.
//
// NOTE: Cluster indexes returned by this function  does  not  correspond  to
//       indexes returned in Rep.Z/PZ/PM. Either you work  with  hierarchical
//       representation of the dataset (dendrogram), or you work with  "flat"
//       representation returned by this function.  Each  of  representations
//       has its own clusters indexing system (former uses [0, 2*NPoints-2]),
//       while latter uses [0..K-1]), although  it  is  possible  to  perform
//       conversion from one system to another by means of CZ array, returned
//       by this function, which allows you to convert indexes stored in CIdx
//       to the numeration system used by Rep.Z.
//
// NOTE: this subroutine is optimized for moderate values of K. Say, for  K=5
//       it will perform many times faster than  for  K=100.  Its  worst-case
//       performance is O(N*K), although in average case  it  perform  better
//       (up to O(N*log(K))).
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
// API: void clusterizerseparatedbycorr(const ahcreport &rep, const double r, ae_int_t &k, integer_1d_array &cidx, integer_1d_array &cz);
void clusterizerseparatedbycorr(ahcreport *rep, double r, ae_int_t *k, ZVector *cidx, ZVector *cz) {
   *k = 0;
   SetVector(cidx);
   SetVector(cz);
   ae_assert(isfinite(r) && r >= -1.0 && r <= 1.0, "ClusterizerSeparatedByCorr: R is infinite or less than 0");
   *k = 1;
   while (*k < rep->npoints && rep->mergedist.xR[rep->npoints - 1 - (*k)] >= 1 - r) {
      ++*k;
   }
   clusterizergetkclusters(rep, *k, cidx, cz);
}

// K-means++ initialization
//
// Inputs:
//     Buf         -   special reusable structure which stores previously allocated
//                     memory, intended to avoid memory fragmentation when solving
//                     multiple subsequent problems. Must be initialized prior to
//                     usage.
//
// Outputs:
//     Buf         -   initialized structure
// ALGLIB: Copyright 24.07.2015 by Sergey Bochkanov
void kmeansinitbuf(kmeansbuffers *buf) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   NewObj(apbuffers, updateseed);
   ae_shared_pool_set_seed(&buf->updatepool, &updateseed, sizeof(updateseed), apbuffers_init, apbuffers_copy, apbuffers_free);
   ae_frame_leave();
}

// This function selects initial centers according to specified initialization
// algorithm.
//
// IMPORTANT: this function provides no  guarantees  regarding  selection  of
//            DIFFERENT  centers.  Centers  returned  by  this  function  may
//            include duplicates (say, when random sampling is  used). It  is
//            also possible that some centers are empty.
//            Algorithm which uses this function must be able to deal with it.
//            Say, you may want to use FixCenters() in order to fix empty centers.
//
// Inputs:
//     XY          -   dataset, array [0..NPoints-1,0..NVars-1].
//     NPoints     -   points count
//     NVars       -   number of variables, NVars >= 1
//     InitAlgo    -   initialization algorithm:
//                     * 0 - automatic selection of best algorithm
//                     * 1 - random selection
//                     * 2 - k-means++
//                     * 3 - fast-greedy init
//                     *-1 - first K rows of dataset are used (debug algorithm)
//     RS          -   RNG used to select centers
//     K           -   number of centers, K >= 1
//     CT          -   possibly preallocated output buffer, resized if needed
//     InitBuf     -   internal buffer, possibly unitialized instance of
//                     APBuffers. It is recommended to use this instance only
//                     with SelectInitialCenters() and FixCenters() functions,
//                     because these functions may allocate really large storage.
//     UpdatePool  -   shared pool seeded with instance of APBuffers structure
//                     (seed instance can be unitialized). Used internally with
//                     KMeansUpdateDistances() function. It is recommended
//                     to use this pool ONLY with KMeansUpdateDistances()
//                     function.
//
// Outputs:
//     CT          -   set of K clusters, one per row
//
// Result:
//     True on success, False on failure (impossible to create K independent clusters)
// ALGLIB: Copyright 21.01.2015 by Sergey Bochkanov
static void clustering_selectinitialcenters(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t initalgo, hqrndstate *rs, ae_int_t k, RMatrix *ct, apbuffers *initbuf, ae_shared_pool *updatepool) {
   ae_int_t cidx;
   ae_int_t i;
   ae_int_t j;
   double v;
   double vv;
   double s;
   ae_int_t lastnz;
   ae_int_t ptidx;
   ae_int_t samplesize;
   ae_int_t samplescntnew;
   ae_int_t samplescntall;
   double samplescale;
// Check parameters
   ae_assert(npoints > 0, "SelectInitialCenters: internal error");
   ae_assert(nvars > 0, "SelectInitialCenters: internal error");
   ae_assert(k > 0, "SelectInitialCenters: internal error");
   if (initalgo == 0) {
      initalgo = 3;
   }
   matrixsetlengthatleast(ct, k, nvars);
// Random initialization
   if (initalgo == -1) {
      for (i = 0; i < k; i++) {
         ae_v_move(ct->xyR[i], 1, xy->xyR[i % npoints], 1, nvars);
      }
      return;
   }
// Random initialization
   if (initalgo == 1) {
      for (i = 0; i < k; i++) {
         j = hqrnduniformi(rs, npoints);
         ae_v_move(ct->xyR[i], 1, xy->xyR[j], 1, nvars);
      }
      return;
   }
// k-means++ initialization
   if (initalgo == 2) {
   // Prepare distances array.
   // Select initial center at random.
      vectorsetlengthatleast(&initbuf->ra0, npoints);
      for (i = 0; i < npoints; i++) {
         initbuf->ra0.xR[i] = ae_maxrealnumber;
      }
      ptidx = hqrnduniformi(rs, npoints);
      ae_v_move(ct->xyR[0], 1, xy->xyR[ptidx], 1, nvars);
   // For each newly added center repeat:
   // * reevaluate distances from points to best centers
   // * sample points with probability dependent on distance
   // * add new center
      for (cidx = 0; cidx < k - 1; cidx++) {
      // Reevaluate distances
         s = 0.0;
         for (i = 0; i < npoints; i++) {
            v = 0.0;
            for (j = 0; j < nvars; j++) {
               vv = xy->xyR[i][j] - ct->xyR[cidx][j];
               v += vv * vv;
            }
            if (v < initbuf->ra0.xR[i]) {
               initbuf->ra0.xR[i] = v;
            }
            s += initbuf->ra0.xR[i];
         }
      // If all distances are zero, it means that we can not find enough
      // distinct points. In this case we just select non-distinct center
      // at random and continue iterations. This issue will be handled
      // later in the FixCenters() function.
         if (s == 0.0) {
            ptidx = hqrnduniformi(rs, npoints);
            ae_v_move(ct->xyR[cidx + 1], 1, xy->xyR[ptidx], 1, nvars);
            continue;
         }
      // Select point as center using its distance.
      // We also handle situation when because of rounding errors
      // no point was selected - in this case, last non-zero one
      // will be used.
         v = hqrnduniformr(rs);
         vv = 0.0;
         lastnz = -1;
         ptidx = -1;
         for (i = 0; i < npoints; i++) {
            if (initbuf->ra0.xR[i] == 0.0) {
               continue;
            }
            lastnz = i;
            vv += initbuf->ra0.xR[i];
            if (v <= vv / s) {
               ptidx = i;
               break;
            }
         }
         ae_assert(lastnz >= 0, "SelectInitialCenters: integrity error");
         if (ptidx < 0) {
            ptidx = lastnz;
         }
         ae_v_move(ct->xyR[cidx + 1], 1, xy->xyR[ptidx], 1, nvars);
      }
      return;
   }
// "Fast-greedy" algorithm based on "Scalable k-means++".
//
// We perform several rounds, within each round we sample about 0.5*K points
// (not exactly 0.5*K) until we have 2*K points sampled. Before each round
// we calculate distances from dataset points to closest points sampled so far.
// We sample dataset points independently using distance xtimes 0.5*K divided by total
// as probability (similar to k-means++, but each point is sampled independently;
// after each round we have roughtly 0.5*K points added to sample).
//
// After sampling is done, we run "greedy" version of k-means++ on this subsample
// which selects most distant point on every round.
   if (initalgo == 3) {
   // Prepare arrays.
   // Select initial center at random, add it to "new" part of sample,
   // which is stored at the beginning of the array
      samplesize = 2 * k;
      samplescale = 0.5 * k;
      matrixsetlengthatleast(&initbuf->rm0, samplesize, nvars);
      ptidx = hqrnduniformi(rs, npoints);
      ae_v_move(initbuf->rm0.xyR[0], 1, xy->xyR[ptidx], 1, nvars);
      samplescntnew = 1;
      samplescntall = 1;
      vectorsetlengthatleast(&initbuf->ra0, npoints);
      vectorsetlengthatleast(&initbuf->ra1, npoints);
      vectorsetlengthatleast(&initbuf->ia1, npoints);
      for (i = 0; i < npoints; i++) {
         initbuf->ra0.xR[i] = ae_maxrealnumber;
      }
   // Repeat until samples count is 2*K
      while (samplescntall < samplesize) {
      // Evaluate distances from points to NEW centers, store to RA1.
      // Reset counter of "new" centers.
         kmeansupdatedistances(xy, 0, npoints, nvars, &initbuf->rm0, samplescntall - samplescntnew, samplescntall, &initbuf->ia1, &initbuf->ra1, updatepool);
         samplescntnew = 0;
      // Merge new distances with old ones.
      // Calculate sum of distances, if sum is exactly zero - fill sample
      // by randomly selected points and terminate.
         s = 0.0;
         for (i = 0; i < npoints; i++) {
            initbuf->ra0.xR[i] = rmin2(initbuf->ra0.xR[i], initbuf->ra1.xR[i]);
            s += initbuf->ra0.xR[i];
         }
         if (s == 0.0) {
            while (samplescntall < samplesize) {
               ptidx = hqrnduniformi(rs, npoints);
               ae_v_move(initbuf->rm0.xyR[samplescntall], 1, xy->xyR[ptidx], 1, nvars);
               samplescntall++;
               samplescntnew++;
            }
            break;
         }
      // Sample points independently.
         for (i = 0; i < npoints; i++) {
            if (samplescntall == samplesize) {
               break;
            }
            if (initbuf->ra0.xR[i] == 0.0) {
               continue;
            }
            if (hqrnduniformr(rs) <= samplescale * initbuf->ra0.xR[i] / s) {
               ae_v_move(initbuf->rm0.xyR[samplescntall], 1, xy->xyR[i], 1, nvars);
               samplescntall++;
               samplescntnew++;
            }
         }
      }
   // Run greedy version of k-means on sampled points
      vectorsetlengthatleast(&initbuf->ra0, samplescntall);
      for (i = 0; i < samplescntall; i++) {
         initbuf->ra0.xR[i] = ae_maxrealnumber;
      }
      ptidx = hqrnduniformi(rs, samplescntall);
      ae_v_move(ct->xyR[0], 1, initbuf->rm0.xyR[ptidx], 1, nvars);
      for (cidx = 0; cidx < k - 1; cidx++) {
      // Reevaluate distances
         for (i = 0; i < samplescntall; i++) {
            v = 0.0;
            for (j = 0; j < nvars; j++) {
               vv = initbuf->rm0.xyR[i][j] - ct->xyR[cidx][j];
               v += vv * vv;
            }
            if (v < initbuf->ra0.xR[i]) {
               initbuf->ra0.xR[i] = v;
            }
         }
      // Select point as center in greedy manner - most distant
      // point is selected.
         ptidx = 0;
         for (i = 0; i < samplescntall; i++) {
            if (initbuf->ra0.xR[i] > initbuf->ra0.xR[ptidx]) {
               ptidx = i;
            }
         }
         ae_v_move(ct->xyR[cidx + 1], 1, initbuf->rm0.xyR[ptidx], 1, nvars);
      }
      return;
   }
// Internal error
   ae_assert(false, "SelectInitialCenters: internal error");
}

// This function "fixes" centers, i.e. replaces ones which have  no  neighbor
// points by new centers which have at least one neighbor. If it is impossible
// to fix centers (not enough distinct points in the dataset), this function
// returns False.
//
// Inputs:
//     XY          -   dataset, array [0..NPoints-1,0..NVars-1].
//     NPoints     -   points count, >= 1
//     NVars       -   number of variables, NVars >= 1
//     CT          -   centers
//     K           -   number of centers, K >= 1
//     InitBuf     -   internal buffer, possibly unitialized instance of
//                     APBuffers. It is recommended to use this instance only
//                     with SelectInitialCenters() and FixCenters() functions,
//                     because these functions may allocate really large storage.
//     UpdatePool  -   shared pool seeded with instance of APBuffers structure
//                     (seed instance can be unitialized). Used internally with
//                     KMeansUpdateDistances() function. It is recommended
//                     to use this pool ONLY with KMeansUpdateDistances()
//                     function.
//
// Outputs:
//     CT          -   set of K centers, one per row
//
// Result:
//     True on success, False on failure (impossible to create K independent clusters)
// ALGLIB: Copyright 21.01.2015 by Sergey Bochkanov
static bool clustering_fixcenters(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, RMatrix *ct, ae_int_t k, apbuffers *initbuf, ae_shared_pool *updatepool) {
   ae_int_t fixiteration;
   ae_int_t centertofix;
   ae_int_t i;
   ae_int_t j;
   ae_int_t pdistant;
   double ddistant;
   double v;
   bool result;
   ae_assert(npoints >= 1, "FixCenters: internal error");
   ae_assert(nvars >= 1, "FixCenters: internal error");
   ae_assert(k >= 1, "FixCenters: internal error");
// Calculate distances from points to best centers (RA0)
// and best center indexes (IA0)
   vectorsetlengthatleast(&initbuf->ia0, npoints);
   vectorsetlengthatleast(&initbuf->ra0, npoints);
   kmeansupdatedistances(xy, 0, npoints, nvars, ct, 0, k, &initbuf->ia0, &initbuf->ra0, updatepool);
// Repeat loop:
// * find first center which has no corresponding point
// * set it to the most distant (from the rest of the centerset) point
// * recalculate distances, update IA0/RA0
// * repeat
//
// Loop is repeated for at most 2*K iterations. It is stopped once we have
// no "empty" clusters.
   vectorsetlengthatleast(&initbuf->ba0, k);
   for (fixiteration = 0; fixiteration <= 2 * k; fixiteration++) {
   // Select center to fix (one which is not mentioned in IA0),
   // terminate if there is no such center.
   // BA0[] stores True for centers which have at least one point.
      for (i = 0; i < k; i++) {
         initbuf->ba0.xB[i] = false;
      }
      for (i = 0; i < npoints; i++) {
         initbuf->ba0.xB[initbuf->ia0.xZ[i]] = true;
      }
      centertofix = -1;
      for (i = 0; i < k; i++) {
         if (!initbuf->ba0.xB[i]) {
            centertofix = i;
            break;
         }
      }
      if (centertofix < 0) {
         result = true;
         return result;
      }
   // Replace center to fix by the most distant point.
   // Update IA0/RA0
      pdistant = 0;
      ddistant = initbuf->ra0.xR[pdistant];
      for (i = 0; i < npoints; i++) {
         if (initbuf->ra0.xR[i] > ddistant) {
            ddistant = initbuf->ra0.xR[i];
            pdistant = i;
         }
      }
      if (ddistant == 0.0) {
         break;
      }
      ae_v_move(ct->xyR[centertofix], 1, xy->xyR[pdistant], 1, nvars);
      for (i = 0; i < npoints; i++) {
         v = 0.0;
         for (j = 0; j < nvars; j++) {
            v += ae_sqr(xy->xyR[i][j] - ct->xyR[centertofix][j]);
         }
         if (v < initbuf->ra0.xR[i]) {
            initbuf->ra0.xR[i] = v;
            initbuf->ia0.xZ[i] = centertofix;
         }
      }
   }
   result = false;
   return result;
}

// K-means++ clusterization
//
// Inputs:
//     XY          -   dataset, array [0..NPoints-1,0..NVars-1].
//     NPoints     -   dataset size, NPoints >= K
//     NVars       -   number of variables, NVars >= 1
//     K           -   desired number of clusters, K >= 1
//     InitAlgo    -   initialization algorithm:
//                     * 0 - automatic selection of best algorithm
//                     * 1 - random selection of centers
//                     * 2 - k-means++
//                     * 3 - fast-greedy init
//                     *-1 - first K rows of dataset are used
//                           (special debug algorithm)
//     Seed        -   seed value for internal RNG:
//                     * positive value is used to initialize RNG in order to
//                       induce deterministic behavior of algorithm
//                     * zero or negative value means  that random  seed   is
//                       generated
//     MaxIts      -   iterations limit or zero for no limit
//     Restarts    -   number of restarts, Restarts >= 1
//     KMeansDbgNoIts- debug flag; if set, Lloyd's iteration is not performed,
//                     only initialization phase.
//     Buf         -   special reusable structure which stores previously allocated
//                     memory, intended to avoid memory fragmentation when solving
//                     multiple subsequent problems:
//                     * MUST BE INITIALIZED WITH KMeansInitBuffers() CALL BEFORE
//                       FIRST PASS TO THIS FUNCTION!
//                     * subsequent passes must be made without re-initialization
//
// Outputs:
//     Info        -   return code:
//                     * -3, if task is degenerate (number of distinct points is
//                           less than K)
//                     * -1, if incorrect NPoints/NFeatures/K/Restarts was passed
//                     *  1, if subroutine finished successfully
//     IterationsCount- actual number of iterations performed by clusterizer
//     CCol        -   array[0..NVars-1,0..K-1].matrix whose columns store
//                     cluster's centers
//     NeedCCol    -   True in case caller requires to store result in CCol
//     CRow        -   array[0..K-1,0..NVars-1], same as CCol, but centers are
//                     stored in rows
//     NeedCRow    -   True in case caller requires to store result in CCol
//     XYC         -   array[NPoints], which contains cluster indexes
//     Energy      -   merit function of clusterization
// ALGLIB: Copyright 21.03.2009 by Sergey Bochkanov
void kmeansgenerateinternal(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t k, ae_int_t initalgo, ae_int_t seed, ae_int_t maxits, ae_int_t restarts, bool kmeansdbgnoits, ae_int_t *info, ae_int_t *iterationscount, RMatrix *ccol, bool needccol, RMatrix *crow, bool needcrow, ZVector *xyc, double *energy, kmeansbuffers *buf) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t i1;
   double e;
   double eprev;
   double v;
   double vv;
   bool waschanges;
   bool zerosizeclusters;
   ae_int_t pass;
   ae_int_t itcnt;
   ae_frame_make(&_frame_block);
   *info = 0;
   *iterationscount = 0;
   SetMatrix(ccol);
   SetMatrix(crow);
   SetVector(xyc);
   *energy = 0;
   NewObj(hqrndstate, rs);
// Test parameters
   if (npoints < k || nvars < 1 || k < 1 || restarts < 1) {
      *info = -1;
      *iterationscount = 0;
      ae_frame_leave();
      return;
   }
// TODO: special case K=1
// TODO: special case K=NPoints
   *info = 1;
   *iterationscount = 0;
// Multiple passes of k-means++ algorithm
   if (seed <= 0) {
      hqrndrandomize(&rs);
   } else {
      hqrndseed(325355, seed, &rs);
   }
   ae_vector_set_length(xyc, npoints);
   matrixsetlengthatleast(&buf->ct, k, nvars);
   matrixsetlengthatleast(&buf->ctbest, k, nvars);
   vectorsetlengthatleast(&buf->xycprev, npoints);
   vectorsetlengthatleast(&buf->xycbest, npoints);
   vectorsetlengthatleast(&buf->d2, npoints);
   vectorsetlengthatleast(&buf->csizes, k);
   *energy = ae_maxrealnumber;
   for (pass = 1; pass <= restarts; pass++) {
   // Select initial centers.
   //
   // Note that for performance reasons centers are stored in ROWS of CT, not
   // in columns. We'll transpose CT in the end and store it in the C.
   //
   // Also note that SelectInitialCenters() may return degenerate set of centers
   // (some of them have no corresponding points in dataset, some are non-distinct).
   // Algorithm below is robust enough to deal with such set.
      clustering_selectinitialcenters(xy, npoints, nvars, initalgo, &rs, k, &buf->ct, &buf->initbuf, &buf->updatepool);
   // Lloyd's iteration
      if (!kmeansdbgnoits) {
      // Perform iteration as usual, in normal mode
         for (i = 0; i < npoints; i++) {
            xyc->xZ[i] = -1;
         }
         eprev = ae_maxrealnumber;
         e = ae_maxrealnumber;
         itcnt = 0;
         while (maxits == 0 || itcnt < maxits) {
         // Update iteration counter
            itcnt++;
            ++*iterationscount;
         // Call KMeansUpdateDistances(), fill XYC with center numbers,
         // D2 with center distances.
            for (i = 0; i < npoints; i++) {
               buf->xycprev.xZ[i] = xyc->xZ[i];
            }
            kmeansupdatedistances(xy, 0, npoints, nvars, &buf->ct, 0, k, xyc, &buf->d2, &buf->updatepool);
            waschanges = false;
            for (i = 0; i < npoints; i++) {
               waschanges = waschanges || xyc->xZ[i] != buf->xycprev.xZ[i];
            }
         // Update centers
            for (j = 0; j < k; j++) {
               buf->csizes.xZ[j] = 0;
            }
            for (i = 0; i < k; i++) {
               for (j = 0; j < nvars; j++) {
                  buf->ct.xyR[i][j] = 0.0;
               }
            }
            for (i = 0; i < npoints; i++) {
               buf->csizes.xZ[xyc->xZ[i]]++;
               ae_v_add(buf->ct.xyR[xyc->xZ[i]], 1, xy->xyR[i], 1, nvars);
            }
            zerosizeclusters = false;
            for (j = 0; j < k; j++) {
               if (buf->csizes.xZ[j] != 0) {
                  v = 1.0 / (double)buf->csizes.xZ[j];
                  ae_v_muld(buf->ct.xyR[j], 1, nvars, v);
               }
               zerosizeclusters = zerosizeclusters || buf->csizes.xZ[j] == 0;
            }
            if (zerosizeclusters) {
            // Some clusters have zero size - rare, but possible.
            // We'll choose new centers for such clusters using k-means++ rule
            // and restart algorithm, decrementing iteration counter
            // in order to allow one more iteration (this one was useless
            // and should not be counted).
               if (!clustering_fixcenters(xy, npoints, nvars, &buf->ct, k, &buf->initbuf, &buf->updatepool)) {
                  *info = -3;
                  ae_frame_leave();
                  return;
               }
               itcnt--;
               continue;
            }
         // Stop if one of two conditions is met:
         // 1. nothing has changed during iteration
         // 2. energy function increased after recalculation on new centers
            e = 0.0;
            for (i = 0; i < npoints; i++) {
               v = 0.0;
               i1 = xyc->xZ[i];
               for (j = 0; j < nvars; j++) {
                  vv = xy->xyR[i][j] - buf->ct.xyR[i1][j];
                  v += vv * vv;
               }
               e += v;
            }
            if (!waschanges || e >= eprev) {
               break;
            }
         // Update EPrev
            eprev = e;
         }
      } else {
      // Debug mode: no Lloyd's iteration.
      // We just calculate potential E.
         kmeansupdatedistances(xy, 0, npoints, nvars, &buf->ct, 0, k, xyc, &buf->d2, &buf->updatepool);
         e = 0.0;
         for (i = 0; i < npoints; i++) {
            e += buf->d2.xR[i];
         }
      }
   // Compare E with best centers found so far
      if (e < *energy) {
      // store partition.
         *energy = e;
         copymatrix(&buf->ct, 0, k - 1, 0, nvars - 1, &buf->ctbest, 0, k - 1, 0, nvars - 1);
         for (i = 0; i < npoints; i++) {
            buf->xycbest.xZ[i] = xyc->xZ[i];
         }
      }
   }
// Copy and transpose
   if (needccol) {
      ae_matrix_set_length(ccol, nvars, k);
      copyandtranspose(&buf->ctbest, 0, k - 1, 0, nvars - 1, ccol, 0, nvars - 1, 0, k - 1);
   }
   if (needcrow) {
      ae_matrix_set_length(crow, k, nvars);
      rmatrixcopy(k, nvars, &buf->ctbest, 0, 0, crow, 0, 0);
   }
   for (i = 0; i < npoints; i++) {
      xyc->xZ[i] = buf->xycbest.xZ[i];
   }
   ae_frame_leave();
}

// This procedure recalculates distances from points to centers  and  assigns
// each point to closest center.
//
// Inputs:
//     XY          -   dataset, array [0..NPoints-1,0..NVars-1].
//     Idx0,Idx1   -   define range of dataset [Idx0,Idx1) to process;
//                     right boundary is not included.
//     NVars       -   number of variables, NVars >= 1
//     CT          -   matrix of centers, centers are stored in rows
//     CIdx0,CIdx1 -   define range of centers [CIdx0,CIdx1) to process;
//                     right boundary is not included.
//     XYC         -   preallocated output buffer,
//     XYDist2     -   preallocated output buffer
//     Tmp         -   temporary buffer, automatically reallocated if needed
//     BufferPool  -   shared pool seeded with instance of APBuffers structure
//                     (seed instance can be unitialized). It is recommended
//                     to use this pool only with KMeansUpdateDistances()
//                     function.
//
// Outputs:
//     XYC         -   new assignment of points to centers are stored
//                     in [Idx0,Idx1)
//     XYDist2     -   squared distances from points to their centers are
//                     stored in [Idx0,Idx1)
// ALGLIB: Copyright 21.01.2015 by Sergey Bochkanov
void kmeansupdatedistances(RMatrix *xy, ae_int_t idx0, ae_int_t idx1, ae_int_t nvars, RMatrix *ct, ae_int_t cidx0, ae_int_t cidx1, ZVector *xyc, RVector *xydist2, ae_shared_pool *bufferpool) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t i0;
   ae_int_t i1;
   ae_int_t j;
   ae_int_t cclosest;
   double dclosest;
   double vv;
   double rcomplexity;
   ae_int_t task0;
   ae_int_t task1;
   ae_int_t pblkcnt;
   ae_int_t cblkcnt;
   ae_int_t vblkcnt;
   ae_int_t pblk;
   ae_int_t cblk;
   ae_int_t vblk;
   ae_int_t p0;
   ae_int_t p1;
   ae_int_t c0;
   ae_int_t c1;
   ae_int_t v0;
   ae_int_t v1;
   double v00;
   double v01;
   double v10;
   double v11;
   double vp0;
   double vp1;
   double vc0;
   double vc1;
   ae_int_t pcnt;
   ae_int_t pcntpadded;
   ae_int_t ccnt;
   ae_int_t ccntpadded;
   ae_int_t offs0;
   ae_int_t offs00;
   ae_int_t offs01;
   ae_int_t offs10;
   ae_int_t offs11;
   ae_int_t vcnt;
   ae_int_t stride;
   ae_frame_make(&_frame_block);
   RefObj(apbuffers, buf);
// Quick exit for special cases
   if (idx1 <= idx0) {
      ae_frame_leave();
      return;
   }
   if (cidx1 <= cidx0) {
      ae_frame_leave();
      return;
   }
   if (nvars <= 0) {
      ae_frame_leave();
      return;
   }
// Try to recursively divide/process dataset
//
// NOTE: real arithmetics is used to avoid integer overflow on large problem sizes
   rcomplexity = 2.0 * (idx1 - idx0) * (cidx1 - cidx0) * nvars;
// Parallelism was activated if: rcomplexity >= smpactivationlevel() && idx1 - idx0 >= 2 * clustering_kmeansblocksize
   if (rcomplexity >= spawnlevel() && idx1 - idx0 >= 2 * clustering_kmeansblocksize && nvars >= clustering_kmeansparalleldim && cidx1 - cidx0 >= clustering_kmeansparallelk) {
      task0 = splitlength(idx1 - idx0, clustering_kmeansblocksize), task1 = idx1 - idx0 - task0;
      kmeansupdatedistances(xy, idx0, idx0 + task0, nvars, ct, cidx0, cidx1, xyc, xydist2, bufferpool);
      kmeansupdatedistances(xy, idx0 + task0, idx1, nvars, ct, cidx0, cidx1, xyc, xydist2, bufferpool);
      ae_frame_leave();
      return;
   }
// Dataset chunk is selected.
//
// Process it with blocked algorithm:
// * iterate over points, process them in KMeansBlockSize-ed chunks
// * for each chunk of dataset, iterate over centers, process them in KMeansBlockSize-ed chunks
// * for each chunk of dataset/centerset, iterate over variables, process them in KMeansBlockSize-ed chunks
   ae_assert(clustering_kmeansblocksize % 2 == 0, "KMeansUpdateDistances: internal error");
   ae_shared_pool_retrieve(bufferpool, &_buf);
   vectorsetlengthatleast(&buf->ra0, clustering_kmeansblocksize * clustering_kmeansblocksize);
   vectorsetlengthatleast(&buf->ra1, clustering_kmeansblocksize * clustering_kmeansblocksize);
   vectorsetlengthatleast(&buf->ra2, clustering_kmeansblocksize * clustering_kmeansblocksize);
   vectorsetlengthatleast(&buf->ra3, clustering_kmeansblocksize);
   vectorsetlengthatleast(&buf->ia3, clustering_kmeansblocksize);
   pblkcnt = chunkscount(idx1 - idx0, clustering_kmeansblocksize);
   cblkcnt = chunkscount(cidx1 - cidx0, clustering_kmeansblocksize);
   vblkcnt = chunkscount(nvars, clustering_kmeansblocksize);
   for (pblk = 0; pblk < pblkcnt; pblk++) {
   // Process PBlk-th chunk of dataset.
      p0 = idx0 + pblk * clustering_kmeansblocksize;
      p1 = imin2(p0 + clustering_kmeansblocksize, idx1);
   // Prepare RA3[]/IA3[] for storage of best distances and best cluster numbers.
      for (i = 0; i < clustering_kmeansblocksize; i++) {
         buf->ra3.xR[i] = ae_maxrealnumber;
         buf->ia3.xZ[i] = -1;
      }
   // Iterare over chunks of centerset.
      for (cblk = 0; cblk < cblkcnt; cblk++) {
      // Process CBlk-th chunk of centerset
         c0 = cidx0 + cblk * clustering_kmeansblocksize;
         c1 = imin2(c0 + clustering_kmeansblocksize, cidx1);
      // At this point we have to calculate a set of pairwise distances
      // between points [P0,P1) and centers [C0,C1) and select best center
      // for each point. It can also be done with blocked algorithm
      // (blocking for variables).
      //
      // Following arrays are used:
      // * RA0[] - matrix of distances, padded by zeros for even size,
      //           rows are stored with stride KMeansBlockSize.
      // * RA1[] - matrix of points (variables corresponding to current
      //           block are extracted), padded by zeros for even size,
      //           rows are stored with stride KMeansBlockSize.
      // * RA2[] - matrix of centers (variables corresponding to current
      //           block are extracted), padded by zeros for even size,
      //           rows are stored with stride KMeansBlockSize.
      //
         pcnt = p1 - p0;
         pcntpadded = pcnt + pcnt % 2;
         ccnt = c1 - c0;
         ccntpadded = ccnt + ccnt % 2;
         stride = clustering_kmeansblocksize;
         ae_assert(pcntpadded <= clustering_kmeansblocksize, "KMeansUpdateDistances: integrity error");
         ae_assert(ccntpadded <= clustering_kmeansblocksize, "KMeansUpdateDistances: integrity error");
         for (i = 0; i < pcntpadded; i++) {
            for (j = 0; j < ccntpadded; j++) {
               buf->ra0.xR[i * stride + j] = 0.0;
            }
         }
         for (vblk = 0; vblk < vblkcnt; vblk++) {
         // Fetch VBlk-th block of variables to arrays RA1 (points) and RA2 (centers).
         // Pad points and centers with zeros.
            v0 = vblk * clustering_kmeansblocksize;
            v1 = imin2(v0 + clustering_kmeansblocksize, nvars);
            vcnt = v1 - v0;
            for (i = 0; i < pcnt; i++) {
               for (j = 0; j < vcnt; j++) {
                  buf->ra1.xR[i * stride + j] = xy->xyR[p0 + i][v0 + j];
               }
            }
            for (i = pcnt; i < pcntpadded; i++) {
               for (j = 0; j < vcnt; j++) {
                  buf->ra1.xR[i * stride + j] = 0.0;
               }
            }
            for (i = 0; i < ccnt; i++) {
               for (j = 0; j < vcnt; j++) {
                  buf->ra2.xR[i * stride + j] = ct->xyR[c0 + i][v0 + j];
               }
            }
            for (i = ccnt; i < ccntpadded; i++) {
               for (j = 0; j < vcnt; j++) {
                  buf->ra2.xR[i * stride + j] = 0.0;
               }
            }
         // Update distance matrix with sums-of-squared-differences of RA1 and RA2
            i0 = 0;
            while (i0 < pcntpadded) {
               i1 = 0;
               while (i1 < ccntpadded) {
                  offs0 = i0 * stride + i1;
                  v00 = buf->ra0.xR[offs0];
                  v01 = buf->ra0.xR[offs0 + 1];
                  v10 = buf->ra0.xR[offs0 + stride];
                  v11 = buf->ra0.xR[offs0 + stride + 1];
                  offs00 = i0 * stride;
                  offs01 = offs00 + stride;
                  offs10 = i1 * stride;
                  offs11 = offs10 + stride;
                  for (j = 0; j < vcnt; j++) {
                     vp0 = buf->ra1.xR[offs00 + j];
                     vp1 = buf->ra1.xR[offs01 + j];
                     vc0 = buf->ra2.xR[offs10 + j];
                     vc1 = buf->ra2.xR[offs11 + j];
                     vv = vp0 - vc0;
                     v00 += vv * vv;
                     vv = vp0 - vc1;
                     v01 += vv * vv;
                     vv = vp1 - vc0;
                     v10 += vv * vv;
                     vv = vp1 - vc1;
                     v11 += vv * vv;
                  }
                  offs0 = i0 * stride + i1;
                  buf->ra0.xR[offs0] = v00;
                  buf->ra0.xR[offs0 + 1] = v01;
                  buf->ra0.xR[offs0 + stride] = v10;
                  buf->ra0.xR[offs0 + stride + 1] = v11;
                  i1 += 2;
               }
               i0 += 2;
            }
         }
         for (i = 0; i < pcnt; i++) {
            cclosest = buf->ia3.xZ[i];
            dclosest = buf->ra3.xR[i];
            for (j = 0; j < ccnt; j++) {
               if (buf->ra0.xR[i * stride + j] < dclosest) {
                  dclosest = buf->ra0.xR[i * stride + j];
                  cclosest = c0 + j;
               }
            }
            buf->ia3.xZ[i] = cclosest;
            buf->ra3.xR[i] = dclosest;
         }
      }
   // Store best centers to XYC[]
      for (i = p0; i < p1; i++) {
         xyc->xZ[i] = buf->ia3.xZ[i - p0];
         xydist2->xR[i] = buf->ra3.xR[i - p0];
      }
   }
   ae_shared_pool_recycle(bufferpool, &_buf);
   ae_frame_leave();
}

void kmeansbuffers_init(void *_p, bool make_automatic) {
   kmeansbuffers *p = (kmeansbuffers *)_p;
   ae_matrix_init(&p->ct, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->ctbest, 0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->xycbest, 0, DT_INT, make_automatic);
   ae_vector_init(&p->xycprev, 0, DT_INT, make_automatic);
   ae_vector_init(&p->d2, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->csizes, 0, DT_INT, make_automatic);
   apbuffers_init(&p->initbuf, make_automatic);
   ae_shared_pool_init(&p->updatepool, make_automatic);
}

void kmeansbuffers_copy(void *_dst, void *_src, bool make_automatic) {
   kmeansbuffers *dst = (kmeansbuffers *)_dst;
   kmeansbuffers *src = (kmeansbuffers *)_src;
   ae_matrix_copy(&dst->ct, &src->ct, make_automatic);
   ae_matrix_copy(&dst->ctbest, &src->ctbest, make_automatic);
   ae_vector_copy(&dst->xycbest, &src->xycbest, make_automatic);
   ae_vector_copy(&dst->xycprev, &src->xycprev, make_automatic);
   ae_vector_copy(&dst->d2, &src->d2, make_automatic);
   ae_vector_copy(&dst->csizes, &src->csizes, make_automatic);
   apbuffers_copy(&dst->initbuf, &src->initbuf, make_automatic);
   ae_shared_pool_copy(&dst->updatepool, &src->updatepool, make_automatic);
}

void kmeansbuffers_free(void *_p, bool make_automatic) {
   kmeansbuffers *p = (kmeansbuffers *)_p;
   ae_matrix_free(&p->ct, make_automatic);
   ae_matrix_free(&p->ctbest, make_automatic);
   ae_vector_free(&p->xycbest, make_automatic);
   ae_vector_free(&p->xycprev, make_automatic);
   ae_vector_free(&p->d2, make_automatic);
   ae_vector_free(&p->csizes, make_automatic);
   apbuffers_free(&p->initbuf, make_automatic);
   ae_shared_pool_free(&p->updatepool, make_automatic);
}

void clusterizerstate_init(void *_p, bool make_automatic) {
   clusterizerstate *p = (clusterizerstate *)_p;
   ae_matrix_init(&p->xy, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->d, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->tmpd, 0, 0, DT_REAL, make_automatic);
   apbuffers_init(&p->distbuf, make_automatic);
   kmeansbuffers_init(&p->kmeanstmp, make_automatic);
}

void clusterizerstate_copy(void *_dst, void *_src, bool make_automatic) {
   clusterizerstate *dst = (clusterizerstate *)_dst;
   clusterizerstate *src = (clusterizerstate *)_src;
   dst->npoints = src->npoints;
   dst->nfeatures = src->nfeatures;
   dst->disttype = src->disttype;
   ae_matrix_copy(&dst->xy, &src->xy, make_automatic);
   ae_matrix_copy(&dst->d, &src->d, make_automatic);
   dst->ahcalgo = src->ahcalgo;
   dst->kmeansrestarts = src->kmeansrestarts;
   dst->kmeansmaxits = src->kmeansmaxits;
   dst->kmeansinitalgo = src->kmeansinitalgo;
   dst->kmeansdbgnoits = src->kmeansdbgnoits;
   dst->seed = src->seed;
   ae_matrix_copy(&dst->tmpd, &src->tmpd, make_automatic);
   apbuffers_copy(&dst->distbuf, &src->distbuf, make_automatic);
   kmeansbuffers_copy(&dst->kmeanstmp, &src->kmeanstmp, make_automatic);
}

void clusterizerstate_free(void *_p, bool make_automatic) {
   clusterizerstate *p = (clusterizerstate *)_p;
   ae_matrix_free(&p->xy, make_automatic);
   ae_matrix_free(&p->d, make_automatic);
   ae_matrix_free(&p->tmpd, make_automatic);
   apbuffers_free(&p->distbuf, make_automatic);
   kmeansbuffers_free(&p->kmeanstmp, make_automatic);
}

void ahcreport_init(void *_p, bool make_automatic) {
   ahcreport *p = (ahcreport *)_p;
   ae_vector_init(&p->p, 0, DT_INT, make_automatic);
   ae_matrix_init(&p->z, 0, 0, DT_INT, make_automatic);
   ae_matrix_init(&p->pz, 0, 0, DT_INT, make_automatic);
   ae_matrix_init(&p->pm, 0, 0, DT_INT, make_automatic);
   ae_vector_init(&p->mergedist, 0, DT_REAL, make_automatic);
}

void ahcreport_copy(void *_dst, void *_src, bool make_automatic) {
   ahcreport *dst = (ahcreport *)_dst;
   ahcreport *src = (ahcreport *)_src;
   dst->terminationtype = src->terminationtype;
   dst->npoints = src->npoints;
   ae_vector_copy(&dst->p, &src->p, make_automatic);
   ae_matrix_copy(&dst->z, &src->z, make_automatic);
   ae_matrix_copy(&dst->pz, &src->pz, make_automatic);
   ae_matrix_copy(&dst->pm, &src->pm, make_automatic);
   ae_vector_copy(&dst->mergedist, &src->mergedist, make_automatic);
}

void ahcreport_free(void *_p, bool make_automatic) {
   ahcreport *p = (ahcreport *)_p;
   ae_vector_free(&p->p, make_automatic);
   ae_matrix_free(&p->z, make_automatic);
   ae_matrix_free(&p->pz, make_automatic);
   ae_matrix_free(&p->pm, make_automatic);
   ae_vector_free(&p->mergedist, make_automatic);
}

void kmeansreport_init(void *_p, bool make_automatic) {
   kmeansreport *p = (kmeansreport *)_p;
   ae_matrix_init(&p->c, 0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->cidx, 0, DT_INT, make_automatic);
}

void kmeansreport_copy(void *_dst, void *_src, bool make_automatic) {
   kmeansreport *dst = (kmeansreport *)_dst;
   kmeansreport *src = (kmeansreport *)_src;
   dst->npoints = src->npoints;
   dst->nfeatures = src->nfeatures;
   dst->terminationtype = src->terminationtype;
   dst->iterationscount = src->iterationscount;
   dst->energy = src->energy;
   dst->k = src->k;
   ae_matrix_copy(&dst->c, &src->c, make_automatic);
   ae_vector_copy(&dst->cidx, &src->cidx, make_automatic);
}

void kmeansreport_free(void *_p, bool make_automatic) {
   kmeansreport *p = (kmeansreport *)_p;
   ae_matrix_free(&p->c, make_automatic);
   ae_vector_free(&p->cidx, make_automatic);
}
} // end of namespace alglib_impl

namespace alglib {
// This structure is a clusterization engine.
// You should not try to access its fields directly.
// Use ALGLIB functions in order to work with this object.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
DefClass(clusterizerstate, EndD)

// This structure  is used to store results of the agglomerative hierarchical
// clustering (AHC).
//
// Following information is returned:
//
// * TerminationType - completion code:
//   * 1   for successful completion of algorithm
//   * -5  inappropriate combination of  clustering  algorithm  and  distance
//         function was used. As for now, it  is  possible  only when  Ward's
//         method is called for dataset with non-Euclidean distance function.
//   In case negative completion code is returned,  other  fields  of  report
//   structure are invalid and should not be used.
//
// * NPoints contains number of points in the original dataset
//
// * Z contains information about merges performed  (see below).  Z  contains
//   indexes from the original (unsorted) dataset and it can be used when you
//   need to know what points were merged. However, it is not convenient when
//   you want to build a dendrograd (see below).
//
// * if  you  want  to  build  dendrogram, you  can use Z, but it is not good
//   option, because Z contains  indexes from  unsorted  dataset.  Dendrogram
//   built from such dataset is likely to have intersections. So, you have to
//   reorder you points before building dendrogram.
//   Permutation which reorders point is returned in P. Another representation
//   of  merges,  which  is  more  convenient for dendorgram construction, is
//   returned in PM.
//
// * more information on format of Z, P and PM can be found below and in the
//   examples from ALGLIB Reference Manual.
//
// FORMAL DESCRIPTION OF FIELDS:
//     NPoints         number of points
//     Z               array[NPoints-1,2],  contains   indexes   of  clusters
//                     linked in pairs to  form  clustering  tree.  I-th  row
//                     corresponds to I-th merge:
//                     * Z[I,0] - index of the first cluster to merge
//                     * Z[I,1] - index of the second cluster to merge
//                     * Z[I,0] < Z[I,1]
//                     * clusters are  numbered  from 0 to 2*NPoints-2,  with
//                       indexes from 0 to NPoints-1 corresponding to  points
//                       of the original dataset, and indexes from NPoints to
//                       2*NPoints-2  correspond  to  clusters  generated  by
//                       subsequent  merges  (I-th  row  of Z creates cluster
//                       with index NPoints+I).
//
//                     IMPORTANT: indexes in Z[] are indexes in the ORIGINAL,
//                     unsorted dataset. In addition to  Z algorithm  outputs
//                     permutation which rearranges points in such  way  that
//                     subsequent merges are  performed  on  adjacent  points
//                     (such order is needed if you want to build dendrogram).
//                     However,  indexes  in  Z  are  related  to   original,
//                     unrearranged sequence of points.
//
//     P               array[NPoints], permutation which reorders points  for
//                     dendrogram  construction.  P[i] contains  index of the
//                     position  where  we  should  move  I-th  point  of the
//                     original dataset in order to apply merges PZ/PM.
//
//     PZ              same as Z, but for permutation of points given  by  P.
//                     The  only  thing  which  changed  are  indexes  of the
//                     original points; indexes of clusters remained same.
//
//     MergeDist       array[NPoints-1], contains distances between  clusters
//                     being merged (MergeDist[i] correspond to merge  stored
//                     in Z[i,...]):
//                     * CLINK, SLINK and  average  linkage algorithms report
//                       "raw", unmodified distance metric.
//                     * Ward's   method   reports   weighted   intra-cluster
//                       variance, which is equal to ||Ca-Cb||^2 * Sa*Sb/(Sa+Sb).
//                       Here  A  and  B  are  clusters being merged, Ca is a
//                       center of A, Cb is a center of B, Sa is a size of A,
//                       Sb is a size of B.
//
//     PM              array[NPoints-1,6], another representation of  merges,
//                     which is suited for dendrogram construction. It  deals
//                     with rearranged points (permutation P is applied)  and
//                     represents merges in a form which different  from  one
//                     used by Z.
//                     For each I from 0 to NPoints-2, I-th row of PM represents
//                     merge performed on two clusters C0 and C1. Here:
//                     * C0 contains points with indexes PM[I,0]...PM[I,1]
//                     * C1 contains points with indexes PM[I,2]...PM[I,3]
//                     * indexes stored in PM are given for dataset sorted
//                       according to permutation P
//                     * PM[I,1]=PM[I,2]-1 (only adjacent clusters are merged)
//                     * PM[I,0] <= PM[I,1], PM[I,2] <= PM[I,3], i.e. both
//                       clusters contain at least one point
//                     * heights of "subdendrograms" corresponding  to  C0/C1
//                       are stored in PM[I,4]  and  PM[I,5].  Subdendrograms
//                       corresponding   to   single-point   clusters    have
//                       height=0. Dendrogram of the merge result has  height
//                       H=max(H0,H1)+1.
//
// NOTE: there is one-to-one correspondence between merges described by Z and
//       PM. I-th row of Z describes same merge of clusters as I-th row of PM,
//       with "left" cluster from Z corresponding to the "left" one from PM.
// ALGLIB: Copyright 10.07.2012 by Sergey Bochkanov
DefClass(ahcreport, AndD DecVal(terminationtype) AndD DecVal(npoints) AndD DecVar(p) AndD DecVar(z) AndD DecVar(pz) AndD DecVar(pm) AndD DecVar(mergedist))

// This  structure   is  used  to  store  results of the  k-means  clustering
// algorithm.
//
// Following information is always returned:
// * NPoints contains number of points in the original dataset
// * TerminationType contains completion code, negative on failure, positive
//   on success
// * K contains number of clusters
//
// For positive TerminationType we return:
// * NFeatures contains number of variables in the original dataset
// * C, which contains centers found by algorithm
// * CIdx, which maps points of the original dataset to clusters
//
// FORMAL DESCRIPTION OF FIELDS:
//     NPoints         number of points, >= 0
//     NFeatures       number of variables, >= 1
//     TerminationType completion code:
//                     * -5 if  distance  type  is  anything  different  from
//                          Euclidean metric
//                     * -3 for degenerate dataset: a) less  than  K  distinct
//                          points, b) K=0 for non-empty dataset.
//                     * +1 for successful completion
//     K               number of clusters
//     C               array[K,NFeatures], rows of the array store centers
//     CIdx            array[NPoints], which contains cluster indexes
//     IterationsCount actual number of iterations performed by clusterizer.
//                     If algorithm performed more than one random restart,
//                     total number of iterations is returned.
//     Energy          merit function, "energy", sum  of  squared  deviations
//                     from cluster centers
// ALGLIB: Copyright 27.11.2012 by Sergey Bochkanov
DefClass(kmeansreport, AndD DecVal(npoints) AndD DecVal(nfeatures) AndD DecVal(terminationtype) AndD DecVal(iterationscount) AndD DecVal(energy) AndD DecVal(k) AndD DecVar(c) AndD DecVar(cidx))

void clusterizercreate(clusterizerstate &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizercreate(ConstT(clusterizerstate, s));
   alglib_impl::ae_state_clear();
}

void clusterizersetpoints(const clusterizerstate &s, const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nfeatures, const ae_int_t disttype) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizersetpoints(ConstT(clusterizerstate, s), ConstT(ae_matrix, xy), npoints, nfeatures, disttype);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void clusterizersetpoints(const clusterizerstate &s, const real_2d_array &xy, const ae_int_t disttype) {
   ae_int_t npoints = xy.rows();
   ae_int_t nfeatures = xy.cols();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizersetpoints(ConstT(clusterizerstate, s), ConstT(ae_matrix, xy), npoints, nfeatures, disttype);
   alglib_impl::ae_state_clear();
}
#endif

void clusterizersetdistances(const clusterizerstate &s, const real_2d_array &d, const ae_int_t npoints, const bool isupper) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizersetdistances(ConstT(clusterizerstate, s), ConstT(ae_matrix, d), npoints, isupper);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void clusterizersetdistances(const clusterizerstate &s, const real_2d_array &d, const bool isupper) {
   if (d.rows() != d.cols()) ThrowError("Error while calling 'clusterizersetdistances': looks like one of arguments has wrong size");
   ae_int_t npoints = d.rows();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizersetdistances(ConstT(clusterizerstate, s), ConstT(ae_matrix, d), npoints, isupper);
   alglib_impl::ae_state_clear();
}
#endif

void clusterizersetahcalgo(const clusterizerstate &s, const ae_int_t algo) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizersetahcalgo(ConstT(clusterizerstate, s), algo);
   alglib_impl::ae_state_clear();
}

void clusterizersetkmeanslimits(const clusterizerstate &s, const ae_int_t restarts, const ae_int_t maxits) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizersetkmeanslimits(ConstT(clusterizerstate, s), restarts, maxits);
   alglib_impl::ae_state_clear();
}

void clusterizersetkmeansinit(const clusterizerstate &s, const ae_int_t initalgo) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizersetkmeansinit(ConstT(clusterizerstate, s), initalgo);
   alglib_impl::ae_state_clear();
}

void clusterizersetseed(const clusterizerstate &s, const ae_int_t seed) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizersetseed(ConstT(clusterizerstate, s), seed);
   alglib_impl::ae_state_clear();
}

void clusterizerrunahc(const clusterizerstate &s, ahcreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizerrunahc(ConstT(clusterizerstate, s), ConstT(ahcreport, rep));
   alglib_impl::ae_state_clear();
}

void clusterizerrunkmeans(const clusterizerstate &s, const ae_int_t k, kmeansreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizerrunkmeans(ConstT(clusterizerstate, s), k, ConstT(kmeansreport, rep));
   alglib_impl::ae_state_clear();
}

void clusterizergetdistances(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nfeatures, const ae_int_t disttype, real_2d_array &d) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizergetdistances(ConstT(ae_matrix, xy), npoints, nfeatures, disttype, ConstT(ae_matrix, d));
   alglib_impl::ae_state_clear();
}

void clusterizergetkclusters(const ahcreport &rep, const ae_int_t k, integer_1d_array &cidx, integer_1d_array &cz) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizergetkclusters(ConstT(ahcreport, rep), k, ConstT(ae_vector, cidx), ConstT(ae_vector, cz));
   alglib_impl::ae_state_clear();
}

void clusterizerseparatedbydist(const ahcreport &rep, const double r, ae_int_t &k, integer_1d_array &cidx, integer_1d_array &cz) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizerseparatedbydist(ConstT(ahcreport, rep), r, &k, ConstT(ae_vector, cidx), ConstT(ae_vector, cz));
   alglib_impl::ae_state_clear();
}

void clusterizerseparatedbycorr(const ahcreport &rep, const double r, ae_int_t &k, integer_1d_array &cidx, integer_1d_array &cz) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::clusterizerseparatedbycorr(ConstT(ahcreport, rep), r, &k, ConstT(ae_vector, cidx), ConstT(ae_vector, cz));
   alglib_impl::ae_state_clear();
}
} // end of namespace alglib

// === DFOREST Package ===
// Depends on: (AlgLibInternal) SCODES
// Depends on: (AlgLibMisc) HQRND
// Depends on: BDSS
namespace alglib_impl {
static const ae_int_t dforest_innernodewidth = 3;
static const ae_int_t dforest_leafnodewidth = 2;
static const ae_int_t dforest_dfusestrongsplits = 1;
static const ae_int_t dforest_dfuseevs = 2;
static const ae_int_t dforest_dfuncompressedv0 = 0;
static const ae_int_t dforest_dfcompressedv0 = 1;
static const ae_int_t dforest_needtrngini = 1;
static const ae_int_t dforest_needoobgini = 2;
static const ae_int_t dforest_needpermutation = 3;
static const ae_int_t dforest_permutationimportancebatchsize = 512;

// This function creates buffer  structure  which  can  be  used  to  perform
// parallel inference requests.
//
// DF subpackage  provides two sets of computing functions - ones  which  use
// internal buffer of DF model  (these  functions are single-threaded because
// they use same buffer, which can not  shared  between  threads),  and  ones
// which use external buffer.
//
// This function is used to initialize external buffer.
//
// Inputs:
//     Model       -   DF model which is associated with newly created buffer
//
// Outputs:
//     Buf         -   external buffer.
//
// IMPORTANT: buffer object should be used only with model which was used  to
//            initialize buffer. Any attempt to  use  buffer  with  different
//            object is dangerous - you  may   get  integrity  check  failure
//            (exception) because sizes of internal  arrays  do  not  fit  to
//            dimensions of the model structure.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void dfcreatebuffer(const decisionforest &model, decisionforestbuffer &buf);
void dfcreatebuffer(decisionforest *model, decisionforestbuffer *buf) {
   SetObj(decisionforestbuffer, buf);
   ae_vector_set_length(&buf->x, model->nvars);
   ae_vector_set_length(&buf->y, model->nclasses);
}

// This subroutine builds random decision forest.
//
// --------- DEPRECATED VERSION! USE DECISION FOREST BUILDER OBJECT ---------
// ALGLIB: Copyright 19.02.2009 by Sergey Bochkanov
// API: void dfbuildrandomdecisionforest(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, const ae_int_t ntrees, const double r, ae_int_t &info, decisionforest &df, dfreport &rep);
void dfbuildrandomdecisionforest(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t ntrees, double r, ae_int_t *info, decisionforest *df, dfreport *rep) {
   ae_int_t samplesize;
   *info = 0;
   SetObj(decisionforest, df);
   SetObj(dfreport, rep);
   if (r <= 0.0 || r > 1.0) {
      *info = -1;
      return;
   }
   samplesize = imax2(RoundZ(r * npoints), 1);
   dfbuildinternal(xy, npoints, nvars, nclasses, ntrees, samplesize, imax2(nvars / 2, 1), dforest_dfusestrongsplits + dforest_dfuseevs, info, df, rep);
}

// This subroutine builds random decision forest.
//
// --------- DEPRECATED VERSION! USE DECISION FOREST BUILDER OBJECT ---------
// ALGLIB: Copyright 19.02.2009 by Sergey Bochkanov
// API: void dfbuildrandomdecisionforestx1(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, const ae_int_t ntrees, const ae_int_t nrndvars, const double r, ae_int_t &info, decisionforest &df, dfreport &rep);
void dfbuildrandomdecisionforestx1(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t ntrees, ae_int_t nrndvars, double r, ae_int_t *info, decisionforest *df, dfreport *rep) {
   ae_int_t samplesize;
   *info = 0;
   SetObj(decisionforest, df);
   SetObj(dfreport, rep);
   if (r <= 0.0 || r > 1.0) {
      *info = -1;
      return;
   }
   if (nrndvars <= 0 || nrndvars > nvars) {
      *info = -1;
      return;
   }
   samplesize = imax2(RoundZ(r * npoints), 1);
   dfbuildinternal(xy, npoints, nvars, nclasses, ntrees, samplesize, nrndvars, dforest_dfusestrongsplits + dforest_dfuseevs, info, df, rep);
}

void dfbuildinternal(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t ntrees, ae_int_t samplesize, ae_int_t nfeatures, ae_int_t flags, ae_int_t *info, decisionforest *df, dfreport *rep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(decisionforest, df);
   SetObj(dfreport, rep);
   NewObj(decisionforestbuilder, builder);
// Test for inputs
   if (npoints < 1 || samplesize < 1 || samplesize > npoints || nvars < 1 || nclasses < 1 || ntrees < 1 || nfeatures < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   if (nclasses > 1) {
      for (i = 0; i < npoints; i++) {
         if (RoundZ(xy->xyR[i][nvars]) < 0 || RoundZ(xy->xyR[i][nvars]) >= nclasses) {
            *info = -2;
            ae_frame_leave();
            return;
         }
      }
   }
   *info = 1;
   dfbuildercreate(&builder);
   dfbuildersetdataset(&builder, xy, npoints, nvars, nclasses);
   dfbuildersetsubsampleratio(&builder, (double)samplesize / (double)npoints);
   dfbuildersetrndvars(&builder, nfeatures);
   dfbuilderbuildrandomforest(&builder, ntrees, df, rep);
   ae_frame_leave();
}

// This subroutine creates DecisionForestBuilder  object  which  is  used  to
// train decision forests.
//
// By default, new builder stores empty dataset and some  reasonable  default
// settings. At the very least, you should specify dataset prior to  building
// decision forest. You can also tweak settings of  the  forest  construction
// algorithm (recommended, although default setting should work well).
//
// Following actions are mandatory:
// * calling dfbuildersetdataset() to specify dataset
// * calling dfbuilderbuildrandomforest()  to  build  decision  forest  using
//   current dataset and default settings
//
// Additionally, you may call:
// * dfbuildersetrndvars() or dfbuildersetrndvarsratio() to specify number of
//   variables randomly chosen for each split
// * dfbuildersetsubsampleratio() to specify fraction of the dataset randomly
//   subsampled to build each tree
// * dfbuildersetseed() to control random seed chosen for tree construction
//
// Inputs:
//     none
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuildercreate(decisionforestbuilder &s);
void dfbuildercreate(decisionforestbuilder *s) {
   SetObj(decisionforestbuilder, s);
// Empty dataset
   s->dstype = -1;
   s->npoints = 0;
   s->nvars = 0;
   s->nclasses = 1;
// Default training settings
   s->rdfalgo = 0;
   s->rdfratio = 0.5;
   s->rdfvars = 0.0;
   s->rdfglobalseed = 0;
   s->rdfsplitstrength = 2;
   s->rdfimportance = 0;
// Other fields
   s->rdfprogress = 0;
   s->rdftotal = 1;
}

// This subroutine adds dense dataset to the internal storage of the  builder
// object. Specifying your dataset in the dense format means that  the  dense
// version of the forest construction algorithm will be invoked.
//
// Inputs:
//     S           -   decision forest builder object
//     XY          -   array[NPoints,NVars+1] (minimum size; actual size  can
//                     be larger, only leading part is used anyway), dataset:
//                     * first NVars elements of each row store values of the
//                       independent variables
//                     * last  column  store class number (in 0...NClasses-1)
//                       or real value of the dependent variable
//     NPoints     -   number of rows in the dataset, NPoints >= 1
//     NVars       -   number of independent variables, NVars >= 1
//     NClasses    -   indicates type of the problem being solved:
//                     * NClasses >= 2 means  that  classification  problem  is
//                       solved  (last  column  of  the  dataset stores class
//                       number)
//                     * NClasses=1 means that regression problem  is  solved
//                       (last column of the dataset stores variable value)
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuildersetdataset(const decisionforestbuilder &s, const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses);
void dfbuildersetdataset(decisionforestbuilder *s, RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses) {
   ae_int_t i;
   ae_int_t j;
// Check parameters
   ae_assert(npoints >= 1, "dfbuildersetdataset: npoints<1");
   ae_assert(nvars >= 1, "dfbuildersetdataset: nvars<1");
   ae_assert(nclasses >= 1, "dfbuildersetdataset: nclasses<1");
   ae_assert(xy->rows >= npoints, "dfbuildersetdataset: rows(xy)<npoints");
   ae_assert(xy->cols >= nvars + 1, "dfbuildersetdataset: cols(xy)<nvars+1");
   ae_assert(apservisfinitematrix(xy, npoints, nvars + 1), "dfbuildersetdataset: xy parameter contains INFs or NANs");
   if (nclasses > 1) {
      for (i = 0; i < npoints; i++) {
         j = RoundZ(xy->xyR[i][nvars]);
         ae_assert(j >= 0 && j < nclasses, "dfbuildersetdataset: last column of xy contains invalid class number");
      }
   }
// Set dataset
   s->dstype = 0;
   s->npoints = npoints;
   s->nvars = nvars;
   s->nclasses = nclasses;
   vectorsetlengthatleast(&s->dsdata, npoints * nvars);
   for (i = 0; i < npoints; i++) {
      for (j = 0; j < nvars; j++) {
         s->dsdata.xR[j * npoints + i] = xy->xyR[i][j];
      }
   }
   if (nclasses > 1) {
      vectorsetlengthatleast(&s->dsival, npoints);
      for (i = 0; i < npoints; i++) {
         s->dsival.xZ[i] = RoundZ(xy->xyR[i][nvars]);
      }
   } else {
      vectorsetlengthatleast(&s->dsrval, npoints);
      for (i = 0; i < npoints; i++) {
         s->dsrval.xR[i] = xy->xyR[i][nvars];
      }
   }
}

// This function sets number  of  variables  (in  [1,NVars]  range)  used  by
// decision forest construction algorithm.
//
// The default option is to use roughly sqrt(NVars) variables.
//
// Inputs:
//     S           -   decision forest builder object
//     RndVars     -   number of randomly selected variables; values  outside
//                     of [1,NVars] range are silently clipped.
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuildersetrndvars(const decisionforestbuilder &s, const ae_int_t rndvars);
void dfbuildersetrndvars(decisionforestbuilder *s, ae_int_t rndvars) {
   s->rdfvars = (double)(imax2(rndvars, 1));
}

// This function sets number of variables used by decision forest construction
// algorithm as a fraction of total variable count (0,1) range.
//
// The default option is to use roughly sqrt(NVars) variables.
//
// Inputs:
//     S           -   decision forest builder object
//     F           -   round(NVars*F) variables are selected
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuildersetrndvarsratio(const decisionforestbuilder &s, const double f);
void dfbuildersetrndvarsratio(decisionforestbuilder *s, double f) {
   ae_assert(isfinite(f), "dfbuildersetrndvarsratio: F is INF or NAN");
   s->rdfvars = -rmax2(f, ae_machineepsilon);
}

// This function tells decision forest builder to automatically choose number
// of  variables  used  by  decision forest construction  algorithm.  Roughly
// sqrt(NVars) variables will be used.
//
// Inputs:
//     S           -   decision forest builder object
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuildersetrndvarsauto(const decisionforestbuilder &s);
void dfbuildersetrndvarsauto(decisionforestbuilder *s) {
   s->rdfvars = 0.0;
}

// This function sets size of dataset subsample generated the decision forest
// construction algorithm. Size is specified as a fraction of  total  dataset
// size.
//
// The default option is to use 50% of the dataset for training, 50% for  the
// OOB estimates. You can decrease fraction F down to 10%, 1% or  even  below
// in order to reduce overfitting.
//
// Inputs:
//     S           -   decision forest builder object
//     F           -   fraction of the dataset to use, in (0,1] range. Values
//                     outside of this range will  be  silently  clipped.  At
//                     least one element is always selected for the  training
//                     set.
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuildersetsubsampleratio(const decisionforestbuilder &s, const double f);
void dfbuildersetsubsampleratio(decisionforestbuilder *s, double f) {
   ae_assert(isfinite(f), "dfbuildersetrndvarsfraction: F is INF or NAN");
   s->rdfratio = rmax2(f, ae_machineepsilon);
}

// This function sets seed used by internal RNG for  random  subsampling  and
// random selection of variable subsets.
//
// By default random seed is used, i.e. every time you build decision forest,
// we seed generator with new value  obtained  from  system-wide  RNG.  Thus,
// decision forest builder returns non-deterministic results. You can  change
// such behavior by specyfing fixed positive seed value.
//
// Inputs:
//     S           -   decision forest builder object
//     SeedVal     -   seed value:
//                     * positive values are used for seeding RNG with fixed
//                       seed, i.e. subsequent runs on same data will return
//                       same decision forests
//                     * non-positive seed means that random seed is used
//                       for every run of builder, i.e. subsequent  runs  on
//                       same  datasets  will  return   slightly   different
//                       decision forests
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuildersetseed(const decisionforestbuilder &s, const ae_int_t seedval);
void dfbuildersetseed(decisionforestbuilder *s, ae_int_t seedval) {
   s->rdfglobalseed = seedval;
}

// This function sets random decision forest construction algorithm.
//
// As for now, only one decision forest construction algorithm is supported -
// a dense "baseline" RDF algorithm.
//
// Inputs:
//     S           -   decision forest builder object
//     AlgoType    -   algorithm type:
//                     * 0 = baseline dense RDF
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuildersetrdfalgo(const decisionforestbuilder &s, const ae_int_t algotype);
void dfbuildersetrdfalgo(decisionforestbuilder *s, ae_int_t algotype) {
   ae_assert(algotype == 0, "dfbuildersetrdfalgo: unexpected algotype");
   s->rdfalgo = algotype;
}

// This  function  sets  split  selection  algorithm used by decision  forest
// classifier. You may choose several algorithms, with  different  speed  and
// quality of the results.
//
// Inputs:
//     S           -   decision forest builder object
//     SplitStrength-  split type:
//                     * 0 = split at the random position, fastest one
//                     * 1 = split at the middle of the range
//                     * 2 = strong split at the best point of the range (default)
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuildersetrdfsplitstrength(const decisionforestbuilder &s, const ae_int_t splitstrength);
void dfbuildersetrdfsplitstrength(decisionforestbuilder *s, ae_int_t splitstrength) {
   ae_assert(splitstrength == 0 || splitstrength == 1 || splitstrength == 2, "dfbuildersetrdfsplitstrength: unexpected split type");
   s->rdfsplitstrength = splitstrength;
}

// This  function  tells  decision  forest  construction  algorithm  to   use
// Gini impurity based variable importance estimation (also known as MDI).
//
// This version of importance estimation algorithm analyzes mean decrease  in
// impurity (MDI) on training sample during  splits.  The result  is  divided
// by impurity at the root node in order to produce estimate in [0,1] range.
//
// Such estimates are fast to calculate and beautifully  normalized  (sum  to
// one) but have following downsides:
// * They ALWAYS sum to 1.0, even if output is completely unpredictable. I.e.
//   MDI allows to order variables by importance, but does not  tell us about
//   "absolute" importances of variables
// * there exist some bias towards continuous and high-cardinality categorical
//   variables
//
// NOTE: informally speaking, MDA (permutation importance) rating answers the
//       question  "what  part  of  the  model  predictive power is ruined by
//       permuting k-th variable?" while MDI tells us "what part of the model
//       predictive power was achieved due to usage of k-th variable".
//
//       Thus, MDA rates each variable independently at "0 to 1"  scale while
//       MDI (and OOB-MDI too) tends to divide "unit  amount  of  importance"
//       between several important variables.
//
//       If  all  variables  are  equally  important,  they  will  have  same
//       MDI/OOB-MDI rating, equal (for OOB-MDI: roughly equal)  to  1/NVars.
//       However, roughly  same  picture  will  be  produced   for  the  "all
//       variables provide information no one is critical" situation  and for
//       the "all variables are critical, drop any one, everything is ruined"
//       situation.
//
//       Contrary to that, MDA will rate critical variable as ~1.0 important,
//       and important but non-critical variable will  have  less  than  unit
//       rating.
//
// NOTE: quite an often MDA and MDI return same results. It generally happens
//       on problems with low test set error (a few  percents  at  most)  and
//       large enough training set to avoid overfitting.
//
//       The difference between MDA, MDI and OOB-MDI becomes  important  only
//       on "hard" tasks with high test set error and/or small training set.
//
// Inputs:
//     S           -   decision forest builder object
//
// Outputs:
//     S           -   decision forest builder object. Next call to the forest
//                     construction function will produce:
//                     * importance estimates in rep.varimportances field
//                     * variable ranks in rep.topvars field
// ALGLIB: Copyright 29.07.2019 by Sergey Bochkanov
// API: void dfbuildersetimportancetrngini(const decisionforestbuilder &s);
void dfbuildersetimportancetrngini(decisionforestbuilder *s) {
   s->rdfimportance = dforest_needtrngini;
}

// This  function  tells  decision  forest  construction  algorithm  to   use
// out-of-bag version of Gini variable importance estimation (also  known  as
// OOB-MDI).
//
// This version of importance estimation algorithm analyzes mean decrease  in
// impurity (MDI) on out-of-bag sample during splits. The result  is  divided
// by impurity at the root node in order to produce estimate in [0,1] range.
//
// Such estimates are fast to calculate and resistant to  overfitting  issues
// (thanks to the  out-of-bag  estimates  used). However, OOB Gini rating has
// following downsides:
// * there exist some bias towards continuous and high-cardinality categorical
//   variables
// * Gini rating allows us to order variables by importance, but it  is  hard
//   to define importance of the variable by itself.
//
// NOTE: informally speaking, MDA (permutation importance) rating answers the
//       question  "what  part  of  the  model  predictive power is ruined by
//       permuting k-th variable?" while MDI tells us "what part of the model
//       predictive power was achieved due to usage of k-th variable".
//
//       Thus, MDA rates each variable independently at "0 to 1"  scale while
//       MDI (and OOB-MDI too) tends to divide "unit  amount  of  importance"
//       between several important variables.
//
//       If  all  variables  are  equally  important,  they  will  have  same
//       MDI/OOB-MDI rating, equal (for OOB-MDI: roughly equal)  to  1/NVars.
//       However, roughly  same  picture  will  be  produced   for  the  "all
//       variables provide information no one is critical" situation  and for
//       the "all variables are critical, drop any one, everything is ruined"
//       situation.
//
//       Contrary to that, MDA will rate critical variable as ~1.0 important,
//       and important but non-critical variable will  have  less  than  unit
//       rating.
//
// NOTE: quite an often MDA and MDI return same results. It generally happens
//       on problems with low test set error (a few  percents  at  most)  and
//       large enough training set to avoid overfitting.
//
//       The difference between MDA, MDI and OOB-MDI becomes  important  only
//       on "hard" tasks with high test set error and/or small training set.
//
// Inputs:
//     S           -   decision forest builder object
//
// Outputs:
//     S           -   decision forest builder object. Next call to the forest
//                     construction function will produce:
//                     * importance estimates in rep.varimportances field
//                     * variable ranks in rep.topvars field
// ALGLIB: Copyright 29.07.2019 by Sergey Bochkanov
// API: void dfbuildersetimportanceoobgini(const decisionforestbuilder &s);
void dfbuildersetimportanceoobgini(decisionforestbuilder *s) {
   s->rdfimportance = dforest_needoobgini;
}

// This  function  tells  decision  forest  construction  algorithm  to   use
// permutation variable importance estimator (also known as MDA).
//
// This version of importance estimation algorithm analyzes mean increase  in
// out-of-bag sum of squared  residuals  after  random  permutation  of  J-th
// variable. The result is divided by error computed with all variables being
// perturbed in order to produce R-squared-like estimate in [0,1] range.
//
// Such estimate  is  slower to calculate than Gini-based rating  because  it
// needs multiple inference runs for each of variables being studied.
//
// ALGLIB uses parallelized and highly  optimized  algorithm  which  analyzes
// path through the decision tree and allows  to  handle  most  perturbations
// in O(1) time; nevertheless, requesting MDA importances may increase forest
// construction time from 10% to 200% (or more,  if  you  have  thousands  of
// variables).
//
// However, MDA rating has following benefits over Gini-based ones:
// * no bias towards specific variable types
// * ability to directly evaluate "absolute" importance of some  variable  at
//   "0 to 1" scale (contrary to Gini-based rating, which returns comparative
//   importances).
//
// NOTE: informally speaking, MDA (permutation importance) rating answers the
//       question  "what  part  of  the  model  predictive power is ruined by
//       permuting k-th variable?" while MDI tells us "what part of the model
//       predictive power was achieved due to usage of k-th variable".
//
//       Thus, MDA rates each variable independently at "0 to 1"  scale while
//       MDI (and OOB-MDI too) tends to divide "unit  amount  of  importance"
//       between several important variables.
//
//       If  all  variables  are  equally  important,  they  will  have  same
//       MDI/OOB-MDI rating, equal (for OOB-MDI: roughly equal)  to  1/NVars.
//       However, roughly  same  picture  will  be  produced   for  the  "all
//       variables provide information no one is critical" situation  and for
//       the "all variables are critical, drop any one, everything is ruined"
//       situation.
//
//       Contrary to that, MDA will rate critical variable as ~1.0 important,
//       and important but non-critical variable will  have  less  than  unit
//       rating.
//
// NOTE: quite an often MDA and MDI return same results. It generally happens
//       on problems with low test set error (a few  percents  at  most)  and
//       large enough training set to avoid overfitting.
//
//       The difference between MDA, MDI and OOB-MDI becomes  important  only
//       on "hard" tasks with high test set error and/or small training set.
//
// Inputs:
//     S           -   decision forest builder object
//
// Outputs:
//     S           -   decision forest builder object. Next call to the forest
//                     construction function will produce:
//                     * importance estimates in rep.varimportances field
//                     * variable ranks in rep.topvars field
// ALGLIB: Copyright 29.07.2019 by Sergey Bochkanov
// API: void dfbuildersetimportancepermutation(const decisionforestbuilder &s);
void dfbuildersetimportancepermutation(decisionforestbuilder *s) {
   s->rdfimportance = dforest_needpermutation;
}

// This  function  tells  decision  forest  construction  algorithm  to  skip
// variable importance estimation.
//
// Inputs:
//     S           -   decision forest builder object
//
// Outputs:
//     S           -   decision forest builder object. Next call to the forest
//                     construction function will result in forest being built
//                     without variable importance estimation.
// ALGLIB: Copyright 29.07.2019 by Sergey Bochkanov
// API: void dfbuildersetimportancenone(const decisionforestbuilder &s);
void dfbuildersetimportancenone(decisionforestbuilder *s) {
   s->rdfimportance = 0;
}

// This function is used to peek into  decision  forest  construction process
// from some other thread and get current progress indicator.
//
// It returns value in [0,1].
//
// Inputs:
//     S           -   decision forest builder object used  to  build  forest
//                     in some other thread
//
// Result:
//     progress value, in [0,1]
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: double dfbuilderpeekprogress(const decisionforestbuilder &s);
double dfbuilderpeekprogress(decisionforestbuilder *s) {
   double result;
   result = s->rdfprogress / rmax2((double)(s->rdftotal), 1.0);
   result = rmax2(result, 0.0);
   result = rmin2(result, 1.0);
   return result;
}

// This function is an alias for dfbuilderpeekprogress(), left in ALGLIB  for
// backward compatibility reasons.
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: double dfbuildergetprogress(const decisionforestbuilder &s);
double dfbuildergetprogress(decisionforestbuilder *s) {
   double result;
   result = dfbuilderpeekprogress(s);
   return result;
}

// Classifier split
static void dforest_classifiersplit(decisionforestbuilder *s, dfworkbuf *workbuf, RVector *x, ZVector *c, ae_int_t n, hqrndstate *rs, ae_int_t *info, double *threshold, double *e, RVector *sortrbuf, ZVector *sortibuf) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t n0;
   ae_int_t n0prev;
   double v;
   ae_int_t advanceby;
   double rms;
   ae_int_t k0;
   ae_int_t k1;
   double v0;
   double v1;
   ae_int_t nclasses;
   double vmin;
   double vmax;
   *info = 0;
   *threshold = 0;
   *e = 0;
   ae_assert(s->rdfsplitstrength == 0 || s->rdfsplitstrength == 1 || s->rdfsplitstrength == 2, "RDF: unexpected split type at ClassifierSplit()");
   nclasses = s->nclasses;
   advanceby = 1;
   if (n >= 20) {
      advanceby = imax2(2, RoundZ(n * 0.05));
   }
   *info = -1;
   *threshold = 0.0;
   *e = ae_maxrealnumber;
// Random split
   if (s->rdfsplitstrength == 0) {
   // Evaluate minimum, maximum and randomly selected values
      vmin = x->xR[0];
      vmax = x->xR[0];
      for (i = 1; i < n; i++) {
         v = x->xR[i];
         if (v < vmin) {
            vmin = v;
         }
         if (v > vmax) {
            vmax = v;
         }
      }
      if (vmin == vmax) {
         return;
      }
      v = x->xR[hqrnduniformi(rs, n)];
      if (v == vmin) {
         v = vmax;
      }
   // Calculate RMS error associated with the split
      for (i = 0; i < nclasses; i++) {
         workbuf->classtotals0.xZ[i] = 0;
      }
      n0 = 0;
      for (i = 0; i < n; i++) {
         if (x->xR[i] < v) {
            k = c->xZ[i];
            workbuf->classtotals0.xZ[k]++;
            n0++;
         }
      }
      ae_assert(n0 > 0 && n0 < n, "RDF: critical integrity check failed at ClassifierSplit()");
      v0 = 1.0 / (double)n0;
      v1 = 1.0 / (double)(n - n0);
      rms = 0.0;
      for (j = 0; j < nclasses; j++) {
         k0 = workbuf->classtotals0.xZ[j];
         k1 = workbuf->classpriors.xZ[j] - k0;
         rms += k0 * (1 - v0 * k0) + k1 * (1 - v1 * k1);
      }
      *threshold = v;
      *info = 1;
      *e = rms;
      return;
   }
// Stronger splits which require us to sort the data
// Quick check for degeneracy
   tagsortfasti(x, c, sortrbuf, sortibuf, n);
   v = 0.5 * (x->xR[0] + x->xR[n - 1]);
   if (!(x->xR[0] < v && v < x->xR[n - 1])) {
      return;
   }
// Split at the middle
   if (s->rdfsplitstrength == 1) {
   // Select split position
      vmin = x->xR[0];
      vmax = x->xR[n - 1];
      v = x->xR[n / 2];
      if (v == vmin) {
         v = vmin + 0.001 * (vmax - vmin);
      }
      if (v == vmin) {
         v = vmax;
      }
   // Calculate RMS error associated with the split
      for (i = 0; i < nclasses; i++) {
         workbuf->classtotals0.xZ[i] = 0;
      }
      n0 = 0;
      for (i = 0; i < n; i++) {
         if (x->xR[i] < v) {
            k = c->xZ[i];
            workbuf->classtotals0.xZ[k]++;
            n0++;
         }
      }
      ae_assert(n0 > 0 && n0 < n, "RDF: critical integrity check failed at ClassifierSplit()");
      v0 = 1.0 / (double)n0;
      v1 = 1.0 / (double)(n - n0);
      rms = 0.0;
      for (j = 0; j < nclasses; j++) {
         k0 = workbuf->classtotals0.xZ[j];
         k1 = workbuf->classpriors.xZ[j] - k0;
         rms += k0 * (1 - v0 * k0) + k1 * (1 - v1 * k1);
      }
      *threshold = v;
      *info = 1;
      *e = rms;
      return;
   }
// Strong split
   if (s->rdfsplitstrength == 2) {
   // Prepare initial split.
   // Evaluate current split, prepare next one, repeat.
      for (i = 0; i < nclasses; i++) {
         workbuf->classtotals0.xZ[i] = 0;
      }
      n0 = 1;
      while (n0 < n && x->xR[n0] == x->xR[n0 - 1]) {
         n0++;
      }
      ae_assert(n0 < n, "RDF: critical integrity check failed in ClassifierSplit()");
      for (i = 0; i < n0; i++) {
         k = c->xZ[i];
         workbuf->classtotals0.xZ[k]++;
      }
      *info = -1;
      *threshold = x->xR[n - 1];
      *e = ae_maxrealnumber;
      while (n0 < n) {
      // RMS error associated with current split
         v0 = 1.0 / (double)n0;
         v1 = 1.0 / (double)(n - n0);
         rms = 0.0;
         for (j = 0; j < nclasses; j++) {
            k0 = workbuf->classtotals0.xZ[j];
            k1 = workbuf->classpriors.xZ[j] - k0;
            rms += k0 * (1 - v0 * k0) + k1 * (1 - v1 * k1);
         }
         if (*info < 0 || rms < *e) {
            *info = 1;
            *e = rms;
            *threshold = 0.5 * (x->xR[n0 - 1] + x->xR[n0]);
            if (*threshold <= x->xR[n0 - 1]) {
               *threshold = x->xR[n0];
            }
         }
      // Advance
         n0prev = n0;
         while (n0 < n && n0 - n0prev < advanceby) {
            v = x->xR[n0];
            while (n0 < n && x->xR[n0] == v) {
               k = c->xZ[n0];
               workbuf->classtotals0.xZ[k]++;
               n0++;
            }
         }
      }
      if (*info > 0) {
         *e = sqrt(*e / (nclasses * n));
      }
      return;
   }
   ae_assert(false, "RDF: ClassifierSplit(), critical error");
}

// Regression model split
static void dforest_regressionsplit(decisionforestbuilder *s, dfworkbuf *workbuf, RVector *x, RVector *y, ae_int_t n, ae_int_t *info, double *threshold, double *e, RVector *sortrbuf, RVector *sortrbuf2) {
   ae_int_t i;
   double vmin;
   double vmax;
   double bnd01;
   double bnd12;
   double bnd23;
   ae_int_t total0;
   ae_int_t total1;
   ae_int_t total2;
   ae_int_t total3;
   ae_int_t cnt0;
   ae_int_t cnt1;
   ae_int_t cnt2;
   ae_int_t cnt3;
   ae_int_t n0;
   ae_int_t advanceby;
   double v;
   double v0;
   double v1;
   double rms;
   ae_int_t n0prev;
   ae_int_t k0;
   ae_int_t k1;
   *info = 0;
   *threshold = 0;
   *e = 0;
   advanceby = 1;
   if (n >= 20) {
      advanceby = imax2(2, RoundZ(n * 0.05));
   }
// Sort data
// Quick check for degeneracy
   tagsortfastr(x, y, sortrbuf, sortrbuf2, n);
   v = 0.5 * (x->xR[0] + x->xR[n - 1]);
   if (!(x->xR[0] < v && v < x->xR[n - 1])) {
      *info = -1;
      *threshold = x->xR[n - 1];
      *e = ae_maxrealnumber;
      return;
   }
// Prepare initial split.
// Evaluate current split, prepare next one, repeat.
   vmin = y->xR[0];
   vmax = y->xR[0];
   for (i = 1; i < n; i++) {
      v = y->xR[i];
      if (v < vmin) {
         vmin = v;
      }
      if (v > vmax) {
         vmax = v;
      }
   }
   bnd12 = 0.5 * (vmin + vmax);
   bnd01 = 0.5 * (vmin + bnd12);
   bnd23 = 0.5 * (vmax + bnd12);
   total0 = 0;
   total1 = 0;
   total2 = 0;
   total3 = 0;
   for (i = 0; i < n; i++) {
      v = y->xR[i];
      if (v < bnd12) {
         if (v < bnd01) {
            total0++;
         } else {
            total1++;
         }
      } else {
         if (v < bnd23) {
            total2++;
         } else {
            total3++;
         }
      }
   }
   n0 = 1;
   while (n0 < n && x->xR[n0] == x->xR[n0 - 1]) {
      n0++;
   }
   ae_assert(n0 < n, "RDF: critical integrity check failed in ClassifierSplit()");
   cnt0 = 0;
   cnt1 = 0;
   cnt2 = 0;
   cnt3 = 0;
   for (i = 0; i < n0; i++) {
      v = y->xR[i];
      if (v < bnd12) {
         if (v < bnd01) {
            cnt0++;
         } else {
            cnt1++;
         }
      } else {
         if (v < bnd23) {
            cnt2++;
         } else {
            cnt3++;
         }
      }
   }
   *info = -1;
   *threshold = x->xR[n - 1];
   *e = ae_maxrealnumber;
   while (n0 < n) {
   // RMS error associated with current split
      v0 = 1.0 / (double)n0;
      v1 = 1.0 / (double)(n - n0);
      rms = 0.0;
      k0 = cnt0;
      k1 = total0 - cnt0;
      rms += k0 * (1 - v0 * k0) + k1 * (1 - v1 * k1);
      k0 = cnt1;
      k1 = total1 - cnt1;
      rms += k0 * (1 - v0 * k0) + k1 * (1 - v1 * k1);
      k0 = cnt2;
      k1 = total2 - cnt2;
      rms += k0 * (1 - v0 * k0) + k1 * (1 - v1 * k1);
      k0 = cnt3;
      k1 = total3 - cnt3;
      rms += k0 * (1 - v0 * k0) + k1 * (1 - v1 * k1);
      if (*info < 0 || rms < *e) {
         *info = 1;
         *e = rms;
         *threshold = 0.5 * (x->xR[n0 - 1] + x->xR[n0]);
         if (*threshold <= x->xR[n0 - 1]) {
            *threshold = x->xR[n0];
         }
      }
   // Advance
      n0prev = n0;
      while (n0 < n && n0 - n0prev < advanceby) {
         v0 = x->xR[n0];
         while (n0 < n && x->xR[n0] == v0) {
            v = y->xR[n0];
            if (v < bnd12) {
               if (v < bnd01) {
                  cnt0++;
               } else {
                  cnt1++;
               }
            } else {
               if (v < bnd23) {
                  cnt2++;
               } else {
                  cnt3++;
               }
            }
            n0++;
         }
      }
   }
   if (*info > 0) {
      *e = sqrt(*e / (4 * n));
   }
}

// Returns split: either deterministic split at the middle of [A,B], or randomly
// chosen split.
//
// It is guaranteed that A<Split <= B.
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static double dforest_getsplit(decisionforestbuilder *s, double a, double b, hqrndstate *rs) {
   double result;
   result = 0.5 * (a + b);
   if (result <= a) {
      result = b;
   }
   return result;
}

// This function performs split on some specific dense variable whose values
// are stored in WorkBuf.CurVals[Idx0,Idx1) and labels are stored in
// WorkBuf.TrnLabelsR/I[Idx0,Idx1).
//
// It returns split value and associated RMS error. It is responsibility of
// the caller to make sure that variable has at least two distinct values,
// i.e. it is possible to make a split.
//
// Precomputed values of following fields of WorkBuf are used:
// * ClassPriors
//
// Following fields of WorkBuf are used as temporaries:
// * ClassTotals0,1,01
// * Tmp0I, Tmp1I, Tmp0R, Tmp1R, Tmp2R, Tmp3R
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_evaluatedensesplit(decisionforestbuilder *s, dfworkbuf *workbuf, hqrndstate *rs, ae_int_t splitvar, ae_int_t idx0, ae_int_t idx1, ae_int_t *info, double *split, double *rms) {
   ae_int_t nclasses;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k0;
   ae_int_t k1;
   double v;
   double v0;
   double v1;
   double v2;
   ae_int_t sl;
   ae_int_t sr;
   *info = 0;
   *split = 0;
   *rms = 0;
   ae_assert(idx0 < idx1, "BuildRandomTreeRec: integrity check failed (8754)");
   nclasses = s->nclasses;
   if (s->dsbinary.xB[splitvar]) {
   // Try simple binary split, if possible
   // Split can be inferred from minimum/maximum values, just calculate RMS error
      *info = 1;
      *split = dforest_getsplit(s, s->dsmin.xR[splitvar], s->dsmax.xR[splitvar], rs);
      if (nclasses > 1) {
      // Classification problem
         for (j = 0; j < nclasses; j++) {
            workbuf->classtotals0.xZ[j] = 0;
         }
         sl = 0;
         for (i = idx0; i < idx1; i++) {
            if (workbuf->curvals.xR[i] < *split) {
               j = workbuf->trnlabelsi.xZ[i];
               workbuf->classtotals0.xZ[j]++;
               sl++;
            }
         }
         sr = idx1 - idx0 - sl;
         ae_assert(sl != 0 && sr != 0, "BuildRandomTreeRec: something strange, impossible failure!");
         v0 = 1.0 / (double)sl;
         v1 = 1.0 / (double)sr;
         *rms = 0.0;
         for (j = 0; j < nclasses; j++) {
            k0 = workbuf->classtotals0.xZ[j];
            k1 = workbuf->classpriors.xZ[j] - k0;
            *rms += k0 * (1 - v0 * k0) + k1 * (1 - v1 * k1);
         }
         *rms = sqrt(*rms / (nclasses * (idx1 - idx0 + 1)));
      } else {
      // regression-specific code
         sl = 0;
         sr = 0;
         v1 = 0.0;
         v2 = 0.0;
         for (j = idx0; j < idx1; j++) {
            if (workbuf->curvals.xR[j] < *split) {
               v1 += workbuf->trnlabelsr.xR[j];
               sl++;
            } else {
               v2 += workbuf->trnlabelsr.xR[j];
               sr++;
            }
         }
         ae_assert(sl != 0 && sr != 0, "BuildRandomTreeRec: something strange, impossible failure!");
         v1 /= sl;
         v2 /= sr;
         *rms = 0.0;
         for (j = 0; j < idx1 - idx0; j++) {
            v = workbuf->trnlabelsr.xR[idx0 + j];
            if (workbuf->curvals.xR[j] < *split) {
               v -= v1;
            } else {
               v -= v2;
            }
            *rms += v * v;
         }
         *rms = sqrt(*rms / (idx1 - idx0 + 1));
      }
   } else {
   // General split
      *info = 0;
      if (nclasses > 1) {
         for (i = 0; i < idx1 - idx0; i++) {
            workbuf->tmp0r.xR[i] = workbuf->curvals.xR[idx0 + i];
            workbuf->tmp0i.xZ[i] = workbuf->trnlabelsi.xZ[idx0 + i];
         }
         dforest_classifiersplit(s, workbuf, &workbuf->tmp0r, &workbuf->tmp0i, idx1 - idx0, rs, info, split, rms, &workbuf->tmp1r, &workbuf->tmp1i);
      } else {
         for (i = 0; i < idx1 - idx0; i++) {
            workbuf->tmp0r.xR[i] = workbuf->curvals.xR[idx0 + i];
            workbuf->tmp1r.xR[i] = workbuf->trnlabelsr.xR[idx0 + i];
         }
         dforest_regressionsplit(s, workbuf, &workbuf->tmp0r, &workbuf->tmp1r, idx1 - idx0, info, split, rms, &workbuf->tmp2r, &workbuf->tmp3r);
      }
   }
}

// This function is a part of the recurrent tree construction function; it
// selects variable for splitting according to current tree construction
// algorithm.
//
// Note: modifies VarsInPool, may decrease it if some variables become non-informative
// and leave the pool.
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_choosecurrentsplitdense(decisionforestbuilder *s, dfworkbuf *workbuf, ae_int_t *varsinpool, ae_int_t varstoselect, hqrndstate *rs, ae_int_t idx0, ae_int_t idx1, ae_int_t *varbest, double *splitbest) {
   ae_int_t npoints;
   double errbest;
   ae_int_t varstried;
   ae_int_t varcur;
   bool valuesaresame;
   ae_int_t offs;
   double split;
   ae_int_t i;
   double v;
   double v0;
   double currms;
   ae_int_t info;
   *varbest = 0;
   *splitbest = 0;
   ae_assert(s->dstype == 0, "sparsity is not supported 4terg!");
   ae_assert(s->rdfalgo == 0, "BuildRandomTreeRec: integrity check failed (1657)");
   ae_assert(idx0 < idx1, "BuildRandomTreeRec: integrity check failed (3445)");
   npoints = s->npoints;
// Select split according to dense direct RDF algorithm
   *varbest = -1;
   errbest = ae_maxrealnumber;
   *splitbest = 0.0;
   varstried = 0;
   while (varstried < imin2(varstoselect, *varsinpool)) {
   // select variables from pool
      swapelementsi(&workbuf->varpool, varstried, varstried + hqrnduniformi(rs, *varsinpool - varstried));
      varcur = workbuf->varpool.xZ[varstried];
   // Load variable values to working array.
   // If all variable values are same, variable is excluded from pool and we re-run variable selection.
      valuesaresame = true;
      ae_assert(s->dstype == 0, "not supported segsv34fs");
      offs = npoints * varcur;
      v0 = s->dsdata.xR[offs + workbuf->trnset.xZ[idx0]];
      for (i = idx0; i < idx1; i++) {
         v = s->dsdata.xR[offs + workbuf->trnset.xZ[i]];
         workbuf->curvals.xR[i] = v;
         valuesaresame = valuesaresame && v == v0;
      }
      if (valuesaresame) {
      // Variable does not change across current subset.
      // Exclude variable from pool, go to the next iteration.
      // VarsTried is not increased.
      //
      // NOTE: it is essential that updated VarsInPool is passed
      //       down to children but not up to caller - it is
      //       possible that one level higher this variable is
      //       not-fixed.
         swapelementsi(&workbuf->varpool, varstried, *varsinpool - 1);
         --*varsinpool;
         continue;
      }
   // Now we are ready to infer the split
      dforest_evaluatedensesplit(s, workbuf, rs, varcur, idx0, idx1, &info, &split, &currms);
      if (info > 0 && (*varbest < 0 || currms <= errbest)) {
         errbest = currms;
         *varbest = varcur;
         *splitbest = split;
         for (i = idx0; i < idx1; i++) {
            workbuf->bestvals.xR[i] = workbuf->curvals.xR[i];
         }
      }
   // Next iteration
      varstried++;
   }
}

// This function returns NRMS2 loss (sum of squared residuals) for a constant-
// output model:
// * model output is a mean over TRN set being passed (for classification
//   problems - NClasses-dimensional vector of class probabilities)
// * model is evaluated over TST set being passed, with L2 loss being returned
//
// Inputs:
//     NClasses            -   ">1" for classification, "=1" for regression
//     TrnLabelsI          -   training set labels, class indexes (for NClasses>1)
//     TrnLabelsR          -   training set output values (for NClasses=1)
//     TrnIdx0, TrnIdx1    -   a range [Idx0,Idx1) of elements in LabelsI/R is considered
//     TstLabelsI          -   training set labels, class indexes (for NClasses>1)
//     TstLabelsR          -   training set output values (for NClasses=1)
//     TstIdx0, TstIdx1    -   a range [Idx0,Idx1) of elements in LabelsI/R is considered
//     TmpI        -   temporary array, reallocated as needed
//
// Result:
//     sum of squared residuals;
//     for NClasses >= 2 it coincides with Gini impurity times (Idx1-Idx0)
//
// Following fields of WorkBuf are used as temporaries:
// * TmpMeanNRMS2
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static double dforest_meannrms2(ae_int_t nclasses, ZVector *trnlabelsi, RVector *trnlabelsr, ae_int_t trnidx0, ae_int_t trnidx1, ZVector *tstlabelsi, RVector *tstlabelsr, ae_int_t tstidx0, ae_int_t tstidx1, ZVector *tmpi) {
   ae_int_t i;
   ae_int_t k;
   ae_int_t ntrn;
   ae_int_t ntst;
   double v;
   double vv;
   double invntrn;
   double pitrn;
   double nitst;
   double result;
   ae_assert(trnidx0 <= trnidx1, "MeanNRMS2: integrity check failed (8754)");
   ae_assert(tstidx0 <= tstidx1, "MeanNRMS2: integrity check failed (8754)");
   result = 0.0;
   ntrn = trnidx1 - trnidx0;
   ntst = tstidx1 - tstidx0;
   if (ntrn == 0 || ntst == 0) {
      return result;
   }
   invntrn = 1.0 / ntrn;
   if (nclasses > 1) {
   // Classification problem
      vectorsetlengthatleast(tmpi, 2 * nclasses);
      for (i = 0; i < 2 * nclasses; i++) {
         tmpi->xZ[i] = 0;
      }
      for (i = trnidx0; i < trnidx1; i++) {
         k = trnlabelsi->xZ[i];
         tmpi->xZ[k]++;
      }
      for (i = tstidx0; i < tstidx1; i++) {
         k = tstlabelsi->xZ[i];
         tmpi->xZ[k + nclasses]++;
      }
      for (i = 0; i < nclasses; i++) {
         pitrn = tmpi->xZ[i] * invntrn;
         nitst = (double)(tmpi->xZ[i + nclasses]);
         result += nitst * (1 - pitrn) * (1 - pitrn);
         result += (ntst - nitst) * pitrn * pitrn;
      }
   } else {
   // regression-specific code
      v = 0.0;
      for (i = trnidx0; i < trnidx1; i++) {
         v += trnlabelsr->xR[i];
      }
      v *= invntrn;
      for (i = tstidx0; i < tstidx1; i++) {
         vv = tstlabelsr->xR[i] - v;
         result += vv * vv;
      }
   }
   return result;
}

// Outputs leaf to the tree
//
// Following items of TRN and OOB sets are updated in the voting buffer:
// * items [Idx0,Idx1) of WorkBuf.TrnSet
// * items [OOBIdx0, OOBIdx1) of WorkBuf.OOBSet
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_outputleaf(decisionforestbuilder *s, dfworkbuf *workbuf, RVector *treebuf, dfvotebuf *votebuf, ae_int_t idx0, ae_int_t idx1, ae_int_t oobidx0, ae_int_t oobidx1, ae_int_t *treesize, double leafval) {
   ae_int_t leafvali;
   ae_int_t nclasses;
   ae_int_t i;
   ae_int_t j;
   nclasses = s->nclasses;
   if (nclasses == 1) {
   // Store split to the tree
      treebuf->xR[*treesize] = -1.0;
      treebuf->xR[*treesize + 1] = leafval;
   // Update training and OOB voting stats
      for (i = idx0; i < idx1; i++) {
         j = workbuf->trnset.xZ[i];
         votebuf->trntotals.xR[j] += leafval;
         votebuf->trncounts.xZ[j]++;
      }
      for (i = oobidx0; i < oobidx1; i++) {
         j = workbuf->oobset.xZ[i];
         votebuf->oobtotals.xR[j] += leafval;
         votebuf->oobcounts.xZ[j]++;
      }
   } else {
   // Store split to the tree
      treebuf->xR[*treesize] = -1.0;
      treebuf->xR[*treesize + 1] = leafval;
   // Update training and OOB voting stats
      leafvali = RoundZ(leafval);
      for (i = idx0; i < idx1; i++) {
         j = workbuf->trnset.xZ[i];
         votebuf->trntotals.xR[j * nclasses + leafvali]++;
         votebuf->trncounts.xZ[j]++;
      }
      for (i = oobidx0; i < oobidx1; i++) {
         j = workbuf->oobset.xZ[i];
         votebuf->oobtotals.xR[j * nclasses + leafvali]++;
         votebuf->oobcounts.xZ[j]++;
      }
   }
   *treesize += dforest_leafnodewidth;
}

// Recurrent tree construction function using  caller-allocated  buffers  and
// caller-initialized RNG.
//
// Following iterms are processed:
// * items [Idx0,Idx1) of WorkBuf.TrnSet
// * items [OOBIdx0, OOBIdx1) of WorkBuf.OOBSet
//
// TreeSize on input must be 1 (header element of the tree), on output it
// contains size of the tree.
//
// OOBLoss on input must contain value of MeanNRMS2(...) computed for entire
// dataset.
//
// Variables from #0 to #WorkingSet-1 from WorkBuf.VarPool are used (for
// block algorithm: blocks, not vars)
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_buildrandomtreerec(decisionforestbuilder *s, dfworkbuf *workbuf, ae_int_t workingset, ae_int_t varstoselect, RVector *treebuf, dfvotebuf *votebuf, hqrndstate *rs, ae_int_t idx0, ae_int_t idx1, ae_int_t oobidx0, ae_int_t oobidx1, double meanloss, double topmostmeanloss, ae_int_t *treesize) {
   ae_int_t npoints;
   ae_int_t nclasses;
   ae_int_t i;
   ae_int_t j;
   ae_int_t j0;
   double v;
   bool labelsaresame;
   ae_int_t offs;
   ae_int_t varbest;
   double splitbest;
   ae_int_t i1;
   ae_int_t i2;
   ae_int_t idxtrn;
   ae_int_t idxoob;
   double meanloss0;
   double meanloss1;
   ae_assert(s->dstype == 0, "not supported skbdgfsi!");
   ae_assert(idx0 < idx1, "BuildRandomTreeRec: integrity check failed (3445)");
   ae_assert(oobidx0 <= oobidx1, "BuildRandomTreeRec: integrity check failed (7452)");
   npoints = s->npoints;
   nclasses = s->nclasses;
// Check labels: all same or not?
   if (nclasses > 1) {
      labelsaresame = true;
      for (i = 0; i < nclasses; i++) {
         workbuf->classpriors.xZ[i] = 0;
      }
      j0 = workbuf->trnlabelsi.xZ[idx0];
      for (i = idx0; i < idx1; i++) {
         j = workbuf->trnlabelsi.xZ[i];
         workbuf->classpriors.xZ[j]++;
         labelsaresame = labelsaresame && j0 == j;
      }
   } else {
      labelsaresame = false;
   }
// Leaf node
   if (idx1 - idx0 == 1 || labelsaresame) {
      if (nclasses == 1) {
         dforest_outputleaf(s, workbuf, treebuf, votebuf, idx0, idx1, oobidx0, oobidx1, treesize, workbuf->trnlabelsr.xR[idx0]);
      } else {
         dforest_outputleaf(s, workbuf, treebuf, votebuf, idx0, idx1, oobidx0, oobidx1, treesize, (double)(workbuf->trnlabelsi.xZ[idx0]));
      }
      return;
   }
// Non-leaf node.
// Investigate possible splits.
   ae_assert(s->rdfalgo == 0, "BuildRandomForest: unexpected algo");
   dforest_choosecurrentsplitdense(s, workbuf, &workingset, varstoselect, rs, idx0, idx1, &varbest, &splitbest);
   if (varbest < 0) {
   // No good split was found; make leaf (label is randomly chosen) and exit.
      if (nclasses > 1) {
         v = (double)(workbuf->trnlabelsi.xZ[idx0 + hqrnduniformi(rs, idx1 - idx0)]);
      } else {
         v = workbuf->trnlabelsr.xR[idx0 + hqrnduniformi(rs, idx1 - idx0)];
      }
      dforest_outputleaf(s, workbuf, treebuf, votebuf, idx0, idx1, oobidx0, oobidx1, treesize, v);
      return;
   }
// Good split WAS found, we can perform it:
// * first, we split training set
// * then, we similarly split OOB set
   ae_assert(s->dstype == 0, "not supported 54bfdh");
   offs = npoints * varbest;
   i1 = idx0;
   i2 = idx1 - 1;
   while (i1 <= i2) {
   // Reorder indexes so that left partition is in [Idx0..I1),
   // and right partition is in [I2+1..Idx1)
      if (workbuf->bestvals.xR[i1] < splitbest) {
         i1++;
         continue;
      }
      if (workbuf->bestvals.xR[i2] >= splitbest) {
         i2--;
         continue;
      }
      swapi(&workbuf->trnset.xZ[i1], &workbuf->trnset.xZ[i2]);
      if (nclasses > 1) {
         swapi(&workbuf->trnlabelsi.xZ[i1], &workbuf->trnlabelsi.xZ[i2]);
      } else {
         swapr(&workbuf->trnlabelsr.xR[i1], &workbuf->trnlabelsr.xR[i2]);
      }
      i1++;
      i2--;
   }
   ae_assert(i1 == i2 + 1, "BuildRandomTreeRec: integrity check failed (45rds3)");
   idxtrn = i1;
   if (oobidx0 < oobidx1) {
   // Unlike the training subset, the out-of-bag subset corresponding to the
   // current sequence of decisions can be empty; thus, we have to explicitly
   // handle situation of zero OOB subset.
      i1 = oobidx0;
      i2 = oobidx1 - 1;
      while (i1 <= i2) {
      // Reorder indexes so that left partition is in [Idx0..I1),
      // and right partition is in [I2+1..Idx1)
         if (s->dsdata.xR[offs + workbuf->oobset.xZ[i1]] < splitbest) {
            i1++;
            continue;
         }
         if (s->dsdata.xR[offs + workbuf->oobset.xZ[i2]] >= splitbest) {
            i2--;
            continue;
         }
         swapi(&workbuf->oobset.xZ[i1], &workbuf->oobset.xZ[i2]);
         if (nclasses > 1) {
            swapi(&workbuf->ooblabelsi.xZ[i1], &workbuf->ooblabelsi.xZ[i2]);
         } else {
            swapr(&workbuf->ooblabelsr.xR[i1], &workbuf->ooblabelsr.xR[i2]);
         }
         i1++;
         i2--;
      }
      ae_assert(i1 == i2 + 1, "BuildRandomTreeRec: integrity check failed (643fs3)");
      idxoob = i1;
   } else {
      idxoob = oobidx0;
   }
// Compute estimates of NRMS2 loss over TRN or OOB subsets, update Gini importances
   if (s->rdfimportance == dforest_needtrngini) {
      meanloss0 = dforest_meannrms2(nclasses, &workbuf->trnlabelsi, &workbuf->trnlabelsr, idx0, idxtrn, &workbuf->trnlabelsi, &workbuf->trnlabelsr, idx0, idxtrn, &workbuf->tmpnrms2);
      meanloss1 = dforest_meannrms2(nclasses, &workbuf->trnlabelsi, &workbuf->trnlabelsr, idxtrn, idx1, &workbuf->trnlabelsi, &workbuf->trnlabelsr, idxtrn, idx1, &workbuf->tmpnrms2);
   } else {
      meanloss0 = dforest_meannrms2(nclasses, &workbuf->trnlabelsi, &workbuf->trnlabelsr, idx0, idxtrn, &workbuf->ooblabelsi, &workbuf->ooblabelsr, oobidx0, idxoob, &workbuf->tmpnrms2);
      meanloss1 = dforest_meannrms2(nclasses, &workbuf->trnlabelsi, &workbuf->trnlabelsr, idxtrn, idx1, &workbuf->ooblabelsi, &workbuf->ooblabelsr, idxoob, oobidx1, &workbuf->tmpnrms2);
   }
   votebuf->giniimportances.xR[varbest] += (meanloss - (meanloss0 + meanloss1)) / (topmostmeanloss + 1.0e-20);
// Generate tree node and subtrees (recursively)
   treebuf->xR[*treesize] = (double)varbest;
   treebuf->xR[*treesize + 1] = splitbest;
   i = *treesize;
   *treesize += dforest_innernodewidth;
   dforest_buildrandomtreerec(s, workbuf, workingset, varstoselect, treebuf, votebuf, rs, idx0, idxtrn, oobidx0, idxoob, meanloss0, topmostmeanloss, treesize);
   treebuf->xR[i + 2] = (double)(*treesize);
   dforest_buildrandomtreerec(s, workbuf, workingset, varstoselect, treebuf, votebuf, rs, idxtrn, idx1, idxoob, oobidx1, meanloss1, topmostmeanloss, treesize);
}

// Builds a range of random trees [TreeIdx0,TreeIdx1) using decision forest
// algorithm. Tree index is used to seed per-tree RNG.
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_buildrandomtree(decisionforestbuilder *s, ae_int_t treeidx0, ae_int_t treeidx1) {
   ae_frame _frame_block;
   ae_int_t treeidx;
   ae_int_t i;
   ae_int_t j;
   ae_int_t npoints;
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t treesize;
   ae_int_t varstoselect;
   ae_int_t workingsetsize;
   double meanloss;
   ae_frame_make(&_frame_block);
   NewObj(hqrndstate, rs);
   RefObj(dfworkbuf, workbuf);
   RefObj(dfvotebuf, votebuf);
   RefObj(dftreebuf, treebuf);
// Perform parallelization
// Parallelism was activated if treeidx1 - treeidx0 > 1.
   if (treeidx1 - treeidx0 > 1) {
      j = (treeidx1 - treeidx0) / 2;
      dforest_buildrandomtree(s, treeidx0, treeidx0 + j);
      dforest_buildrandomtree(s, treeidx0 + j, treeidx1);
      ae_frame_leave();
      return;
   } else {
      ae_assert(treeidx1 - treeidx0 == 1, "RDF: integrity check failed");
      treeidx = treeidx0;
   }
// Prepare
   npoints = s->npoints;
   nvars = s->nvars;
   nclasses = s->nclasses;
   if (s->rdfglobalseed > 0) {
      hqrndseed(s->rdfglobalseed, 1 + treeidx, &rs);
   } else {
      hqrndseed(ae_randominteger(30000), 1 + treeidx, &rs);
   }
// Retrieve buffers.
   ae_shared_pool_retrieve(&s->workpool, &_workbuf);
   ae_shared_pool_retrieve(&s->votepool, &_votebuf);
// Prepare everything for tree construction.
   ae_assert(workbuf->trnsize >= 1, "DForest: integrity check failed (34636)");
   ae_assert(workbuf->oobsize >= 0, "DForest: integrity check failed (45745)");
   ae_assert(workbuf->trnsize + workbuf->oobsize == npoints, "DForest: integrity check failed (89415)");
   workingsetsize = -1;
   workbuf->varpoolsize = 0;
   for (i = 0; i < nvars; i++) {
      if (s->dsmin.xR[i] != s->dsmax.xR[i]) {
         workbuf->varpool.xZ[workbuf->varpoolsize] = i;
         workbuf->varpoolsize++;
      }
   }
   workingsetsize = workbuf->varpoolsize;
   ae_assert(workingsetsize >= 0, "DForest: integrity check failed (73f5)");
   for (i = 0; i < npoints; i++) {
      workbuf->tmp0i.xZ[i] = i;
   }
   for (i = 0; i < workbuf->trnsize; i++) {
      j = hqrnduniformi(&rs, npoints - i);
      swapelementsi(&workbuf->tmp0i, i, i + j);
      workbuf->trnset.xZ[i] = workbuf->tmp0i.xZ[i];
      if (nclasses > 1) {
         workbuf->trnlabelsi.xZ[i] = s->dsival.xZ[workbuf->tmp0i.xZ[i]];
      } else {
         workbuf->trnlabelsr.xR[i] = s->dsrval.xR[workbuf->tmp0i.xZ[i]];
      }
      if (s->neediobmatrix) {
         s->iobmatrix.xyB[treeidx][workbuf->trnset.xZ[i]] = true;
      }
   }
   for (i = 0; i < workbuf->oobsize; i++) {
      j = workbuf->tmp0i.xZ[workbuf->trnsize + i];
      workbuf->oobset.xZ[i] = j;
      if (nclasses > 1) {
         workbuf->ooblabelsi.xZ[i] = s->dsival.xZ[j];
      } else {
         workbuf->ooblabelsr.xR[i] = s->dsrval.xR[j];
      }
   }
   varstoselect = RoundZ(sqrt((double)nvars));
   if (s->rdfvars > 0.0) {
      varstoselect = RoundZ(s->rdfvars);
   }
   if (s->rdfvars < 0.0) {
      varstoselect = RoundZ(-nvars * s->rdfvars);
   }
   varstoselect = imax2(varstoselect, 1);
   varstoselect = imin2(varstoselect, nvars);
// Perform recurrent construction
   if (s->rdfimportance == dforest_needtrngini) {
      meanloss = dforest_meannrms2(nclasses, &workbuf->trnlabelsi, &workbuf->trnlabelsr, 0, workbuf->trnsize, &workbuf->trnlabelsi, &workbuf->trnlabelsr, 0, workbuf->trnsize, &workbuf->tmpnrms2);
   } else {
      meanloss = dforest_meannrms2(nclasses, &workbuf->trnlabelsi, &workbuf->trnlabelsr, 0, workbuf->trnsize, &workbuf->ooblabelsi, &workbuf->ooblabelsr, 0, workbuf->oobsize, &workbuf->tmpnrms2);
   }
   treesize = 1;
   dforest_buildrandomtreerec(s, workbuf, workingsetsize, varstoselect, &workbuf->treebuf, votebuf, &rs, 0, workbuf->trnsize, 0, workbuf->oobsize, meanloss, meanloss, &treesize);
   workbuf->treebuf.xR[0] = (double)treesize;
// Store tree
   ae_shared_pool_retrieve(&s->treefactory, &_treebuf);
   ae_vector_set_length(&treebuf->treebuf, treesize);
   for (i = 0; i < treesize; i++) {
      treebuf->treebuf.xR[i] = workbuf->treebuf.xR[i];
   }
   treebuf->treeidx = treeidx;
   ae_shared_pool_recycle(&s->treepool, &_treebuf);
// Return other buffers to appropriate pools
   ae_shared_pool_recycle(&s->workpool, &_workbuf);
   ae_shared_pool_recycle(&s->votepool, &_votebuf);
// Update progress indicator
   s->rdfprogress += npoints;
   ae_frame_leave();
}

// Internal subroutine for processing one decision tree stored in uncompressed
// format starting at SubtreeRoot (this index points to the header of the tree,
// not its first node). First node being processed is located at NodeOffs.
static void dforest_dfprocessinternaluncompressed(decisionforest *df, ae_int_t subtreeroot, ae_int_t nodeoffs, RVector *x, RVector *y) {
   ae_int_t idx;
   ae_assert(df->forestformat == dforest_dfuncompressedv0, "DFProcessInternal: unexpected forest format");
// Navigate through the tree
   while (true) {
      if (df->trees.xR[nodeoffs] == -1.0) {
         if (df->nclasses == 1) {
            y->xR[0] += df->trees.xR[nodeoffs + 1];
         } else {
            idx = RoundZ(df->trees.xR[nodeoffs + 1]);
            y->xR[idx]++;
         }
         break;
      }
      if (x->xR[RoundZ(df->trees.xR[nodeoffs])] < df->trees.xR[nodeoffs + 1]) {
         nodeoffs += dforest_innernodewidth;
      } else {
         nodeoffs = subtreeroot + RoundZ(df->trees.xR[nodeoffs + 2]);
      }
   }
}

// Estimates permutation variable importance ratings for a range of dataset
// points.
//
// Initial call to this function should span entire range of the dataset,
// [Idx0,Idx1)=[0,NPoints), because function performs initialization of some
// internal structures when called with these arguments.
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_estimatepermutationimportances(decisionforestbuilder *s, decisionforest *df, ae_int_t ntrees, ae_shared_pool *permpool, ae_int_t idx0, ae_int_t idx1) {
   ae_frame _frame_block;
   ae_int_t npoints;
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t nperm;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   double v;
   ae_int_t treeroot;
   ae_int_t nodeoffs;
   double prediction;
   ae_int_t varidx;
   ae_int_t oobcounts;
   ae_int_t srcidx;
   ae_frame_make(&_frame_block);
   RefObj(dfpermimpbuf, permimpbuf);
   npoints = s->npoints;
   nvars = s->nvars;
   nclasses = s->nclasses;
   ae_assert(df->forestformat == dforest_dfuncompressedv0, "EstimateVariableImportance: integrity check failed (ff)");
   ae_assert(idx0 >= 0 && idx0 <= idx1 && idx1 <= npoints, "EstimateVariableImportance: integrity check failed (idx)");
   ae_assert(s->iobmatrix.rows >= ntrees && s->iobmatrix.cols >= npoints, "EstimateVariableImportance: integrity check failed (IOB)");
// Perform parallelization if batch is too large
   if (idx1 - idx0 > dforest_permutationimportancebatchsize) {
      j = (idx1 - idx0) / 2;
      dforest_estimatepermutationimportances(s, df, ntrees, permpool, idx0, idx0 + j);
      dforest_estimatepermutationimportances(s, df, ntrees, permpool, idx0 + j, idx1);
      ae_frame_leave();
      return;
   }
// Retrieve buffer object from pool
   ae_shared_pool_retrieve(permpool, &_permimpbuf);
// Process range of points [idx0,idx1)
   nperm = nvars + 2;
   for (i = idx0; i < idx1; i++) {
      ae_assert(s->dstype == 0, "EstimateVariableImportance: unexpected dataset type");
      for (j = 0; j < nvars; j++) {
         permimpbuf->xraw.xR[j] = s->dsdata.xR[j * npoints + i];
         srcidx = s->varimpshuffle2.xZ[(i + s->varimpshuffle2.xZ[npoints + j]) % npoints];
         permimpbuf->xdist.xR[j] = s->dsdata.xR[j * npoints + srcidx];
      }
      if (nclasses > 1) {
         for (j = 0; j < nclasses; j++) {
            permimpbuf->targety.xR[j] = 0.0;
         }
         permimpbuf->targety.xR[s->dsival.xZ[i]] = 1.0;
      } else {
         permimpbuf->targety.xR[0] = s->dsrval.xR[i];
      }
   // Process all trees, for each tree compute NPerm losses corresponding
   // to various permutations of variable values
      for (j = 0; j < nperm * nclasses; j++) {
         permimpbuf->yv.xR[j] = 0.0;
      }
      oobcounts = 0;
      treeroot = 0;
      for (k = 0; k < ntrees; k++) {
         if (!s->iobmatrix.xyB[k][i]) {
         // Process original (unperturbed) point and analyze path from the
         // tree root to the final leaf. Output prediction to RawPrediction.
         //
         // Additionally, for each variable in [0,NVars-1] save offset of
         // the first split on this variable. It allows us to quickly compute
         // tree decision when perturbation does not change decision path.
            ae_assert(df->forestformat == dforest_dfuncompressedv0, "EstimateVariableImportance: integrity check failed (ff)");
            nodeoffs = treeroot + 1;
            for (j = 0; j < nvars; j++) {
               permimpbuf->startnodes.xZ[j] = -1;
            }
            prediction = 0.0;
            while (true) {
               if (df->trees.xR[nodeoffs] == -1.0) {
                  prediction = df->trees.xR[nodeoffs + 1];
                  break;
               }
               j = RoundZ(df->trees.xR[nodeoffs]);
               if (permimpbuf->startnodes.xZ[j] < 0) {
                  permimpbuf->startnodes.xZ[j] = nodeoffs;
               }
               if (permimpbuf->xraw.xR[j] < df->trees.xR[nodeoffs + 1]) {
                  nodeoffs += dforest_innernodewidth;
               } else {
                  nodeoffs = treeroot + RoundZ(df->trees.xR[nodeoffs + 2]);
               }
            }
         // Save loss for unperturbed point
            varidx = nvars + 1;
            if (nclasses > 1) {
               j = RoundZ(prediction);
               permimpbuf->yv.xR[varidx * nclasses + j]++;
            } else {
               permimpbuf->yv.xR[varidx] += prediction;
            }
         // Save loss for all variables being perturbed (XDist).
         // This loss is used as a reference loss when we compute R-squared.
            varidx = nvars;
            for (j = 0; j < nclasses; j++) {
               permimpbuf->y.xR[j] = 0.0;
            }
            dforest_dfprocessinternaluncompressed(df, treeroot, treeroot + 1, &permimpbuf->xdist, &permimpbuf->y);
            for (j = 0; j < nclasses; j++) {
               permimpbuf->yv.xR[varidx * nclasses + j] += permimpbuf->y.xR[j];
            }
         // Compute losses for variable #VarIdx being perturbed. Quite an often decision
         // process does not actually depend on the variable #VarIdx (path from the tree
         // root does not include splits on this variable). In such cases we perform
         // quick exit from the loop with precomputed value.
            for (j = 0; j < nvars; j++) {
               permimpbuf->xcur.xR[j] = permimpbuf->xraw.xR[j];
            }
            for (varidx = 0; varidx < nvars; varidx++) {
               if (permimpbuf->startnodes.xZ[varidx] >= 0) {
               // Path from tree root to the final leaf involves split on variable #VarIdx.
               // Restart computation from the position first split on #VarIdx.
                  ae_assert(df->forestformat == dforest_dfuncompressedv0, "EstimateVariableImportance: integrity check failed (ff)");
                  permimpbuf->xcur.xR[varidx] = permimpbuf->xdist.xR[varidx];
                  nodeoffs = permimpbuf->startnodes.xZ[varidx];
                  while (true) {
                     if (df->trees.xR[nodeoffs] == -1.0) {
                        if (nclasses > 1) {
                           j = RoundZ(df->trees.xR[nodeoffs + 1]);
                           permimpbuf->yv.xR[varidx * nclasses + j]++;
                        } else {
                           permimpbuf->yv.xR[varidx] += df->trees.xR[nodeoffs + 1];
                        }
                        break;
                     }
                     j = RoundZ(df->trees.xR[nodeoffs]);
                     if (permimpbuf->xcur.xR[j] < df->trees.xR[nodeoffs + 1]) {
                        nodeoffs += dforest_innernodewidth;
                     } else {
                        nodeoffs = treeroot + RoundZ(df->trees.xR[nodeoffs + 2]);
                     }
                  }
                  permimpbuf->xcur.xR[varidx] = permimpbuf->xraw.xR[varidx];
               } else {
               // Path from tree root to the final leaf does NOT involve split on variable #VarIdx.
               // Permutation does not change tree output, reuse already computed value.
                  if (nclasses > 1) {
                     j = RoundZ(prediction);
                     permimpbuf->yv.xR[varidx * nclasses + j]++;
                  } else {
                     permimpbuf->yv.xR[varidx] += prediction;
                  }
               }
            }
         // update OOB counter
            oobcounts++;
         }
         treeroot += RoundZ(df->trees.xR[treeroot]);
      }
   // Now YV[] stores NPerm versions of the forest output for various permutations of variable values.
   // Update losses.
      for (j = 0; j < nperm; j++) {
         for (k = 0; k < nclasses; k++) {
            permimpbuf->yv.xR[j * nclasses + k] /= coalesce((double)oobcounts, 1.0);
         }
         v = 0.0;
         for (k = 0; k < nclasses; k++) {
            v += ae_sqr(permimpbuf->yv.xR[j * nclasses + k] - permimpbuf->targety.xR[k]);
         }
         permimpbuf->losses.xR[j] += v;
      }
   // Update progress indicator
      s->rdfprogress += ntrees;
   }
// Recycle buffer object with updated Losses[] field
   ae_shared_pool_recycle(permpool, &_permimpbuf);
   ae_frame_leave();
}

// Estimates permutation variable importance ratings for a range of dataset
// points.
//
// Initial call to this function should span entire range of the dataset,
// [Idx0,Idx1)=[0,NPoints), because function performs initialization of some
// internal structures when called with these arguments.
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_estimatevariableimportance(decisionforestbuilder *s, ae_int_t sessionseed, decisionforest *df, ae_int_t ntrees, dfreport *rep) {
   ae_frame _frame_block;
   ae_int_t npoints;
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t nperm;
   ae_int_t i;
   ae_int_t j;
   double nopermloss;
   double totalpermloss;
   ae_frame_make(&_frame_block);
   RefObj(dfvotebuf, vote);
   NewVector(tmpr0, 0, DT_REAL);
   NewVector(tmpr1, 0, DT_REAL);
   NewVector(tmpi0, 0, DT_INT);
   NewVector(losses, 0, DT_REAL);
   NewObj(dfpermimpbuf, permseed);
   RefObj(dfpermimpbuf, permresult);
   NewObj(ae_shared_pool, permpool);
   NewObj(hqrndstate, varimprs);
   npoints = s->npoints;
   nvars = s->nvars;
   nclasses = s->nclasses;
// No importance rating
   if (s->rdfimportance == 0) {
      ae_frame_leave();
      return;
   }
// Gini importance
   if (s->rdfimportance == dforest_needtrngini || s->rdfimportance == dforest_needoobgini) {
   // Merge OOB Gini importances computed during tree generation
      for (ae_shared_pool_first_recycled(&s->votepool, &_vote); vote != NULL; ae_shared_pool_next_recycled(&s->votepool, &_vote)) {
         for (i = 0; i < nvars; i++) {
            rep->varimportances.xR[i] += vote->giniimportances.xR[i] / ntrees;
         }
      }
      for (i = 0; i < nvars; i++) {
         rep->varimportances.xR[i] = rboundval(rep->varimportances.xR[i], 0.0, 1.0);
      }
   // Compute topvars[] array
      ae_vector_set_length(&tmpr0, nvars);
      for (j = 0; j < nvars; j++) {
         tmpr0.xR[j] = -rep->varimportances.xR[j];
         rep->topvars.xZ[j] = j;
      }
      tagsortfasti(&tmpr0, &rep->topvars, &tmpr1, &tmpi0, nvars);
      ae_frame_leave();
      return;
   }
// Permutation importance
   if (s->rdfimportance == dforest_needpermutation) {
      ae_assert(df->forestformat == dforest_dfuncompressedv0, "EstimateVariableImportance: integrity check failed (ff)");
      ae_assert(s->iobmatrix.rows >= ntrees && s->iobmatrix.cols >= npoints, "EstimateVariableImportance: integrity check failed (IOB)");
   // Generate packed representation of the shuffle which is applied to all variables
   //
   // Ideally we want to apply different permutations to different variables,
   // i.e. we have to generate and store NPoints*NVars random numbers.
   // However due to performance and memory restrictions we prefer to use compact
   // representation:
   // * we store one "reference" permutation P_ref in VarImpShuffle2[0:NPoints-1]
   // * a permutation P_j applied to variable J is obtained by circularly shifting
   //   elements in P_ref by VarImpShuffle2[NPoints+J]
      hqrndseed(sessionseed, 1117, &varimprs);
      vectorsetlengthatleast(&s->varimpshuffle2, npoints + nvars);
      for (i = 0; i < npoints; i++) {
         s->varimpshuffle2.xZ[i] = i;
      }
      for (i = 0; i < npoints - 1; i++) {
         j = i + hqrnduniformi(&varimprs, npoints - i);
         swapi(&s->varimpshuffle2.xZ[i], &s->varimpshuffle2.xZ[j]);
      }
      for (i = 0; i < nvars; i++) {
         s->varimpshuffle2.xZ[npoints + i] = hqrnduniformi(&varimprs, npoints);
      }
   // Prepare buffer object, seed pool
      nperm = nvars + 2;
      ae_vector_set_length(&permseed.losses, nperm);
      for (j = 0; j < nperm; j++) {
         permseed.losses.xR[j] = 0.0;
      }
      ae_vector_set_length(&permseed.yv, nperm * nclasses);
      ae_vector_set_length(&permseed.xraw, nvars);
      ae_vector_set_length(&permseed.xdist, nvars);
      ae_vector_set_length(&permseed.xcur, nvars);
      ae_vector_set_length(&permseed.targety, nclasses);
      ae_vector_set_length(&permseed.startnodes, nvars);
      ae_vector_set_length(&permseed.y, nclasses);
      ae_shared_pool_set_seed(&permpool, &permseed, sizeof(permseed), dfpermimpbuf_init, dfpermimpbuf_copy, dfpermimpbuf_free);
   // Recursively split subset and process (using parallel capabilities, if possible)
      dforest_estimatepermutationimportances(s, df, ntrees, &permpool, 0, npoints);
   // Merge results
      ae_vector_set_length(&losses, nperm);
      for (j = 0; j < nperm; j++) {
         losses.xR[j] = 1.0e-20;
      }
      for (ae_shared_pool_first_recycled(&permpool, &_permresult); permresult != NULL; ae_shared_pool_next_recycled(&permpool, &_permresult)) {
         for (j = 0; j < nperm; j++) {
            losses.xR[j] += permresult->losses.xR[j];
         }
      }
   // Compute importances
      nopermloss = losses.xR[nvars + 1];
      totalpermloss = losses.xR[nvars];
      for (i = 0; i < nvars; i++) {
         rep->varimportances.xR[i] = 1 - nopermloss / totalpermloss - (1 - losses.xR[i] / totalpermloss);
         rep->varimportances.xR[i] = rboundval(rep->varimportances.xR[i], 0.0, 1.0);
      }
   // Compute topvars[] array
      ae_vector_set_length(&tmpr0, nvars);
      for (j = 0; j < nvars; j++) {
         tmpr0.xR[j] = -rep->varimportances.xR[j];
         rep->topvars.xZ[j] = j;
      }
      tagsortfasti(&tmpr0, &rep->topvars, &tmpr1, &tmpi0, nvars);
      ae_frame_leave();
      return;
   }
   ae_assert(false, "EstimateVariableImportance: unexpected importance type");
   ae_frame_leave();
}

// Sets report fields to their default values
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_cleanreport(decisionforestbuilder *s, dfreport *rep) {
   ae_int_t i;
   rep->relclserror = 0.0;
   rep->avgce = 0.0;
   rep->rmserror = 0.0;
   rep->avgerror = 0.0;
   rep->avgrelerror = 0.0;
   rep->oobrelclserror = 0.0;
   rep->oobavgce = 0.0;
   rep->oobrmserror = 0.0;
   rep->oobavgerror = 0.0;
   rep->oobavgrelerror = 0.0;
   ae_vector_set_length(&rep->topvars, s->nvars);
   ae_vector_set_length(&rep->varimportances, s->nvars);
   for (i = 0; i < s->nvars; i++) {
      rep->topvars.xZ[i] = i;
      rep->varimportances.xR[i] = 0.0;
   }
}

// This function performs generic and algorithm-specific preprocessing of the
// dataset
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_analyzeandpreprocessdataset(decisionforestbuilder *s) {
   ae_frame _frame_block;
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t npoints;
   ae_int_t i;
   ae_int_t j;
   bool isbinary;
   double v;
   double v0;
   double v1;
   ae_frame_make(&_frame_block);
   NewObj(hqrndstate, rs);
   ae_assert(s->dstype == 0, "no sparsity");
   npoints = s->npoints;
   nvars = s->nvars;
   nclasses = s->nclasses;
// seed local RNG
   if (s->rdfglobalseed > 0) {
      hqrndseed(s->rdfglobalseed, 3532, &rs);
   } else {
      hqrndseed(ae_randominteger(30000), 3532, &rs);
   }
// Generic processing
   ae_assert(npoints >= 1, "BuildRandomForest: integrity check failed");
   vectorsetlengthatleast(&s->dsmin, nvars);
   vectorsetlengthatleast(&s->dsmax, nvars);
   vectorsetlengthatleast(&s->dsbinary, nvars);
   for (i = 0; i < nvars; i++) {
      v0 = s->dsdata.xR[i * npoints];
      v1 = s->dsdata.xR[i * npoints];
      for (j = 1; j < npoints; j++) {
         v = s->dsdata.xR[i * npoints + j];
         if (v < v0) {
            v0 = v;
         }
         if (v > v1) {
            v1 = v;
         }
      }
      s->dsmin.xR[i] = v0;
      s->dsmax.xR[i] = v1;
      ae_assert(v0 <= v1, "BuildRandomForest: strange integrity check failure");
      isbinary = true;
      for (j = 0; j < npoints; j++) {
         v = s->dsdata.xR[i * npoints + j];
         isbinary = isbinary && (v == v0 || v == v1);
      }
      s->dsbinary.xB[i] = isbinary;
   }
   if (nclasses == 1) {
      s->dsravg = 0.0;
      for (i = 0; i < npoints; i++) {
         s->dsravg += s->dsrval.xR[i];
      }
      s->dsravg /= npoints;
   } else {
      vectorsetlengthatleast(&s->dsctotals, nclasses);
      for (i = 0; i < nclasses; i++) {
         s->dsctotals.xZ[i] = 0;
      }
      for (i = 0; i < npoints; i++) {
         s->dsctotals.xZ[s->dsival.xZ[i]]++;
      }
   }
   ae_frame_leave();
}

// This function merges together trees generated during training and outputs
// it to the decision forest.
//
// Inputs:
//     S           -   decision forest builder object
//     NTrees      -   NTrees >= 1, number of trees to train
//
// Outputs:
//     DF          -   decision forest
//     Rep         -   report
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_mergetrees(decisionforestbuilder *s, decisionforest *df) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t cursize;
   ae_int_t offs;
   ae_frame_make(&_frame_block);
   RefObj(dftreebuf, tree);
   NewVector(treesizes, 0, DT_INT);
   NewVector(treeoffsets, 0, DT_INT);
   df->forestformat = dforest_dfuncompressedv0;
   df->nvars = s->nvars;
   df->nclasses = s->nclasses;
   df->bufsize = 0;
   df->ntrees = 0;
// Determine trees count
   for (ae_shared_pool_first_recycled(&s->treepool, &_tree); tree != NULL; ae_shared_pool_next_recycled(&s->treepool, &_tree)) {
      df->ntrees++;
   }
   ae_assert(df->ntrees > 0, "MergeTrees: integrity check failed, zero trees count");
// Determine individual tree sizes and total buffer size
   ae_vector_set_length(&treesizes, df->ntrees);
   for (i = 0; i < df->ntrees; i++) {
      treesizes.xZ[i] = -1;
   }
   for (ae_shared_pool_first_recycled(&s->treepool, &_tree); tree != NULL; ae_shared_pool_next_recycled(&s->treepool, &_tree)) {
      ae_assert(tree->treeidx >= 0 && tree->treeidx < df->ntrees, "MergeTrees: integrity check failed (wrong TreeIdx)");
      ae_assert(treesizes.xZ[tree->treeidx] < 0, "MergeTrees: integrity check failed (duplicate TreeIdx)");
      df->bufsize += RoundZ(tree->treebuf.xR[0]);
      treesizes.xZ[tree->treeidx] = RoundZ(tree->treebuf.xR[0]);
   }
   for (i = 0; i < df->ntrees; i++) {
      ae_assert(treesizes.xZ[i] > 0, "MergeTrees: integrity check failed (wrong TreeSize)");
   }
// Determine offsets for individual trees in output buffer
   ae_vector_set_length(&treeoffsets, df->ntrees);
   treeoffsets.xZ[0] = 0;
   for (i = 1; i < df->ntrees; i++) {
      treeoffsets.xZ[i] = treeoffsets.xZ[i - 1] + treesizes.xZ[i - 1];
   }
// Output trees
//
// NOTE: since ALGLIB 3.16.0 trees are sorted by tree index prior to
//       output (necessary for variable importance estimation), that's
//       why we need array of tree offsets
   ae_vector_set_length(&df->trees, df->bufsize);
   for (ae_shared_pool_first_recycled(&s->treepool, &_tree); tree != NULL; ae_shared_pool_next_recycled(&s->treepool, &_tree)) {
      cursize = RoundZ(tree->treebuf.xR[0]);
      offs = treeoffsets.xZ[tree->treeidx];
      for (i = 0; i < cursize; i++) {
         df->trees.xR[offs + i] = tree->treebuf.xR[i];
      }
   }
   ae_frame_leave();
}

// This function post-processes voting array and calculates TRN and OOB errors.
//
// Inputs:
//     S           -   decision forest builder object
//     NTrees      -   number of trees in the forest
//     Buf         -   possibly preallocated vote buffer, its contents is
//                     overwritten by this function
//
// Outputs:
//     Rep         -   report fields corresponding to errors are updated
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
static void dforest_processvotingresults(decisionforestbuilder *s, ae_int_t ntrees, dfvotebuf *buf, dfreport *rep) {
   ae_frame _frame_block;
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t npoints;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t k1;
   double v;
   ae_int_t avgrelcnt;
   ae_int_t oobavgrelcnt;
   ae_frame_make(&_frame_block);
   RefObj(dfvotebuf, vote);
   npoints = s->npoints;
   nvars = s->nvars;
   nclasses = s->nclasses;
   ae_assert(npoints > 0, "DFOREST: integrity check failed");
   ae_assert(nvars > 0, "DFOREST: integrity check failed");
   ae_assert(nclasses > 0, "DFOREST: integrity check failed");
// Prepare vote buffer
   vectorsetlengthatleast(&buf->trntotals, npoints * nclasses);
   vectorsetlengthatleast(&buf->oobtotals, npoints * nclasses);
   for (i = 0; i < npoints * nclasses; i++) {
      buf->trntotals.xR[i] = 0.0;
      buf->oobtotals.xR[i] = 0.0;
   }
   vectorsetlengthatleast(&buf->trncounts, npoints);
   vectorsetlengthatleast(&buf->oobcounts, npoints);
   for (i = 0; i < npoints; i++) {
      buf->trncounts.xZ[i] = 0;
      buf->oobcounts.xZ[i] = 0;
   }
// Merge voting arrays
   for (ae_shared_pool_first_recycled(&s->votepool, &_vote); vote != NULL; ae_shared_pool_next_recycled(&s->votepool, &_vote)) {
      for (i = 0; i < npoints * nclasses; i++) {
         buf->trntotals.xR[i] += vote->trntotals.xR[i] + vote->oobtotals.xR[i];
         buf->oobtotals.xR[i] += vote->oobtotals.xR[i];
      }
      for (i = 0; i < npoints; i++) {
         buf->trncounts.xZ[i] += vote->trncounts.xZ[i] + vote->oobcounts.xZ[i];
         buf->oobcounts.xZ[i] += vote->oobcounts.xZ[i];
      }
   }
   for (i = 0; i < npoints; i++) {
      v = 1 / coalesce((double)(buf->trncounts.xZ[i]), 1.0);
      for (j = 0; j < nclasses; j++) {
         buf->trntotals.xR[i * nclasses + j] *= v;
      }
      v = 1 / coalesce((double)(buf->oobcounts.xZ[i]), 1.0);
      for (j = 0; j < nclasses; j++) {
         buf->oobtotals.xR[i * nclasses + j] *= v;
      }
   }
// Use aggregated voting data to output error metrics
   avgrelcnt = 0;
   oobavgrelcnt = 0;
   rep->rmserror = 0.0;
   rep->avgerror = 0.0;
   rep->avgrelerror = 0.0;
   rep->relclserror = 0.0;
   rep->avgce = 0.0;
   rep->oobrmserror = 0.0;
   rep->oobavgerror = 0.0;
   rep->oobavgrelerror = 0.0;
   rep->oobrelclserror = 0.0;
   rep->oobavgce = 0.0;
   for (i = 0; i < npoints; i++) {
      if (nclasses > 1) {
      // classification-specific code
         k = s->dsival.xZ[i];
         for (j = 0; j < nclasses; j++) {
            v = buf->trntotals.xR[i * nclasses + j];
            if (j == k) {
               rep->avgce -= log(coalesce(v, ae_minrealnumber));
               rep->rmserror += ae_sqr(v - 1);
               rep->avgerror += fabs(v - 1);
               rep->avgrelerror += fabs(v - 1);
               avgrelcnt++;
            } else {
               rep->rmserror += ae_sqr(v);
               rep->avgerror += fabs(v);
            }
            v = buf->oobtotals.xR[i * nclasses + j];
            if (j == k) {
               rep->oobavgce -= log(coalesce(v, ae_minrealnumber));
               rep->oobrmserror += ae_sqr(v - 1);
               rep->oobavgerror += fabs(v - 1);
               rep->oobavgrelerror += fabs(v - 1);
               oobavgrelcnt++;
            } else {
               rep->oobrmserror += ae_sqr(v);
               rep->oobavgerror += fabs(v);
            }
         }
      // Classification errors are handled separately
         k1 = 0;
         for (j = 1; j < nclasses; j++) {
            if (buf->trntotals.xR[i * nclasses + j] > buf->trntotals.xR[i * nclasses + k1]) {
               k1 = j;
            }
         }
         if (k1 != k) {
            rep->relclserror++;
         }
         k1 = 0;
         for (j = 1; j < nclasses; j++) {
            if (buf->oobtotals.xR[i * nclasses + j] > buf->oobtotals.xR[i * nclasses + k1]) {
               k1 = j;
            }
         }
         if (k1 != k) {
            rep->oobrelclserror++;
         }
      } else {
      // regression-specific code
         v = buf->trntotals.xR[i] - s->dsrval.xR[i];
         rep->rmserror += ae_sqr(v);
         rep->avgerror += fabs(v);
         if (s->dsrval.xR[i] != 0.0) {
            rep->avgrelerror += fabs(v / s->dsrval.xR[i]);
            avgrelcnt++;
         }
         v = buf->oobtotals.xR[i] - s->dsrval.xR[i];
         rep->oobrmserror += ae_sqr(v);
         rep->oobavgerror += fabs(v);
         if (s->dsrval.xR[i] != 0.0) {
            rep->oobavgrelerror += fabs(v / s->dsrval.xR[i]);
            oobavgrelcnt++;
         }
      }
   }
   rep->relclserror /= npoints;
   rep->avgce /= npoints; //(@) Added to the original.
   rep->rmserror = sqrt(rep->rmserror / (npoints * nclasses));
   rep->avgerror /= npoints * nclasses;
   rep->avgrelerror /= coalesce((double)avgrelcnt, 1.0);
   rep->oobrelclserror /= npoints;
   rep->oobavgce /= npoints; //(@) Added to the original.
   rep->oobrmserror = sqrt(rep->oobrmserror / (npoints * nclasses));
   rep->oobavgerror /= npoints * nclasses;
   rep->oobavgrelerror /= coalesce((double)oobavgrelcnt, 1.0);
   ae_frame_leave();
}

// This subroutine builds decision forest according to current settings using
// dataset internally stored in the builder object. Dense algorithm is used.
//
// NOTE: this   function   uses   dense  algorithm  for  forest  construction
//       independently from the dataset format (dense or sparse).
//
// NOTE: forest built with this function is  stored  in-memory  using  64-bit
//       data structures for offsets/indexes/split values. It is possible  to
//       convert  forest  into  more  memory-efficient   compressed    binary
//       representation.  Depending  on  the  problem  properties,  3.7x-5.7x
//       compression factors are possible.
//
//       The downsides of compression are (a) slight reduction in  the  model
//       accuracy and (b) ~1.5x reduction in  the  inference  speed  (due  to
//       increased complexity of the storage format).
//
//       See comments on dfbinarycompression() for more info.
//
// Default settings are used by the algorithm; you can tweak  them  with  the
// help of the following functions:
// * dfbuildersetrfactor() - to control a fraction of the  dataset  used  for
//   subsampling
// * dfbuildersetrandomvars() - to control number of variables randomly chosen
//   for decision rule creation
//
// Inputs:
//     S           -   decision forest builder object
//     NTrees      -   NTrees >= 1, number of trees to train
//
// Outputs:
//     DF          -   decision forest. You can compress this forest to  more
//                     compact 16-bit representation with dfbinarycompression()
//     Rep         -   report, see below for information on its fields.
//
// ==== report information produced by forest construction function ====
//
// Decision forest training report includes following information:
// * training set errors
// * out-of-bag estimates of errors
// * variable importance ratings
//
// Following fields are used to store information:
// * training set errors are stored in rep.relclserror, rep.avgce, rep.rmserror,
//   rep.avgerror and rep.avgrelerror
// * out-of-bag estimates of errors are stored in rep.oobrelclserror, rep.oobavgce,
//   rep.oobrmserror, rep.oobavgerror and rep.oobavgrelerror
//
// Variable importance reports, if requested by dfbuildersetimportancegini(),
// dfbuildersetimportancetrngini() or dfbuildersetimportancepermutation()
// call, are stored in:
// * rep.varimportances field stores importance ratings
// * rep.topvars stores variable indexes ordered from the most important to
//   less important ones
//
// You can find more information about report fields in:
// * comments on dfreport structure
// * comments on dfbuildersetimportancegini function
// * comments on dfbuildersetimportancetrngini function
// * comments on dfbuildersetimportancepermutation function
// ALGLIB: Copyright 21.05.2018 by Sergey Bochkanov
// API: void dfbuilderbuildrandomforest(const decisionforestbuilder &s, const ae_int_t ntrees, decisionforest &df, dfreport &rep);
void dfbuilderbuildrandomforest(decisionforestbuilder *s, ae_int_t ntrees, decisionforest *df, dfreport *rep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t npoints;
   ae_int_t trnsize;
   ae_int_t maxtreesize;
   ae_int_t sessionseed;
   ae_frame_make(&_frame_block);
   SetObj(decisionforest, df);
   SetObj(dfreport, rep);
   NewObj(dfworkbuf, workbufseed);
   NewObj(dfvotebuf, votebufseed);
   NewObj(dftreebuf, treebufseed);
   ae_assert(ntrees >= 1, "DFBuilderBuildRandomForest: ntrees<1");
   dforest_cleanreport(s, rep);
   npoints = s->npoints;
   nvars = s->nvars;
   nclasses = s->nclasses;
// Set up progress counter
   s->rdfprogress = 0;
   s->rdftotal = ntrees * npoints;
   if (s->rdfimportance == dforest_needpermutation) {
      s->rdftotal += ntrees * npoints;
   }
// Quick exit for empty dataset
   if (s->dstype == -1 || npoints == 0) {
      ae_assert(dforest_leafnodewidth == 2, "DFBuilderBuildRandomForest: integrity check failed");
      df->forestformat = dforest_dfuncompressedv0;
      df->nvars = s->nvars;
      df->nclasses = s->nclasses;
      df->ntrees = 1;
      df->bufsize = 1 + dforest_leafnodewidth;
      ae_vector_set_length(&df->trees, 1 + dforest_leafnodewidth);
      df->trees.xR[0] = (double)(1 + dforest_leafnodewidth);
      df->trees.xR[1] = -1.0;
      df->trees.xR[2] = 0.0;
      dfcreatebuffer(df, &df->buffer);
      ae_frame_leave();
      return;
   }
   ae_assert(npoints > 0, "DFBuilderBuildRandomForest: integrity check failed");
// Analyze dataset statistics, perform preprocessing
   dforest_analyzeandpreprocessdataset(s);
// Prepare "work", "vote" and "tree" pools and other settings
   trnsize = RoundZ(npoints * s->rdfratio);
   trnsize = imax2(trnsize, 1);
   trnsize = imin2(trnsize, npoints);
   maxtreesize = 1 + dforest_innernodewidth * (trnsize - 1) + dforest_leafnodewidth * trnsize;
   ae_vector_set_length(&workbufseed.varpool, nvars);
   ae_vector_set_length(&workbufseed.trnset, trnsize);
   ae_vector_set_length(&workbufseed.oobset, npoints - trnsize);
   ae_vector_set_length(&workbufseed.tmp0i, npoints);
   ae_vector_set_length(&workbufseed.tmp1i, npoints);
   ae_vector_set_length(&workbufseed.tmp0r, npoints);
   ae_vector_set_length(&workbufseed.tmp1r, npoints);
   ae_vector_set_length(&workbufseed.tmp2r, npoints);
   ae_vector_set_length(&workbufseed.tmp3r, npoints);
   ae_vector_set_length(&workbufseed.trnlabelsi, npoints);
   ae_vector_set_length(&workbufseed.trnlabelsr, npoints);
   ae_vector_set_length(&workbufseed.ooblabelsi, npoints);
   ae_vector_set_length(&workbufseed.ooblabelsr, npoints);
   ae_vector_set_length(&workbufseed.curvals, npoints);
   ae_vector_set_length(&workbufseed.bestvals, npoints);
   ae_vector_set_length(&workbufseed.classpriors, nclasses);
   ae_vector_set_length(&workbufseed.classtotals0, nclasses);
   ae_vector_set_length(&workbufseed.classtotals1, nclasses);
   ae_vector_set_length(&workbufseed.classtotals01, 2 * nclasses);
   ae_vector_set_length(&workbufseed.treebuf, maxtreesize);
   workbufseed.trnsize = trnsize;
   workbufseed.oobsize = npoints - trnsize;
   ae_vector_set_length(&votebufseed.trntotals, npoints * nclasses);
   ae_vector_set_length(&votebufseed.oobtotals, npoints * nclasses);
   for (i = 0; i < npoints * nclasses; i++) {
      votebufseed.trntotals.xR[i] = 0.0;
      votebufseed.oobtotals.xR[i] = 0.0;
   }
   ae_vector_set_length(&votebufseed.trncounts, npoints);
   ae_vector_set_length(&votebufseed.oobcounts, npoints);
   for (i = 0; i < npoints; i++) {
      votebufseed.trncounts.xZ[i] = 0;
      votebufseed.oobcounts.xZ[i] = 0;
   }
   ae_vector_set_length(&votebufseed.giniimportances, nvars);
   for (i = 0; i < nvars; i++) {
      votebufseed.giniimportances.xR[i] = 0.0;
   }
   treebufseed.treeidx = -1;
   ae_shared_pool_set_seed(&s->workpool, &workbufseed, sizeof(workbufseed), dfworkbuf_init, dfworkbuf_copy, dfworkbuf_free);
   ae_shared_pool_set_seed(&s->votepool, &votebufseed, sizeof(votebufseed), dfvotebuf_init, dfvotebuf_copy, dfvotebuf_free);
   ae_shared_pool_set_seed(&s->treepool, &treebufseed, sizeof(treebufseed), dftreebuf_init, dftreebuf_copy, dftreebuf_free);
   ae_shared_pool_set_seed(&s->treefactory, &treebufseed, sizeof(treebufseed), dftreebuf_init, dftreebuf_copy, dftreebuf_free);
// Select session seed (individual trees are constructed using
// combination of session and local seeds).
   sessionseed = s->rdfglobalseed;
   if (s->rdfglobalseed <= 0) {
      sessionseed = ae_randominteger(30000);
   }
// Prepare In-and-Out-of-Bag matrix, if needed
   s->neediobmatrix = s->rdfimportance == dforest_needpermutation;
   if (s->neediobmatrix) {
   // Prepare default state of In-and-Out-of-Bag matrix
      matrixsetlengthatleast(&s->iobmatrix, ntrees, npoints);
      for (i = 0; i < ntrees; i++) {
         for (j = 0; j < npoints; j++) {
            s->iobmatrix.xyB[i][j] = false;
         }
      }
   }
// Build trees (in parallel, if possible)
   dforest_buildrandomtree(s, 0, ntrees);
// Merge trees and output result
   dforest_mergetrees(s, df);
// Process voting results and output training set and OOB errors.
// Finalize tree construction.
   dforest_processvotingresults(s, ntrees, &votebufseed, rep);
   dfcreatebuffer(df, &df->buffer);
// Perform variable importance estimation
   dforest_estimatevariableimportance(s, sessionseed, df, ntrees, rep);
// Update progress counter
   s->rdfprogress = s->rdftotal;
   ae_frame_leave();
}

// This function returns exact number of bytes required to  store  compressed
// unsigned integer number (negative  arguments  result  in  assertion  being
// generated).
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
static ae_int_t dforest_computecompresseduintsize(ae_int_t v) {
   ae_int_t result;
   ae_assert(v >= 0, "Assertion failed");
   result = 1;
   while (v >= 128) {
      v /= 128;
      result++;
   }
   return result;
}

// This function returns exact number of bytes required to  store  compressed
// version of the tree starting at location TreeBase.
//
// PARAMETERS:
//     DF              -   decision forest
//     UseMantissa8    -   whether 8-bit or 16-bit mantissas are used to store
//                         floating point numbers
//     TreeRoot        -   root of the specific tree being stored (offset in DF.Trees)
//     TreePos         -   position within tree (first location in the tree
//                         is TreeRoot+1)
//     CompressedSizes -   not referenced if SaveCompressedSizes is False;
//                         otherwise, values computed by this function for
//                         specific values of TreePos are stored to
//                         CompressedSizes[TreePos-TreeRoot] (other elements
//                         of the array are not referenced).
//                         This array must be preallocated by caller.
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
static ae_int_t dforest_computecompressedsizerec(decisionforest *df, bool usemantissa8, ae_int_t treeroot, ae_int_t treepos, ZVector *compressedsizes, bool savecompressedsizes) {
   ae_int_t jmponbranch;
   ae_int_t child0size;
   ae_int_t child1size;
   ae_int_t fpwidth;
   ae_int_t result;
   if (usemantissa8) {
      fpwidth = 2;
   } else {
      fpwidth = 3;
   }
// Leaf or split?
   if (df->trees.xR[treepos] == -1.0) {
   // Leaf
      result = dforest_computecompresseduintsize(2 * df->nvars);
      if (df->nclasses == 1) {
         result += fpwidth;
      } else {
         result += dforest_computecompresseduintsize(RoundZ(df->trees.xR[treepos + 1]));
      }
   } else {
   // Split
      jmponbranch = RoundZ(df->trees.xR[treepos + 2]);
      child0size = dforest_computecompressedsizerec(df, usemantissa8, treeroot, treepos + dforest_innernodewidth, compressedsizes, savecompressedsizes);
      child1size = dforest_computecompressedsizerec(df, usemantissa8, treeroot, treeroot + jmponbranch, compressedsizes, savecompressedsizes);
      if (child0size <= child1size) {
      // Child #0 comes first because it is shorter
         result = dforest_computecompresseduintsize(RoundZ(df->trees.xR[treepos]));
         result += fpwidth;
         result += dforest_computecompresseduintsize(child0size);
      } else {
      // Child #1 comes first because it is shorter
         result = dforest_computecompresseduintsize(RoundZ(df->trees.xR[treepos]) + df->nvars);
         result += fpwidth;
         result += dforest_computecompresseduintsize(child1size);
      }
      result += child0size + child1size;
   }
// Do we have to save compressed sizes?
   if (savecompressedsizes) {
      ae_assert(treepos - treeroot < compressedsizes->cnt, "ComputeCompressedSizeRec: integrity check failed");
      compressedsizes->xZ[treepos - treeroot] = result;
   }
   return result;
}

// This function stores compressed unsigned integer number (negative arguments
// result in assertion being generated) to byte array at  location  Offs  and
// increments Offs by number of bytes being stored.
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
static void dforest_streamuint(ae_vector *buf, ae_int_t *offs, ae_int_t v) {
   ae_int_t v0;
   ae_assert(v >= 0, "Assertion failed");
   while (true) {
   // Save 7 least significant bits of V, use 8th bit as a flag which
   // tells us whether subsequent 7-bit packages will be sent.
      v0 = v % 128;
      if (v >= 128) {
         v0 += 128;
      }
      buf->xU[*(offs)] = (unsigned char)(v0);
      ++*offs;
      v /= 128;
      if (v == 0) {
         break;
      }
   }
}

// This function stores compressed floating point number  to  byte  array  at
// location  Offs and increments Offs by number of bytes being stored.
//
// Either 8-bit mantissa or 16-bit mantissa is used. The exponent  is  always
// 7 bits of exponent + sign. Values which do not fit into exponent range are
// truncated to fit.
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
static void dforest_streamfloat(ae_vector *buf, bool usemantissa8, ae_int_t *offs, double v) {
   ae_int_t signbit;
   ae_int_t e;
   ae_int_t m;
   double twopow30;
   double twopowm30;
   double twopow10;
   double twopowm10;
   ae_assert(isfinite(v), "StreamFloat: V is not finite number");
// Special case: zero
   if (v == 0.0) {
      if (usemantissa8) {
         buf->xU[*offs] = (unsigned char)(0);
         buf->xU[*offs + 1] = (unsigned char)(0);
         *offs += 2;
      } else {
         buf->xU[*offs] = (unsigned char)(0);
         buf->xU[*offs + 1] = (unsigned char)(0);
         buf->xU[*offs + 2] = (unsigned char)(0);
         *offs += 3;
      }
      return;
   }
// Handle sign
   signbit = 0;
   if (v < 0.0) {
      v = -v;
      signbit = 128;
   }
// Compute exponent
   twopow30 = 1073741824.0;
   twopow10 = 1024.0;
   twopowm30 = 1.0 / twopow30;
   twopowm10 = 1.0 / twopow10;
   e = 0;
   while (v >= twopow30) {
      v *= twopowm30;
      e += 30;
   }
   while (v >= twopow10) {
      v *= twopowm10;
      e += 10;
   }
   while (v >= 1.0) {
      v *= 0.5;
      e++;
   }
   while (v < twopowm30) {
      v *= twopow30;
      e -= 30;
   }
   while (v < twopowm10) {
      v *= twopow10;
      e -= 10;
   }
   while (v < 0.5) {
      v *= 2;
      e--;
   }
   ae_assert(v >= 0.5 && v < 1.0, "StreamFloat: integrity check failed");
// Handle exponent underflow/overflow
   if (e < -63) {
      signbit = 0;
      e = 0;
      v = 0.0;
   }
   if (e > 63) {
      e = 63;
      v = 1.0;
   }
// Save to stream
   if (usemantissa8) {
      m = RoundZ(v * 256);
      if (m == 256) {
         m /= 2;
         e = imin2(e + 1, 63);
      }
      buf->xU[*offs] = (unsigned char)(e + 64 + signbit);
      buf->xU[*offs + 1] = (unsigned char)(m);
      *offs += 2;
   } else {
      m = RoundZ(v * 65536);
      if (m == 65536) {
         m /= 2;
         e = imin2(e + 1, 63);
      }
      buf->xU[*offs] = (unsigned char)(e + 64 + signbit);
      buf->xU[*offs + 1] = (unsigned char)(m % 256);
      buf->xU[*offs + 2] = (unsigned char)(m / 256);
      *offs += 3;
   }
}

// This function returns exact number of bytes required to  store  compressed
// version of the tree starting at location TreeBase.
//
// PARAMETERS:
//     DF              -   decision forest
//     UseMantissa8    -   whether 8-bit or 16-bit mantissas are used to store
//                         floating point numbers
//     TreeRoot        -   root of the specific tree being stored (offset in DF.Trees)
//     TreePos         -   position within tree (first location in the tree
//                         is TreeRoot+1)
//     CompressedSizes -   not referenced if SaveCompressedSizes is False;
//                         otherwise, values computed by this function for
//                         specific values of TreePos are stored to
//                         CompressedSizes[TreePos-TreeRoot] (other elements
//                         of the array are not referenced).
//                         This array must be preallocated by caller.
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
static void dforest_compressrec(decisionforest *df, bool usemantissa8, ae_int_t treeroot, ae_int_t treepos, ZVector *compressedsizes, ae_vector *buf, ae_int_t *dstoffs) {
   ae_int_t jmponbranch;
   ae_int_t child0size;
   ae_int_t child1size;
   ae_int_t varidx;
   double leafval;
   double splitval;
   ae_int_t dstoffsold;
   dstoffsold = *dstoffs;
// Leaf or split?
   varidx = RoundZ(df->trees.xR[treepos]);
   if (varidx == -1) {
   // Leaf node:
   // * stream special value which denotes leaf (2*NVars)
   // * then, stream scalar value (floating point) or class number (unsigned integer)
      leafval = df->trees.xR[treepos + 1];
      dforest_streamuint(buf, dstoffs, 2 * df->nvars);
      if (df->nclasses == 1) {
         dforest_streamfloat(buf, usemantissa8, dstoffs, leafval);
      } else {
         dforest_streamuint(buf, dstoffs, RoundZ(leafval));
      }
   } else {
   // Split node:
   // * fetch compressed sizes of child nodes, decide which child goes first
      jmponbranch = RoundZ(df->trees.xR[treepos + 2]);
      splitval = df->trees.xR[treepos + 1];
      child0size = compressedsizes->xZ[treepos + dforest_innernodewidth - treeroot];
      child1size = compressedsizes->xZ[treeroot + jmponbranch - treeroot];
      if (child0size <= child1size) {
      // Child #0 comes first because it is shorter:
      // * stream variable index used for splitting;
      //   value in [0,NVars) range indicates that split is
      //   "if VAR<VAL then BRANCH0 else BRANCH1"
      // * stream value used for splitting
      // * stream children #0 and #1
         dforest_streamuint(buf, dstoffs, varidx);
         dforest_streamfloat(buf, usemantissa8, dstoffs, splitval);
         dforest_streamuint(buf, dstoffs, child0size);
         dforest_compressrec(df, usemantissa8, treeroot, treepos + dforest_innernodewidth, compressedsizes, buf, dstoffs);
         dforest_compressrec(df, usemantissa8, treeroot, treeroot + jmponbranch, compressedsizes, buf, dstoffs);
      } else {
      // Child #1 comes first because it is shorter:
      // * stream variable index used for splitting + NVars;
      //   value in [NVars,2*NVars) range indicates that split is
      //   "if VAR >= VAL then BRANCH0 else BRANCH1"
      // * stream value used for splitting
      // * stream children #0 and #1
         dforest_streamuint(buf, dstoffs, varidx + df->nvars);
         dforest_streamfloat(buf, usemantissa8, dstoffs, splitval);
         dforest_streamuint(buf, dstoffs, child1size);
         dforest_compressrec(df, usemantissa8, treeroot, treeroot + jmponbranch, compressedsizes, buf, dstoffs);
         dforest_compressrec(df, usemantissa8, treeroot, treepos + dforest_innernodewidth, compressedsizes, buf, dstoffs);
      }
   }
// Integrity check at the end
   ae_assert(*dstoffs - dstoffsold == compressedsizes->xZ[treepos - treeroot], "CompressRec: integrity check failed (compressed size at leaf)");
}

// This function performs binary compression of decision forest, using either
// 8-bit mantissa (a bit more compact representation) or 16-bit mantissa  for
// splits and regression outputs.
//
// Forest is compressed in-place.
//
// Return value is a compression factor.
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
static double dforest_binarycompression(decisionforest *df, bool usemantissa8) {
   ae_frame _frame_block;
   ae_int_t size8;
   ae_int_t size8i;
   ae_int_t offssrc;
   ae_int_t offsdst;
   ae_int_t i;
   ae_int_t maxrawtreesize;
   double result;
   ae_frame_make(&_frame_block);
   NewVector(dummyi, 0, DT_INT);
   NewVector(compressedsizes, 0, DT_INT);
// Quick exit if already compressed
   if (df->forestformat == dforest_dfcompressedv0) {
      result = 1.0;
      ae_frame_leave();
      return result;
   }
// Check that source format is supported
   ae_assert(df->forestformat == dforest_dfuncompressedv0, "BinaryCompression: unexpected forest format");
// Compute sizes of uncompressed and compressed trees.
   size8 = 0;
   offssrc = 0;
   maxrawtreesize = 0;
   for (i = 0; i < df->ntrees; i++) {
      size8i = dforest_computecompressedsizerec(df, usemantissa8, offssrc, offssrc + 1, &dummyi, false);
      size8 += dforest_computecompresseduintsize(size8i) + size8i;
      maxrawtreesize = imax2(maxrawtreesize, RoundZ(df->trees.xR[offssrc]));
      offssrc += RoundZ(df->trees.xR[offssrc]);
   }
   result = (double)(8 * df->trees.cnt) / (double)(size8 + 1);
// Allocate memory and perform compression
   ae_vector_set_length(&(df->trees8), size8);
   ae_vector_set_length(&compressedsizes, maxrawtreesize);
   offssrc = 0;
   offsdst = 0;
   for (i = 0; i < df->ntrees; i++) {
   // Call compressed size evaluator one more time, now saving subtree sizes into temporary array
      size8i = dforest_computecompressedsizerec(df, usemantissa8, offssrc, offssrc + 1, &compressedsizes, true);
   // Output tree header (length in bytes)
      dforest_streamuint(&df->trees8, &offsdst, size8i);
   // Compress recursively
      dforest_compressrec(df, usemantissa8, offssrc, offssrc + 1, &compressedsizes, &df->trees8, &offsdst);
   // Next tree
      offssrc += RoundZ(df->trees.xR[offssrc]);
   }
   ae_assert(offsdst == size8, "BinaryCompression: integrity check failed (stream length)");
// Finalize forest conversion, clear previously allocated memory
   df->forestformat = dforest_dfcompressedv0;
   df->usemantissa8 = usemantissa8;
   ae_vector_set_length(&df->trees, 0);
   ae_frame_leave();
   return result;
}

// This function performs binary compression of the decision forest.
//
// Original decision forest produced by the  forest  builder  is stored using
// 64-bit representation for all numbers - offsets, variable  indexes,  split
// points.
//
// It is possible to significantly reduce model size by means of:
// * using compressed  dynamic encoding for integers  (offsets  and  variable
//   indexes), which uses just 1 byte to store small ints  (less  than  128),
//   just 2 bytes for larger values (less than 128^2) and so on
// * storing floating point numbers using 8-bit exponent and 16-bit mantissa
//
// As  result,  model  needs  significantly  less  memory (compression factor
// depends on  variable and class counts). In particular:
// * NVars < 128   and NClasses < 128 result in 4.4x-5.7x model size reduction
// * NVars < 16384 and NClasses < 128 result in 3.7x-4.5x model size reduction
//
// Such storage format performs lossless compression  of  all  integers,  but
// compression of floating point values (split values) is lossy, with roughly
// 0.01% relative error introduced during rounding. Thus, we recommend you to
// re-evaluate model accuracy after compression.
//
// Another downside  of  compression  is  ~1.5x reduction  in  the  inference
// speed due to necessity of dynamic decompression of the compressed model.
//
// Inputs:
//     DF      -   decision forest built by forest builder
//
// Outputs:
//     DF      -   replaced by compressed forest
//
// Result:
//     compression factor (in-RAM size of the compressed model vs than of the
//     uncompressed one), positive number larger than 1.0
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
// API: double dfbinarycompression(const decisionforest &df);
double dfbinarycompression(decisionforest *df) {
   double result;
   result = dforest_binarycompression(df, false);
   return result;
}

// This is a 8-bit version of dfbinarycompression.
// Not recommended for external use because it is too lossy.
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
double dfbinarycompression8(decisionforest *df) {
   double result;
   result = dforest_binarycompression(df, true);
   return result;
}

// Fast pow
// ALGLIB: Copyright 24.08.2009 by Sergey Bochkanov
static double dforest_xfastpow(double r, ae_int_t n) {
   double result;
   result = 0.0;
   if (n > 0) {
      if (n % 2 == 0) {
         result = dforest_xfastpow(r, n / 2);
         result *= result;
      } else {
         result = r * dforest_xfastpow(r, n - 1);
      }
      return result;
   }
   if (n == 0) {
      result = 1.0;
   }
   if (n < 0) {
      result = dforest_xfastpow(1 / r, -n);
   }
   return result;
}

// This function reads compressed floating point number from the byte array
// starting from location Offs and increments Offs by number of bytes being
// read.
//
// Either 8-bit mantissa or 16-bit mantissa is used. The exponent  is  always
// 7 bits of exponent + sign. Values which do not fit into exponent range are
// truncated to fit.
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
static double dforest_unstreamfloat(ae_vector *buf, bool usemantissa8, ae_int_t *offs) {
   ae_int_t e;
   double v;
   double inv256;
   double result;
// Read from stream
   inv256 = 1.0 / 256.0;
   if (usemantissa8) {
      e = buf->xU[*offs];
      v = buf->xU[*offs + 1] * inv256;
      *offs += 2;
   } else {
      e = buf->xU[*offs];
      v = (buf->xU[*offs + 1] * inv256 + buf->xU[*offs + 2]) * inv256;
      *offs += 3;
   }
// Decode
   if (e > 128) {
      v = -v;
      e -= 128;
   }
   e -= 64;
   result = dforest_xfastpow(2.0, e) * v;
   return result;
}

// This function reads compressed unsigned integer number from byte array
// starting at location Offs and increments Offs by number of bytes being
// read.
// ALGLIB: Copyright 22.07.2019 by Sergey Bochkanov
static ae_int_t dforest_unstreamuint(ae_vector *buf, ae_int_t *offs) {
   ae_int_t v0;
   ae_int_t p;
   ae_int_t result;
   result = 0;
   p = 1;
   while (true) {
   // Rad 7 bits of V, use 8th bit as a flag which tells us whether
   // subsequent 7-bit packages will be received.
      v0 = buf->xU[*(offs)];
      ++*offs;
      result += v0 % 128 * p;
      if (v0 < 128) {
         break;
      }
      p *= 128;
   }
   return result;
}

// Internal subroutine for processing one decision tree stored in compressed
// format starting at Offs (this index points to the first node of the tree,
// right past the header field).
static void dforest_dfprocessinternalcompressed(decisionforest *df, ae_int_t offs, RVector *x, RVector *y) {
   ae_int_t leafindicator;
   ae_int_t varidx;
   double splitval;
   ae_int_t jmplen;
   double leafval;
   ae_int_t leafcls;
   ae_assert(df->forestformat == dforest_dfcompressedv0, "DFProcessInternal: unexpected forest format");
// Navigate through the tree
   leafindicator = 2 * df->nvars;
   while (true) {
   // Read variable idx
      varidx = dforest_unstreamuint(&df->trees8, &offs);
   // Is it leaf?
      if (varidx == leafindicator) {
         if (df->nclasses == 1) {
         // Regression forest
            leafval = dforest_unstreamfloat(&df->trees8, df->usemantissa8, &offs);
            y->xR[0] += leafval;
         } else {
         // Classification forest
            leafcls = dforest_unstreamuint(&df->trees8, &offs);
            y->xR[leafcls]++;
         }
         break;
      }
   // Process node
      splitval = dforest_unstreamfloat(&df->trees8, df->usemantissa8, &offs);
      jmplen = dforest_unstreamuint(&df->trees8, &offs);
      if (varidx < df->nvars) {
      // The split rule is "if VAR<VAL then BRANCH0 else BRANCH1"
         if (x->xR[varidx] >= splitval) {
            offs += jmplen;
         }
      } else {
      // The split rule is "if VAR >= VAL then BRANCH0 else BRANCH1"
         varidx -= df->nvars;
         if (x->xR[varidx] < splitval) {
            offs += jmplen;
         }
      }
   }
}

// Inference using decision forest
//
// IMPORTANT: this  function  is  thread-unsafe  and  may   modify   internal
//            structures of the model! You can not use same model  object for
//            parallel evaluation from several threads.
//
//            Use dftsprocess()  with  independent  thread-local  buffers  if
//            you need thread-safe evaluation.
//
// Inputs:
//     DF      -   decision forest model
//     X       -   input vector,  array[NVars]
//     Y       -   possibly preallocated buffer, reallocated if too small
//
// Outputs:
//     Y       -   result. Regression estimate when solving regression  task,
//                 vector of posterior probabilities for classification task.
//
// See also DFProcessI.
// ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
// API: void dfprocess(const decisionforest &df, const real_1d_array &x, real_1d_array &y);
void dfprocess(decisionforest *df, RVector *x, RVector *y) {
   ae_int_t offs;
   ae_int_t i;
   double v;
   ae_int_t treesize;
   bool processed;
// Process
//
// Although comments above warn you about thread-unsafety of this
// function, it is de facto thread-safe. However, thread safety is
// an accidental side-effect of the specific inference algorithm
// being used. It may disappear in the future versions of the DF
// models, so you should NOT rely on it.
   if (y->cnt < df->nclasses) {
      ae_vector_set_length(y, df->nclasses);
   }
   for (i = 0; i < df->nclasses; i++) {
      y->xR[i] = 0.0;
   }
   processed = false;
   if (df->forestformat == dforest_dfuncompressedv0) {
   // Process trees stored in uncompressed format
      offs = 0;
      for (i = 0; i < df->ntrees; i++) {
         dforest_dfprocessinternaluncompressed(df, offs, offs + 1, x, y);
         offs += RoundZ(df->trees.xR[offs]);
      }
      processed = true;
   }
   if (df->forestformat == dforest_dfcompressedv0) {
   // Process trees stored in compressed format
      offs = 0;
      for (i = 0; i < df->ntrees; i++) {
         treesize = dforest_unstreamuint(&df->trees8, &offs);
         dforest_dfprocessinternalcompressed(df, offs, x, y);
         offs += treesize;
      }
      processed = true;
   }
   ae_assert(processed, "DFProcess: integrity check failed (unexpected format?)");
   v = 1.0 / (double)df->ntrees;
   ae_v_muld(y->xR, 1, df->nclasses, v);
}

// 'interactive' variant of DFProcess for languages like Python which support
// constructs like "Y = DFProcessI(DF,X)" and interactive mode of interpreter
//
// This function allocates new array on each call,  so  it  is  significantly
// slower than its 'non-interactive' counterpart, but it is  more  convenient
// when you call it from command line.
//
// IMPORTANT: this  function  is  thread-unsafe  and  may   modify   internal
//            structures of the model! You can not use same model  object for
//            parallel evaluation from several threads.
//
//            Use dftsprocess()  with  independent  thread-local  buffers  if
//            you need thread-safe evaluation.
// ALGLIB: Copyright 28.02.2010 by Sergey Bochkanov
// API: void dfprocessi(const decisionforest &df, const real_1d_array &x, real_1d_array &y);
void dfprocessi(decisionforest *df, RVector *x, RVector *y) {
   SetVector(y);
   dfprocess(df, x, y);
}

// This function returns first component of the  inferred  vector  (i.e.  one
// with index #0).
//
// It is a convenience wrapper for dfprocess() intended for either:
// * 1-dimensional regression problems
// * 2-class classification problems
//
// In the former case this function returns inference result as scalar, which
// is definitely more convenient that wrapping it as vector.  In  the  latter
// case it returns probability of object belonging to class #0.
//
// If you call it for anything different from two cases above, it  will  work
// as defined, i.e. return y[0], although it is of less use in such cases.
//
// IMPORTANT: this function is thread-unsafe and modifies internal structures
//            of the model! You can not use same model  object  for  parallel
//            evaluation from several threads.
//
//            Use dftsprocess() with  independent  thread-local  buffers,  if
//            you need thread-safe evaluation.
//
// Inputs:
//     Model   -   DF model
//     X       -   input vector,  array[0..NVars-1].
//
// Result:
//     Y[0]
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: double dfprocess0(const decisionforest &model, const real_1d_array &x);
double dfprocess0(decisionforest *model, RVector *x) {
   ae_int_t i;
   ae_int_t nvars;
   double result;
   nvars = model->nvars;
   for (i = 0; i < nvars; i++) {
      model->buffer.x.xR[i] = x->xR[i];
   }
   dfprocess(model, &model->buffer.x, &model->buffer.y);
   result = model->buffer.y.xR[0];
   return result;
}

// This function returns most probable class number for an  input  X.  It  is
// same as calling  dfprocess(model,x,y), then determining i=argmax(y[i]) and
// returning i.
//
// A class number in [0,NOut) range in returned for classification  problems,
// -1 is returned when this function is called for regression problems.
//
// IMPORTANT: this function is thread-unsafe and modifies internal structures
//            of the model! You can not use same model  object  for  parallel
//            evaluation from several threads.
//
//            Use dftsprocess()  with independent  thread-local  buffers,  if
//            you need thread-safe evaluation.
//
// Inputs:
//     Model   -   decision forest model
//     X       -   input vector,  array[0..NVars-1].
//
// Result:
//     class number, -1 for regression tasks
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: ae_int_t dfclassify(const decisionforest &model, const real_1d_array &x);
ae_int_t dfclassify(decisionforest *model, RVector *x) {
   ae_int_t i;
   ae_int_t nvars;
   ae_int_t nout;
   ae_int_t result;
   if (model->nclasses < 2) {
      result = -1;
      return result;
   }
   nvars = model->nvars;
   nout = model->nclasses;
   for (i = 0; i < nvars; i++) {
      model->buffer.x.xR[i] = x->xR[i];
   }
   dfprocess(model, &model->buffer.x, &model->buffer.y);
   result = 0;
   for (i = 1; i < nout; i++) {
      if (model->buffer.y.xR[i] > model->buffer.y.xR[result]) {
         result = i;
      }
   }
   return result;
}

// Inference using decision forest
//
// Thread-safe procesing using external buffer for temporaries.
//
// This function is thread-safe (i.e.  you  can  use  same  DF   model  from
// multiple threads) as long as you use different buffer objects for different
// threads.
//
// Inputs:
//     DF      -   decision forest model
//     Buf     -   buffer object, must be  allocated  specifically  for  this
//                 model with dfcreatebuffer().
//     X       -   input vector,  array[NVars]
//     Y       -   possibly preallocated buffer, reallocated if too small
//
// Outputs:
//     Y       -   result. Regression estimate when solving regression  task,
//                 vector of posterior probabilities for classification task.
//
// See also DFProcessI.
// ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
// API: void dftsprocess(const decisionforest &df, const decisionforestbuffer &buf, const real_1d_array &x, real_1d_array &y);
void dftsprocess(decisionforest *df, decisionforestbuffer *buf, RVector *x, RVector *y) {
// Although docs warn you about thread-unsafety of the dfprocess()
// function, it is de facto thread-safe. However, thread safety is
// an accidental side-effect of the specific inference algorithm
// being used. It may disappear in the future versions of the DF
// models, so you should NOT rely on it.
   dfprocess(df, x, y);
}

// Classification error
static ae_int_t dforest_dfclserror(decisionforest *df, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t tmpi;
   ae_int_t result;
   ae_frame_make(&_frame_block);
   NewVector(x, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   if (df->nclasses <= 1) {
      result = 0;
      ae_frame_leave();
      return result;
   }
   ae_vector_set_length(&x, df->nvars);
   ae_vector_set_length(&y, df->nclasses);
   result = 0;
   for (i = 0; i < npoints; i++) {
      ae_v_move(x.xR, 1, xy->xyR[i], 1, df->nvars);
      dfprocess(df, &x, &y);
      k = RoundZ(xy->xyR[i][df->nvars]);
      tmpi = 0;
      for (j = 1; j < df->nclasses; j++) {
         if (y.xR[j] > y.xR[tmpi]) {
            tmpi = j;
         }
      }
      if (tmpi != k) {
         result++;
      }
   }
   ae_frame_leave();
   return result;
}

// Relative classification error on the test set
//
// Inputs:
//     DF      -   decision forest model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     percent of incorrectly classified cases.
//     Zero if model solves regression task.
// ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
// API: double dfrelclserror(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints);
double dfrelclserror(decisionforest *df, RMatrix *xy, ae_int_t npoints) {
   double result;
   result = (double)dforest_dfclserror(df, xy, npoints) / (double)npoints;
   return result;
}

// Average cross-entropy (in bits per element) on the test set
//
// Inputs:
//     DF      -   decision forest model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     CrossEntropy/(NPoints*LN(2)).
//     Zero if model solves regression task.
// ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
// API: double dfavgce(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints);
double dfavgce(decisionforest *df, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t tmpi;
   double result;
   ae_frame_make(&_frame_block);
   NewVector(x, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   ae_vector_set_length(&x, df->nvars);
   ae_vector_set_length(&y, df->nclasses);
   result = 0.0;
   for (i = 0; i < npoints; i++) {
      ae_v_move(x.xR, 1, xy->xyR[i], 1, df->nvars);
      dfprocess(df, &x, &y);
      if (df->nclasses > 1) {
      // classification-specific code
         k = RoundZ(xy->xyR[i][df->nvars]);
         tmpi = 0;
         for (j = 1; j < df->nclasses; j++) {
            if (y.xR[j] > y.xR[tmpi]) {
               tmpi = j;
            }
         }
         if (y.xR[k] != 0.0) {
            result -= log(y.xR[k]);
         } else {
            result -= log(ae_minrealnumber);
         }
      }
   }
   result /= npoints;
   ae_frame_leave();
   return result;
}

// RMS error on the test set
//
// Inputs:
//     DF      -   decision forest model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     root mean square error.
//     Its meaning for regression task is obvious. As for
//     classification task, RMS error means error when estimating posterior
//     probabilities.
// ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
// API: double dfrmserror(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints);
double dfrmserror(decisionforest *df, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t tmpi;
   double result;
   ae_frame_make(&_frame_block);
   NewVector(x, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   ae_vector_set_length(&x, df->nvars);
   ae_vector_set_length(&y, df->nclasses);
   result = 0.0;
   for (i = 0; i < npoints; i++) {
      ae_v_move(x.xR, 1, xy->xyR[i], 1, df->nvars);
      dfprocess(df, &x, &y);
      if (df->nclasses > 1) {
      // classification-specific code
         k = RoundZ(xy->xyR[i][df->nvars]);
         tmpi = 0;
         for (j = 1; j < df->nclasses; j++) {
            if (y.xR[j] > y.xR[tmpi]) {
               tmpi = j;
            }
         }
         for (j = 0; j < df->nclasses; j++) {
            if (j == k) {
               result += ae_sqr(y.xR[j] - 1);
            } else {
               result += ae_sqr(y.xR[j]);
            }
         }
      } else {
      // regression-specific code
         result += ae_sqr(y.xR[0] - xy->xyR[i][df->nvars]);
      }
   }
   result = sqrt(result / (npoints * df->nclasses));
   ae_frame_leave();
   return result;
}

// Average error on the test set
//
// Inputs:
//     DF      -   decision forest model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     Its meaning for regression task is obvious. As for
//     classification task, it means average error when estimating posterior
//     probabilities.
// ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
// API: double dfavgerror(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints);
double dfavgerror(decisionforest *df, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   double result;
   ae_frame_make(&_frame_block);
   NewVector(x, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   ae_vector_set_length(&x, df->nvars);
   ae_vector_set_length(&y, df->nclasses);
   result = 0.0;
   for (i = 0; i < npoints; i++) {
      ae_v_move(x.xR, 1, xy->xyR[i], 1, df->nvars);
      dfprocess(df, &x, &y);
      if (df->nclasses > 1) {
      // classification-specific code
         k = RoundZ(xy->xyR[i][df->nvars]);
         for (j = 0; j < df->nclasses; j++) {
            if (j == k) {
               result += fabs(y.xR[j] - 1);
            } else {
               result += fabs(y.xR[j]);
            }
         }
      } else {
      // regression-specific code
         result += fabs(y.xR[0] - xy->xyR[i][df->nvars]);
      }
   }
   result /= npoints * df->nclasses;
   ae_frame_leave();
   return result;
}

// Average relative error on the test set
//
// Inputs:
//     DF      -   decision forest model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     Its meaning for regression task is obvious. As for
//     classification task, it means average relative error when estimating
//     posterior probability of belonging to the correct class.
// ALGLIB: Copyright 16.02.2009 by Sergey Bochkanov
// API: double dfavgrelerror(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints);
double dfavgrelerror(decisionforest *df, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   ae_int_t relcnt;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   double result;
   ae_frame_make(&_frame_block);
   NewVector(x, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   ae_vector_set_length(&x, df->nvars);
   ae_vector_set_length(&y, df->nclasses);
   result = 0.0;
   relcnt = 0;
   for (i = 0; i < npoints; i++) {
      ae_v_move(x.xR, 1, xy->xyR[i], 1, df->nvars);
      dfprocess(df, &x, &y);
      if (df->nclasses > 1) {
      // classification-specific code
         k = RoundZ(xy->xyR[i][df->nvars]);
         for (j = 0; j < df->nclasses; j++) {
            if (j == k) {
               result += fabs(y.xR[j] - 1);
               relcnt++;
            }
         }
      } else {
      // regression-specific code
         if (xy->xyR[i][df->nvars] != 0.0) {
            result += fabs((y.xR[0] - xy->xyR[i][df->nvars]) / xy->xyR[i][df->nvars]);
            relcnt++;
         }
      }
   }
   if (relcnt > 0) {
      result /= relcnt;
   }
   ae_frame_leave();
   return result;
}

// Copying of DecisionForest structure
//
// Inputs:
//     DF1 -   original
//
// Outputs:
//     DF2 -   copy
// ALGLIB: Copyright 13.02.2009 by Sergey Bochkanov
void dfcopy(decisionforest *df1, decisionforest *df2) {
   ae_int_t i;
   ae_int_t bufsize;
   SetObj(decisionforest, df2);
   if (df1->forestformat == dforest_dfuncompressedv0) {
      df2->forestformat = df1->forestformat;
      df2->nvars = df1->nvars;
      df2->nclasses = df1->nclasses;
      df2->ntrees = df1->ntrees;
      df2->bufsize = df1->bufsize;
      ae_vector_set_length(&df2->trees, df1->bufsize);
      ae_v_move(df2->trees.xR, 1, df1->trees.xR, 1, df1->bufsize);
      dfcreatebuffer(df2, &df2->buffer);
      return;
   }
   if (df1->forestformat == dforest_dfcompressedv0) {
      df2->forestformat = df1->forestformat;
      df2->usemantissa8 = df1->usemantissa8;
      df2->nvars = df1->nvars;
      df2->nclasses = df1->nclasses;
      df2->ntrees = df1->ntrees;
      bufsize = df1->trees8.cnt;
      ae_vector_set_length(&(df2->trees8), bufsize);
      for (i = 0; i < bufsize; i++) {
         df2->trees8.xU[i] = (unsigned char)(df1->trees8.xU[i]);
      }
      dfcreatebuffer(df2, &df2->buffer);
      return;
   }
   ae_assert(false, "DFCopy: unexpected forest format");
}

// Serializer: allocation
// ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
void dfalloc(ae_serializer *s, decisionforest *forest) {
   if (forest->forestformat == dforest_dfuncompressedv0) {
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      allocrealarray(s, &forest->trees, forest->bufsize);
      return;
   }
   if (forest->forestformat == dforest_dfcompressedv0) {
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_entry(s);
      ae_serializer_alloc_byte_array(s, &forest->trees8);
      return;
   }
   ae_assert(false, "dfalloc: unexpected forest format");
}

// Serializer: serialization
// These functions serialize a data structure to a C++ string or stream.
// * serialization can be freely moved across 32-bit and 64-bit systems,
//   and different byte orders. For example, you can serialize a string
//   on a SPARC and unserialize it on an x86.
// * ALGLIB++ serialization is compatible with serialization in ALGLIB,
//   in both directions.
// Important properties of s_out:
// * it contains alphanumeric characters, dots, underscores, minus signs
// * these symbols are grouped into words, which are separated by spaces
//   and Windows-style (CR+LF) newlines
// ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
// API: void dfserialize(decisionforest &obj, std::string &s_out);
// API: void dfserialize(decisionforest &obj, std::ostream &s_out);
void dfserialize(ae_serializer *s, decisionforest *forest) {
   if (forest->forestformat == dforest_dfuncompressedv0) {
      ae_serializer_serialize_int(s, getrdfserializationcode());
      ae_serializer_serialize_int(s, dforest_dfuncompressedv0);
      ae_serializer_serialize_int(s, forest->nvars);
      ae_serializer_serialize_int(s, forest->nclasses);
      ae_serializer_serialize_int(s, forest->ntrees);
      ae_serializer_serialize_int(s, forest->bufsize);
      serializerealarray(s, &forest->trees, forest->bufsize);
      return;
   }
   if (forest->forestformat == dforest_dfcompressedv0) {
      ae_serializer_serialize_int(s, getrdfserializationcode());
      ae_serializer_serialize_int(s, forest->forestformat);
      ae_serializer_serialize_bool(s, forest->usemantissa8);
      ae_serializer_serialize_int(s, forest->nvars);
      ae_serializer_serialize_int(s, forest->nclasses);
      ae_serializer_serialize_int(s, forest->ntrees);
      ae_serializer_serialize_byte_array(s, &forest->trees8);
      return;
   }
   ae_assert(false, "dfserialize: unexpected forest format");
}

// Serializer: unserialization
// These functions unserialize a data structure from a C++ string or stream.
// Important properties of s_in:
// * any combination of spaces, tabs, Windows or Unix stype newlines can
//   be used as separators, so as to allow flexible reformatting of the
//   stream or string from text or XML files.
// * But you should not insert separators into the middle of the "words"
//   nor you should change case of letters.
// ALGLIB: Copyright 14.03.2011 by Sergey Bochkanov
// API: void dfunserialize(const std::string &s_in, decisionforest &obj);
// API: void dfunserialize(const std::istream &s_in, decisionforest &obj);
void dfunserialize(ae_serializer *s, decisionforest *forest) {
   ae_int_t i0;
   ae_int_t forestformat;
   bool processed;
   SetObj(decisionforest, forest);
// check correctness of header
   i0 = ae_serializer_unserialize_int(s);
   ae_assert(i0 == getrdfserializationcode(), "dfunserialize: stream header corrupted");
// Read forest
   forestformat = ae_serializer_unserialize_int(s);
   processed = false;
   if (forestformat == dforest_dfuncompressedv0) {
   // Unserialize data
      forest->forestformat = forestformat;
      forest->nvars = ae_serializer_unserialize_int(s);
      forest->nclasses = ae_serializer_unserialize_int(s);
      forest->ntrees = ae_serializer_unserialize_int(s);
      forest->bufsize = ae_serializer_unserialize_int(s);
      unserializerealarray(s, &forest->trees);
      processed = true;
   }
   if (forestformat == dforest_dfcompressedv0) {
   // Unserialize data
      forest->forestformat = forestformat;
      forest->usemantissa8 = ae_serializer_unserialize_bool(s);
      forest->nvars = ae_serializer_unserialize_int(s);
      forest->nclasses = ae_serializer_unserialize_int(s);
      forest->ntrees = ae_serializer_unserialize_int(s);
      ae_serializer_unserialize_byte_array(s, &forest->trees8);
      processed = true;
   }
   ae_assert(processed, "dfunserialize: unexpected forest format");
// Prepare buffer
   dfcreatebuffer(forest, &forest->buffer);
}

void decisionforestbuilder_init(void *_p, bool make_automatic) {
   decisionforestbuilder *p = (decisionforestbuilder *)_p;
   ae_vector_init(&p->dsdata, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->dsrval, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->dsival, 0, DT_INT, make_automatic);
   ae_vector_init(&p->dsmin, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->dsmax, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->dsbinary, 0, DT_BOOL, make_automatic);
   ae_vector_init(&p->dsctotals, 0, DT_INT, make_automatic);
   ae_shared_pool_init(&p->workpool, make_automatic);
   ae_shared_pool_init(&p->votepool, make_automatic);
   ae_shared_pool_init(&p->treepool, make_automatic);
   ae_shared_pool_init(&p->treefactory, make_automatic);
   ae_matrix_init(&p->iobmatrix, 0, 0, DT_BOOL, make_automatic);
   ae_vector_init(&p->varimpshuffle2, 0, DT_INT, make_automatic);
}

void decisionforestbuilder_copy(void *_dst, void *_src, bool make_automatic) {
   decisionforestbuilder *dst = (decisionforestbuilder *)_dst;
   decisionforestbuilder *src = (decisionforestbuilder *)_src;
   dst->dstype = src->dstype;
   dst->npoints = src->npoints;
   dst->nvars = src->nvars;
   dst->nclasses = src->nclasses;
   ae_vector_copy(&dst->dsdata, &src->dsdata, make_automatic);
   ae_vector_copy(&dst->dsrval, &src->dsrval, make_automatic);
   ae_vector_copy(&dst->dsival, &src->dsival, make_automatic);
   dst->rdfalgo = src->rdfalgo;
   dst->rdfratio = src->rdfratio;
   dst->rdfvars = src->rdfvars;
   dst->rdfglobalseed = src->rdfglobalseed;
   dst->rdfsplitstrength = src->rdfsplitstrength;
   dst->rdfimportance = src->rdfimportance;
   ae_vector_copy(&dst->dsmin, &src->dsmin, make_automatic);
   ae_vector_copy(&dst->dsmax, &src->dsmax, make_automatic);
   ae_vector_copy(&dst->dsbinary, &src->dsbinary, make_automatic);
   dst->dsravg = src->dsravg;
   ae_vector_copy(&dst->dsctotals, &src->dsctotals, make_automatic);
   dst->rdfprogress = src->rdfprogress;
   dst->rdftotal = src->rdftotal;
   ae_shared_pool_copy(&dst->workpool, &src->workpool, make_automatic);
   ae_shared_pool_copy(&dst->votepool, &src->votepool, make_automatic);
   ae_shared_pool_copy(&dst->treepool, &src->treepool, make_automatic);
   ae_shared_pool_copy(&dst->treefactory, &src->treefactory, make_automatic);
   dst->neediobmatrix = src->neediobmatrix;
   ae_matrix_copy(&dst->iobmatrix, &src->iobmatrix, make_automatic);
   ae_vector_copy(&dst->varimpshuffle2, &src->varimpshuffle2, make_automatic);
}

void decisionforestbuilder_free(void *_p, bool make_automatic) {
   decisionforestbuilder *p = (decisionforestbuilder *)_p;
   ae_vector_free(&p->dsdata, make_automatic);
   ae_vector_free(&p->dsrval, make_automatic);
   ae_vector_free(&p->dsival, make_automatic);
   ae_vector_free(&p->dsmin, make_automatic);
   ae_vector_free(&p->dsmax, make_automatic);
   ae_vector_free(&p->dsbinary, make_automatic);
   ae_vector_free(&p->dsctotals, make_automatic);
   ae_shared_pool_free(&p->workpool, make_automatic);
   ae_shared_pool_free(&p->votepool, make_automatic);
   ae_shared_pool_free(&p->treepool, make_automatic);
   ae_shared_pool_free(&p->treefactory, make_automatic);
   ae_matrix_free(&p->iobmatrix, make_automatic);
   ae_vector_free(&p->varimpshuffle2, make_automatic);
}

void dfworkbuf_init(void *_p, bool make_automatic) {
   dfworkbuf *p = (dfworkbuf *)_p;
   ae_vector_init(&p->classpriors, 0, DT_INT, make_automatic);
   ae_vector_init(&p->varpool, 0, DT_INT, make_automatic);
   ae_vector_init(&p->trnset, 0, DT_INT, make_automatic);
   ae_vector_init(&p->trnlabelsr, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->trnlabelsi, 0, DT_INT, make_automatic);
   ae_vector_init(&p->oobset, 0, DT_INT, make_automatic);
   ae_vector_init(&p->ooblabelsr, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->ooblabelsi, 0, DT_INT, make_automatic);
   ae_vector_init(&p->treebuf, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->curvals, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->bestvals, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->tmp0i, 0, DT_INT, make_automatic);
   ae_vector_init(&p->tmp1i, 0, DT_INT, make_automatic);
   ae_vector_init(&p->tmp0r, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->tmp1r, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->tmp2r, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->tmp3r, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->tmpnrms2, 0, DT_INT, make_automatic);
   ae_vector_init(&p->classtotals0, 0, DT_INT, make_automatic);
   ae_vector_init(&p->classtotals1, 0, DT_INT, make_automatic);
   ae_vector_init(&p->classtotals01, 0, DT_INT, make_automatic);
}

void dfworkbuf_copy(void *_dst, void *_src, bool make_automatic) {
   dfworkbuf *dst = (dfworkbuf *)_dst;
   dfworkbuf *src = (dfworkbuf *)_src;
   ae_vector_copy(&dst->classpriors, &src->classpriors, make_automatic);
   ae_vector_copy(&dst->varpool, &src->varpool, make_automatic);
   dst->varpoolsize = src->varpoolsize;
   ae_vector_copy(&dst->trnset, &src->trnset, make_automatic);
   dst->trnsize = src->trnsize;
   ae_vector_copy(&dst->trnlabelsr, &src->trnlabelsr, make_automatic);
   ae_vector_copy(&dst->trnlabelsi, &src->trnlabelsi, make_automatic);
   ae_vector_copy(&dst->oobset, &src->oobset, make_automatic);
   dst->oobsize = src->oobsize;
   ae_vector_copy(&dst->ooblabelsr, &src->ooblabelsr, make_automatic);
   ae_vector_copy(&dst->ooblabelsi, &src->ooblabelsi, make_automatic);
   ae_vector_copy(&dst->treebuf, &src->treebuf, make_automatic);
   ae_vector_copy(&dst->curvals, &src->curvals, make_automatic);
   ae_vector_copy(&dst->bestvals, &src->bestvals, make_automatic);
   ae_vector_copy(&dst->tmp0i, &src->tmp0i, make_automatic);
   ae_vector_copy(&dst->tmp1i, &src->tmp1i, make_automatic);
   ae_vector_copy(&dst->tmp0r, &src->tmp0r, make_automatic);
   ae_vector_copy(&dst->tmp1r, &src->tmp1r, make_automatic);
   ae_vector_copy(&dst->tmp2r, &src->tmp2r, make_automatic);
   ae_vector_copy(&dst->tmp3r, &src->tmp3r, make_automatic);
   ae_vector_copy(&dst->tmpnrms2, &src->tmpnrms2, make_automatic);
   ae_vector_copy(&dst->classtotals0, &src->classtotals0, make_automatic);
   ae_vector_copy(&dst->classtotals1, &src->classtotals1, make_automatic);
   ae_vector_copy(&dst->classtotals01, &src->classtotals01, make_automatic);
}

void dfworkbuf_free(void *_p, bool make_automatic) {
   dfworkbuf *p = (dfworkbuf *)_p;
   ae_vector_free(&p->classpriors, make_automatic);
   ae_vector_free(&p->varpool, make_automatic);
   ae_vector_free(&p->trnset, make_automatic);
   ae_vector_free(&p->trnlabelsr, make_automatic);
   ae_vector_free(&p->trnlabelsi, make_automatic);
   ae_vector_free(&p->oobset, make_automatic);
   ae_vector_free(&p->ooblabelsr, make_automatic);
   ae_vector_free(&p->ooblabelsi, make_automatic);
   ae_vector_free(&p->treebuf, make_automatic);
   ae_vector_free(&p->curvals, make_automatic);
   ae_vector_free(&p->bestvals, make_automatic);
   ae_vector_free(&p->tmp0i, make_automatic);
   ae_vector_free(&p->tmp1i, make_automatic);
   ae_vector_free(&p->tmp0r, make_automatic);
   ae_vector_free(&p->tmp1r, make_automatic);
   ae_vector_free(&p->tmp2r, make_automatic);
   ae_vector_free(&p->tmp3r, make_automatic);
   ae_vector_free(&p->tmpnrms2, make_automatic);
   ae_vector_free(&p->classtotals0, make_automatic);
   ae_vector_free(&p->classtotals1, make_automatic);
   ae_vector_free(&p->classtotals01, make_automatic);
}

void dfvotebuf_init(void *_p, bool make_automatic) {
   dfvotebuf *p = (dfvotebuf *)_p;
   ae_vector_init(&p->trntotals, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->oobtotals, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->trncounts, 0, DT_INT, make_automatic);
   ae_vector_init(&p->oobcounts, 0, DT_INT, make_automatic);
   ae_vector_init(&p->giniimportances, 0, DT_REAL, make_automatic);
}

void dfvotebuf_copy(void *_dst, void *_src, bool make_automatic) {
   dfvotebuf *dst = (dfvotebuf *)_dst;
   dfvotebuf *src = (dfvotebuf *)_src;
   ae_vector_copy(&dst->trntotals, &src->trntotals, make_automatic);
   ae_vector_copy(&dst->oobtotals, &src->oobtotals, make_automatic);
   ae_vector_copy(&dst->trncounts, &src->trncounts, make_automatic);
   ae_vector_copy(&dst->oobcounts, &src->oobcounts, make_automatic);
   ae_vector_copy(&dst->giniimportances, &src->giniimportances, make_automatic);
}

void dfvotebuf_free(void *_p, bool make_automatic) {
   dfvotebuf *p = (dfvotebuf *)_p;
   ae_vector_free(&p->trntotals, make_automatic);
   ae_vector_free(&p->oobtotals, make_automatic);
   ae_vector_free(&p->trncounts, make_automatic);
   ae_vector_free(&p->oobcounts, make_automatic);
   ae_vector_free(&p->giniimportances, make_automatic);
}

void dfpermimpbuf_init(void *_p, bool make_automatic) {
   dfpermimpbuf *p = (dfpermimpbuf *)_p;
   ae_vector_init(&p->losses, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->xraw, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->xdist, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->xcur, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->y, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->yv, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->targety, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->startnodes, 0, DT_INT, make_automatic);
}

void dfpermimpbuf_copy(void *_dst, void *_src, bool make_automatic) {
   dfpermimpbuf *dst = (dfpermimpbuf *)_dst;
   dfpermimpbuf *src = (dfpermimpbuf *)_src;
   ae_vector_copy(&dst->losses, &src->losses, make_automatic);
   ae_vector_copy(&dst->xraw, &src->xraw, make_automatic);
   ae_vector_copy(&dst->xdist, &src->xdist, make_automatic);
   ae_vector_copy(&dst->xcur, &src->xcur, make_automatic);
   ae_vector_copy(&dst->y, &src->y, make_automatic);
   ae_vector_copy(&dst->yv, &src->yv, make_automatic);
   ae_vector_copy(&dst->targety, &src->targety, make_automatic);
   ae_vector_copy(&dst->startnodes, &src->startnodes, make_automatic);
}

void dfpermimpbuf_free(void *_p, bool make_automatic) {
   dfpermimpbuf *p = (dfpermimpbuf *)_p;
   ae_vector_free(&p->losses, make_automatic);
   ae_vector_free(&p->xraw, make_automatic);
   ae_vector_free(&p->xdist, make_automatic);
   ae_vector_free(&p->xcur, make_automatic);
   ae_vector_free(&p->y, make_automatic);
   ae_vector_free(&p->yv, make_automatic);
   ae_vector_free(&p->targety, make_automatic);
   ae_vector_free(&p->startnodes, make_automatic);
}

void dftreebuf_init(void *_p, bool make_automatic) {
   dftreebuf *p = (dftreebuf *)_p;
   ae_vector_init(&p->treebuf, 0, DT_REAL, make_automatic);
}

void dftreebuf_copy(void *_dst, void *_src, bool make_automatic) {
   dftreebuf *dst = (dftreebuf *)_dst;
   dftreebuf *src = (dftreebuf *)_src;
   ae_vector_copy(&dst->treebuf, &src->treebuf, make_automatic);
   dst->treeidx = src->treeidx;
}

void dftreebuf_free(void *_p, bool make_automatic) {
   dftreebuf *p = (dftreebuf *)_p;
   ae_vector_free(&p->treebuf, make_automatic);
}

void decisionforestbuffer_init(void *_p, bool make_automatic) {
   decisionforestbuffer *p = (decisionforestbuffer *)_p;
   ae_vector_init(&p->x, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->y, 0, DT_REAL, make_automatic);
}

void decisionforestbuffer_copy(void *_dst, void *_src, bool make_automatic) {
   decisionforestbuffer *dst = (decisionforestbuffer *)_dst;
   decisionforestbuffer *src = (decisionforestbuffer *)_src;
   ae_vector_copy(&dst->x, &src->x, make_automatic);
   ae_vector_copy(&dst->y, &src->y, make_automatic);
}

void decisionforestbuffer_free(void *_p, bool make_automatic) {
   decisionforestbuffer *p = (decisionforestbuffer *)_p;
   ae_vector_free(&p->x, make_automatic);
   ae_vector_free(&p->y, make_automatic);
}

void decisionforest_init(void *_p, bool make_automatic) {
   decisionforest *p = (decisionforest *)_p;
   ae_vector_init(&p->trees, 0, DT_REAL, make_automatic);
   decisionforestbuffer_init(&p->buffer, make_automatic);
   ae_vector_init(&p->trees8, 0, DT_BYTE, make_automatic);
}

void decisionforest_copy(void *_dst, void *_src, bool make_automatic) {
   decisionforest *dst = (decisionforest *)_dst;
   decisionforest *src = (decisionforest *)_src;
   dst->forestformat = src->forestformat;
   dst->usemantissa8 = src->usemantissa8;
   dst->nvars = src->nvars;
   dst->nclasses = src->nclasses;
   dst->ntrees = src->ntrees;
   dst->bufsize = src->bufsize;
   ae_vector_copy(&dst->trees, &src->trees, make_automatic);
   decisionforestbuffer_copy(&dst->buffer, &src->buffer, make_automatic);
   ae_vector_copy(&dst->trees8, &src->trees8, make_automatic);
}

void decisionforest_free(void *_p, bool make_automatic) {
   decisionforest *p = (decisionforest *)_p;
   ae_vector_free(&p->trees, make_automatic);
   decisionforestbuffer_free(&p->buffer, make_automatic);
   ae_vector_free(&p->trees8, make_automatic);
}

void dfreport_init(void *_p, bool make_automatic) {
   dfreport *p = (dfreport *)_p;
   ae_vector_init(&p->topvars, 0, DT_INT, make_automatic);
   ae_vector_init(&p->varimportances, 0, DT_REAL, make_automatic);
}

void dfreport_copy(void *_dst, void *_src, bool make_automatic) {
   dfreport *dst = (dfreport *)_dst;
   dfreport *src = (dfreport *)_src;
   dst->relclserror = src->relclserror;
   dst->avgce = src->avgce;
   dst->rmserror = src->rmserror;
   dst->avgerror = src->avgerror;
   dst->avgrelerror = src->avgrelerror;
   dst->oobrelclserror = src->oobrelclserror;
   dst->oobavgce = src->oobavgce;
   dst->oobrmserror = src->oobrmserror;
   dst->oobavgerror = src->oobavgerror;
   dst->oobavgrelerror = src->oobavgrelerror;
   ae_vector_copy(&dst->topvars, &src->topvars, make_automatic);
   ae_vector_copy(&dst->varimportances, &src->varimportances, make_automatic);
}

void dfreport_free(void *_p, bool make_automatic) {
   dfreport *p = (dfreport *)_p;
   ae_vector_free(&p->topvars, make_automatic);
   ae_vector_free(&p->varimportances, make_automatic);
}

void dfinternalbuffers_init(void *_p, bool make_automatic) {
   dfinternalbuffers *p = (dfinternalbuffers *)_p;
   ae_vector_init(&p->treebuf, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->idxbuf, 0, DT_INT, make_automatic);
   ae_vector_init(&p->tmpbufr, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->tmpbufr2, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->tmpbufi, 0, DT_INT, make_automatic);
   ae_vector_init(&p->classibuf, 0, DT_INT, make_automatic);
   ae_vector_init(&p->sortrbuf, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->sortrbuf2, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->sortibuf, 0, DT_INT, make_automatic);
   ae_vector_init(&p->varpool, 0, DT_INT, make_automatic);
   ae_vector_init(&p->evsbin, 0, DT_BOOL, make_automatic);
   ae_vector_init(&p->evssplits, 0, DT_REAL, make_automatic);
}

void dfinternalbuffers_copy(void *_dst, void *_src, bool make_automatic) {
   dfinternalbuffers *dst = (dfinternalbuffers *)_dst;
   dfinternalbuffers *src = (dfinternalbuffers *)_src;
   ae_vector_copy(&dst->treebuf, &src->treebuf, make_automatic);
   ae_vector_copy(&dst->idxbuf, &src->idxbuf, make_automatic);
   ae_vector_copy(&dst->tmpbufr, &src->tmpbufr, make_automatic);
   ae_vector_copy(&dst->tmpbufr2, &src->tmpbufr2, make_automatic);
   ae_vector_copy(&dst->tmpbufi, &src->tmpbufi, make_automatic);
   ae_vector_copy(&dst->classibuf, &src->classibuf, make_automatic);
   ae_vector_copy(&dst->sortrbuf, &src->sortrbuf, make_automatic);
   ae_vector_copy(&dst->sortrbuf2, &src->sortrbuf2, make_automatic);
   ae_vector_copy(&dst->sortibuf, &src->sortibuf, make_automatic);
   ae_vector_copy(&dst->varpool, &src->varpool, make_automatic);
   ae_vector_copy(&dst->evsbin, &src->evsbin, make_automatic);
   ae_vector_copy(&dst->evssplits, &src->evssplits, make_automatic);
}

void dfinternalbuffers_free(void *_p, bool make_automatic) {
   dfinternalbuffers *p = (dfinternalbuffers *)_p;
   ae_vector_free(&p->treebuf, make_automatic);
   ae_vector_free(&p->idxbuf, make_automatic);
   ae_vector_free(&p->tmpbufr, make_automatic);
   ae_vector_free(&p->tmpbufr2, make_automatic);
   ae_vector_free(&p->tmpbufi, make_automatic);
   ae_vector_free(&p->classibuf, make_automatic);
   ae_vector_free(&p->sortrbuf, make_automatic);
   ae_vector_free(&p->sortrbuf2, make_automatic);
   ae_vector_free(&p->sortibuf, make_automatic);
   ae_vector_free(&p->varpool, make_automatic);
   ae_vector_free(&p->evsbin, make_automatic);
   ae_vector_free(&p->evssplits, make_automatic);
}
} // end of namespace alglib_impl

namespace alglib {
// A random forest (decision forest) builder object.
//
// Used to store dataset and specify decision forest training algorithm settings.
DefClass(decisionforestbuilder, EndD)

// Buffer object which is used to perform  various  requests  (usually  model
// inference) in the multithreaded mode (multiple threads working  with  same
// DF object).
//
// This object should be created with DFCreateBuffer().
DefClass(decisionforestbuffer, EndD)

// Decision forest (random forest) model.
DefClass(decisionforest, EndD)

// Decision forest training report.
//
// ==== training/oob errors ====
//
// Following fields store training set errors:
// * relclserror           -   fraction of misclassified cases, [0,1]
// * avgce                 -   average cross-entropy in bits per symbol
// * rmserror              -   root-mean-square error
// * avgerror              -   average error
// * avgrelerror           -   average relative error
//
// Out-of-bag estimates are stored in fields with same names, but "oob" prefix.
//
// For classification problems:
// * RMS, AVG and AVGREL errors are calculated for posterior probabilities
//
// For regression problems:
// * RELCLS and AVGCE errors are zero
//
// ==== variable importance ====
//
// Following fields are used to store variable importance information:
//
// * topvars               -   variables ordered from the most  important  to
//                             less  important  ones  (according  to  current
//                             choice of importance raiting).
//                             For example, topvars[0] contains index of  the
//                             most important variable, and topvars[0:2]  are
//                             indexes of 3 most important ones and so on.
//
// * varimportances        -   array[nvars], ratings (the  larger,  the  more
//                             important the variable  is,  always  in  [0,1]
//                             range).
//                             By default, filled  by  zeros  (no  importance
//                             ratings are  provided  unless  you  explicitly
//                             request them).
//                             Zero rating means that variable is not important,
//                             however you will rarely encounter such a thing,
//                             in many cases  unimportant  variables  produce
//                             nearly-zero (but nonzero) ratings.
//
// Variable importance report must be EXPLICITLY requested by calling:
// * dfbuildersetimportancegini() function, if you need out-of-bag Gini-based
//   importance rating also known as MDI  (fast to  calculate,  resistant  to
//   overfitting  issues,   but   has   some   bias  towards  continuous  and
//   high-cardinality categorical variables)
// * dfbuildersetimportancetrngini() function, if you need training set Gini-
//   -based importance rating (what other packages typically report).
// * dfbuildersetimportancepermutation() function, if you  need  permutation-
//   based importance rating also known as MDA (slower to calculate, but less
//   biased)
// * dfbuildersetimportancenone() function,  if  you  do  not  need  importance
//   ratings - ratings will be zero, topvars[] will be [0,1,2,...]
//
// Different importance ratings (Gini or permutation) produce  non-comparable
// values. Although in all cases rating values lie in [0,1] range, there  are
// exist differences:
// * informally speaking, Gini importance rating tends to divide "unit amount
//   of importance"  between  several  important  variables, i.e. it produces
//   estimates which roughly sum to 1.0 (or less than 1.0, if your  task  can
//   not be solved exactly). If all variables  are  equally  important,  they
//   will have same rating,  roughly  1/NVars,  even  if  every  variable  is
//   critically important.
// * from the other side, permutation importance tells us what percentage  of
//   the model predictive power will be ruined  by  permuting  this  specific
//   variable. It does not produce estimates which  sum  to  one.  Critically
//   important variable will have rating close  to  1.0,  and  you  may  have
//   multiple variables with such a rating.
//
// More information on variable importance ratings can be found  in  comments
// on the dfbuildersetimportancegini() and dfbuildersetimportancepermutation()
// functions.
DefClass(dfreport, AndD DecVal(relclserror) AndD DecVal(avgce) AndD DecVal(rmserror) AndD DecVal(avgerror) AndD DecVal(avgrelerror) AndD DecVal(oobrelclserror) AndD DecVal(oobavgce) AndD DecVal(oobrmserror) AndD DecVal(oobavgerror) AndD DecVal(oobavgrelerror) AndD DecVar(topvars) AndD DecVar(varimportances))

void dfserialize(decisionforest &obj, std::string &s_out) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_alloc_start(&serializer);
   alglib_impl::dfalloc(&serializer, obj.c_ptr());
   ae_int_t ssize = alglib_impl::ae_serializer_get_alloc_size(&serializer);
   s_out.clear();
   s_out.reserve((size_t)(ssize + 1));
   alglib_impl::ae_serializer_sstart_str(&serializer, &s_out);
   alglib_impl::dfserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_assert(s_out.length() <= (size_t)ssize, "dfserialize: serialization integrity error");
   alglib_impl::ae_state_clear();
}
void dfserialize(decisionforest &obj, std::ostream &s_out) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_alloc_start(&serializer);
   alglib_impl::dfalloc(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_get_alloc_size(&serializer); // not actually needed, but we have to ask
   alglib_impl::ae_serializer_sstart_stream(&serializer, &s_out);
   alglib_impl::dfserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}

void dfunserialize(const std::string &s_in, decisionforest &obj) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_ustart_str(&serializer, &s_in);
   alglib_impl::dfunserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}
void dfunserialize(const std::istream &s_in, decisionforest &obj) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_ustart_stream(&serializer, &s_in);
   alglib_impl::dfunserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}

void dfcreatebuffer(const decisionforest &model, decisionforestbuffer &buf) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfcreatebuffer(ConstT(decisionforest, model), ConstT(decisionforestbuffer, buf));
   alglib_impl::ae_state_clear();
}

void dfbuildrandomdecisionforest(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, const ae_int_t ntrees, const double r, ae_int_t &info, decisionforest &df, dfreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildrandomdecisionforest(ConstT(ae_matrix, xy), npoints, nvars, nclasses, ntrees, r, &info, ConstT(decisionforest, df), ConstT(dfreport, rep));
   alglib_impl::ae_state_clear();
}

void dfbuildrandomdecisionforestx1(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, const ae_int_t ntrees, const ae_int_t nrndvars, const double r, ae_int_t &info, decisionforest &df, dfreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildrandomdecisionforestx1(ConstT(ae_matrix, xy), npoints, nvars, nclasses, ntrees, nrndvars, r, &info, ConstT(decisionforest, df), ConstT(dfreport, rep));
   alglib_impl::ae_state_clear();
}

void dfbuildercreate(decisionforestbuilder &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildercreate(ConstT(decisionforestbuilder, s));
   alglib_impl::ae_state_clear();
}

void dfbuildersetdataset(const decisionforestbuilder &s, const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetdataset(ConstT(decisionforestbuilder, s), ConstT(ae_matrix, xy), npoints, nvars, nclasses);
   alglib_impl::ae_state_clear();
}

void dfbuildersetrndvars(const decisionforestbuilder &s, const ae_int_t rndvars) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetrndvars(ConstT(decisionforestbuilder, s), rndvars);
   alglib_impl::ae_state_clear();
}

void dfbuildersetrndvarsratio(const decisionforestbuilder &s, const double f) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetrndvarsratio(ConstT(decisionforestbuilder, s), f);
   alglib_impl::ae_state_clear();
}

void dfbuildersetrndvarsauto(const decisionforestbuilder &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetrndvarsauto(ConstT(decisionforestbuilder, s));
   alglib_impl::ae_state_clear();
}

void dfbuildersetsubsampleratio(const decisionforestbuilder &s, const double f) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetsubsampleratio(ConstT(decisionforestbuilder, s), f);
   alglib_impl::ae_state_clear();
}

void dfbuildersetseed(const decisionforestbuilder &s, const ae_int_t seedval) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetseed(ConstT(decisionforestbuilder, s), seedval);
   alglib_impl::ae_state_clear();
}

void dfbuildersetrdfalgo(const decisionforestbuilder &s, const ae_int_t algotype) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetrdfalgo(ConstT(decisionforestbuilder, s), algotype);
   alglib_impl::ae_state_clear();
}

void dfbuildersetrdfsplitstrength(const decisionforestbuilder &s, const ae_int_t splitstrength) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetrdfsplitstrength(ConstT(decisionforestbuilder, s), splitstrength);
   alglib_impl::ae_state_clear();
}

void dfbuildersetimportancetrngini(const decisionforestbuilder &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetimportancetrngini(ConstT(decisionforestbuilder, s));
   alglib_impl::ae_state_clear();
}

void dfbuildersetimportanceoobgini(const decisionforestbuilder &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetimportanceoobgini(ConstT(decisionforestbuilder, s));
   alglib_impl::ae_state_clear();
}

void dfbuildersetimportancepermutation(const decisionforestbuilder &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetimportancepermutation(ConstT(decisionforestbuilder, s));
   alglib_impl::ae_state_clear();
}

void dfbuildersetimportancenone(const decisionforestbuilder &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuildersetimportancenone(ConstT(decisionforestbuilder, s));
   alglib_impl::ae_state_clear();
}

double dfbuilderpeekprogress(const decisionforestbuilder &s) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::dfbuilderpeekprogress(ConstT(decisionforestbuilder, s));
   alglib_impl::ae_state_clear();
   return D;
}

double dfbuildergetprogress(const decisionforestbuilder &s) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::dfbuildergetprogress(ConstT(decisionforestbuilder, s));
   alglib_impl::ae_state_clear();
   return D;
}

void dfbuilderbuildrandomforest(const decisionforestbuilder &s, const ae_int_t ntrees, decisionforest &df, dfreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfbuilderbuildrandomforest(ConstT(decisionforestbuilder, s), ntrees, ConstT(decisionforest, df), ConstT(dfreport, rep));
   alglib_impl::ae_state_clear();
}

double dfbinarycompression(const decisionforest &df) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::dfbinarycompression(ConstT(decisionforest, df));
   alglib_impl::ae_state_clear();
   return D;
}

void dfprocess(const decisionforest &df, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfprocess(ConstT(decisionforest, df), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

void dfprocessi(const decisionforest &df, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dfprocessi(ConstT(decisionforest, df), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

double dfprocess0(const decisionforest &model, const real_1d_array &x) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::dfprocess0(ConstT(decisionforest, model), ConstT(ae_vector, x));
   alglib_impl::ae_state_clear();
   return D;
}

ae_int_t dfclassify(const decisionforest &model, const real_1d_array &x) {
   alglib_impl::ae_state_init();
   TryCatch(0)
   ae_int_t Z = alglib_impl::dfclassify(ConstT(decisionforest, model), ConstT(ae_vector, x));
   alglib_impl::ae_state_clear();
   return Z;
}

void dftsprocess(const decisionforest &df, const decisionforestbuffer &buf, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::dftsprocess(ConstT(decisionforest, df), ConstT(decisionforestbuffer, buf), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

double dfrelclserror(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::dfrelclserror(ConstT(decisionforest, df), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double dfavgce(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::dfavgce(ConstT(decisionforest, df), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double dfrmserror(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::dfrmserror(ConstT(decisionforest, df), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double dfavgerror(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::dfavgerror(ConstT(decisionforest, df), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double dfavgrelerror(const decisionforest &df, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::dfavgrelerror(ConstT(decisionforest, df), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}
} // end of namespace alglib

// === LINREG Package ===
// Depends on: (SpecialFunctions) IGAMMAF
// Depends on: (LinAlg) SVD
// Depends on: (Statistics) BASESTAT
namespace alglib_impl {
static const ae_int_t linreg_lrvnum = 5;

// Linear regression
//
// Subroutine builds model:
//
//     Y = A(0)*X[0] + ... + A(N-1)*X[N-1] + A(N)
//
// and model found in ALGLIB format, covariation matrix, training set  errors
// (rms,  average,  average  relative)   and  leave-one-out  cross-validation
// estimate of the generalization error. CV  estimate calculated  using  fast
// algorithm with O(NPoints*NVars) complexity.
//
// When  covariation  matrix  is  calculated  standard deviations of function
// values are assumed to be equal to RMS error on the training set.
//
// Inputs:
//     XY          -   training set, array [0..NPoints-1,0..NVars]:
//                     * NVars columns - independent variables
//                     * last column - dependent variable
//     NPoints     -   training set size, NPoints > NVars+1
//     NVars       -   number of independent variables
//
// Outputs:
//     Info        -   return code:
//                     * -255, in case of unknown internal error
//                     * -4, if internal SVD subroutine haven't converged
//                     * -1, if incorrect parameters was passed (NPoints < NVars+2, NVars < 1).
//                     *  1, if subroutine successfully finished
//     LM          -   linear model in the ALGLIB format. Use subroutines of
//                     this unit to work with the model.
//     AR          -   additional results
// ALGLIB: Copyright 02.08.2008 by Sergey Bochkanov
// API: void lrbuild(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, linearmodel &lm, lrreport &ar);
void lrbuild(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t *info, linearmodel *lm, lrreport *ar) {
   ae_frame _frame_block;
   ae_int_t i;
   double sigma2;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(linearmodel, lm);
   SetObj(lrreport, ar);
   NewVector(s, 0, DT_REAL);
   if (npoints <= nvars + 1 || nvars < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   ae_vector_set_length(&s, npoints);
   for (i = 0; i < npoints; i++) {
      s.xR[i] = 1.0;
   }
   lrbuilds(xy, &s, npoints, nvars, info, lm, ar);
   if (*info < 0) {
      ae_frame_leave();
      return;
   }
   sigma2 = ae_sqr(ar->rmserror) * npoints / (npoints - nvars - 1);
   for (i = 0; i <= nvars; i++) {
      ae_v_muld(ar->c.xyR[i], 1, nvars + 1, sigma2);
   }
   ae_frame_leave();
}

// Internal linear regression subroutine
static void linreg_lrinternal(RMatrix *xy, RVector *s, ae_int_t npoints, ae_int_t nvars, ae_int_t *info, linearmodel *lm, lrreport *ar) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t ncv;
   ae_int_t na;
   ae_int_t nacv;
   double r;
   double p;
   double epstol;
   ae_int_t offs;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(linearmodel, lm);
   SetObj(lrreport, ar);
   NewMatrix(a, 0, 0, DT_REAL);
   NewMatrix(u, 0, 0, DT_REAL);
   NewMatrix(vt, 0, 0, DT_REAL);
   NewMatrix(vm, 0, 0, DT_REAL);
   NewMatrix(xym, 0, 0, DT_REAL);
   NewVector(b, 0, DT_REAL);
   NewVector(sv, 0, DT_REAL);
   NewVector(t, 0, DT_REAL);
   NewVector(svi, 0, DT_REAL);
   NewVector(work, 0, DT_REAL);
   NewObj(lrreport, ar2);
   NewObj(linearmodel, tlm);
   epstol = 1000.0;
// Check for errors in data
   if (npoints < nvars || nvars < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   for (i = 0; i < npoints; i++) {
      if (s->xR[i] <= 0.0) {
         *info = -2;
         ae_frame_leave();
         return;
      }
   }
   *info = 1;
// Create design matrix
   ae_matrix_set_length(&a, npoints, nvars);
   ae_vector_set_length(&b, npoints);
   for (i = 0; i < npoints; i++) {
      r = 1 / s->xR[i];
      ae_v_moved(a.xyR[i], 1, xy->xyR[i], 1, nvars, r);
      b.xR[i] = xy->xyR[i][nvars] / s->xR[i];
   }
// Allocate W:
// W[0]     array size
// W[1]     version number, 0
// W[2]     NVars (minus 1, to be compatible with external representation)
// W[3]     coefficients offset
   ae_vector_set_length(&lm->w, 4 + nvars);
   offs = 4;
   lm->w.xR[0] = (double)(4 + nvars);
   lm->w.xR[1] = (double)linreg_lrvnum;
   lm->w.xR[2] = (double)(nvars - 1);
   lm->w.xR[3] = (double)offs;
// Solve problem using SVD:
//
// 0. check for degeneracy (different types)
// 1. A = U*diag(sv)*V'
// 2. T = b'*U
// 3. w = SUM((T[i]/sv[i])*V[..,i])
// 4. cov(wi,wj) = SUM(Vji*Vjk/sv[i]^2,K=1..M)
//
// see $15.4 of "Numerical Recipes in C" for more information
   ae_vector_set_length(&t, nvars);
   ae_vector_set_length(&svi, nvars);
   ae_matrix_set_length(&ar->c, nvars, nvars);
   ae_matrix_set_length(&vm, nvars, nvars);
   if (!rmatrixsvd(&a, npoints, nvars, 1, 1, 2, &sv, &u, &vt)) {
      *info = -4;
      ae_frame_leave();
      return;
   }
   if (sv.xR[0] <= 0.0) {
   // Degenerate case: zero design matrix.
      for (i = offs; i < offs + nvars; i++) {
         lm->w.xR[i] = 0.0;
      }
      ar->rmserror = lrrmserror(lm, xy, npoints);
      ar->avgerror = lravgerror(lm, xy, npoints);
      ar->avgrelerror = lravgrelerror(lm, xy, npoints);
      ar->cvrmserror = ar->rmserror;
      ar->cvavgerror = ar->avgerror;
      ar->cvavgrelerror = ar->avgrelerror;
      ar->ncvdefects = 0;
      ae_vector_set_length(&ar->cvdefects, nvars);
      for (i = 0; i < nvars; i++) {
         ar->cvdefects.xZ[i] = -1;
      }
      ae_matrix_set_length(&ar->c, nvars, nvars);
      for (i = 0; i < nvars; i++) {
         for (j = 0; j < nvars; j++) {
            ar->c.xyR[i][j] = 0.0;
         }
      }
      ae_frame_leave();
      return;
   }
   if (sv.xR[nvars - 1] <= epstol * ae_machineepsilon * sv.xR[0]) {
   // Degenerate case, non-zero design matrix.
   //
   // We can leave it and solve task in SVD least squares fashion.
   // Solution and covariance matrix will be obtained correctly,
   // but CV error estimates - will not. It is better to reduce
   // it to non-degenerate task and to obtain correct CV estimates.
      for (k = nvars; k >= 1; k--) {
         if (sv.xR[k - 1] > epstol * ae_machineepsilon * sv.xR[0]) {
         // Reduce
            ae_matrix_set_length(&xym, npoints, k + 1);
            for (i = 0; i < npoints; i++) {
               for (j = 0; j < k; j++) {
                  r = ae_v_dotproduct(xy->xyR[i], 1, vt.xyR[j], 1, nvars);
                  xym.xyR[i][j] = r;
               }
               xym.xyR[i][k] = xy->xyR[i][nvars];
            }
         // Solve
            linreg_lrinternal(&xym, s, npoints, k, info, &tlm, &ar2);
            if (*info != 1) {
               ae_frame_leave();
               return;
            }
         // Convert back to un-reduced format
            for (j = 0; j < nvars; j++) {
               lm->w.xR[offs + j] = 0.0;
            }
            for (j = 0; j < k; j++) {
               r = tlm.w.xR[offs + j];
               ae_v_addd(&lm->w.xR[offs], 1, vt.xyR[j], 1, nvars, r);
            }
            ar->rmserror = ar2.rmserror;
            ar->avgerror = ar2.avgerror;
            ar->avgrelerror = ar2.avgrelerror;
            ar->cvrmserror = ar2.cvrmserror;
            ar->cvavgerror = ar2.cvavgerror;
            ar->cvavgrelerror = ar2.cvavgrelerror;
            ar->ncvdefects = ar2.ncvdefects;
            ae_vector_set_length(&ar->cvdefects, nvars);
            for (j = 0; j < ar->ncvdefects; j++) {
               ar->cvdefects.xZ[j] = ar2.cvdefects.xZ[j];
            }
            for (j = ar->ncvdefects; j < nvars; j++) {
               ar->cvdefects.xZ[j] = -1;
            }
            ae_matrix_set_length(&ar->c, nvars, nvars);
            ae_vector_set_length(&work, nvars + 1);
            matrixmatrixmultiply(&ar2.c, 0, k - 1, 0, k - 1, false, &vt, 0, k - 1, 0, nvars - 1, false, 1.0, &vm, 0, k - 1, 0, nvars - 1, 0.0, &work);
            matrixmatrixmultiply(&vt, 0, k - 1, 0, nvars - 1, true, &vm, 0, k - 1, 0, nvars - 1, false, 1.0, &ar->c, 0, nvars - 1, 0, nvars - 1, 0.0, &work);
            ae_frame_leave();
            return;
         }
      }
      *info = -255;
      ae_frame_leave();
      return;
   }
   for (i = 0; i < nvars; i++) {
      if (sv.xR[i] > epstol * ae_machineepsilon * sv.xR[0]) {
         svi.xR[i] = 1 / sv.xR[i];
      } else {
         svi.xR[i] = 0.0;
      }
   }
   for (i = 0; i < nvars; i++) {
      t.xR[i] = 0.0;
   }
   for (i = 0; i < npoints; i++) {
      r = b.xR[i];
      ae_v_addd(t.xR, 1, u.xyR[i], 1, nvars, r);
   }
   for (i = 0; i < nvars; i++) {
      lm->w.xR[offs + i] = 0.0;
   }
   for (i = 0; i < nvars; i++) {
      r = t.xR[i] * svi.xR[i];
      ae_v_addd(&lm->w.xR[offs], 1, vt.xyR[i], 1, nvars, r);
   }
   for (j = 0; j < nvars; j++) {
      r = svi.xR[j];
      ae_v_moved(&vm.xyR[0][j], vm.stride, vt.xyR[j], 1, nvars, r);
   }
   for (i = 0; i < nvars; i++) {
      for (j = i; j < nvars; j++) {
         r = ae_v_dotproduct(vm.xyR[i], 1, vm.xyR[j], 1, nvars);
         ar->c.xyR[i][j] = r;
         ar->c.xyR[j][i] = r;
      }
   }
// Leave-1-out cross-validation error.
//
// NOTATIONS:
// A            design matrix
// A*x = b      original linear least squares task
// U*S*V'       SVD of A
// ai           i-th row of the A
// bi           i-th element of the b
// xf           solution of the original LLS task
//
// Cross-validation error of i-th element from a sample is
// calculated using following formula:
//
//     ERRi = ai*xf - (ai*xf-bi*(ui*ui'))/(1-ui*ui')     (1)
//
// This formula can be derived from normal equations of the
// original task
//
//     (A'*A)x = A'*b                                    (2)
//
// by applying modification (zeroing out i-th row of A) to (2):
//
//     (A-ai)'*(A-ai) = (A-ai)'*b
//
// and using Sherman-Morrison formula for updating matrix inverse
//
// NOTE 1: b is not zeroed out since it is much simpler and
// does not influence final result.
//
// NOTE 2: some design matrices A have such ui that 1-ui*ui'=0.
// Formula (1) can't be applied for such cases and they are skipped
// from CV calculation (which distorts resulting CV estimate).
// But from the properties of U we can conclude that there can
// be no more than NVars such vectors. Usually
// NVars << NPoints, so in a normal case it only slightly
// influences result.
   ncv = 0;
   na = 0;
   nacv = 0;
   ar->rmserror = 0.0;
   ar->avgerror = 0.0;
   ar->avgrelerror = 0.0;
   ar->cvrmserror = 0.0;
   ar->cvavgerror = 0.0;
   ar->cvavgrelerror = 0.0;
   ar->ncvdefects = 0;
   ae_vector_set_length(&ar->cvdefects, nvars);
   for (i = 0; i < nvars; i++) {
      ar->cvdefects.xZ[i] = -1;
   }
   for (i = 0; i < npoints; i++) {
   // Error on a training set
      r = ae_v_dotproduct(xy->xyR[i], 1, &lm->w.xR[offs], 1, nvars);
      ar->rmserror += ae_sqr(r - xy->xyR[i][nvars]);
      ar->avgerror += fabs(r - xy->xyR[i][nvars]);
      if (xy->xyR[i][nvars] != 0.0) {
         ar->avgrelerror += fabs((r - xy->xyR[i][nvars]) / xy->xyR[i][nvars]);
         na++;
      }
   // Error using fast leave-one-out cross-validation
      p = ae_v_dotproduct(u.xyR[i], 1, u.xyR[i], 1, nvars);
      if (p > 1 - epstol * ae_machineepsilon) {
         ar->cvdefects.xZ[ar->ncvdefects] = i;
         ar->ncvdefects++;
         continue;
      }
      r = s->xR[i] * (r / s->xR[i] - b.xR[i] * p) / (1 - p);
      ar->cvrmserror += ae_sqr(r - xy->xyR[i][nvars]);
      ar->cvavgerror += fabs(r - xy->xyR[i][nvars]);
      if (xy->xyR[i][nvars] != 0.0) {
         ar->cvavgrelerror += fabs((r - xy->xyR[i][nvars]) / xy->xyR[i][nvars]);
         nacv++;
      }
      ncv++;
   }
   if (ncv == 0) {
   // Something strange: ALL ui are degenerate.
   // Unexpected...
      *info = -255;
      ae_frame_leave();
      return;
   }
   ar->rmserror = sqrt(ar->rmserror / npoints);
   ar->avgerror /= npoints;
   if (na != 0) {
      ar->avgrelerror /= na;
   }
   ar->cvrmserror = sqrt(ar->cvrmserror / ncv);
   ar->cvavgerror /= ncv;
   if (nacv != 0) {
      ar->cvavgrelerror /= nacv;
   }
   ae_frame_leave();
}

// Linear regression
//
// Variant of LRBuild which uses vector of standatd deviations (errors in
// function values).
//
// Inputs:
//     XY          -   training set, array [0..NPoints-1,0..NVars]:
//                     * NVars columns - independent variables
//                     * last column - dependent variable
//     S           -   standard deviations (errors in function values)
//                     array[0..NPoints-1], S[i] > 0.
//     NPoints     -   training set size, NPoints > NVars+1
//     NVars       -   number of independent variables
//
// Outputs:
//     Info        -   return code:
//                     * -255, in case of unknown internal error
//                     * -4, if internal SVD subroutine haven't converged
//                     * -1, if incorrect parameters was passed (NPoints < NVars+2, NVars < 1).
//                     * -2, if S[I] <= 0
//                     *  1, if subroutine successfully finished
//     LM          -   linear model in the ALGLIB format. Use subroutines of
//                     this unit to work with the model.
//     AR          -   additional results
// ALGLIB: Copyright 02.08.2008 by Sergey Bochkanov
// API: void lrbuilds(const real_2d_array &xy, const real_1d_array &s, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, linearmodel &lm, lrreport &ar);
void lrbuilds(RMatrix *xy, RVector *s, ae_int_t npoints, ae_int_t nvars, ae_int_t *info, linearmodel *lm, lrreport *ar) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   double v;
   ae_int_t offs;
   double mean;
   double variance;
   double skewness;
   double kurtosis;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(linearmodel, lm);
   SetObj(lrreport, ar);
   NewMatrix(xyi, 0, 0, DT_REAL);
   NewVector(x, 0, DT_REAL);
   NewVector(means, 0, DT_REAL);
   NewVector(sigmas, 0, DT_REAL);
// Test parameters
   if (npoints <= nvars + 1 || nvars < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
// Copy data, add one more column (constant term)
   ae_matrix_set_length(&xyi, npoints, nvars + 1 + 1);
   for (i = 0; i < npoints; i++) {
      ae_v_move(xyi.xyR[i], 1, xy->xyR[i], 1, nvars);
      xyi.xyR[i][nvars] = 1.0;
      xyi.xyR[i][nvars + 1] = xy->xyR[i][nvars];
   }
// Standartization
   ae_vector_set_length(&x, npoints);
   ae_vector_set_length(&means, nvars);
   ae_vector_set_length(&sigmas, nvars);
   for (j = 0; j < nvars; j++) {
      ae_v_move(x.xR, 1, &xy->xyR[0][j], xy->stride, npoints);
      samplemoments(&x, npoints, &mean, &variance, &skewness, &kurtosis);
      means.xR[j] = mean;
      sigmas.xR[j] = sqrt(variance);
      if (sigmas.xR[j] == 0.0) {
         sigmas.xR[j] = 1.0;
      }
      for (i = 0; i < npoints; i++) {
         xyi.xyR[i][j] = (xyi.xyR[i][j] - means.xR[j]) / sigmas.xR[j];
      }
   }
// Internal processing
   linreg_lrinternal(&xyi, s, npoints, nvars + 1, info, lm, ar);
   if (*info < 0) {
      ae_frame_leave();
      return;
   }
// Un-standartization
   offs = RoundZ(lm->w.xR[3]);
   for (j = 0; j < nvars; j++) {
   // Constant term is updated (and its covariance too,
   // since it gets some variance from J-th component)
      lm->w.xR[offs + nvars] -= lm->w.xR[offs + j] * means.xR[j] / sigmas.xR[j];
      v = means.xR[j] / sigmas.xR[j];
      ae_v_subd(ar->c.xyR[nvars], 1, ar->c.xyR[j], 1, nvars + 1, v);
      ae_v_subd(&ar->c.xyR[0][nvars], ar->c.stride, &ar->c.xyR[0][j], ar->c.stride, nvars + 1, v);
   // J-th term is updated
      lm->w.xR[offs + j] /= sigmas.xR[j];
      v = 1 / sigmas.xR[j];
      ae_v_muld(ar->c.xyR[j], 1, nvars + 1, v);
      ae_v_muld(&ar->c.xyR[0][j], ar->c.stride, nvars + 1, v);
   }
   ae_frame_leave();
}

// Like LRBuildS, but builds model
//
//     Y = A(0)*X[0] + ... + A(N-1)*X[N-1]
//
// i.e. with zero constant term.
// ALGLIB: Copyright 30.10.2008 by Sergey Bochkanov
// API: void lrbuildzs(const real_2d_array &xy, const real_1d_array &s, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, linearmodel &lm, lrreport &ar);
void lrbuildzs(RMatrix *xy, RVector *s, ae_int_t npoints, ae_int_t nvars, ae_int_t *info, linearmodel *lm, lrreport *ar) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   double v;
   ae_int_t offs;
   double mean;
   double variance;
   double skewness;
   double kurtosis;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(linearmodel, lm);
   SetObj(lrreport, ar);
   NewMatrix(xyi, 0, 0, DT_REAL);
   NewVector(x, 0, DT_REAL);
   NewVector(c, 0, DT_REAL);
// Test parameters
   if (npoints <= nvars + 1 || nvars < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
// Copy data, add one more column (constant term)
   ae_matrix_set_length(&xyi, npoints, nvars + 1 + 1);
   for (i = 0; i < npoints; i++) {
      ae_v_move(xyi.xyR[i], 1, xy->xyR[i], 1, nvars);
      xyi.xyR[i][nvars] = 0.0;
      xyi.xyR[i][nvars + 1] = xy->xyR[i][nvars];
   }
// Standartization: unusual scaling
   ae_vector_set_length(&x, npoints);
   ae_vector_set_length(&c, nvars);
   for (j = 0; j < nvars; j++) {
      ae_v_move(x.xR, 1, &xy->xyR[0][j], xy->stride, npoints);
      samplemoments(&x, npoints, &mean, &variance, &skewness, &kurtosis);
      if (!SmallAtR(mean, sqrt(variance))) {
      // variation is relatively small, it is better to
      // bring mean value to 1
         c.xR[j] = mean;
      } else {
      // variation is large, it is better to bring variance to 1
         if (variance == 0.0) {
            variance = 1.0;
         }
         c.xR[j] = sqrt(variance);
      }
      for (i = 0; i < npoints; i++) {
         xyi.xyR[i][j] /= c.xR[j];
      }
   }
// Internal processing
   linreg_lrinternal(&xyi, s, npoints, nvars + 1, info, lm, ar);
   if (*info < 0) {
      ae_frame_leave();
      return;
   }
// Un-standartization
   offs = RoundZ(lm->w.xR[3]);
   for (j = 0; j < nvars; j++) {
   // J-th term is updated
      lm->w.xR[offs + j] /= c.xR[j];
      v = 1 / c.xR[j];
      ae_v_muld(ar->c.xyR[j], 1, nvars + 1, v);
      ae_v_muld(&ar->c.xyR[0][j], ar->c.stride, nvars + 1, v);
   }
   ae_frame_leave();
}

// Like LRBuild but builds model
//
//     Y = A(0)*X[0] + ... + A(N-1)*X[N-1]
//
// i.e. with zero constant term.
// ALGLIB: Copyright 30.10.2008 by Sergey Bochkanov
// API: void lrbuildz(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, linearmodel &lm, lrreport &ar);
void lrbuildz(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t *info, linearmodel *lm, lrreport *ar) {
   ae_frame _frame_block;
   ae_int_t i;
   double sigma2;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(linearmodel, lm);
   SetObj(lrreport, ar);
   NewVector(s, 0, DT_REAL);
   if (npoints <= nvars + 1 || nvars < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   ae_vector_set_length(&s, npoints);
   for (i = 0; i < npoints; i++) {
      s.xR[i] = 1.0;
   }
   lrbuildzs(xy, &s, npoints, nvars, info, lm, ar);
   if (*info < 0) {
      ae_frame_leave();
      return;
   }
   sigma2 = ae_sqr(ar->rmserror) * npoints / (npoints - nvars - 1);
   for (i = 0; i <= nvars; i++) {
      ae_v_muld(ar->c.xyR[i], 1, nvars + 1, sigma2);
   }
   ae_frame_leave();
}

// Unpacks coefficients of linear model.
//
// Inputs:
//     LM          -   linear model in ALGLIB format
//
// Outputs:
//     V           -   coefficients, array[0..NVars]
//                     constant term (intercept) is stored in the V[NVars].
//     NVars       -   number of independent variables (one less than number
//                     of coefficients)
// ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
// API: void lrunpack(const linearmodel &lm, real_1d_array &v, ae_int_t &nvars);
void lrunpack(linearmodel *lm, RVector *v, ae_int_t *nvars) {
   ae_int_t offs;
   SetVector(v);
   *nvars = 0;
   ae_assert(RoundZ(lm->w.xR[1]) == linreg_lrvnum, "LINREG: Incorrect LINREG version!");
   *nvars = RoundZ(lm->w.xR[2]);
   offs = RoundZ(lm->w.xR[3]);
   ae_vector_set_length(v, *nvars + 1);
   ae_v_move(v->xR, 1, &lm->w.xR[offs], 1, *nvars + 1);
}

// "Packs" coefficients and creates linear model in ALGLIB format (LRUnpack
// reversed).
//
// Inputs:
//     V           -   coefficients, array[0..NVars]
//     NVars       -   number of independent variables
//
// Outputs:
//     LM          -   linear model.
// ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
// API: void lrpack(const real_1d_array &v, const ae_int_t nvars, linearmodel &lm);
void lrpack(RVector *v, ae_int_t nvars, linearmodel *lm) {
   ae_int_t offs;
   SetObj(linearmodel, lm);
   ae_vector_set_length(&lm->w, 4 + nvars + 1);
   offs = 4;
   lm->w.xR[0] = (double)(4 + nvars + 1);
   lm->w.xR[1] = (double)linreg_lrvnum;
   lm->w.xR[2] = (double)nvars;
   lm->w.xR[3] = (double)offs;
   ae_v_move(&lm->w.xR[offs], 1, v->xR, 1, nvars + 1);
}

// Procesing
//
// Inputs:
//     LM      -   linear model
//     X       -   input vector,  array[0..NVars-1].
//
// Result:
//     value of linear model regression estimate
// ALGLIB: Copyright 03.09.2008 by Sergey Bochkanov
// API: double lrprocess(const linearmodel &lm, const real_1d_array &x);
double lrprocess(linearmodel *lm, RVector *x) {
   double v;
   ae_int_t offs;
   ae_int_t nvars;
   double result;
   ae_assert(RoundZ(lm->w.xR[1]) == linreg_lrvnum, "LINREG: Incorrect LINREG version!");
   nvars = RoundZ(lm->w.xR[2]);
   offs = RoundZ(lm->w.xR[3]);
   v = ae_v_dotproduct(x->xR, 1, &lm->w.xR[offs], 1, nvars);
   result = v + lm->w.xR[offs + nvars];
   return result;
}

// RMS error on the test set
//
// Inputs:
//     LM      -   linear model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     root mean square error.
// ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
// API: double lrrmserror(const linearmodel &lm, const real_2d_array &xy, const ae_int_t npoints);
double lrrmserror(linearmodel *lm, RMatrix *xy, ae_int_t npoints) {
   ae_int_t i;
   double v;
   ae_int_t offs;
   ae_int_t nvars;
   double result;
   ae_assert(RoundZ(lm->w.xR[1]) == linreg_lrvnum, "LINREG: Incorrect LINREG version!");
   nvars = RoundZ(lm->w.xR[2]);
   offs = RoundZ(lm->w.xR[3]);
   result = 0.0;
   for (i = 0; i < npoints; i++) {
      v = ae_v_dotproduct(xy->xyR[i], 1, &lm->w.xR[offs], 1, nvars);
      v += lm->w.xR[offs + nvars];
      result += ae_sqr(v - xy->xyR[i][nvars]);
   }
   result = sqrt(result / npoints);
   return result;
}

// Average error on the test set
//
// Inputs:
//     LM      -   linear model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     average error.
// ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
// API: double lravgerror(const linearmodel &lm, const real_2d_array &xy, const ae_int_t npoints);
double lravgerror(linearmodel *lm, RMatrix *xy, ae_int_t npoints) {
   ae_int_t i;
   double v;
   ae_int_t offs;
   ae_int_t nvars;
   double result;
   ae_assert(RoundZ(lm->w.xR[1]) == linreg_lrvnum, "LINREG: Incorrect LINREG version!");
   nvars = RoundZ(lm->w.xR[2]);
   offs = RoundZ(lm->w.xR[3]);
   result = 0.0;
   for (i = 0; i < npoints; i++) {
      v = ae_v_dotproduct(xy->xyR[i], 1, &lm->w.xR[offs], 1, nvars);
      v += lm->w.xR[offs + nvars];
      result += fabs(v - xy->xyR[i][nvars]);
   }
   result /= npoints;
   return result;
}

// RMS error on the test set
//
// Inputs:
//     LM      -   linear model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     average relative error.
// ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
// API: double lravgrelerror(const linearmodel &lm, const real_2d_array &xy, const ae_int_t npoints);
double lravgrelerror(linearmodel *lm, RMatrix *xy, ae_int_t npoints) {
   ae_int_t i;
   ae_int_t k;
   double v;
   ae_int_t offs;
   ae_int_t nvars;
   double result;
   ae_assert(RoundZ(lm->w.xR[1]) == linreg_lrvnum, "LINREG: Incorrect LINREG version!");
   nvars = RoundZ(lm->w.xR[2]);
   offs = RoundZ(lm->w.xR[3]);
   result = 0.0;
   k = 0;
   for (i = 0; i < npoints; i++) {
      if (xy->xyR[i][nvars] != 0.0) {
         v = ae_v_dotproduct(xy->xyR[i], 1, &lm->w.xR[offs], 1, nvars);
         v += lm->w.xR[offs + nvars];
         result += fabs((v - xy->xyR[i][nvars]) / xy->xyR[i][nvars]);
         k++;
      }
   }
   if (k != 0) {
      result /= k;
   }
   return result;
}

// Copying of LinearModel structure
//
// Inputs:
//     LM1 -   original
//
// Outputs:
//     LM2 -   copy
// ALGLIB: Copyright 15.03.2009 by Sergey Bochkanov
void lrcopy(linearmodel *lm1, linearmodel *lm2) {
   ae_int_t k;
   SetObj(linearmodel, lm2);
   k = RoundZ(lm1->w.xR[0]);
   ae_vector_set_length(&lm2->w, k);
   ae_v_move(lm2->w.xR, 1, lm1->w.xR, 1, k);
}

void lrlines(RMatrix *xy, RVector *s, ae_int_t n, ae_int_t *info, double *a, double *b, double *vara, double *varb, double *covab, double *corrab, double *p) {
   ae_int_t i;
   double ss;
   double sx;
   double sxx;
   double sy;
   double stt;
   double e1;
   double e2;
   double t;
   double chi2;
   *info = 0;
   *a = 0;
   *b = 0;
   *vara = 0;
   *varb = 0;
   *covab = 0;
   *corrab = 0;
   *p = 0;
   if (n < 2) {
      *info = -1;
      return;
   }
   for (i = 0; i < n; i++) {
      if (s->xR[i] <= 0.0) {
         *info = -2;
         return;
      }
   }
   *info = 1;
// Calculate S, SX, SY, SXX
   ss = 0.0;
   sx = 0.0;
   sy = 0.0;
   sxx = 0.0;
   for (i = 0; i < n; i++) {
      t = ae_sqr(s->xR[i]);
      ss += 1.0 / t;
      sx += xy->xyR[i][0] / t;
      sy += xy->xyR[i][1] / t;
      sxx += ae_sqr(xy->xyR[i][0]) / t;
   }
// Test for condition number
   t = sqrt(4 * ae_sqr(sx) + ae_sqr(ss - sxx));
   e1 = 0.5 * (ss + sxx + t);
   e2 = 0.5 * (ss + sxx - t);
   if (rmin2(e1, e2) <= 1000 * ae_machineepsilon * rmax2(e1, e2)) {
      *info = -3;
      return;
   }
// Calculate A, B
   *a = 0.0;
   *b = 0.0;
   stt = 0.0;
   for (i = 0; i < n; i++) {
      t = (xy->xyR[i][0] - sx / ss) / s->xR[i];
      *b += t * xy->xyR[i][1] / s->xR[i];
      stt += ae_sqr(t);
   }
   *b /= stt;
   *a = (sy - sx * (*b)) / ss;
// Calculate goodness-of-fit
   if (n > 2) {
      chi2 = 0.0;
      for (i = 0; i < n; i++) {
         chi2 += ae_sqr((xy->xyR[i][1] - (*a) - *b * xy->xyR[i][0]) / s->xR[i]);
      }
      *p = incompletegammac((double)(n - 2) / 2.0, chi2 / 2);
   } else {
      *p = 1.0;
   }
// Calculate other parameters
   *vara = (1 + ae_sqr(sx) / (ss * stt)) / ss;
   *varb = 1 / stt;
   *covab = -sx / (ss * stt);
   *corrab = *covab / sqrt(*vara * (*varb));
}

void lrline(RMatrix *xy, ae_int_t n, ae_int_t *info, double *a, double *b) {
   ae_frame _frame_block;
   ae_int_t i;
   double vara;
   double varb;
   double covab;
   double corrab;
   double p;
   ae_frame_make(&_frame_block);
   *info = 0;
   *a = 0;
   *b = 0;
   NewVector(s, 0, DT_REAL);
   if (n < 2) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   ae_vector_set_length(&s, n);
   for (i = 0; i < n; i++) {
      s.xR[i] = 1.0;
   }
   lrlines(xy, &s, n, info, a, b, &vara, &varb, &covab, &corrab, &p);
   ae_frame_leave();
}

void linearmodel_init(void *_p, bool make_automatic) {
   linearmodel *p = (linearmodel *)_p;
   ae_vector_init(&p->w, 0, DT_REAL, make_automatic);
}

void linearmodel_copy(void *_dst, void *_src, bool make_automatic) {
   linearmodel *dst = (linearmodel *)_dst;
   linearmodel *src = (linearmodel *)_src;
   ae_vector_copy(&dst->w, &src->w, make_automatic);
}

void linearmodel_free(void *_p, bool make_automatic) {
   linearmodel *p = (linearmodel *)_p;
   ae_vector_free(&p->w, make_automatic);
}

void lrreport_init(void *_p, bool make_automatic) {
   lrreport *p = (lrreport *)_p;
   ae_matrix_init(&p->c, 0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->cvdefects, 0, DT_INT, make_automatic);
}

void lrreport_copy(void *_dst, void *_src, bool make_automatic) {
   lrreport *dst = (lrreport *)_dst;
   lrreport *src = (lrreport *)_src;
   ae_matrix_copy(&dst->c, &src->c, make_automatic);
   dst->rmserror = src->rmserror;
   dst->avgerror = src->avgerror;
   dst->avgrelerror = src->avgrelerror;
   dst->cvrmserror = src->cvrmserror;
   dst->cvavgerror = src->cvavgerror;
   dst->cvavgrelerror = src->cvavgrelerror;
   dst->ncvdefects = src->ncvdefects;
   ae_vector_copy(&dst->cvdefects, &src->cvdefects, make_automatic);
}

void lrreport_free(void *_p, bool make_automatic) {
   lrreport *p = (lrreport *)_p;
   ae_matrix_free(&p->c, make_automatic);
   ae_vector_free(&p->cvdefects, make_automatic);
}
} // end of namespace alglib_impl

namespace alglib {
DefClass(linearmodel, EndD)

// LRReport structure contains additional information about linear model:
// * C             -   covariation matrix,  array[0..NVars,0..NVars].
//                     C[i,j] = Cov(A[i],A[j])
// * RMSError      -   root mean square error on a training set
// * AvgError      -   average error on a training set
// * AvgRelError   -   average relative error on a training set (excluding
//                     observations with zero function value).
// * CVRMSError    -   leave-one-out cross-validation estimate of
//                     generalization error. Calculated using fast algorithm
//                     with O(NVars*NPoints) complexity.
// * CVAvgError    -   cross-validation estimate of average error
// * CVAvgRelError -   cross-validation estimate of average relative error
//
// All other fields of the structure are intended for internal use and should
// not be used outside ALGLIB.
DefClass(lrreport, AndD DecVar(c) AndD DecVal(rmserror) AndD DecVal(avgerror) AndD DecVal(avgrelerror) AndD DecVal(cvrmserror) AndD DecVal(cvavgerror) AndD DecVal(cvavgrelerror) AndD DecVal(ncvdefects) AndD DecVar(cvdefects))

void lrbuild(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, linearmodel &lm, lrreport &ar) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::lrbuild(ConstT(ae_matrix, xy), npoints, nvars, &info, ConstT(linearmodel, lm), ConstT(lrreport, ar));
   alglib_impl::ae_state_clear();
}

void lrbuilds(const real_2d_array &xy, const real_1d_array &s, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, linearmodel &lm, lrreport &ar) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::lrbuilds(ConstT(ae_matrix, xy), ConstT(ae_vector, s), npoints, nvars, &info, ConstT(linearmodel, lm), ConstT(lrreport, ar));
   alglib_impl::ae_state_clear();
}

void lrbuildzs(const real_2d_array &xy, const real_1d_array &s, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, linearmodel &lm, lrreport &ar) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::lrbuildzs(ConstT(ae_matrix, xy), ConstT(ae_vector, s), npoints, nvars, &info, ConstT(linearmodel, lm), ConstT(lrreport, ar));
   alglib_impl::ae_state_clear();
}

void lrbuildz(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, ae_int_t &info, linearmodel &lm, lrreport &ar) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::lrbuildz(ConstT(ae_matrix, xy), npoints, nvars, &info, ConstT(linearmodel, lm), ConstT(lrreport, ar));
   alglib_impl::ae_state_clear();
}

void lrunpack(const linearmodel &lm, real_1d_array &v, ae_int_t &nvars) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::lrunpack(ConstT(linearmodel, lm), ConstT(ae_vector, v), &nvars);
   alglib_impl::ae_state_clear();
}

void lrpack(const real_1d_array &v, const ae_int_t nvars, linearmodel &lm) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::lrpack(ConstT(ae_vector, v), nvars, ConstT(linearmodel, lm));
   alglib_impl::ae_state_clear();
}

double lrprocess(const linearmodel &lm, const real_1d_array &x) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::lrprocess(ConstT(linearmodel, lm), ConstT(ae_vector, x));
   alglib_impl::ae_state_clear();
   return D;
}

double lrrmserror(const linearmodel &lm, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::lrrmserror(ConstT(linearmodel, lm), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double lravgerror(const linearmodel &lm, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::lravgerror(ConstT(linearmodel, lm), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double lravgrelerror(const linearmodel &lm, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::lravgrelerror(ConstT(linearmodel, lm), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}
} // end of namespace alglib

// === FILTERS Package ===
// Depends on: LINREG
namespace alglib_impl {
// Filters: simple moving averages (unsymmetric).
//
// This filter replaces array by results of SMA(K) filter. SMA(K) is defined
// as filter which averages at most K previous points (previous - not points
// AROUND central point) - or less, in case of the first K-1 points.
//
// Inputs:
//     X           -   array[N], array to process. It can be larger than N,
//                     in this case only first N points are processed.
//     N           -   points count, N >= 0
//     K           -   K >= 1 (K can be larger than N ,  such  cases  will  be
//                     correctly handled). Window width. K=1 corresponds  to
//                     identity transformation (nothing changes).
//
// Outputs:
//     X           -   array, whose first N elements were processed with SMA(K)
//
// NOTE 1: this function uses efficient in-place  algorithm  which  does not
//         allocate temporary arrays.
//
// NOTE 2: this algorithm makes only one pass through array and uses running
//         sum  to speed-up calculation of the averages. Additional measures
//         are taken to ensure that running sum on a long sequence  of  zero
//         elements will be correctly reset to zero even in the presence  of
//         round-off error.
//
// NOTE 3: this  is  unsymmetric version of the algorithm,  which  does  NOT
//         averages points after the current one. Only X[i], X[i-1], ... are
//         used when calculating new value of X[i]. We should also note that
//         this algorithm uses BOTH previous points and  current  one,  i.e.
//         new value of X[i] depends on BOTH previous point and X[i] itself.
// ALGLIB: Copyright 25.10.2011 by Sergey Bochkanov
// API: void filtersma(real_1d_array &x, const ae_int_t n, const ae_int_t k);
// API: void filtersma(real_1d_array &x, const ae_int_t k);
void filtersma(RVector *x, ae_int_t n, ae_int_t k) {
   ae_int_t i;
   double runningsum;
   double termsinsum;
   ae_int_t zeroprefix;
   double v;
   ae_assert(n >= 0, "FilterSMA: N<0");
   ae_assert(x->cnt >= n, "FilterSMA: Length(X)<N");
   ae_assert(isfinitevector(x, n), "FilterSMA: X contains INF or NAN");
   ae_assert(k >= 1, "FilterSMA: K<1");
// Quick exit, if necessary
   if (n <= 1 || k == 1) {
      return;
   }
// Prepare variables (see below for explanation)
   runningsum = 0.0;
   termsinsum = 0.0;
   for (i = imax2(n - k, 0); i < n; i++) {
      runningsum += x->xR[i];
      termsinsum++;
   }
   i = imax2(n - k, 0);
   zeroprefix = 0;
   while (i < n && x->xR[i] == 0.0) {
      zeroprefix++;
      i++;
   }
// General case: we assume that N > 1 and K > 1
//
// Make one pass through all elements. At the beginning of
// the iteration we have:
// * I              element being processed
// * RunningSum     current value of the running sum
//                  (including I-th element)
// * TermsInSum     number of terms in sum, 0 <= TermsInSum <= K
// * ZeroPrefix     length of the sequence of zero elements
//                  which starts at X[I-K+1] and continues towards X[I].
//                  Equal to zero in case X[I-K+1] is non-zero.
//                  This value is used to make RunningSum exactly zero
//                  when it follows from the problem properties.
   for (i = n - 1; i >= 0; i--) {
   // Store new value of X[i], save old value in V
      v = x->xR[i];
      x->xR[i] = runningsum / termsinsum;
   // Update RunningSum and TermsInSum
      if (i - k >= 0) {
         runningsum -= v - x->xR[i - k];
      } else {
         runningsum -= v;
         termsinsum--;
      }
   // Update ZeroPrefix.
   // In case we have ZeroPrefix=TermsInSum,
   // RunningSum is reset to zero.
      if (i - k >= 0) {
         if (x->xR[i - k] != 0.0) {
            zeroprefix = 0;
         } else {
            zeroprefix = imin2(zeroprefix + 1, k);
         }
      } else {
         zeroprefix = imin2(zeroprefix, i + 1);
      }
      if ((double)zeroprefix == termsinsum) {
         runningsum = 0.0;
      }
   }
}

// Filters: exponential moving averages.
//
// This filter replaces array by results of EMA(alpha) filter. EMA(alpha) is
// defined as filter which replaces X[] by S[]:
//     S[0] = X[0]
//     S[t] = alpha*X[t] + (1-alpha)*S[t-1]
//
// Inputs:
//     X           -   array[N], array to process. It can be larger than N,
//                     in this case only first N points are processed.
//     N           -   points count, N >= 0
//     alpha       -   0 < alpha <= 1, smoothing parameter.
//
// Outputs:
//     X           -   array, whose first N elements were processed
//                     with EMA(alpha)
//
// NOTE 1: this function uses efficient in-place  algorithm  which  does not
//         allocate temporary arrays.
//
// NOTE 2: this algorithm uses BOTH previous points and  current  one,  i.e.
//         new value of X[i] depends on BOTH previous point and X[i] itself.
//
// NOTE 3: technical analytis users quite often work  with  EMA  coefficient
//         expressed in DAYS instead of fractions. If you want to  calculate
//         EMA(N), where N is a number of days, you can use alpha=2/(N+1).
// ALGLIB: Copyright 25.10.2011 by Sergey Bochkanov
// API: void filterema(real_1d_array &x, const ae_int_t n, const double alpha);
// API: void filterema(real_1d_array &x, const double alpha);
void filterema(RVector *x, ae_int_t n, double alpha) {
   ae_int_t i;
   ae_assert(n >= 0, "FilterEMA: N<0");
   ae_assert(x->cnt >= n, "FilterEMA: Length(X)<N");
   ae_assert(isfinitevector(x, n), "FilterEMA: X contains INF or NAN");
   ae_assert(alpha > 0.0, "FilterEMA: Alpha <= 0");
   ae_assert(alpha <= 1.0, "FilterEMA: Alpha>1");
// Quick exit, if necessary
   if (n <= 1 || alpha == 1.0) {
      return;
   }
// Process
   for (i = 1; i < n; i++) {
      x->xR[i] = alpha * x->xR[i] + (1 - alpha) * x->xR[i - 1];
   }
}

// Filters: linear regression moving averages.
//
// This filter replaces array by results of LRMA(K) filter.
//
// LRMA(K) is defined as filter which, for each data  point,  builds  linear
// regression  model  using  K  prevous  points (point itself is included in
// these K points) and calculates value of this linear model at the point in
// question.
//
// Inputs:
//     X           -   array[N], array to process. It can be larger than N,
//                     in this case only first N points are processed.
//     N           -   points count, N >= 0
//     K           -   K >= 1 (K can be larger than N ,  such  cases  will  be
//                     correctly handled). Window width. K=1 corresponds  to
//                     identity transformation (nothing changes).
//
// Outputs:
//     X           -   array, whose first N elements were processed with SMA(K)
//
// NOTE 1: this function uses efficient in-place  algorithm  which  does not
//         allocate temporary arrays.
//
// NOTE 2: this algorithm makes only one pass through array and uses running
//         sum  to speed-up calculation of the averages. Additional measures
//         are taken to ensure that running sum on a long sequence  of  zero
//         elements will be correctly reset to zero even in the presence  of
//         round-off error.
//
// NOTE 3: this  is  unsymmetric version of the algorithm,  which  does  NOT
//         averages points after the current one. Only X[i], X[i-1], ... are
//         used when calculating new value of X[i]. We should also note that
//         this algorithm uses BOTH previous points and  current  one,  i.e.
//         new value of X[i] depends on BOTH previous point and X[i] itself.
// ALGLIB: Copyright 25.10.2011 by Sergey Bochkanov
// API: void filterlrma(real_1d_array &x, const ae_int_t n, const ae_int_t k);
// API: void filterlrma(real_1d_array &x, const ae_int_t k);
void filterlrma(RVector *x, ae_int_t n, ae_int_t k) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t m;
   ae_int_t info;
   double a;
   double b;
   double vara;
   double varb;
   double covab;
   double corrab;
   double p;
   ae_frame_make(&_frame_block);
   NewMatrix(xy, 0, 0, DT_REAL);
   NewVector(s, 0, DT_REAL);
   ae_assert(n >= 0, "FilterLRMA: N<0");
   ae_assert(x->cnt >= n, "FilterLRMA: Length(X)<N");
   ae_assert(isfinitevector(x, n), "FilterLRMA: X contains INF or NAN");
   ae_assert(k >= 1, "FilterLRMA: K<1");
// Quick exit, if necessary:
// * either N is equal to 1 (nothing to average)
// * or K is 1 (only point itself is used) or 2 (model is too simple,
//   we will always get identity transformation)
   if (n <= 1 || k <= 2) {
      ae_frame_leave();
      return;
   }
// General case: K>2, N > 1.
// We do not process points with I<2 because first two points (I=0 and I=1) will be
// left unmodified by LRMA filter in any case.
   ae_matrix_set_length(&xy, k, 2);
   ae_vector_set_length(&s, k);
   for (i = 0; i < k; i++) {
      xy.xyR[i][0] = (double)i;
      s.xR[i] = 1.0;
   }
   for (i = n - 1; i >= 2; i--) {
      m = imin2(i + 1, k);
      ae_v_move(&xy.xyR[0][1], xy.stride, &x->xR[i - m + 1], 1, m);
      lrlines(&xy, &s, m, &info, &a, &b, &vara, &varb, &covab, &corrab, &p);
      ae_assert(info == 1, "FilterLRMA: internal error");
      x->xR[i] = a + b * (m - 1);
   }
   ae_frame_leave();
}
} // end of namespace alglib_impl

namespace alglib {
void filtersma(real_1d_array &x, const ae_int_t n, const ae_int_t k) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::filtersma(ConstT(ae_vector, x), n, k);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void filtersma(real_1d_array &x, const ae_int_t k) {
   ae_int_t n = x.length();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::filtersma(ConstT(ae_vector, x), n, k);
   alglib_impl::ae_state_clear();
}
#endif

void filterema(real_1d_array &x, const ae_int_t n, const double alpha) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::filterema(ConstT(ae_vector, x), n, alpha);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void filterema(real_1d_array &x, const double alpha) {
   ae_int_t n = x.length();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::filterema(ConstT(ae_vector, x), n, alpha);
   alglib_impl::ae_state_clear();
}
#endif

void filterlrma(real_1d_array &x, const ae_int_t n, const ae_int_t k) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::filterlrma(ConstT(ae_vector, x), n, k);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void filterlrma(real_1d_array &x, const ae_int_t k) {
   ae_int_t n = x.length();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::filterlrma(ConstT(ae_vector, x), n, k);
   alglib_impl::ae_state_clear();
}
#endif
} // end of namespace alglib

// === SSA Package ===
// Depends on: (LinAlg) SVD, EVD
namespace alglib_impl {
// This function creates SSA model object.  Right after creation model is  in
// "dummy" mode - you can add data,  but   analyzing/prediction  will  return
// just zeros (it assumes that basis is empty).
//
// HOW TO USE SSA MODEL:
//
// 1. create model with ssacreate()
// 2. add data with one/many ssaaddsequence() calls
// 3. choose SSA algorithm with one of ssasetalgo...() functions:
//    * ssasetalgotopkdirect() for direct one-run analysis
//    * ssasetalgotopkrealtime() for algorithm optimized for many  subsequent
//      runs with warm-start capabilities
//    * ssasetalgoprecomputed() for user-supplied basis
// 4. set window width with ssasetwindow()
// 5. perform one of the analysis-related activities:
//    a) call ssagetbasis() to get basis
//    b) call ssaanalyzelast() ssaanalyzesequence() or ssaanalyzelastwindow()
//       to perform analysis (trend/noise separation)
//    c) call  one  of   the   forecasting   functions  (ssaforecastlast() or
//       ssaforecastsequence()) to perform prediction; alternatively, you can
//       extract linear recurrence coefficients with ssagetlrr().
//    SSA analysis will be performed during first  call  to  analysis-related
//    function. SSA model is smart enough to track all changes in the dataset
//    and  model  settings,  to  cache  previously  computed  basis  and   to
//    re-evaluate basis only when necessary.
//
// Additionally, if your setting involves constant stream  of  incoming data,
// you can perform quick update already calculated  model  with  one  of  the
// incremental   append-and-update   functions:  ssaappendpointandupdate() or
// ssaappendsequenceandupdate().
//
// NOTE: steps (2), (3), (4) can be performed in arbitrary order.
//
// Inputs:
//     none
//
// Outputs:
//     S               -   structure which stores model state
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssacreate(ssamodel &s);
void ssacreate(ssamodel *s) {
   SetObj(ssamodel, s);
// Model data, algorithms and settings
   s->nsequences = 0;
   ae_vector_set_length(&s->sequenceidx, 1);
   s->sequenceidx.xZ[0] = 0;
   s->algotype = 0;
   s->windowwidth = 1;
   s->rtpowerup = 1;
   s->arebasisandsolvervalid = false;
   s->rngseed = 1;
   s->defaultsubspaceits = 10;
   s->memorylimit = 50000000;
// Debug counters
   s->dbgcntevd = 0;
}

// This function sets window width for SSA model. You should call  it  before
// analysis phase. Default window width is 1 (not for real use).
//
// Special notes:
// * this function call can be performed at any moment before  first call  to
//   analysis-related functions
// * changing window width invalidates internally stored basis; if you change
//   window width AFTER you call analysis-related  function,  next  analysis
//   phase will require re-calculation of  the  basis  according  to  current
//   algorithm.
// * calling this function with exactly  same window width as current one has
//   no effect
// * if you specify window width larger  than any data sequence stored in the
//   model, analysis will return zero basis.
//
// Inputs:
//     S               -   SSA model created with ssacreate()
//     WindowWidth     - >= 1, new window width
//
// Outputs:
//     S               -   SSA model, updated
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssasetwindow(const ssamodel &s, const ae_int_t windowwidth);
void ssasetwindow(ssamodel *s, ae_int_t windowwidth) {
   ae_assert(windowwidth >= 1, "SSASetWindow: WindowWidth<1");
   if (windowwidth == s->windowwidth) {
      return;
   }
   s->windowwidth = windowwidth;
   s->arebasisandsolvervalid = false;
}

// This  function  sets  seed  which  is used to initialize internal RNG when
// we make pseudorandom decisions on model updates.
//
// By default, deterministic seed is used - which results in same sequence of
// pseudorandom decisions every time you run SSA model. If you  specify  non-
// deterministic seed value, then SSA  model  may  return  slightly different
// results after each run.
//
// This function can be useful when you have several SSA models updated  with
// sseappendpointandupdate() called with 0 < UpdateIts < 1 (fractional value) and
// due to performance limitations want them to perform updates  at  different
// moments.
//
// Inputs:
//     S       -   SSA model
//     Seed    -   seed:
//                 * positive values = use deterministic seed for each run of
//                   algorithms which depend on random initialization
//                 * zero or negative values = use non-deterministic seed
// ALGLIB: Copyright 03.11.2017 by Sergey Bochkanov
// API: void ssasetseed(const ssamodel &s, const ae_int_t seed);
void ssasetseed(ssamodel *s, ae_int_t seed) {
   s->rngseed = seed;
}

// This function sets length of power-up cycle for real-time algorithm.
//
// By default, this algorithm performs costly O(N*WindowWidth^2)  init  phase
// followed by full run of truncated  EVD.  However,  if  you  are  ready  to
// live with a bit lower-quality basis during first few iterations,  you  can
// split this O(N*WindowWidth^2) initialization  between  several  subsequent
// append-and-update rounds. It results in better latency of the algorithm.
//
// This function invalidates basis/solver, next analysis call will result  in
// full recalculation of everything.
//
// Inputs:
//     S       -   SSA model
//     PWLen   -   length of the power-up stage:
//                 * 0 means that no power-up is requested
//                 * 1 is the same as 0
//                 * > 1 means that delayed power-up is performed
// ALGLIB: Copyright 03.11.2017 by Sergey Bochkanov
// API: void ssasetpoweruplength(const ssamodel &s, const ae_int_t pwlen);
void ssasetpoweruplength(ssamodel *s, ae_int_t pwlen) {
   ae_assert(pwlen >= 0, "SSASetPowerUpLength: PWLen<0");
   s->rtpowerup = imax2(pwlen, 1);
   s->arebasisandsolvervalid = false;
}

// This function sets memory limit of SSA analysis.
//
// Straightforward SSA with sequence length T and window width W needs O(T*W)
// memory. It is possible to reduce memory consumption by splitting task into
// smaller chunks.
//
// Thus function allows you to specify approximate memory limit (measured  in
// double precision numbers used for buffers). Actual memory consumption will
// be comparable to the number specified by you.
//
// Default memory limit is 50.000.000 (400Mbytes) in current version.
//
// Inputs:
//     S       -   SSA model
//     MemLimit-   memory limit, >= 0. Zero value means no limit.
// ALGLIB: Copyright 20.12.2017 by Sergey Bochkanov
// API: void ssasetmemorylimit(const ssamodel &s, const ae_int_t memlimit);
void ssasetmemorylimit(ssamodel *s, ae_int_t memlimit) {
   if (memlimit < 0) {
      memlimit = 0;
   }
   s->memorylimit = memlimit;
}

// This function adds data sequence to SSA  model.  Only   single-dimensional
// sequences are supported.
//
// What is a sequences? Following definitions/requirements apply:
// * a sequence  is  an  array of  values  measured  in  subsequent,  equally
//   separated time moments (ticks).
// * you may have many sequences  in your  dataset;  say,  one  sequence  may
//   correspond to one trading session.
// * sequence length should be larger  than current  window  length  (shorter
//   sequences will be ignored during analysis).
// * analysis is performed within a  sequence; different  sequences  are  NOT
//   stacked together to produce one large contiguous stream of data.
// * analysis is performed for all  sequences at once, i.e. same set of basis
//   vectors is computed for all sequences
//
// INCREMENTAL ANALYSIS
//
// This function is non intended for  incremental updates of previously found
// SSA basis. Calling it invalidates  all previous analysis results (basis is
// reset and will be recalculated from zero during next analysis).
//
// If  you  want  to  perform   incremental/real-time  SSA,  consider   using
// following functions:
// * ssaappendpointandupdate() for appending one point
// * ssaappendsequenceandupdate() for appending new sequence
//
// Inputs:
//     S               -   SSA model created with ssacreate()
//     X               -   array[N], data, can be larger (additional values
//                         are ignored)
//     N               -   data length, can be automatically determined from
//                         the array length. N >= 0.
//
// Outputs:
//     S               -   SSA model, updated
//
// NOTE: you can clear dataset with ssacleardata()
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaaddsequence(const ssamodel &s, const real_1d_array &x, const ae_int_t n);
// API: void ssaaddsequence(const ssamodel &s, const real_1d_array &x);
void ssaaddsequence(ssamodel *s, RVector *x, ae_int_t n) {
   ae_int_t i;
   ae_int_t offs;
   ae_assert(n >= 0, "SSAAddSequence: N<0");
   ae_assert(x->cnt >= n, "SSAAddSequence: X is too short");
   ae_assert(isfinitevector(x, n), "SSAAddSequence: X contains infinities NANs");
// Invalidate model
   s->arebasisandsolvervalid = false;
// Add sequence
   ivectorgrowto(&s->sequenceidx, s->nsequences + 2);
   s->sequenceidx.xZ[s->nsequences + 1] = s->sequenceidx.xZ[s->nsequences] + n;
   rvectorgrowto(&s->sequencedata, s->sequenceidx.xZ[s->nsequences + 1]);
   offs = s->sequenceidx.xZ[s->nsequences];
   for (i = 0; i < n; i++) {
      s->sequencedata.xR[offs + i] = x->xR[i];
   }
   s->nsequences++;
}

// This function prepares batch buffer for XXT update. The idea  is  that  we
// send a stream of "XXT += u*u'" updates, and we want to package  them  into
// one big matrix update U*U', applied with SYRK() kernel, but U can  consume
// too much memory, so we want to transparently divide it  into  few  smaller
// chunks.
//
// This set of functions solves this problem:
// * UpdateXXTPrepare() prepares temporary buffers
// * UpdateXXTSend() sends next u to the buffer, possibly initiating next SYRK()
// * UpdateXXTFinalize() performs last SYRK() update
//
// Inputs:
//     S                   -   model, only fields with UX prefix are used
//     UpdateSize          -   number of updates
//     WindowWidth         -   window width, > 0
//     MemoryLimit         -   memory limit, non-positive value means no limit
//
// Outputs:
//     S                   -   UX temporaries updated
// ALGLIB: Copyright 20.12.2017 by Sergey Bochkanov
static void ssa_updatexxtprepare(ssamodel *s, ae_int_t updatesize, ae_int_t windowwidth, ae_int_t memorylimit) {
   ae_assert(windowwidth > 0, "UpdateXXTPrepare: WinW <= 0");
   s->uxbatchlimit = imax2(updatesize, 1);
   if (memorylimit > 0) {
      s->uxbatchlimit = imin2(s->uxbatchlimit, imax2(memorylimit / windowwidth, 4 * windowwidth));
   }
   s->uxbatchwidth = windowwidth;
   s->uxbatchsize = 0;
   if (s->uxbatch.cols != windowwidth) {
      ae_matrix_set_length(&s->uxbatch, 0, 0);
   }
   matrixsetlengthatleast(&s->uxbatch, s->uxbatchlimit, windowwidth);
}

// This function sends update u*u' to the batch buffer.
//
// Inputs:
//     S                   -   model, only fields with UX prefix are used
//     U                   -   WindowWidth-sized update, starts at I0
//     I0                  -   starting position for update
//
// Outputs:
//     S                   -   UX temporaries updated
//     XXT                 -   array[WindowWidth,WindowWidth], in the middle
//                             of update. All intermediate updates are
//                             applied to the upper triangle.
// ALGLIB: Copyright 20.12.2017 by Sergey Bochkanov
static void ssa_updatexxtsend(ssamodel *s, RVector *u, ae_int_t i0, RMatrix *xxt) {
   ae_assert(i0 + s->uxbatchwidth - 1 < u->cnt, "UpdateXXTSend: incorrect U size");
   ae_assert(s->uxbatchsize >= 0, "UpdateXXTSend: integrity check failure");
   ae_assert(s->uxbatchsize <= s->uxbatchlimit, "UpdateXXTSend: integrity check failure");
   ae_assert(s->uxbatchlimit >= 1, "UpdateXXTSend: integrity check failure");
// Send pending batch if full
   if (s->uxbatchsize == s->uxbatchlimit) {
      rmatrixsyrk(s->uxbatchwidth, s->uxbatchsize, 1.0, &s->uxbatch, 0, 0, 2, 1.0, xxt, 0, 0, true);
      s->uxbatchsize = 0;
   }
// Append update to batch
   ae_v_move(s->uxbatch.xyR[s->uxbatchsize], 1, &u->xR[i0], 1, s->uxbatchwidth);
   s->uxbatchsize++;
}

// This function finalizes batch buffer. Call it after the last update.
//
// Inputs:
//     S                   -   model, only fields with UX prefix are used
//
// Outputs:
//     S                   -   UX temporaries updated
//     XXT                 -   array[WindowWidth,WindowWidth], updated with
//                             all previous updates, both triangles of the
//                             symmetric matrix are present.
// ALGLIB: Copyright 20.12.2017 by Sergey Bochkanov
static void ssa_updatexxtfinalize(ssamodel *s, RMatrix *xxt) {
   ae_assert(s->uxbatchsize >= 0, "UpdateXXTFinalize: integrity check failure");
   ae_assert(s->uxbatchsize <= s->uxbatchlimit, "UpdateXXTFinalize: integrity check failure");
   ae_assert(s->uxbatchlimit >= 1, "UpdateXXTFinalize: integrity check failure");
   if (s->uxbatchsize > 0) {
      rmatrixsyrk(s->uxbatchwidth, s->uxbatchsize, 1.0, &s->uxbatch, 0, 0, 2, 1.0, &s->xxt, 0, 0, true);
      s->uxbatchsize = 0;
   }
   rmatrixenforcesymmetricity(&s->xxt, s->uxbatchwidth, true);
}

// This function extracts updates from real-time queue and  applies  them  to
// the S.XXT matrix. XXT is premultiplied by  Beta,  which  can  be  0.0  for
// initial creation, 1.0 for subsequent updates, or even within (0,1) for some
// kind of updates with decay.
//
// Inputs:
//     S                   -   model
//     Beta                - >= 0, coefficient to premultiply XXT
//     Cnt                 -   0<Cnt <= S.RTQueueCnt, number of updates to extract
//                             from the end of the queue
//
// Outputs:
//     S                   -   S.XXT updated, S.RTQueueCnt decreased
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
static void ssa_realtimedequeue(ssamodel *s, double beta, ae_int_t cnt) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t winw;
   ae_assert(cnt > 0, "SSA: RealTimeDequeue() integrity check failed / 43tdv");
   ae_assert(isfinite(beta) && beta >= 0.0, "SSA: RealTimeDequeue() integrity check failed / 5gdg6");
   ae_assert(cnt <= s->rtqueuecnt, "SSA: RealTimeDequeue() integrity check failed / 547yh");
   ae_assert(s->xxt.cols >= s->windowwidth, "SSA: RealTimeDequeue() integrity check failed / 54bf4");
   ae_assert(s->xxt.rows >= s->windowwidth, "SSA: RealTimeDequeue() integrity check failed / 9gdfn");
   winw = s->windowwidth;
// Premultiply XXT by Beta
   if (beta != 0.0) {
      for (i = 0; i < winw; i++) {
         for (j = 0; j < winw; j++) {
            s->xxt.xyR[i][j] *= beta;
         }
      }
   } else {
      for (i = 0; i < winw; i++) {
         for (j = 0; j < winw; j++) {
            s->xxt.xyR[i][j] = 0.0;
         }
      }
   }
// Dequeue
   ssa_updatexxtprepare(s, cnt, winw, s->memorylimit);
   for (i = 0; i < cnt; i++) {
      ssa_updatexxtsend(s, &s->sequencedata, s->rtqueue.xZ[s->rtqueuecnt - 1], &s->xxt);
      s->rtqueuecnt--;
   }
   ssa_updatexxtfinalize(s, &s->xxt);
}

// This function performs basis update. Either full update (recalculated from
// the very beginning) or partial update (handles append to the  end  of  the
// dataset).
//
// With AppendLen=0 this function behaves as follows:
// * if AreBasisAndSolverValid=False, then  solver  object  is  created  from
//   scratch, initial calculations are performed according  to  specific  SSA
//   algorithm being chosen. Basis/Solver validity flag is set to True,  then
//   we immediately return.
// * if AreBasisAndSolverValid=True, then nothing is done  -  we  immediately
//   return.
//
// With AppendLen>0 this function behaves as follows:
// * if AreBasisAndSolverValid=False, then exception is  generated;  you  can
//   append points only to fully constructed basis. Call this  function  with
//   zero AppendLen BEFORE append, then perform append, then call it one more
//   time with non-zero AppendLen.
// * if AreBasisAndSolverValid=True, then basis is incrementally updated.  It
//   also updates recurrence relation used for prediction. It is expected that
//   either AppendLen=1, or AppendLen=length(last_sequence). Basis update  is
//   performed with probability UpdateIts (larger-than-one values  mean  that
//   some amount of iterations is always performed).
//
// In any case, after calling this function we either:
// * have an exception
// * have completely valid basis
//
// IMPORTANT: this function expects that we do NOT call it for degenerate tasks
//            (no data). So, call it after check with HasSomethingToAnalyze()
//            returned True.
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
static void ssa_updatebasis(ssamodel *s, ae_int_t appendlen, double updateits) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t srcoffs;
   ae_int_t dstoffs;
   ae_int_t winw;
   ae_int_t windowstotal;
   ae_int_t requesttype;
   ae_int_t requestsize;
   double v;
   bool degeneraterecurrence;
   double nu2;
   ae_int_t subspaceits;
   bool needevd;
   winw = s->windowwidth;
// Critical checks
   ae_assert(appendlen >= 0, "SSA: incorrect parameters passed to UpdateBasis(), integrity check failed");
   ae_assert(!(!s->arebasisandsolvervalid && appendlen != 0), "SSA: incorrect parameters passed to UpdateBasis(), integrity check failed");
   ae_assert(!(appendlen == 0 && updateits > 0.0), "SSA: incorrect parameters passed to UpdateBasis(), integrity check failed");
// Everything is OK, nothing to do
   if (s->arebasisandsolvervalid && appendlen == 0) {
      return;
   }
// Seed RNG with fixed or random seed.
//
// RNG used when pseudorandomly deciding whether
// to re-evaluate basis or not. Sandom seed is
// important when we have several simultaneously
// calculated SSA models - we do not want them
// to be re-evaluated in same moments).
   if (!s->arebasisandsolvervalid) {
      if (s->rngseed > 0) {
         hqrndseed(s->rngseed, s->rngseed + 235, &s->rs);
      } else {
         hqrndrandomize(&s->rs);
      }
   }
// Compute XXT for algorithms which need it
   if (!s->arebasisandsolvervalid) {
      ae_assert(appendlen == 0, "SSA: integrity check failed / 34cx6");
      if (s->algotype == 2) {
      // Compute X*X^T for direct algorithm.
      // Quite straightforward, no subtle optimizations.
         matrixsetlengthatleast(&s->xxt, winw, winw);
         windowstotal = 0;
         for (i = 0; i < s->nsequences; i++) {
            windowstotal += imax2(s->sequenceidx.xZ[i + 1] - s->sequenceidx.xZ[i] - winw + 1, 0);
         }
         ae_assert(windowstotal > 0, "SSA: integrity check in UpdateBasis() failed / 76t34");
         for (i = 0; i < winw; i++) {
            for (j = 0; j < winw; j++) {
               s->xxt.xyR[i][j] = 0.0;
            }
         }
         ssa_updatexxtprepare(s, windowstotal, winw, s->memorylimit);
         for (i = 0; i < s->nsequences; i++) {
            for (j = 0; j < imax2(s->sequenceidx.xZ[i + 1] - s->sequenceidx.xZ[i] - winw + 1, 0); j++) {
               ssa_updatexxtsend(s, &s->sequencedata, s->sequenceidx.xZ[i] + j, &s->xxt);
            }
         }
         ssa_updatexxtfinalize(s, &s->xxt);
      }
      if (s->algotype == 3) {
      // Compute X*X^T for real-time algorithm:
      // * prepare queue of windows to merge into XXT
      // * shuffle queue in order to avoid time-related biases in algorithm
      // * dequeue first chunk
         matrixsetlengthatleast(&s->xxt, winw, winw);
         windowstotal = 0;
         for (i = 0; i < s->nsequences; i++) {
            windowstotal += imax2(s->sequenceidx.xZ[i + 1] - s->sequenceidx.xZ[i] - winw + 1, 0);
         }
         ae_assert(windowstotal > 0, "SSA: integrity check in UpdateBasis() failed / 76t34");
         vectorsetlengthatleast(&s->rtqueue, windowstotal);
         dstoffs = 0;
         for (i = 0; i < s->nsequences; i++) {
            for (j = 0; j < imax2(s->sequenceidx.xZ[i + 1] - s->sequenceidx.xZ[i] - winw + 1, 0); j++) {
               srcoffs = s->sequenceidx.xZ[i] + j;
               s->rtqueue.xZ[dstoffs] = srcoffs;
               dstoffs++;
            }
         }
         ae_assert(dstoffs == windowstotal, "SSA: integrity check in UpdateBasis() failed / fh45f");
         if (s->rtpowerup > 1) {
         // Shuffle queue, it helps to avoid time-related bias in algorithm
            for (i = 0; i < windowstotal; i++) {
               j = i + hqrnduniformi(&s->rs, windowstotal - i);
               swapelementsi(&s->rtqueue, i, j);
            }
         }
         s->rtqueuecnt = windowstotal;
         s->rtqueuechunk = 1;
         s->rtqueuechunk = imax2(s->rtqueuechunk, s->rtqueuecnt / s->rtpowerup);
         s->rtqueuechunk = imax2(s->rtqueuechunk, 2 * s->topk);
         ssa_realtimedequeue(s, 0.0, imin2(s->rtqueuechunk, s->rtqueuecnt));
      }
   }
// Handle possible updates for XXT:
// * check that append involves either last point of last sequence,
//   or entire last sequence
// * if last sequence is shorter than window width, perform quick exit -
//   we have nothing to update - no windows to insert into XXT
// * update XXT
   if (appendlen > 0) {
      ae_assert(s->arebasisandsolvervalid, "SSA: integrity check failed / 5gvz3");
      ae_assert(s->nsequences >= 1, "SSA: integrity check failed / 658ev");
      ae_assert(appendlen == 1 || appendlen == s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] - winw + 1, "SSA: integrity check failed / sd3g7");
      if (s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] < winw) {
      // Last sequence is too short, nothing to update
         return;
      }
      if (s->algotype == 2 || s->algotype == 3) {
         if (appendlen > 1) {
         // Long append, use GEMM for updates
            ssa_updatexxtprepare(s, appendlen, winw, s->memorylimit);
            for (j = 0; j < imax2(s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] - winw + 1, 0); j++) {
               ssa_updatexxtsend(s, &s->sequencedata, s->sequenceidx.xZ[s->nsequences - 1] + j, &s->xxt);
            }
            ssa_updatexxtfinalize(s, &s->xxt);
         } else {
         // Just one element is added, use rank-1 update
            rmatrixger(winw, winw, &s->xxt, 0, 0, 1.0, &s->sequencedata, s->sequenceidx.xZ[s->nsequences] - winw, &s->sequencedata, s->sequenceidx.xZ[s->nsequences] - winw);
         }
      }
   }
// Now, perform basis calculation - either full recalculation (AppendLen=0)
// or quick update (AppendLen>0).
   if (s->algotype == 1) {
   // Precomputed basis
      if (winw != s->precomputedwidth) {
      // Window width has changed, reset basis to zeros
         s->nbasis = 1;
         matrixsetlengthatleast(&s->basis, winw, 1);
         vectorsetlengthatleast(&s->sv, 1);
         for (i = 0; i < winw; i++) {
            s->basis.xyR[i][0] = 0.0;
         }
         s->sv.xR[0] = 0.0;
      } else {
      // OK, use precomputed basis
         s->nbasis = s->precomputednbasis;
         matrixsetlengthatleast(&s->basis, winw, s->nbasis);
         vectorsetlengthatleast(&s->sv, s->nbasis);
         for (j = 0; j < s->nbasis; j++) {
            s->sv.xR[j] = 0.0;
            for (i = 0; i < winw; i++) {
               s->basis.xyR[i][j] = s->precomputedbasis.xyR[i][j];
            }
         }
      }
      matrixsetlengthatleast(&s->basist, s->nbasis, winw);
      rmatrixtranspose(winw, s->nbasis, &s->basis, 0, 0, &s->basist, 0, 0);
   } else {
      if (s->algotype == 2) {
      // Direct top-K algorithm
      //
      // Calculate eigenvectors with SMatrixEVD(), reorder by descending
      // of magnitudes.
      //
      // Update is performed for invalid basis or for non-zero UpdateIts.
         needevd = !s->arebasisandsolvervalid;
         needevd = needevd || updateits >= 1.0;
         needevd = needevd || hqrnduniformr(&s->rs) < updateits - FloorZ(updateits);
         if (needevd) {
            s->dbgcntevd++;
            s->nbasis = imin2(winw, s->topk);
            if (!smatrixevd(&s->xxt, winw, 1, true, &s->sv, &s->basis)) {
               ae_assert(false, "SSA: SMatrixEVD failed");
            }
            for (i = 0; i < winw; i++) {
               k = winw - 1 - i;
               if (i >= k) {
                  break;
               }
               swapr(&s->sv.xR[i], &s->sv.xR[k]);
               for (j = 0; j < winw; j++) {
                  swapr(&s->basis.xyR[j][i], &s->basis.xyR[j][k]);
               }
            }
            for (i = 0; i < s->nbasis; i++) {
               s->sv.xR[i] = sqrt(rmax2(s->sv.xR[i], 0.0));
            }
            matrixsetlengthatleast(&s->basist, s->nbasis, winw);
            rmatrixtranspose(winw, s->nbasis, &s->basis, 0, 0, &s->basist, 0, 0);
         }
      } else {
         if (s->algotype == 3) {
         // Real-time top-K.
         //
         // Determine actual number of basis components, prepare subspace
         // solver (either create from scratch or reuse).
         //
         // Update is always performed for invalid basis; for a valid basis
         // it is performed with probability UpdateIts.
            if (s->rtpowerup == 1) {
               subspaceits = s->defaultsubspaceits;
            } else {
               subspaceits = 3;
            }
            if (appendlen > 0) {
               ae_assert(s->arebasisandsolvervalid, "SSA: integrity check in UpdateBasis() failed / srg6f");
               ae_assert(updateits >= 0.0, "SSA: integrity check in UpdateBasis() failed / srg4f");
               subspaceits = FloorZ(updateits);
               if (hqrnduniformr(&s->rs) < updateits - FloorZ(updateits)) {
                  subspaceits++;
               }
               ae_assert(subspaceits >= 0, "SSA: integrity check in UpdateBasis() failed / srg9f");
            }
         // Dequeue pending dataset and merge it into XXT.
         //
         // Dequeuing is done only for appends, and only when we have
         // non-empty queue.
            if (appendlen > 0 && s->rtqueuecnt > 0) {
               ssa_realtimedequeue(s, 1.0, imin2(s->rtqueuechunk, s->rtqueuecnt));
            }
         // Now, proceed to solver
            if (subspaceits > 0) {
               if (appendlen == 0) {
                  s->nbasis = imin2(winw, s->topk);
                  eigsubspacecreatebuf(winw, s->nbasis, &s->solver);
               } else {
                  eigsubspacesetwarmstart(&s->solver, true);
               }
               eigsubspacesetcond(&s->solver, 0.0, subspaceits);
            // Perform initial basis estimation
               s->dbgcntevd++;
               for (eigsubspaceoocstart(&s->solver, 0); eigsubspaceooccontinue(&s->solver); ) {
                  eigsubspaceoocgetrequestinfo(&s->solver, &requesttype, &requestsize);
                  ae_assert(requesttype == 0, "SSA: integrity check in UpdateBasis() failed / 346372");
                  rmatrixgemm(winw, requestsize, winw, 1.0, &s->xxt, 0, 0, 0, &s->solver.x, 0, 0, 0, 0.0, &s->solver.ax, 0, 0);
               }
               eigsubspaceoocstop(&s->solver, &s->sv, &s->basis, &s->solverrep);
               for (i = 0; i < s->nbasis; i++) {
                  s->sv.xR[i] = sqrt(rmax2(s->sv.xR[i], 0.0));
               }
               matrixsetlengthatleast(&s->basist, s->nbasis, winw);
               rmatrixtranspose(winw, s->nbasis, &s->basis, 0, 0, &s->basist, 0, 0);
            }
         } else {
            ae_assert(false, "SSA: integrity check in UpdateBasis() failed / dfgs34");
         }
      }
   }
// Update recurrent relation
   vectorsetlengthatleast(&s->forecasta, imax2(winw - 1, 1));
   degeneraterecurrence = false;
   if (winw > 1) {
   // Non-degenerate case
      vectorsetlengthatleast(&s->tmp0, s->nbasis);
      nu2 = 0.0;
      for (i = 0; i < s->nbasis; i++) {
         v = s->basist.xyR[i][winw - 1];
         s->tmp0.xR[i] = v;
         nu2 += v * v;
      }
      if (nu2 < 1 - 1000 * ae_machineepsilon) {
         rmatrixgemv(winw - 1, s->nbasis, 1 / (1 - nu2), &s->basist, 0, 0, 1, &s->tmp0, 0, 0.0, &s->forecasta, 0);
      } else {
         degeneraterecurrence = true;
      }
   } else {
      degeneraterecurrence = true;
   }
   if (degeneraterecurrence) {
      for (i = 0; i < imax2(winw - 1, 1); i++) {
         s->forecasta.xR[i] = 0.0;
      }
      s->forecasta.xR[imax2(winw - 1, 1) - 1] = 1.0;
   }
// Set validity flag
   s->arebasisandsolvervalid = true;
}

// An indication of whether or not the current model does not have any data which can be analyzed by the current algorithm.
// No analysis can be done in the following degenerate cases:
// *	the data set is empty,
// *	all sequences are shorter than the window length,
// *	no algorithm is specified.
// AlgLib: Copyright 30.10.2017 by Sergey Bochkanov
static bool ssa_isdegenerate(ssamodel *s) {
   if (s->algotype == 0 || s->nsequences == 0) return true;
   for (ae_int_t i = 0; i < s->nsequences; i++)
      if (s->sequenceidx.xZ[i + 1] >= s->sequenceidx.xZ[i] + s->windowwidth) return false;
   return true;
}

// This function appends single point to last data sequence stored in the SSA
// model and tries to update model in the  incremental  manner  (if  possible
// with current algorithm).
//
// If you want to add more than one point at once:
// * if you want to add M points to the same sequence, perform M-1 calls with
//   UpdateIts parameter set to 0.0, and last call with non-zero UpdateIts.
// * if you want to add new sequence, use ssaappendsequenceandupdate()
//
// Running time of this function does NOT depend on  dataset  size,  only  on
// window width and number of singular vectors. Depending on algorithm  being
// used, incremental update has complexity:
// * for top-K real time   - O(UpdateIts*K*Width^2), with fractional UpdateIts
// * for top-K direct      - O(Width^3) for any non-zero UpdateIts
// * for precomputed basis - O(1), no update is performed
//
// Inputs:
//     S               -   SSA model created with ssacreate()
//     X               -   new point
//     UpdateIts       - >= 0,  floating  point (!)  value,  desired  update
//                         frequency:
//                         * zero value means that point is  stored,  but  no
//                           update is performed
//                         * integer part of the value means  that  specified
//                           number of iterations is always performed
//                         * fractional part of  the  value  means  that  one
//                           iteration is performed with this probability.
//
//                         Recommended value: 0 < UpdateIts <= 1.  Values  larger
//                         than 1 are VERY seldom  needed.  If  your  dataset
//                         changes slowly, you can set it  to  0.1  and  skip
//                         90% of updates.
//
//                         In any case, no information is lost even with zero
//                         value of UpdateIts! It will be  incorporated  into
//                         model, sooner or later.
//
// Outputs:
//     S               -   SSA model, updated
//
// NOTE: this function uses internal  RNG  to  handle  fractional  values  of
//       UpdateIts. By default it  is  initialized  with  fixed  seed  during
//       initial calculation of basis. Thus subsequent calls to this function
//       will result in the same sequence of pseudorandom decisions.
//
//       However, if  you  have  several  SSA  models  which  are  calculated
//       simultaneously, and if you want to reduce computational  bottlenecks
//       by performing random updates at random moments, then fixed  seed  is
//       not an option - all updates will fire at same moments.
//
//       You may change it with ssasetseed() function.
//
// NOTE: this function throws an exception if called for empty dataset (there
//       is no "last" sequence to modify).
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaappendpointandupdate(const ssamodel &s, const double x, const double updateits);
void ssaappendpointandupdate(ssamodel *s, double x, double updateits) {
   ae_assert(isfinite(x), "SSAAppendPointAndUpdate: X is not finite");
   ae_assert(isfinite(updateits), "SSAAppendPointAndUpdate: UpdateIts is not finite");
   ae_assert(updateits >= 0.0, "SSAAppendPointAndUpdate: UpdateIts<0");
   ae_assert(s->nsequences > 0, "SSAAppendPointAndUpdate: dataset is empty, no sequence to modify");
// Append point to dataset
   rvectorgrowto(&s->sequencedata, s->sequenceidx.xZ[s->nsequences] + 1);
   s->sequencedata.xR[s->sequenceidx.xZ[s->nsequences]] = x;
   s->sequenceidx.xZ[s->nsequences]++;
// Do we have something to analyze? If no, invalidate basis
// (just to be sure) and exit.
   if (ssa_isdegenerate(s)) {
      s->arebasisandsolvervalid = false;
      return;
   }
// Well, we have data to analyze and algorithm set, but basis is
// invalid. Let's calculate it from scratch and exit.
   if (!s->arebasisandsolvervalid) {
      ssa_updatebasis(s, 0, 0.0);
      return;
   }
// Update already computed basis
   ssa_updatebasis(s, 1, updateits);
}

// This function appends new sequence to dataset stored in the SSA  model and
// tries to update model in the incremental manner (if possible  with current
// algorithm).
//
// Notes:
// * if you want to add M sequences at once, perform M-1 calls with UpdateIts
//   parameter set to 0.0, and last call with non-zero UpdateIts.
// * if you want to add just one point, use ssaappendpointandupdate()
//
// Running time of this function does NOT depend on  dataset  size,  only  on
// sequence length, window width and number of singular vectors. Depending on
// algorithm being used, incremental update has complexity:
// * for top-K real time   - O(UpdateIts*K*Width^2+(NTicks-Width)*Width^2)
// * for top-K direct      - O(Width^3+(NTicks-Width)*Width^2)
// * for precomputed basis - O(1), no update is performed
//
// Inputs:
//     S               -   SSA model created with ssacreate()
//     X               -   new sequence, array[NTicks] or larget
//     NTicks          - >= 1, number of ticks in the sequence
//     UpdateIts       - >= 0,  floating  point (!)  value,  desired  update
//                         frequency:
//                         * zero value means that point is  stored,  but  no
//                           update is performed
//                         * integer part of the value means  that  specified
//                           number of iterations is always performed
//                         * fractional part of  the  value  means  that  one
//                           iteration is performed with this probability.
//
//                         Recommended value: 0 < UpdateIts <= 1.  Values  larger
//                         than 1 are VERY seldom  needed.  If  your  dataset
//                         changes slowly, you can set it  to  0.1  and  skip
//                         90% of updates.
//
//                         In any case, no information is lost even with zero
//                         value of UpdateIts! It will be  incorporated  into
//                         model, sooner or later.
//
// Outputs:
//     S               -   SSA model, updated
//
// NOTE: this function uses internal  RNG  to  handle  fractional  values  of
//       UpdateIts. By default it  is  initialized  with  fixed  seed  during
//       initial calculation of basis. Thus subsequent calls to this function
//       will result in the same sequence of pseudorandom decisions.
//
//       However, if  you  have  several  SSA  models  which  are  calculated
//       simultaneously, and if you want to reduce computational  bottlenecks
//       by performing random updates at random moments, then fixed  seed  is
//       not an option - all updates will fire at same moments.
//
//       You may change it with ssasetseed() function.
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaappendsequenceandupdate(const ssamodel &s, const real_1d_array &x, const ae_int_t nticks, const double updateits);
// API: void ssaappendsequenceandupdate(const ssamodel &s, const real_1d_array &x, const double updateits);
void ssaappendsequenceandupdate(ssamodel *s, RVector *x, ae_int_t nticks, double updateits) {
   ae_int_t i;
   ae_int_t offs;
   ae_assert(nticks >= 0, "SSAAppendSequenceAndUpdate: NTicks<0");
   ae_assert(x->cnt >= nticks, "SSAAppendSequenceAndUpdate: X is too short");
   ae_assert(isfinitevector(x, nticks), "SSAAppendSequenceAndUpdate: X contains infinities NANs");
// Add sequence
   ivectorgrowto(&s->sequenceidx, s->nsequences + 2);
   s->sequenceidx.xZ[s->nsequences + 1] = s->sequenceidx.xZ[s->nsequences] + nticks;
   rvectorgrowto(&s->sequencedata, s->sequenceidx.xZ[s->nsequences + 1]);
   offs = s->sequenceidx.xZ[s->nsequences];
   for (i = 0; i < nticks; i++) {
      s->sequencedata.xR[offs + i] = x->xR[i];
   }
   s->nsequences++;
// Do we have something to analyze? If no, invalidate basis
// (just to be sure) and exit.
   if (ssa_isdegenerate(s)) {
      s->arebasisandsolvervalid = false;
      return;
   }
// Well, we have data to analyze and algorithm set, but basis is
// invalid. Let's calculate it from scratch and exit.
   if (!s->arebasisandsolvervalid) {
      ssa_updatebasis(s, 0, 0.0);
      return;
   }
// Update already computed basis
   if (nticks >= s->windowwidth) {
      ssa_updatebasis(s, nticks - s->windowwidth + 1, updateits);
   }
}

// This  function sets SSA algorithm to "precomputed vectors" algorithm.
//
// This  algorithm  uses  precomputed  set  of  orthonormal  (orthogonal  AND
// normalized) basis vectors supplied by user. Thus, basis calculation  phase
// is not performed -  we  already  have  our  basis  -  and  only  analysis/
// forecasting phase requires actual calculations.
//
// This algorithm may handle "append" requests which add just  one/few  ticks
// to the end of the last sequence in O(1) time.
//
// NOTE: this algorithm accepts both basis and window  width,  because  these
//       two parameters are naturally aligned.  Calling  this  function  sets
//       window width; if you call ssasetwindow() with  other  window  width,
//       then during analysis stage algorithm will detect conflict and  reset
//       to zero basis.
//
// Inputs:
//     S               -   SSA model
//     A               -   array[WindowWidth,NBasis], orthonormalized  basis;
//                         this function does NOT control  orthogonality  and
//                         does NOT perform any kind of  renormalization.  It
//                         is your responsibility to provide it with  correct
//                         basis.
//     WindowWidth     -   window width, >= 1
//     NBasis          -   number of basis vectors, 1 <= NBasis <= WindowWidth
//
// Outputs:
//     S               -   updated model
//
// NOTE: calling this function invalidates basis in all cases.
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssasetalgoprecomputed(const ssamodel &s, const real_2d_array &a, const ae_int_t windowwidth, const ae_int_t nbasis);
// API: void ssasetalgoprecomputed(const ssamodel &s, const real_2d_array &a);
void ssasetalgoprecomputed(ssamodel *s, RMatrix *a, ae_int_t windowwidth, ae_int_t nbasis) {
   ae_int_t i;
   ae_int_t j;
   ae_assert(windowwidth >= 1, "SSASetAlgoPrecomputed: WindowWidth<1");
   ae_assert(nbasis >= 1, "SSASetAlgoPrecomputed: NBasis<1");
   ae_assert(nbasis <= windowwidth, "SSASetAlgoPrecomputed: NBasis>WindowWidth");
   ae_assert(a->rows >= windowwidth, "SSASetAlgoPrecomputed: Rows(A)<WindowWidth");
   ae_assert(a->cols >= nbasis, "SSASetAlgoPrecomputed: Rows(A)<NBasis");
   ae_assert(apservisfinitematrix(a, windowwidth, nbasis), "SSASetAlgoPrecomputed: Rows(A)<NBasis");
   s->algotype = 1;
   s->precomputedwidth = windowwidth;
   s->precomputednbasis = nbasis;
   s->windowwidth = windowwidth;
   matrixsetlengthatleast(&s->precomputedbasis, windowwidth, nbasis);
   for (i = 0; i < windowwidth; i++) {
      for (j = 0; j < nbasis; j++) {
         s->precomputedbasis.xyR[i][j] = a->xyR[i][j];
      }
   }
   s->arebasisandsolvervalid = false;
}

// This  function sets SSA algorithm to "direct top-K" algorithm.
//
// "Direct top-K" algorithm performs full  SVD  of  the  N*WINDOW  trajectory
// matrix (hence its name - direct solver  is  used),  then  extracts  top  K
// components. Overall running time is O(N*WINDOW^2), where N is a number  of
// ticks in the dataset, WINDOW is window width.
//
// This algorithm may handle "append" requests which add just  one/few  ticks
// to the end of the last sequence in O(WINDOW^3) time,  which  is  ~N/WINDOW
// times faster than re-computing everything from scratch.
//
// Inputs:
//     S               -   SSA model
//     TopK            -   number of components to analyze; TopK >= 1.
//
// Outputs:
//     S               -   updated model
//
// NOTE: TopK > WindowWidth is silently decreased to WindowWidth during analysis
//       phase
//
// NOTE: calling this function invalidates basis, except  for  the  situation
//       when this algorithm was already set with same parameters.
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssasetalgotopkdirect(const ssamodel &s, const ae_int_t topk);
void ssasetalgotopkdirect(ssamodel *s, ae_int_t topk) {
   ae_assert(topk >= 1, "SSASetAlgoTopKDirect: TopK<1");
// Ignore calls which change nothing
   if (s->algotype == 2 && s->topk == topk) {
      return;
   }
// Update settings, invalidate model
   s->algotype = 2;
   s->topk = topk;
   s->arebasisandsolvervalid = false;
}

// This function sets SSA algorithm to "top-K real time algorithm". This algo
// extracts K components with largest singular values.
//
// It  is  real-time  version  of  top-K  algorithm  which  is  optimized for
// incremental processing and  fast  start-up. Internally  it  uses  subspace
// eigensolver for truncated SVD. It results  in  ability  to  perform  quick
// updates of the basis when only a few points/sequences is added to dataset.
//
// Performance profile of the algorithm is given below:
// * O(K*WindowWidth^2) running time for incremental update  of  the  dataset
//   with one of the "append-and-update" functions (ssaappendpointandupdate()
//   or ssaappendsequenceandupdate()).
// * O(N*WindowWidth^2) running time for initial basis evaluation (N=size  of
//   dataset)
// * ability  to  split  costly  initialization  across  several  incremental
//   updates of the basis (so called "Power-Up" functionality,  activated  by
//   ssasetpoweruplength() function)
//
// Inputs:
//     S               -   SSA model
//     TopK            -   number of components to analyze; TopK >= 1.
//
// Outputs:
//     S               -   updated model
//
// NOTE: this  algorithm  is  optimized  for  large-scale  tasks  with  large
//       datasets. On toy problems with just  5-10 points it can return basis
//       which is slightly different from that returned by  direct  algorithm
//       (ssasetalgotopkdirect() function). However, the  difference  becomes
//       negligible as dataset grows.
//
// NOTE: TopK > WindowWidth is silently decreased to WindowWidth during analysis
//       phase
//
// NOTE: calling this function invalidates basis, except  for  the  situation
//       when this algorithm was already set with same parameters.
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssasetalgotopkrealtime(const ssamodel &s, const ae_int_t topk);
void ssasetalgotopkrealtime(ssamodel *s, ae_int_t topk) {
   ae_assert(topk >= 1, "SSASetAlgoTopKRealTime: TopK<1");
// Ignore calls which change nothing
   if (s->algotype == 3 && s->topk == topk) {
      return;
   }
// Update settings, invalidate model
   s->algotype = 3;
   s->topk = topk;
   s->arebasisandsolvervalid = false;
}

// This function clears all data stored in the  model  and  invalidates  all
// basis components found so far.
//
// Inputs:
//     S               -   SSA model created with ssacreate()
//
// Outputs:
//     S               -   SSA model, updated
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssacleardata(const ssamodel &s);
void ssacleardata(ssamodel *s) {
   s->nsequences = 0;
   s->arebasisandsolvervalid = false;
}

// This function executes SSA on internally stored dataset and returns  basis
// found by current method.
//
// Inputs:
//     S               -   SSA model
//
// Outputs:
//     A               -   array[WindowWidth,NBasis],   basis;  vectors  are
//                         stored in matrix columns, by descreasing variance
//     SV              -   array[NBasis]:
//                         * zeros - for model initialized with SSASetAlgoPrecomputed()
//                         * singular values - for other algorithms
//     WindowWidth     -   current window
//     NBasis          -   basis size
//
// CACHING/REUSE OF THE BASIS
//
// Caching/reuse of previous results is performed:
// * first call performs full run of SSA; basis is stored in the cache
// * subsequent calls reuse previously cached basis
// * if you call any function which changes model properties (window  length,
//   algorithm, dataset), internal basis will be invalidated.
// * the only calls which do NOT invalidate basis are listed below:
//   a) ssasetwindow() with same window length
//   b) ssaappendpointandupdate()
//   c) ssaappendsequenceandupdate()
//   d) ssasetalgotopk...() with exactly same K
//   Calling these functions will result in reuse of previously found basis.
//
// HANDLING OF DEGENERATE CASES
//
// Calling  this  function  in  degenerate  cases  (no  data  or all data are
// shorter than window size; no algorithm is specified)  returns  basis  with
// just one zero vector.
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssagetbasis(const ssamodel &s, real_2d_array &a, real_1d_array &sv, ae_int_t &windowwidth, ae_int_t &nbasis);
void ssagetbasis(ssamodel *s, RMatrix *a, RVector *sv, ae_int_t *windowwidth, ae_int_t *nbasis) {
   ae_int_t i;
   SetMatrix(a);
   SetVector(sv);
   *windowwidth = 0;
   *nbasis = 0;
// Is it degenerate case?
   if (ssa_isdegenerate(s)) {
      *windowwidth = s->windowwidth;
      *nbasis = 1;
      ae_matrix_set_length(a, *windowwidth, 1);
      for (i = 0; i < *windowwidth; i++) {
         a->xyR[i][0] = 0.0;
      }
      ae_vector_set_length(sv, 1);
      sv->xR[0] = 0.0;
      return;
   }
// Update basis.
//
// It will take care of basis validity flags. AppendLen=0 which means
// that we perform initial basis evaluation.
   ssa_updatebasis(s, 0, 0.0);
// Output
   ae_assert(s->nbasis > 0, "SSAGetBasis: integrity check failed");
   ae_assert(s->windowwidth > 0, "SSAGetBasis: integrity check failed");
   *nbasis = s->nbasis;
   *windowwidth = s->windowwidth;
   ae_matrix_set_length(a, *windowwidth, *nbasis);
   rmatrixcopy(*windowwidth, *nbasis, &s->basis, 0, 0, a, 0, 0);
   ae_vector_set_length(sv, *nbasis);
   for (i = 0; i < *nbasis; i++) {
      sv->xR[i] = s->sv.xR[i];
   }
}

// This function returns linear recurrence relation (LRR) coefficients  found
// by current SSA algorithm.
//
// Inputs:
//     S               -   SSA model
//
// Outputs:
//     A               -   array[WindowWidth-1]. Coefficients  of  the
//                         linear recurrence of the form:
//                         X[W-1] = X[W-2]*A[W-2] + X[W-3]*A[W-3] + ... + X[0]*A[0].
//                         Empty array for WindowWidth=1.
//     WindowWidth     -   current window width
//
// CACHING/REUSE OF THE BASIS
//
// Caching/reuse of previous results is performed:
// * first call performs full run of SSA; basis is stored in the cache
// * subsequent calls reuse previously cached basis
// * if you call any function which changes model properties (window  length,
//   algorithm, dataset), internal basis will be invalidated.
// * the only calls which do NOT invalidate basis are listed below:
//   a) ssasetwindow() with same window length
//   b) ssaappendpointandupdate()
//   c) ssaappendsequenceandupdate()
//   d) ssasetalgotopk...() with exactly same K
//   Calling these functions will result in reuse of previously found basis.
//
// HANDLING OF DEGENERATE CASES
//
// Calling  this  function  in  degenerate  cases  (no  data  or all data are
// shorter than window size; no algorithm is specified) returns zeros.
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssagetlrr(const ssamodel &s, real_1d_array &a, ae_int_t &windowwidth);
void ssagetlrr(ssamodel *s, RVector *a, ae_int_t *windowwidth) {
   ae_int_t i;
   SetVector(a);
   *windowwidth = 0;
   ae_assert(s->windowwidth > 0, "SSAGetLRR: integrity check failed");
// Is it degenerate case?
   if (ssa_isdegenerate(s)) {
      *windowwidth = s->windowwidth;
      ae_vector_set_length(a, *windowwidth - 1);
      for (i = 0; i < *windowwidth - 1; i++) {
         a->xR[i] = 0.0;
      }
      return;
   }
// Update basis.
//
// It will take care of basis validity flags. AppendLen=0 which means
// that we perform initial basis evaluation.
   ssa_updatebasis(s, 0, 0.0);
// Output
   *windowwidth = s->windowwidth;
   ae_vector_set_length(a, *windowwidth - 1);
   for (i = 0; i < *windowwidth - 1; i++) {
      a->xR[i] = s->forecasta.xR[i];
   }
}

// This function checks whether I-th sequence is big enough for analysis or not.
//
// I=-1 is used to denote last sequence (for NSequences=0)
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
static bool ssa_issequencebigenough(ssamodel *s, ae_int_t i) {
   bool result;
   ae_assert(i >= -1 && i < s->nsequences, "Assertion failed");
   result = false;
   if (s->nsequences == 0) {
      return result;
   }
   if (i < 0) {
      i = s->nsequences - 1;
   }
   result = s->sequenceidx.xZ[i + 1] - s->sequenceidx.xZ[i] >= s->windowwidth;
   return result;
}

// This  function  executes  SSA  on  internally  stored  dataset and returns
// analysis  for  the  last  window  of  the  last sequence. Such analysis is
// an lightweight alternative for full scale reconstruction (see below).
//
// Typical use case for this function is  real-time  setting,  when  you  are
// interested in quick-and-dirty (very quick and very  dirty)  processing  of
// just a few last ticks of the trend.
//
// IMPORTANT: full  scale  SSA  involves  analysis  of  the  ENTIRE  dataset,
//            with reconstruction being done for  all  positions  of  sliding
//            window with subsequent hankelization  (diagonal  averaging)  of
//            the resulting matrix.
//
//            Such analysis requires O((DataLen-Window)*Window*NBasis)  FLOPs
//            and can be quite costly. However, it has  nice  noise-canceling
//            effects due to averaging.
//
//            This function performs REDUCED analysis of the last window.  It
//            is much faster - just O(Window*NBasis),  but  its  results  are
//            DIFFERENT from that of ssaanalyzelast(). In  particular,  first
//            few points of the trend are much more prone to noise.
//
// Inputs:
//     S               -   SSA model
//
// Outputs:
//     Trend           -   array[WindowSize], reconstructed trend line
//     Noise           -   array[WindowSize], the rest of the signal;
//                         it holds that ActualData = Trend+Noise.
//     NTicks          -   current WindowSize
//
// CACHING/REUSE OF THE BASIS
//
// Caching/reuse of previous results is performed:
// * first call performs full run of SSA; basis is stored in the cache
// * subsequent calls reuse previously cached basis
// * if you call any function which changes model properties (window  length,
//   algorithm, dataset), internal basis will be invalidated.
// * the only calls which do NOT invalidate basis are listed below:
//   a) ssasetwindow() with same window length
//   b) ssaappendpointandupdate()
//   c) ssaappendsequenceandupdate()
//   d) ssasetalgotopk...() with exactly same K
//   Calling these functions will result in reuse of previously found basis.
//
// In  any  case,  only  basis  is  reused. Reconstruction is performed  from
// scratch every time you call this function.
//
// HANDLING OF DEGENERATE CASES
//
// Following degenerate cases may happen:
// * dataset is empty (no analysis can be done)
// * all sequences are shorter than the window length,no analysis can be done
// * no algorithm is specified (no analysis can be done)
// * last sequence is shorter than the window length (analysis can  be  done,
//   but we can not perform reconstruction on the last sequence)
//
// Calling this function in degenerate cases returns following result:
// * in any case, WindowWidth ticks is returned
// * trend is assumed to be zero
// * noise is initialized by the last sequence; if last sequence  is  shorter
//   than the window size, it is moved to  the  end  of  the  array, and  the
//   beginning of the noise array is filled by zeros
//
// No analysis is performed in degenerate cases (we immediately return  dummy
// values, no basis is constructed).
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaanalyzelastwindow(const ssamodel &s, real_1d_array &trend, real_1d_array &noise, ae_int_t &nticks);
void ssaanalyzelastwindow(ssamodel *s, RVector *trend, RVector *noise, ae_int_t *nticks) {
   ae_int_t i;
   ae_int_t offs;
   ae_int_t cnt;
   SetVector(trend);
   SetVector(noise);
   *nticks = 0;
// Init
   *nticks = s->windowwidth;
   ae_vector_set_length(trend, s->windowwidth);
   ae_vector_set_length(noise, s->windowwidth);
// Is it degenerate case?
   if (ssa_isdegenerate(s) || !ssa_issequencebigenough(s, -1)) {
      for (i = 0; i < *nticks; i++) {
         trend->xR[i] = 0.0;
         noise->xR[i] = 0.0;
      }
      if (s->nsequences >= 1) {
         cnt = imin2(s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1], *nticks);
         offs = s->sequenceidx.xZ[s->nsequences] - cnt;
         for (i = 0; i < cnt; i++) {
            noise->xR[*nticks - cnt + i] = s->sequencedata.xR[offs + i];
         }
      }
      return;
   }
// Update basis.
//
// It will take care of basis validity flags. AppendLen=0 which means
// that we perform initial basis evaluation.
   ssa_updatebasis(s, 0, 0.0);
// Perform analysis of the last window
   ae_assert(s->sequenceidx.xZ[s->nsequences] - s->windowwidth >= 0, "SSAAnalyzeLastWindow: integrity check failed");
   vectorsetlengthatleast(&s->tmp0, s->nbasis);
   rmatrixgemv(s->nbasis, s->windowwidth, 1.0, &s->basist, 0, 0, 0, &s->sequencedata, s->sequenceidx.xZ[s->nsequences] - s->windowwidth, 0.0, &s->tmp0, 0);
   rmatrixgemv(s->windowwidth, s->nbasis, 1.0, &s->basis, 0, 0, 0, &s->tmp0, 0, 0.0, trend, 0);
   offs = s->sequenceidx.xZ[s->nsequences] - s->windowwidth;
   cnt = s->windowwidth;
   for (i = 0; i < cnt; i++) {
      noise->xR[i] = s->sequencedata.xR[offs + i] - trend->xR[i];
   }
}

// This function performs analysis using current basis. It assumes and checks
// that validity flag AreBasisAndSolverValid is set.
//
// Inputs:
//     S                   -   model
//     Data                -   array which holds data in elements [I0,I1):
//                             * right bound is not included.
//                             * I1-I0 >= WindowWidth (assertion is performed).
//     Trend               -   preallocated output array, large enough
//     Noise               -   preallocated output array, large enough
//     Offs                -   offset in Trend/Noise where result is stored;
//                             I1-I0 elements are written starting at offset
//                             Offs.
//
// Outputs:
//     Trend, Noise - processing results
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
static void ssa_analyzesequence(ssamodel *s, RVector *data, ae_int_t i0, ae_int_t i1, RVector *trend, RVector *noise, ae_int_t offs) {
   ae_int_t winw;
   ae_int_t nwindows;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t cnt;
   ae_int_t batchstart;
   ae_int_t batchlimit;
   ae_int_t batchsize;
   ae_assert(s->arebasisandsolvervalid, "AnalyzeSequence: integrity check failed / d84sz0");
   ae_assert(i1 - i0 >= s->windowwidth, "AnalyzeSequence: integrity check failed / d84sz1");
   ae_assert(s->nbasis >= 1, "AnalyzeSequence: integrity check failed / d84sz2");
   nwindows = i1 - i0 - s->windowwidth + 1;
   winw = s->windowwidth;
   batchlimit = imax2(nwindows, 1);
   if (s->memorylimit > 0) {
      batchlimit = imin2(batchlimit, imax2(s->memorylimit / winw, 4 * winw));
   }
// Zero-initialize trend and counts
   cnt = i1 - i0;
   vectorsetlengthatleast(&s->aseqcounts, cnt);
   for (i = 0; i < cnt; i++) {
      s->aseqcounts.xZ[i] = 0;
      trend->xR[offs + i] = 0.0;
   }
// Reset temporaries if algorithm settings changed since last round
   if (s->aseqtrajectory.cols != winw) {
      ae_matrix_set_length(&s->aseqtrajectory, 0, 0);
   }
   if (s->aseqtbproduct.cols != s->nbasis) {
      ae_matrix_set_length(&s->aseqtbproduct, 0, 0);
   }
// Perform batch processing
   matrixsetlengthatleast(&s->aseqtrajectory, batchlimit, winw);
   matrixsetlengthatleast(&s->aseqtbproduct, batchlimit, s->nbasis);
   batchsize = 0;
   batchstart = offs;
   for (i = 0; i < nwindows; i++) {
   // Enqueue next row of trajectory matrix
      if (batchsize == 0) {
         batchstart = i;
      }
      for (j = 0; j < winw; j++) {
         s->aseqtrajectory.xyR[batchsize][j] = data->xR[i0 + i + j];
      }
      batchsize++;
   // Process batch
      if (batchsize == batchlimit || i == nwindows - 1) {
      // Project onto basis
         rmatrixgemm(batchsize, s->nbasis, winw, 1.0, &s->aseqtrajectory, 0, 0, 0, &s->basist, 0, 0, 1, 0.0, &s->aseqtbproduct, 0, 0);
         rmatrixgemm(batchsize, winw, s->nbasis, 1.0, &s->aseqtbproduct, 0, 0, 0, &s->basist, 0, 0, 0, 0.0, &s->aseqtrajectory, 0, 0);
      // Hankelize
         for (k = 0; k < batchsize; k++) {
            for (j = 0; j < winw; j++) {
               trend->xR[offs + batchstart + k + j] += s->aseqtrajectory.xyR[k][j];
               s->aseqcounts.xZ[batchstart + k + j]++;
            }
         }
      // Reset batch size
         batchsize = 0;
      }
   }
   for (i = 0; i < cnt; i++) {
      trend->xR[offs + i] /= s->aseqcounts.xZ[i];
   }
// Output noise
   for (i = 0; i < cnt; i++) {
      noise->xR[offs + i] = data->xR[i0 + i] - trend->xR[offs + i];
   }
}

// This function:
// * builds SSA basis using internally stored (entire) dataset
// * returns reconstruction for the last NTicks of the last sequence
//
// If you want to analyze some other sequence, use ssaanalyzesequence().
//
// Reconstruction phase involves  generation  of  NTicks-WindowWidth  sliding
// windows, their decomposition using empirical orthogonal functions found by
// SSA, followed by averaging of each data point across  several  overlapping
// windows. Thus, every point in the output trend is reconstructed  using  up
// to WindowWidth overlapping  windows  (WindowWidth windows exactly  in  the
// inner points, just one window at the extremal points).
//
// IMPORTANT: due to averaging this function returns  different  results  for
//            different values of NTicks. It is expected and not a bug.
//
//            For example:
//            * Trend[NTicks-1] is always same because it is not averaged  in
//              any case (same applies to Trend[0]).
//            * Trend[NTicks-2] has different values  for  NTicks=WindowWidth
//              and NTicks=WindowWidth+1 because former  case  means that  no
//              averaging is performed, and latter  case means that averaging
//              using two sliding windows  is  performed.  Larger  values  of
//              NTicks produce same results as NTicks=WindowWidth+1.
//            * ...and so on...
//
// PERFORMANCE: this  function has O((NTicks-WindowWidth)*WindowWidth*NBasis)
//              running time. If you work  in  time-constrained  setting  and
//              have to analyze just a few last ticks, choosing NTicks  equal
//              to WindowWidth+SmoothingLen, with SmoothingLen=1...WindowWidth
//              will result in good compromise between noise cancellation and
//              analysis speed.
//
// Inputs:
//     S               -   SSA model
//     NTicks          -   number of ticks to analyze, Nticks >= 1.
//                         * special case of NTicks <= WindowWidth  is  handled
//                           by analyzing last window and  returning   NTicks
//                           last ticks.
//                         * special case NTicks > LastSequenceLen  is  handled
//                           by prepending result with NTicks-LastSequenceLen
//                           zeros.
//
// Outputs:
//     Trend           -   array[NTicks], reconstructed trend line
//     Noise           -   array[NTicks], the rest of the signal;
//                         it holds that ActualData = Trend+Noise.
//
// CACHING/REUSE OF THE BASIS
//
// Caching/reuse of previous results is performed:
// * first call performs full run of SSA; basis is stored in the cache
// * subsequent calls reuse previously cached basis
// * if you call any function which changes model properties (window  length,
//   algorithm, dataset), internal basis will be invalidated.
// * the only calls which do NOT invalidate basis are listed below:
//   a) ssasetwindow() with same window length
//   b) ssaappendpointandupdate()
//   c) ssaappendsequenceandupdate()
//   d) ssasetalgotopk...() with exactly same K
//   Calling these functions will result in reuse of previously found basis.
//
// In  any  case,  only  basis  is  reused. Reconstruction is performed  from
// scratch every time you call this function.
//
// HANDLING OF DEGENERATE CASES
//
// Following degenerate cases may happen:
// * dataset is empty (no analysis can be done)
// * all sequences are shorter than the window length,no analysis can be done
// * no algorithm is specified (no analysis can be done)
// * last sequence is shorter than the window length (analysis  can  be done,
//   but we can not perform reconstruction on the last sequence)
//
// Calling this function in degenerate cases returns following result:
// * in any case, NTicks ticks is returned
// * trend is assumed to be zero
// * noise is initialized by the last sequence; if last sequence  is  shorter
//   than the window size, it is moved to  the  end  of  the  array, and  the
//   beginning of the noise array is filled by zeros
//
// No analysis is performed in degenerate cases (we immediately return  dummy
// values, no basis is constructed).
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaanalyzelast(const ssamodel &s, const ae_int_t nticks, real_1d_array &trend, real_1d_array &noise);
void ssaanalyzelast(ssamodel *s, ae_int_t nticks, RVector *trend, RVector *noise) {
   ae_int_t i;
   ae_int_t offs;
   ae_int_t cnt;
   ae_int_t cntzeros;
   SetVector(trend);
   SetVector(noise);
   ae_assert(nticks >= 1, "SSAAnalyzeLast: NTicks<1");
// Init
   ae_vector_set_length(trend, nticks);
   ae_vector_set_length(noise, nticks);
// Is it degenerate case?
   if (ssa_isdegenerate(s) || !ssa_issequencebigenough(s, -1)) {
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = 0.0;
         noise->xR[i] = 0.0;
      }
      if (s->nsequences >= 1) {
         cnt = imin2(s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1], nticks);
         offs = s->sequenceidx.xZ[s->nsequences] - cnt;
         for (i = 0; i < cnt; i++) {
            noise->xR[nticks - cnt + i] = s->sequencedata.xR[offs + i];
         }
      }
      return;
   }
// Fast exit: NTicks <= WindowWidth, just last window is analyzed
   if (nticks <= s->windowwidth) {
      ssaanalyzelastwindow(s, &s->alongtrend, &s->alongnoise, &cnt);
      offs = s->windowwidth - nticks;
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = s->alongtrend.xR[offs + i];
         noise->xR[i] = s->alongnoise.xR[offs + i];
      }
      return;
   }
// Update basis.
//
// It will take care of basis validity flags. AppendLen=0 which means
// that we perform initial basis evaluation.
   ssa_updatebasis(s, 0, 0.0);
// Perform analysis:
// * prepend max(NTicks-LastSequenceLength,0) zeros to the beginning
//   of array
// * analyze the rest with AnalyzeSequence() which assumes that we
//   already have basis
   ae_assert(s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] >= s->windowwidth, "SSAAnalyzeLast: integrity check failed / 23vd4");
   cntzeros = imax2(nticks - (s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1]), 0);
   for (i = 0; i < cntzeros; i++) {
      trend->xR[i] = 0.0;
      noise->xR[i] = 0.0;
   }
   cnt = imin2(nticks, s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1]);
   ssa_analyzesequence(s, &s->sequencedata, s->sequenceidx.xZ[s->nsequences] - cnt, s->sequenceidx.xZ[s->nsequences], trend, noise, cntzeros);
}

// This function:
// * builds SSA basis using internally stored (entire) dataset
// * returns reconstruction for the sequence being passed to this function
//
// If  you  want  to  analyze  last  sequence  stored  in   the   model,  use
// ssaanalyzelast().
//
// Reconstruction phase involves  generation  of  NTicks-WindowWidth  sliding
// windows, their decomposition using empirical orthogonal functions found by
// SSA, followed by averaging of each data point across  several  overlapping
// windows. Thus, every point in the output trend is reconstructed  using  up
// to WindowWidth overlapping  windows  (WindowWidth windows exactly  in  the
// inner points, just one window at the extremal points).
//
// PERFORMANCE: this  function has O((NTicks-WindowWidth)*WindowWidth*NBasis)
//              running time. If you work  in  time-constrained  setting  and
//              have to analyze just a few last ticks, choosing NTicks  equal
//              to WindowWidth+SmoothingLen, with SmoothingLen=1...WindowWidth
//              will result in good compromise between noise cancellation and
//              analysis speed.
//
// Inputs:
//     S               -   SSA model
//     Data            -   array[NTicks], can be larger (only NTicks  leading
//                         elements will be used)
//     NTicks          -   number of ticks to analyze, Nticks >= 1.
//                         * special case of NTicks < WindowWidth  is   handled
//                           by returning zeros as trend, and signal as noise
//
// Outputs:
//     Trend           -   array[NTicks], reconstructed trend line
//     Noise           -   array[NTicks], the rest of the signal;
//                         it holds that ActualData = Trend+Noise.
//
// CACHING/REUSE OF THE BASIS
//
// Caching/reuse of previous results is performed:
// * first call performs full run of SSA; basis is stored in the cache
// * subsequent calls reuse previously cached basis
// * if you call any function which changes model properties (window  length,
//   algorithm, dataset), internal basis will be invalidated.
// * the only calls which do NOT invalidate basis are listed below:
//   a) ssasetwindow() with same window length
//   b) ssaappendpointandupdate()
//   c) ssaappendsequenceandupdate()
//   d) ssasetalgotopk...() with exactly same K
//   Calling these functions will result in reuse of previously found basis.
//
// In  any  case,  only  basis  is  reused. Reconstruction is performed  from
// scratch every time you call this function.
//
// HANDLING OF DEGENERATE CASES
//
// Following degenerate cases may happen:
// * dataset is empty (no analysis can be done)
// * all sequences are shorter than the window length,no analysis can be done
// * no algorithm is specified (no analysis can be done)
// * sequence being passed is shorter than the window length
//
// Calling this function in degenerate cases returns following result:
// * in any case, NTicks ticks is returned
// * trend is assumed to be zero
// * noise is initialized by the sequence.
//
// No analysis is performed in degenerate cases (we immediately return  dummy
// values, no basis is constructed).
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaanalyzesequence(const ssamodel &s, const real_1d_array &data, const ae_int_t nticks, real_1d_array &trend, real_1d_array &noise);
// API: void ssaanalyzesequence(const ssamodel &s, const real_1d_array &data, real_1d_array &trend, real_1d_array &noise);
void ssaanalyzesequence(ssamodel *s, RVector *data, ae_int_t nticks, RVector *trend, RVector *noise) {
   ae_int_t i;
   SetVector(trend);
   SetVector(noise);
   ae_assert(nticks >= 1, "SSAAnalyzeSequence: NTicks<1");
   ae_assert(data->cnt >= nticks, "SSAAnalyzeSequence: Data is too short");
   ae_assert(isfinitevector(data, nticks), "SSAAnalyzeSequence: Data contains infinities NANs");
// Init
   ae_vector_set_length(trend, nticks);
   ae_vector_set_length(noise, nticks);
// Is it degenerate case?
   if (ssa_isdegenerate(s) || nticks < s->windowwidth) {
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = 0.0;
         noise->xR[i] = data->xR[i];
      }
      return;
   }
// Update basis.
//
// It will take care of basis validity flags. AppendLen=0 which means
// that we perform initial basis evaluation.
   ssa_updatebasis(s, 0, 0.0);
// Perform analysis
   ssa_analyzesequence(s, data, 0, nticks, trend, noise, 0);
}

// This function builds SSA basis and performs forecasting  for  a  specified
// number of ticks, returning value of trend.
//
// Forecast is performed as follows:
// * SSA  trend  extraction  is  applied  to last WindowWidth elements of the
//   internally stored dataset; this step is basically a noise reduction.
// * linear recurrence relation is applied to extracted trend
//
// This function has following running time:
// * O(NBasis*WindowWidth) for trend extraction phase (always performed)
// * O(WindowWidth*NTicks) for forecast phase
//
// NOTE: noise reduction is ALWAYS applied by this algorithm; if you want  to
//       apply recurrence relation  to  raw  unprocessed  data,  use  another
//       function - ssaforecastsequence() which allows to  turn  on  and  off
//       noise reduction phase.
//
// NOTE: this algorithm performs prediction using only one - last  -  sliding
//       window.  Predictions  produced   by   such   approach   are   smooth
//       continuations of the reconstructed  trend  line,  but  they  can  be
//       easily corrupted by noise. If you need  noise-resistant  prediction,
//       use ssaforecastavglast() function, which averages predictions  built
//       using several sliding windows.
//
// Inputs:
//     S               -   SSA model
//     NTicks          -   number of ticks to forecast, NTicks >= 1
//
// Outputs:
//     Trend           -   array[NTicks], predicted trend line
//
// CACHING/REUSE OF THE BASIS
//
// Caching/reuse of previous results is performed:
// * first call performs full run of SSA; basis is stored in the cache
// * subsequent calls reuse previously cached basis
// * if you call any function which changes model properties (window  length,
//   algorithm, dataset), internal basis will be invalidated.
// * the only calls which do NOT invalidate basis are listed below:
//   a) ssasetwindow() with same window length
//   b) ssaappendpointandupdate()
//   c) ssaappendsequenceandupdate()
//   d) ssasetalgotopk...() with exactly same K
//   Calling these functions will result in reuse of previously found basis.
//
// HANDLING OF DEGENERATE CASES
//
// Following degenerate cases may happen:
// * dataset is empty (no analysis can be done)
// * all sequences are shorter than the window length,no analysis can be done
// * no algorithm is specified (no analysis can be done)
// * last sequence is shorter than the WindowWidth   (analysis  can  be done,
//   but we can not perform forecasting on the last sequence)
// * window length is 1 (impossible to use for forecasting)
// * SSA analysis algorithm is  configured  to  extract  basis  whose size is
//   equal to window length (impossible to use for  forecasting;  only  basis
//   whose size is less than window length can be used).
//
// Calling this function in degenerate cases returns following result:
// * NTicks  copies  of  the  last  value is returned for non-empty task with
//   large enough dataset, but with overcomplete  basis  (window  width=1  or
//   basis size is equal to window width)
// * zero trend with length=NTicks is returned for empty task
//
// No analysis is performed in degenerate cases (we immediately return  dummy
// values, no basis is ever constructed).
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaforecastlast(const ssamodel &s, const ae_int_t nticks, real_1d_array &trend);
void ssaforecastlast(ssamodel *s, ae_int_t nticks, RVector *trend) {
   ae_int_t i;
   ae_int_t j;
   double v;
   ae_int_t winw;
   SetVector(trend);
   ae_assert(nticks >= 1, "SSAForecast: NTicks<1");
// Init
   winw = s->windowwidth;
   ae_vector_set_length(trend, nticks);
// Is it degenerate case?
   if (ssa_isdegenerate(s)) {
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = 0.0;
      }
      return;
   }
   ae_assert(s->nsequences > 0, "SSAForecastLast: integrity check failed");
   if (s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] < winw) {
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = 0.0;
      }
      return;
   }
   if (winw == 1) {
      ae_assert(s->nsequences > 0, "SSAForecast: integrity check failed / 2355");
      ae_assert(s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] > 0, "SSAForecast: integrity check failed");
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = s->sequencedata.xR[s->sequenceidx.xZ[s->nsequences] - 1];
      }
      return;
   }
// Update basis and recurrent relation.
//
// It will take care of basis validity flags. AppendLen=0 which means
// that we perform initial basis evaluation.
   ssa_updatebasis(s, 0, 0.0);
   ae_assert(s->nbasis <= winw && s->nbasis > 0, "SSAForecast: integrity check failed / 4f5et");
   if (s->nbasis == winw) {
   // Handle degenerate situation with basis whose size
   // is equal to window length.
      ae_assert(s->nsequences > 0, "SSAForecast: integrity check failed / 2355");
      ae_assert(s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] > 0, "SSAForecast: integrity check failed");
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = s->sequencedata.xR[s->sequenceidx.xZ[s->nsequences] - 1];
      }
      return;
   }
// Apply recurrent formula for SSA forecasting:
// * first, perform smoothing of the last window
// * second, perform analysis phase
   ae_assert(s->nsequences > 0, "SSAForecastLast: integrity check failed");
   ae_assert(s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] >= s->windowwidth, "SSAForecastLast: integrity check failed");
   vectorsetlengthatleast(&s->tmp0, s->nbasis);
   vectorsetlengthatleast(&s->fctrend, s->windowwidth);
   rmatrixgemv(s->nbasis, s->windowwidth, 1.0, &s->basist, 0, 0, 0, &s->sequencedata, s->sequenceidx.xZ[s->nsequences] - s->windowwidth, 0.0, &s->tmp0, 0);
   rmatrixgemv(s->windowwidth, s->nbasis, 1.0, &s->basis, 0, 0, 0, &s->tmp0, 0, 0.0, &s->fctrend, 0);
   vectorsetlengthatleast(&s->tmp1, winw - 1);
   for (i = 1; i < winw; i++) {
      s->tmp1.xR[i - 1] = s->fctrend.xR[i];
   }
   for (i = 0; i < nticks; i++) {
      v = s->forecasta.xR[0] * s->tmp1.xR[0];
      for (j = 1; j < winw - 1; j++) {
         v += s->forecasta.xR[j] * s->tmp1.xR[j];
         s->tmp1.xR[j - 1] = s->tmp1.xR[j];
      }
      trend->xR[i] = v;
      s->tmp1.xR[winw - 2] = v;
   }
}

// This function builds SSA  basis  and  performs  forecasting  for  a  user-
// specified sequence, returning value of trend.
//
// Forecasting is done in two stages:
// * first,  we  extract  trend  from the WindowWidth  last  elements of  the
//   sequence. This stage is optional, you  can  turn  it  off  if  you  pass
//   data which are already processed with SSA. Of course, you  can  turn  it
//   off even for raw data, but it is not recommended - noise suppression  is
//   very important for correct prediction.
// * then, we apply LRR for last  WindowWidth-1  elements  of  the  extracted
//   trend.
//
// This function has following running time:
// * O(NBasis*WindowWidth) for trend extraction phase
// * O(WindowWidth*NTicks) for forecast phase
//
// NOTE: this algorithm performs prediction using only one - last  -  sliding
//       window.  Predictions  produced   by   such   approach   are   smooth
//       continuations of the reconstructed  trend  line,  but  they  can  be
//       easily corrupted by noise. If you need  noise-resistant  prediction,
//       use ssaforecastavgsequence() function,  which  averages  predictions
//       built using several sliding windows.
//
// Inputs:
//     S               -   SSA model
//     Data            -   array[DataLen], data to forecast
//     DataLen         -   number of ticks in the data, DataLen >= 1
//     ForecastLen     -   number of ticks to predict, ForecastLen >= 1
//     ApplySmoothing  -   whether to apply smoothing trend extraction or not;
//                         if you do not know what to specify, pass True.
//
// Outputs:
//     Trend           -   array[ForecastLen], forecasted trend
//
// CACHING/REUSE OF THE BASIS
//
// Caching/reuse of previous results is performed:
// * first call performs full run of SSA; basis is stored in the cache
// * subsequent calls reuse previously cached basis
// * if you call any function which changes model properties (window  length,
//   algorithm, dataset), internal basis will be invalidated.
// * the only calls which do NOT invalidate basis are listed below:
//   a) ssasetwindow() with same window length
//   b) ssaappendpointandupdate()
//   c) ssaappendsequenceandupdate()
//   d) ssasetalgotopk...() with exactly same K
//   Calling these functions will result in reuse of previously found basis.
//
// HANDLING OF DEGENERATE CASES
//
// Following degenerate cases may happen:
// * dataset is empty (no analysis can be done)
// * all sequences are shorter than the window length,no analysis can be done
// * no algorithm is specified (no analysis can be done)
// * data sequence is shorter than the WindowWidth   (analysis  can  be done,
//   but we can not perform forecasting on the last sequence)
// * window length is 1 (impossible to use for forecasting)
// * SSA analysis algorithm is  configured  to  extract  basis  whose size is
//   equal to window length (impossible to use for  forecasting;  only  basis
//   whose size is less than window length can be used).
//
// Calling this function in degenerate cases returns following result:
// * ForecastLen copies of the last value is returned for non-empty task with
//   large enough dataset, but with overcomplete  basis  (window  width=1  or
//   basis size is equal to window width)
// * zero trend with length=ForecastLen is returned for empty task
//
// No analysis is performed in degenerate cases (we immediately return  dummy
// values, no basis is ever constructed).
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaforecastsequence(const ssamodel &s, const real_1d_array &data, const ae_int_t datalen, const ae_int_t forecastlen, const bool applysmoothing, real_1d_array &trend);
// API: void ssaforecastsequence(const ssamodel &s, const real_1d_array &data, const ae_int_t forecastlen, real_1d_array &trend);
void ssaforecastsequence(ssamodel *s, RVector *data, ae_int_t datalen, ae_int_t forecastlen, bool applysmoothing, RVector *trend) {
   ae_int_t i;
   ae_int_t j;
   double v;
   ae_int_t winw;
   SetVector(trend);
   ae_assert(datalen >= 1, "SSAForecastSequence: DataLen<1");
   ae_assert(data->cnt >= datalen, "SSAForecastSequence: Data is too short");
   ae_assert(isfinitevector(data, datalen), "SSAForecastSequence: Data contains infinities NANs");
   ae_assert(forecastlen >= 1, "SSAForecastSequence: ForecastLen<1");
// Init
   winw = s->windowwidth;
   ae_vector_set_length(trend, forecastlen);
// Is it degenerate case?
   if (ssa_isdegenerate(s) || datalen < winw) {
      for (i = 0; i < forecastlen; i++) {
         trend->xR[i] = 0.0;
      }
      return;
   }
   if (winw == 1) {
      for (i = 0; i < forecastlen; i++) {
         trend->xR[i] = data->xR[datalen - 1];
      }
      return;
   }
// Update basis.
//
// It will take care of basis validity flags. AppendLen=0 which means
// that we perform initial basis evaluation.
   ssa_updatebasis(s, 0, 0.0);
   ae_assert(s->nbasis <= winw && s->nbasis > 0, "SSAForecast: integrity check failed / 4f5et");
   if (s->nbasis == winw) {
   // Handle degenerate situation with basis whose size
   // is equal to window length.
      for (i = 0; i < forecastlen; i++) {
         trend->xR[i] = data->xR[datalen - 1];
      }
      return;
   }
// Perform trend extraction
   vectorsetlengthatleast(&s->fctrend, s->windowwidth);
   if (applysmoothing) {
      ae_assert(datalen >= winw, "SSAForecastSequence: integrity check failed");
      vectorsetlengthatleast(&s->tmp0, s->nbasis);
      rmatrixgemv(s->nbasis, winw, 1.0, &s->basist, 0, 0, 0, data, datalen - winw, 0.0, &s->tmp0, 0);
      rmatrixgemv(winw, s->nbasis, 1.0, &s->basis, 0, 0, 0, &s->tmp0, 0, 0.0, &s->fctrend, 0);
   } else {
      for (i = 0; i < winw; i++) {
         s->fctrend.xR[i] = data->xR[datalen + i - winw];
      }
   }
// Apply recurrent formula for SSA forecasting
   vectorsetlengthatleast(&s->tmp1, winw - 1);
   for (i = 1; i < winw; i++) {
      s->tmp1.xR[i - 1] = s->fctrend.xR[i];
   }
   for (i = 0; i < forecastlen; i++) {
      v = s->forecasta.xR[0] * s->tmp1.xR[0];
      for (j = 1; j < winw - 1; j++) {
         v += s->forecasta.xR[j] * s->tmp1.xR[j];
         s->tmp1.xR[j - 1] = s->tmp1.xR[j];
      }
      trend->xR[i] = v;
      s->tmp1.xR[winw - 2] = v;
   }
}

// This function performs  averaged  forecasting.  It  assumes  that basis is
// already built, everything is valid and checked. See  comments  on  similar
// public functions to find out more about averaged predictions.
//
// Inputs:
//     S                   -   model
//     Data                -   array which holds data in elements [I0,I1):
//                             * right bound is not included.
//                             * I1-I0 >= WindowWidth (assertion is performed).
//     M                   -   number  of  sliding  windows  to combine, M >= 1. If
//                             your dataset has less than M sliding windows, this
//                             parameter will be silently reduced.
//     ForecastLen         -   number of ticks to predict, ForecastLen >= 1
//     Trend               -   preallocated output array, large enough
//     Offs                -   offset in Trend where result is stored;
//                             I1-I0 elements are written starting at offset
//                             Offs.
//
// Outputs:
//     Trend           -   array[ForecastLen], forecasted trend
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
static void ssa_forecastavgsequence(ssamodel *s, RVector *data, ae_int_t i0, ae_int_t i1, ae_int_t m, ae_int_t forecastlen, bool smooth, RVector *trend, ae_int_t offs) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t winw;
   ae_assert(s->arebasisandsolvervalid, "ForecastAvgSequence: integrity check failed / d84sz0");
   ae_assert(i1 - i0 - s->windowwidth + 1 >= m, "ForecastAvgSequence: integrity check failed / d84sz1");
   ae_assert(s->nbasis >= 1, "ForecastAvgSequence: integrity check failed / d84sz2");
   ae_assert(s->windowwidth >= 2, "ForecastAvgSequence: integrity check failed / 5tgdg5");
   ae_assert(s->windowwidth > s->nbasis, "ForecastAvgSequence: integrity check failed / d5g56w");
   winw = s->windowwidth;
// Prepare M synchronized predictions for the last known tick
// (last one is an actual value of the trend, previous M-1 predictions
// are predictions from differently positioned sliding windows).
   matrixsetlengthatleast(&s->fctrendm, m, winw);
   vectorsetlengthatleast(&s->tmp0, imax2(m, s->nbasis));
   vectorsetlengthatleast(&s->tmp1, winw);
   for (k = 0; k < m; k++) {
   // Perform prediction for rows [0,K-1]
      rmatrixgemv(k, winw - 1, 1.0, &s->fctrendm, 0, 1, 0, &s->forecasta, 0, 0.0, &s->tmp0, 0);
      for (i = 0; i < k; i++) {
         for (j = 1; j < winw; j++) {
            s->fctrendm.xyR[i][j - 1] = s->fctrendm.xyR[i][j];
         }
         s->fctrendm.xyR[i][winw - 1] = s->tmp0.xR[i];
      }
   // Perform trend extraction for row K, add it to dataset
      if (smooth) {
         rmatrixgemv(s->nbasis, winw, 1.0, &s->basist, 0, 0, 0, data, i1 - winw - (m - 1 - k), 0.0, &s->tmp0, 0);
         rmatrixgemv(s->windowwidth, s->nbasis, 1.0, &s->basis, 0, 0, 0, &s->tmp0, 0, 0.0, &s->tmp1, 0);
         for (j = 0; j < winw; j++) {
            s->fctrendm.xyR[k][j] = s->tmp1.xR[j];
         }
      } else {
         for (j = 0; j < winw; j++) {
            s->fctrendm.xyR[k][j] = data->xR[i1 - winw - (m - 1 - k) + j];
         }
      }
   }
// Now we have M synchronized predictions of the sequence state at the last
// know moment (last "prediction" is just a copy of the trend). Let's start
// batch prediction!
   for (k = 0; k < forecastlen; k++) {
      rmatrixgemv(m, winw - 1, 1.0, &s->fctrendm, 0, 1, 0, &s->forecasta, 0, 0.0, &s->tmp0, 0);
      trend->xR[offs + k] = 0.0;
      for (i = 0; i < m; i++) {
         for (j = 1; j < winw; j++) {
            s->fctrendm.xyR[i][j - 1] = s->fctrendm.xyR[i][j];
         }
         s->fctrendm.xyR[i][winw - 1] = s->tmp0.xR[i];
         trend->xR[offs + k] += s->tmp0.xR[i];
      }
      trend->xR[offs + k] /= m;
   }
}

// This function builds SSA basis and performs forecasting  for  a  specified
// number of ticks, returning value of trend.
//
// Forecast is performed as follows:
// * SSA  trend  extraction  is  applied to last  M  sliding windows  of  the
//   internally stored dataset
// * for each of M sliding windows, M predictions are built
// * average value of M predictions is returned
//
// This function has following running time:
// * O(NBasis*WindowWidth*M) for trend extraction phase (always performed)
// * O(WindowWidth*NTicks*M) for forecast phase
//
// NOTE: noise reduction is ALWAYS applied by this algorithm; if you want  to
//       apply recurrence relation  to  raw  unprocessed  data,  use  another
//       function - ssaforecastsequence() which allows to  turn  on  and  off
//       noise reduction phase.
//
// NOTE: combination of several predictions results in lesser sensitivity  to
//       noise, but it may produce undesirable discontinuities  between  last
//       point of the trend and first point of the prediction. The reason  is
//       that  last  point  of  the  trend is usually corrupted by noise, but
//       average  value of  several  predictions  is less sensitive to noise,
//       thus discontinuity appears. It is not a bug.
//
// Inputs:
//     S               -   SSA model
//     M               -   number  of  sliding  windows  to combine, M >= 1. If
//                         your dataset has less than M sliding windows, this
//                         parameter will be silently reduced.
//     NTicks          -   number of ticks to forecast, NTicks >= 1
//
// Outputs:
//     Trend           -   array[NTicks], predicted trend line
//
// CACHING/REUSE OF THE BASIS
//
// Caching/reuse of previous results is performed:
// * first call performs full run of SSA; basis is stored in the cache
// * subsequent calls reuse previously cached basis
// * if you call any function which changes model properties (window  length,
//   algorithm, dataset), internal basis will be invalidated.
// * the only calls which do NOT invalidate basis are listed below:
//   a) ssasetwindow() with same window length
//   b) ssaappendpointandupdate()
//   c) ssaappendsequenceandupdate()
//   d) ssasetalgotopk...() with exactly same K
//   Calling these functions will result in reuse of previously found basis.
//
// HANDLING OF DEGENERATE CASES
//
// Following degenerate cases may happen:
// * dataset is empty (no analysis can be done)
// * all sequences are shorter than the window length,no analysis can be done
// * no algorithm is specified (no analysis can be done)
// * last sequence is shorter than the WindowWidth   (analysis  can  be done,
//   but we can not perform forecasting on the last sequence)
// * window length is 1 (impossible to use for forecasting)
// * SSA analysis algorithm is  configured  to  extract  basis  whose size is
//   equal to window length (impossible to use for  forecasting;  only  basis
//   whose size is less than window length can be used).
//
// Calling this function in degenerate cases returns following result:
// * NTicks  copies  of  the  last  value is returned for non-empty task with
//   large enough dataset, but with overcomplete  basis  (window  width=1  or
//   basis size is equal to window width)
// * zero trend with length=NTicks is returned for empty task
//
// No analysis is performed in degenerate cases (we immediately return  dummy
// values, no basis is ever constructed).
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaforecastavglast(const ssamodel &s, const ae_int_t m, const ae_int_t nticks, real_1d_array &trend);
void ssaforecastavglast(ssamodel *s, ae_int_t m, ae_int_t nticks, RVector *trend) {
   ae_int_t i;
   ae_int_t winw;
   SetVector(trend);
   ae_assert(nticks >= 1, "SSAForecastAvgLast: NTicks<1");
   ae_assert(m >= 1, "SSAForecastAvgLast: M < 1");
// Init
   winw = s->windowwidth;
   ae_vector_set_length(trend, nticks);
// Is it degenerate case?
   if (ssa_isdegenerate(s)) {
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = 0.0;
      }
      return;
   }
   ae_assert(s->nsequences > 0, "SSAForecastAvgLast: integrity check failed");
   if (s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] < winw) {
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = 0.0;
      }
      return;
   }
   if (winw == 1) {
      ae_assert(s->nsequences > 0, "SSAForecastAvgLast: integrity check failed / 2355");
      ae_assert(s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] > 0, "SSAForecastAvgLast: integrity check failed");
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = s->sequencedata.xR[s->sequenceidx.xZ[s->nsequences] - 1];
      }
      return;
   }
// Update basis and recurrent relation.
//
// It will take care of basis validity flags. AppendLen=0 which means
// that we perform initial basis evaluation.
   ssa_updatebasis(s, 0, 0.0);
   ae_assert(s->nbasis <= winw && s->nbasis > 0, "SSAForecastAvgLast: integrity check failed / 4f5et");
   if (s->nbasis == winw) {
   // Handle degenerate situation with basis whose size
   // is equal to window length.
      ae_assert(s->nsequences > 0, "SSAForecastAvgLast: integrity check failed / 2355");
      ae_assert(s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] > 0, "SSAForecastAvgLast: integrity check failed");
      for (i = 0; i < nticks; i++) {
         trend->xR[i] = s->sequencedata.xR[s->sequenceidx.xZ[s->nsequences] - 1];
      }
      return;
   }
// Decrease M if we have less than M sliding windows.
// Forecast.
   m = imin2(m, s->sequenceidx.xZ[s->nsequences] - s->sequenceidx.xZ[s->nsequences - 1] - winw + 1);
   ae_assert(m >= 1, "SSAForecastAvgLast: integrity check failed");
   ssa_forecastavgsequence(s, &s->sequencedata, s->sequenceidx.xZ[s->nsequences - 1], s->sequenceidx.xZ[s->nsequences], m, nticks, true, trend, 0);
}

// This function builds SSA  basis  and  performs  forecasting  for  a  user-
// specified sequence, returning value of trend.
//
// Forecasting is done in two stages:
// * first,  we  extract  trend  from M last sliding windows of the sequence.
//   This stage is optional, you can  turn  it  off  if  you  pass data which
//   are already processed with SSA. Of course, you  can  turn  it  off  even
//   for raw data, but it is not recommended  -  noise  suppression  is  very
//   important for correct prediction.
// * then, we apply LRR independently for M sliding windows
// * average of M predictions is returned
//
// This function has following running time:
// * O(NBasis*WindowWidth*M) for trend extraction phase
// * O(WindowWidth*NTicks*M) for forecast phase
//
// NOTE: combination of several predictions results in lesser sensitivity  to
//       noise, but it may produce undesirable discontinuities  between  last
//       point of the trend and first point of the prediction. The reason  is
//       that  last  point  of  the  trend is usually corrupted by noise, but
//       average  value of  several  predictions  is less sensitive to noise,
//       thus discontinuity appears. It is not a bug.
//
// Inputs:
//     S               -   SSA model
//     Data            -   array[NTicks], data to forecast
//     DataLen         -   number of ticks in the data, DataLen >= 1
//     M               -   number  of  sliding  windows  to combine, M >= 1. If
//                         your dataset has less than M sliding windows, this
//                         parameter will be silently reduced.
//     ForecastLen     -   number of ticks to predict, ForecastLen >= 1
//     ApplySmoothing  -   whether to apply smoothing trend extraction or not.
//                         if you do not know what to specify, pass true.
//
// Outputs:
//     Trend           -   array[ForecastLen], forecasted trend
//
// CACHING/REUSE OF THE BASIS
//
// Caching/reuse of previous results is performed:
// * first call performs full run of SSA; basis is stored in the cache
// * subsequent calls reuse previously cached basis
// * if you call any function which changes model properties (window  length,
//   algorithm, dataset), internal basis will be invalidated.
// * the only calls which do NOT invalidate basis are listed below:
//   a) ssasetwindow() with same window length
//   b) ssaappendpointandupdate()
//   c) ssaappendsequenceandupdate()
//   d) ssasetalgotopk...() with exactly same K
//   Calling these functions will result in reuse of previously found basis.
//
// HANDLING OF DEGENERATE CASES
//
// Following degenerate cases may happen:
// * dataset is empty (no analysis can be done)
// * all sequences are shorter than the window length,no analysis can be done
// * no algorithm is specified (no analysis can be done)
// * data sequence is shorter than the WindowWidth   (analysis  can  be done,
//   but we can not perform forecasting on the last sequence)
// * window length is 1 (impossible to use for forecasting)
// * SSA analysis algorithm is  configured  to  extract  basis  whose size is
//   equal to window length (impossible to use for  forecasting;  only  basis
//   whose size is less than window length can be used).
//
// Calling this function in degenerate cases returns following result:
// * ForecastLen copies of the last value is returned for non-empty task with
//   large enough dataset, but with overcomplete  basis  (window  width=1  or
//   basis size is equal to window width)
// * zero trend with length=ForecastLen is returned for empty task
//
// No analysis is performed in degenerate cases (we immediately return  dummy
// values, no basis is ever constructed).
// ALGLIB: Copyright 30.10.2017 by Sergey Bochkanov
// API: void ssaforecastavgsequence(const ssamodel &s, const real_1d_array &data, const ae_int_t datalen, const ae_int_t m, const ae_int_t forecastlen, const bool applysmoothing, real_1d_array &trend);
// API: void ssaforecastavgsequence(const ssamodel &s, const real_1d_array &data, const ae_int_t m, const ae_int_t forecastlen, real_1d_array &trend);
void ssaforecastavgsequence(ssamodel *s, RVector *data, ae_int_t datalen, ae_int_t m, ae_int_t forecastlen, bool applysmoothing, RVector *trend) {
   ae_int_t i;
   ae_int_t winw;
   SetVector(trend);
   ae_assert(datalen >= 1, "SSAForecastAvgSequence: DataLen<1");
   ae_assert(m >= 1, "SSAForecastAvgSequence: M < 1");
   ae_assert(data->cnt >= datalen, "SSAForecastAvgSequence: Data is too short");
   ae_assert(isfinitevector(data, datalen), "SSAForecastAvgSequence: Data contains infinities NANs");
   ae_assert(forecastlen >= 1, "SSAForecastAvgSequence: ForecastLen<1");
// Init
   winw = s->windowwidth;
   ae_vector_set_length(trend, forecastlen);
// Is it degenerate case?
   if (ssa_isdegenerate(s) || datalen < winw) {
      for (i = 0; i < forecastlen; i++) {
         trend->xR[i] = 0.0;
      }
      return;
   }
   if (winw == 1) {
      for (i = 0; i < forecastlen; i++) {
         trend->xR[i] = data->xR[datalen - 1];
      }
      return;
   }
// Update basis.
//
// It will take care of basis validity flags. AppendLen=0 which means
// that we perform initial basis evaluation.
   ssa_updatebasis(s, 0, 0.0);
   ae_assert(s->nbasis <= winw && s->nbasis > 0, "SSAForecast: integrity check failed / 4f5et");
   if (s->nbasis == winw) {
   // Handle degenerate situation with basis whose size
   // is equal to window length.
      for (i = 0; i < forecastlen; i++) {
         trend->xR[i] = data->xR[datalen - 1];
      }
      return;
   }
// Decrease M if we have less than M sliding windows.
// Forecast.
   m = imin2(m, datalen - winw + 1);
   ae_assert(m >= 1, "SSAForecastAvgLast: integrity check failed");
   ssa_forecastavgsequence(s, data, 0, datalen, m, forecastlen, applysmoothing, trend, 0);
}

void ssamodel_init(void *_p, bool make_automatic) {
   ssamodel *p = (ssamodel *)_p;
   ae_vector_init(&p->sequenceidx, 0, DT_INT, make_automatic);
   ae_vector_init(&p->sequencedata, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->precomputedbasis, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->basis, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->basist, 0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->sv, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->forecasta, 0, DT_REAL, make_automatic);
   eigsubspacestate_init(&p->solver, make_automatic);
   ae_matrix_init(&p->xxt, 0, 0, DT_REAL, make_automatic);
   hqrndstate_init(&p->rs, make_automatic);
   ae_vector_init(&p->rtqueue, 0, DT_INT, make_automatic);
   ae_vector_init(&p->tmp0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->tmp1, 0, DT_REAL, make_automatic);
   eigsubspacereport_init(&p->solverrep, make_automatic);
   ae_vector_init(&p->alongtrend, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->alongnoise, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->aseqtrajectory, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->aseqtbproduct, 0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->aseqcounts, 0, DT_INT, make_automatic);
   ae_vector_init(&p->fctrend, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->fcnoise, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->fctrendm, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->uxbatch, 0, 0, DT_REAL, make_automatic);
}

void ssamodel_copy(void *_dst, void *_src, bool make_automatic) {
   ssamodel *dst = (ssamodel *)_dst;
   ssamodel *src = (ssamodel *)_src;
   dst->nsequences = src->nsequences;
   ae_vector_copy(&dst->sequenceidx, &src->sequenceidx, make_automatic);
   ae_vector_copy(&dst->sequencedata, &src->sequencedata, make_automatic);
   dst->algotype = src->algotype;
   dst->windowwidth = src->windowwidth;
   dst->rtpowerup = src->rtpowerup;
   dst->topk = src->topk;
   dst->precomputedwidth = src->precomputedwidth;
   dst->precomputednbasis = src->precomputednbasis;
   ae_matrix_copy(&dst->precomputedbasis, &src->precomputedbasis, make_automatic);
   dst->defaultsubspaceits = src->defaultsubspaceits;
   dst->memorylimit = src->memorylimit;
   dst->arebasisandsolvervalid = src->arebasisandsolvervalid;
   ae_matrix_copy(&dst->basis, &src->basis, make_automatic);
   ae_matrix_copy(&dst->basist, &src->basist, make_automatic);
   ae_vector_copy(&dst->sv, &src->sv, make_automatic);
   ae_vector_copy(&dst->forecasta, &src->forecasta, make_automatic);
   dst->nbasis = src->nbasis;
   eigsubspacestate_copy(&dst->solver, &src->solver, make_automatic);
   ae_matrix_copy(&dst->xxt, &src->xxt, make_automatic);
   hqrndstate_copy(&dst->rs, &src->rs, make_automatic);
   dst->rngseed = src->rngseed;
   ae_vector_copy(&dst->rtqueue, &src->rtqueue, make_automatic);
   dst->rtqueuecnt = src->rtqueuecnt;
   dst->rtqueuechunk = src->rtqueuechunk;
   dst->dbgcntevd = src->dbgcntevd;
   ae_vector_copy(&dst->tmp0, &src->tmp0, make_automatic);
   ae_vector_copy(&dst->tmp1, &src->tmp1, make_automatic);
   eigsubspacereport_copy(&dst->solverrep, &src->solverrep, make_automatic);
   ae_vector_copy(&dst->alongtrend, &src->alongtrend, make_automatic);
   ae_vector_copy(&dst->alongnoise, &src->alongnoise, make_automatic);
   ae_matrix_copy(&dst->aseqtrajectory, &src->aseqtrajectory, make_automatic);
   ae_matrix_copy(&dst->aseqtbproduct, &src->aseqtbproduct, make_automatic);
   ae_vector_copy(&dst->aseqcounts, &src->aseqcounts, make_automatic);
   ae_vector_copy(&dst->fctrend, &src->fctrend, make_automatic);
   ae_vector_copy(&dst->fcnoise, &src->fcnoise, make_automatic);
   ae_matrix_copy(&dst->fctrendm, &src->fctrendm, make_automatic);
   ae_matrix_copy(&dst->uxbatch, &src->uxbatch, make_automatic);
   dst->uxbatchwidth = src->uxbatchwidth;
   dst->uxbatchsize = src->uxbatchsize;
   dst->uxbatchlimit = src->uxbatchlimit;
}

void ssamodel_free(void *_p, bool make_automatic) {
   ssamodel *p = (ssamodel *)_p;
   ae_vector_free(&p->sequenceidx, make_automatic);
   ae_vector_free(&p->sequencedata, make_automatic);
   ae_matrix_free(&p->precomputedbasis, make_automatic);
   ae_matrix_free(&p->basis, make_automatic);
   ae_matrix_free(&p->basist, make_automatic);
   ae_vector_free(&p->sv, make_automatic);
   ae_vector_free(&p->forecasta, make_automatic);
   eigsubspacestate_free(&p->solver, make_automatic);
   ae_matrix_free(&p->xxt, make_automatic);
   hqrndstate_free(&p->rs, make_automatic);
   ae_vector_free(&p->rtqueue, make_automatic);
   ae_vector_free(&p->tmp0, make_automatic);
   ae_vector_free(&p->tmp1, make_automatic);
   eigsubspacereport_free(&p->solverrep, make_automatic);
   ae_vector_free(&p->alongtrend, make_automatic);
   ae_vector_free(&p->alongnoise, make_automatic);
   ae_matrix_free(&p->aseqtrajectory, make_automatic);
   ae_matrix_free(&p->aseqtbproduct, make_automatic);
   ae_vector_free(&p->aseqcounts, make_automatic);
   ae_vector_free(&p->fctrend, make_automatic);
   ae_vector_free(&p->fcnoise, make_automatic);
   ae_matrix_free(&p->fctrendm, make_automatic);
   ae_matrix_free(&p->uxbatch, make_automatic);
}
} // end of namespace alglib_impl

namespace alglib {
// This object stores state of the SSA model.
// You should use ALGLIB functions to work with this object.
DefClass(ssamodel, EndD)

void ssacreate(ssamodel &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssacreate(ConstT(ssamodel, s));
   alglib_impl::ae_state_clear();
}

void ssasetwindow(const ssamodel &s, const ae_int_t windowwidth) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssasetwindow(ConstT(ssamodel, s), windowwidth);
   alglib_impl::ae_state_clear();
}

void ssasetseed(const ssamodel &s, const ae_int_t seed) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssasetseed(ConstT(ssamodel, s), seed);
   alglib_impl::ae_state_clear();
}

void ssasetpoweruplength(const ssamodel &s, const ae_int_t pwlen) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssasetpoweruplength(ConstT(ssamodel, s), pwlen);
   alglib_impl::ae_state_clear();
}

void ssasetmemorylimit(const ssamodel &s, const ae_int_t memlimit) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssasetmemorylimit(ConstT(ssamodel, s), memlimit);
   alglib_impl::ae_state_clear();
}

void ssaaddsequence(const ssamodel &s, const real_1d_array &x, const ae_int_t n) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaaddsequence(ConstT(ssamodel, s), ConstT(ae_vector, x), n);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void ssaaddsequence(const ssamodel &s, const real_1d_array &x) {
   ae_int_t n = x.length();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaaddsequence(ConstT(ssamodel, s), ConstT(ae_vector, x), n);
   alglib_impl::ae_state_clear();
}
#endif

void ssaappendpointandupdate(const ssamodel &s, const double x, const double updateits) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaappendpointandupdate(ConstT(ssamodel, s), x, updateits);
   alglib_impl::ae_state_clear();
}

void ssaappendsequenceandupdate(const ssamodel &s, const real_1d_array &x, const ae_int_t nticks, const double updateits) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaappendsequenceandupdate(ConstT(ssamodel, s), ConstT(ae_vector, x), nticks, updateits);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void ssaappendsequenceandupdate(const ssamodel &s, const real_1d_array &x, const double updateits) {
   ae_int_t nticks = x.length();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaappendsequenceandupdate(ConstT(ssamodel, s), ConstT(ae_vector, x), nticks, updateits);
   alglib_impl::ae_state_clear();
}
#endif

void ssasetalgoprecomputed(const ssamodel &s, const real_2d_array &a, const ae_int_t windowwidth, const ae_int_t nbasis) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssasetalgoprecomputed(ConstT(ssamodel, s), ConstT(ae_matrix, a), windowwidth, nbasis);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void ssasetalgoprecomputed(const ssamodel &s, const real_2d_array &a) {
   ae_int_t windowwidth = a.rows();
   ae_int_t nbasis = a.cols();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssasetalgoprecomputed(ConstT(ssamodel, s), ConstT(ae_matrix, a), windowwidth, nbasis);
   alglib_impl::ae_state_clear();
}
#endif

void ssasetalgotopkdirect(const ssamodel &s, const ae_int_t topk) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssasetalgotopkdirect(ConstT(ssamodel, s), topk);
   alglib_impl::ae_state_clear();
}

void ssasetalgotopkrealtime(const ssamodel &s, const ae_int_t topk) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssasetalgotopkrealtime(ConstT(ssamodel, s), topk);
   alglib_impl::ae_state_clear();
}

void ssacleardata(const ssamodel &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssacleardata(ConstT(ssamodel, s));
   alglib_impl::ae_state_clear();
}

void ssagetbasis(const ssamodel &s, real_2d_array &a, real_1d_array &sv, ae_int_t &windowwidth, ae_int_t &nbasis) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssagetbasis(ConstT(ssamodel, s), ConstT(ae_matrix, a), ConstT(ae_vector, sv), &windowwidth, &nbasis);
   alglib_impl::ae_state_clear();
}

void ssagetlrr(const ssamodel &s, real_1d_array &a, ae_int_t &windowwidth) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssagetlrr(ConstT(ssamodel, s), ConstT(ae_vector, a), &windowwidth);
   alglib_impl::ae_state_clear();
}

void ssaanalyzelastwindow(const ssamodel &s, real_1d_array &trend, real_1d_array &noise, ae_int_t &nticks) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaanalyzelastwindow(ConstT(ssamodel, s), ConstT(ae_vector, trend), ConstT(ae_vector, noise), &nticks);
   alglib_impl::ae_state_clear();
}

void ssaanalyzelast(const ssamodel &s, const ae_int_t nticks, real_1d_array &trend, real_1d_array &noise) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaanalyzelast(ConstT(ssamodel, s), nticks, ConstT(ae_vector, trend), ConstT(ae_vector, noise));
   alglib_impl::ae_state_clear();
}

void ssaanalyzesequence(const ssamodel &s, const real_1d_array &data, const ae_int_t nticks, real_1d_array &trend, real_1d_array &noise) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaanalyzesequence(ConstT(ssamodel, s), ConstT(ae_vector, data), nticks, ConstT(ae_vector, trend), ConstT(ae_vector, noise));
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void ssaanalyzesequence(const ssamodel &s, const real_1d_array &data, real_1d_array &trend, real_1d_array &noise) {
   ae_int_t nticks = data.length();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaanalyzesequence(ConstT(ssamodel, s), ConstT(ae_vector, data), nticks, ConstT(ae_vector, trend), ConstT(ae_vector, noise));
   alglib_impl::ae_state_clear();
}
#endif

void ssaforecastlast(const ssamodel &s, const ae_int_t nticks, real_1d_array &trend) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaforecastlast(ConstT(ssamodel, s), nticks, ConstT(ae_vector, trend));
   alglib_impl::ae_state_clear();
}

void ssaforecastsequence(const ssamodel &s, const real_1d_array &data, const ae_int_t datalen, const ae_int_t forecastlen, const bool applysmoothing, real_1d_array &trend) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaforecastsequence(ConstT(ssamodel, s), ConstT(ae_vector, data), datalen, forecastlen, applysmoothing, ConstT(ae_vector, trend));
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void ssaforecastsequence(const ssamodel &s, const real_1d_array &data, const ae_int_t forecastlen, real_1d_array &trend) {
   ae_int_t datalen = data.length();
   bool applysmoothing = true;
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaforecastsequence(ConstT(ssamodel, s), ConstT(ae_vector, data), datalen, forecastlen, applysmoothing, ConstT(ae_vector, trend));
   alglib_impl::ae_state_clear();
}
#endif

void ssaforecastavglast(const ssamodel &s, const ae_int_t m, const ae_int_t nticks, real_1d_array &trend) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaforecastavglast(ConstT(ssamodel, s), m, nticks, ConstT(ae_vector, trend));
   alglib_impl::ae_state_clear();
}

void ssaforecastavgsequence(const ssamodel &s, const real_1d_array &data, const ae_int_t datalen, const ae_int_t m, const ae_int_t forecastlen, const bool applysmoothing, real_1d_array &trend) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaforecastavgsequence(ConstT(ssamodel, s), ConstT(ae_vector, data), datalen, m, forecastlen, applysmoothing, ConstT(ae_vector, trend));
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void ssaforecastavgsequence(const ssamodel &s, const real_1d_array &data, const ae_int_t m, const ae_int_t forecastlen, real_1d_array &trend) {
   ae_int_t datalen = data.length();
   bool applysmoothing = true;
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::ssaforecastavgsequence(ConstT(ssamodel, s), ConstT(ae_vector, data), datalen, m, forecastlen, applysmoothing, ConstT(ae_vector, trend));
   alglib_impl::ae_state_clear();
}
#endif
} // end of namespace alglib

// === LDA Package ===
// Depends on: (LinAlg) MATINV, EVD
namespace alglib_impl {
// Multiclass Fisher LDA
//
// Subroutine finds coefficients of linear combination which optimally separates
// training set on classes.
//
// Inputs:
//     XY          -   training set, array[0..NPoints-1,0..NVars].
//                     First NVars columns store values of independent
//                     variables, next column stores number of class (from 0
//                     to NClasses-1) which dataset element belongs to. Fractional
//                     values are rounded to nearest integer.
//     NPoints     -   training set size, NPoints >= 0
//     NVars       -   number of independent variables, NVars >= 1
//     NClasses    -   number of classes, NClasses >= 2
//
// Outputs:
//     Info        -   return code:
//                     * -4, if internal EVD subroutine hasn't converged
//                     * -2, if there is a point with class number
//                           outside of [0..NClasses-1].
//                     * -1, if incorrect parameters was passed (NPoints < 0,
//                           NVars < 1, NClasses < 2)
//                     *  1, if task has been solved
//                     *  2, if there was a multicollinearity in training set,
//                           but task has been solved.
//     W           -   linear combination coefficients, array[0..NVars-1]
// ALGLIB: Copyright 31.05.2008 by Sergey Bochkanov
// API: void fisherlda(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, ae_int_t &info, real_1d_array &w);
void fisherlda(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t *info, RVector *w) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetVector(w);
   NewMatrix(w2, 0, 0, DT_REAL);
   fisherldan(xy, npoints, nvars, nclasses, info, &w2);
   if (*info > 0) {
      ae_vector_set_length(w, nvars);
      ae_v_move(w->xR, 1, w2.xyR[0], w2.stride, nvars);
   }
   ae_frame_leave();
}

// N-dimensional multiclass Fisher LDA
//
// Subroutine finds coefficients of linear combinations which optimally separates
// training set on classes. It returns N-dimensional basis whose vector are sorted
// by quality of training set separation (in descending order).
//
// Inputs:
//     XY          -   training set, array[0..NPoints-1,0..NVars].
//                     First NVars columns store values of independent
//                     variables, next column stores number of class (from 0
//                     to NClasses-1) which dataset element belongs to. Fractional
//                     values are rounded to nearest integer.
//     NPoints     -   training set size, NPoints >= 0
//     NVars       -   number of independent variables, NVars >= 1
//     NClasses    -   number of classes, NClasses >= 2
//
// Outputs:
//     Info        -   return code:
//                     * -4, if internal EVD subroutine hasn't converged
//                     * -2, if there is a point with class number
//                           outside of [0..NClasses-1].
//                     * -1, if incorrect parameters was passed (NPoints < 0,
//                           NVars < 1, NClasses < 2)
//                     *  1, if task has been solved
//                     *  2, if there was a multicollinearity in training set,
//                           but task has been solved.
//     W           -   basis, array[0..NVars-1,0..NVars-1]
//                     columns of matrix stores basis vectors, sorted by
//                     quality of training set separation (in descending order)
// ALGLIB: Copyright 31.05.2008 by Sergey Bochkanov
// API: void fisherldan(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, ae_int_t &info, real_2d_array &w);
void fisherldan(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t *info, RMatrix *w) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t m;
   double v;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetMatrix(w);
   NewVector(c, 0, DT_INT);
   NewVector(mu, 0, DT_REAL);
   NewMatrix(muc, 0, 0, DT_REAL);
   NewVector(nc, 0, DT_INT);
   NewMatrix(sw, 0, 0, DT_REAL);
   NewMatrix(st, 0, 0, DT_REAL);
   NewMatrix(z, 0, 0, DT_REAL);
   NewMatrix(z2, 0, 0, DT_REAL);
   NewMatrix(tm, 0, 0, DT_REAL);
   NewMatrix(sbroot, 0, 0, DT_REAL);
   NewMatrix(a, 0, 0, DT_REAL);
   NewMatrix(xyc, 0, 0, DT_REAL);
   NewMatrix(xyproj, 0, 0, DT_REAL);
   NewMatrix(wproj, 0, 0, DT_REAL);
   NewVector(tf, 0, DT_REAL);
   NewVector(d, 0, DT_REAL);
   NewVector(d2, 0, DT_REAL);
   NewVector(work, 0, DT_REAL);
// Test data
   if (npoints < 0 || nvars < 1 || nclasses < 2) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   for (i = 0; i < npoints; i++) {
      if (RoundZ(xy->xyR[i][nvars]) < 0 || RoundZ(xy->xyR[i][nvars]) >= nclasses) {
         *info = -2;
         ae_frame_leave();
         return;
      }
   }
   *info = 1;
// Special case: NPoints <= 1
// Degenerate task.
   if (npoints <= 1) {
      *info = 2;
      ae_matrix_set_length(w, nvars, nvars);
      for (i = 0; i < nvars; i++) {
         for (j = 0; j < nvars; j++) {
            if (i == j) {
               w->xyR[i][j] = 1.0;
            } else {
               w->xyR[i][j] = 0.0;
            }
         }
      }
      ae_frame_leave();
      return;
   }
// Prepare temporaries
   ae_vector_set_length(&tf, nvars);
   ae_vector_set_length(&work, imax2(nvars, npoints) + 1);
   ae_matrix_set_length(&xyc, npoints, nvars);
// Convert class labels from reals to integers (just for convenience)
   ae_vector_set_length(&c, npoints);
   for (i = 0; i < npoints; i++) {
      c.xZ[i] = RoundZ(xy->xyR[i][nvars]);
   }
// Calculate class sizes, class means
   ae_vector_set_length(&mu, nvars);
   ae_matrix_set_length(&muc, nclasses, nvars);
   ae_vector_set_length(&nc, nclasses);
   for (j = 0; j < nvars; j++) {
      mu.xR[j] = 0.0;
   }
   for (i = 0; i < nclasses; i++) {
      nc.xZ[i] = 0;
      for (j = 0; j < nvars; j++) {
         muc.xyR[i][j] = 0.0;
      }
   }
   for (i = 0; i < npoints; i++) {
      ae_v_add(mu.xR, 1, xy->xyR[i], 1, nvars);
      ae_v_add(muc.xyR[c.xZ[i]], 1, xy->xyR[i], 1, nvars);
      nc.xZ[c.xZ[i]]++;
   }
   for (i = 0; i < nclasses; i++) {
      v = 1.0 / (double)nc.xZ[i];
      ae_v_muld(muc.xyR[i], 1, nvars, v);
   }
   v = 1.0 / (double)npoints;
   ae_v_muld(mu.xR, 1, nvars, v);
// Create ST matrix
   ae_matrix_set_length(&st, nvars, nvars);
   for (i = 0; i < nvars; i++) {
      for (j = 0; j < nvars; j++) {
         st.xyR[i][j] = 0.0;
      }
   }
   for (k = 0; k < npoints; k++) {
      ae_v_move(xyc.xyR[k], 1, xy->xyR[k], 1, nvars);
      ae_v_sub(xyc.xyR[k], 1, mu.xR, 1, nvars);
   }
   rmatrixgemm(nvars, nvars, npoints, 1.0, &xyc, 0, 0, 1, &xyc, 0, 0, 0, 0.0, &st, 0, 0);
// Create SW matrix
   ae_matrix_set_length(&sw, nvars, nvars);
   for (i = 0; i < nvars; i++) {
      for (j = 0; j < nvars; j++) {
         sw.xyR[i][j] = 0.0;
      }
   }
   for (k = 0; k < npoints; k++) {
      ae_v_move(xyc.xyR[k], 1, xy->xyR[k], 1, nvars);
      ae_v_sub(xyc.xyR[k], 1, muc.xyR[c.xZ[k]], 1, nvars);
   }
   rmatrixgemm(nvars, nvars, npoints, 1.0, &xyc, 0, 0, 1, &xyc, 0, 0, 0, 0.0, &sw, 0, 0);
// Maximize ratio J=(w'*ST*w)/(w'*SW*w).
//
// First, make transition from w to v such that w'*ST*w becomes v'*v:
//    v  = root(ST)*w = R*w
//    R  = root(D)*Z'
//    w  = (root(ST)^-1)*v = RI*v
//    RI = Z*inv(root(D))
//    J  = (v'*v)/(v'*(RI'*SW*RI)*v)
//    ST = Z*D*Z'
//
//    so we have
//
//    J = (v'*v) / (v'*(inv(root(D))*Z'*SW*Z*inv(root(D)))*v)  =
//      = (v'*v) / (v'*A*v)
   if (!smatrixevd(&st, nvars, 1, true, &d, &z)) {
      *info = -4;
      ae_frame_leave();
      return;
   }
   ae_matrix_set_length(w, nvars, nvars);
   if (d.xR[nvars - 1] <= 0.0 || d.xR[0] <= 1000 * ae_machineepsilon * d.xR[nvars - 1]) {
   // Special case: D[NVars-1] <= 0
   // Degenerate task (all variables takes the same value).
      if (d.xR[nvars - 1] <= 0.0) {
         *info = 2;
         for (i = 0; i < nvars; i++) {
            for (j = 0; j < nvars; j++) {
               if (i == j) {
                  w->xyR[i][j] = 1.0;
               } else {
                  w->xyR[i][j] = 0.0;
               }
            }
         }
         ae_frame_leave();
         return;
      }
   // Special case: degenerate ST matrix, multicollinearity found.
   // Since we know ST eigenvalues/vectors we can translate task to
   // non-degenerate form.
   //
   // Let WG is orthogonal basis of the non zero variance subspace
   // of the ST and let WZ is orthogonal basis of the zero variance
   // subspace.
   //
   // Projection on WG allows us to use LDA on reduced M-dimensional
   // subspace, N-M vectors of WZ allows us to update reduced LDA
   // factors to full N-dimensional subspace.
      m = 0;
      for (k = 0; k < nvars; k++) {
         if (d.xR[k] <= 1000 * ae_machineepsilon * d.xR[nvars - 1]) {
            m = k + 1;
         }
      }
      ae_assert(m != 0, "FisherLDAN: internal error #1");
      ae_matrix_set_length(&xyproj, npoints, nvars - m + 1);
      rmatrixgemm(npoints, nvars - m, nvars, 1.0, xy, 0, 0, 0, &z, 0, m, 0, 0.0, &xyproj, 0, 0);
      for (i = 0; i < npoints; i++) {
         xyproj.xyR[i][nvars - m] = xy->xyR[i][nvars];
      }
      fisherldan(&xyproj, npoints, nvars - m, nclasses, info, &wproj);
      if (*info < 0) {
         ae_frame_leave();
         return;
      }
      rmatrixgemm(nvars, nvars - m, nvars - m, 1.0, &z, 0, m, 0, &wproj, 0, 0, 0, 0.0, w, 0, 0);
      for (k = nvars - m; k < nvars; k++) {
         ae_v_move(&w->xyR[0][k], w->stride, &z.xyR[0][k - (nvars - m)], z.stride, nvars);
      }
      *info = 2;
   } else {
   // General case: no multicollinearity
      ae_matrix_set_length(&tm, nvars, nvars);
      ae_matrix_set_length(&a, nvars, nvars);
      rmatrixgemm(nvars, nvars, nvars, 1.0, &sw, 0, 0, 0, &z, 0, 0, 0, 0.0, &tm, 0, 0);
      rmatrixgemm(nvars, nvars, nvars, 1.0, &z, 0, 0, 1, &tm, 0, 0, 0, 0.0, &a, 0, 0);
      for (i = 0; i < nvars; i++) {
         for (j = 0; j < nvars; j++) {
            a.xyR[i][j] /= sqrt(d.xR[i] * d.xR[j]);
         }
      }
      if (!smatrixevd(&a, nvars, 1, true, &d2, &z2)) {
         *info = -4;
         ae_frame_leave();
         return;
      }
      for (i = 0; i < nvars; i++) {
         for (k = 0; k < nvars; k++) {
            z2.xyR[i][k] /= sqrt(d.xR[i]);
         }
      }
      rmatrixgemm(nvars, nvars, nvars, 1.0, &z, 0, 0, 0, &z2, 0, 0, 0, 0.0, w, 0, 0);
   }
// Post-processing:
// * normalization
// * converting to non-negative form, if possible
   for (k = 0; k < nvars; k++) {
      v = ae_v_dotproduct(&w->xyR[0][k], w->stride, &w->xyR[0][k], w->stride, nvars);
      v = 1 / sqrt(v);
      ae_v_muld(&w->xyR[0][k], w->stride, nvars, v);
      v = 0.0;
      for (i = 0; i < nvars; i++) {
         v += w->xyR[i][k];
      }
      if (v < 0.0) {
         ae_v_muld(&w->xyR[0][k], w->stride, nvars, -1);
      }
   }
   ae_frame_leave();
}
} // end of namespace alglib_impl

namespace alglib {
void fisherlda(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, ae_int_t &info, real_1d_array &w) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::fisherlda(ConstT(ae_matrix, xy), npoints, nvars, nclasses, &info, ConstT(ae_vector, w));
   alglib_impl::ae_state_clear();
}

void fisherldan(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, ae_int_t &info, real_2d_array &w) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::fisherldan(ConstT(ae_matrix, xy), npoints, nvars, nclasses, &info, ConstT(ae_matrix, w));
   alglib_impl::ae_state_clear();
}
} // end of namespace alglib

// === MCPD Package ===
// Depends on: (Optimization) MINBLEIC
namespace alglib_impl {
static const double mcpd_xtol = 1.0E-8;

// Internal initialization function
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
static void mcpd_mcpdinit(ae_int_t n, ae_int_t entrystate, ae_int_t exitstate, mcpdstate *s) {
   ae_int_t i;
   ae_int_t j;
   ae_assert(n >= 1, "MCPDCreate: N<1");
   s->n = n;
   ae_vector_set_length(&s->states, n);
   for (i = 0; i < n; i++) {
      s->states.xZ[i] = 0;
   }
   if (entrystate >= 0) {
      s->states.xZ[entrystate] = 1;
   }
   if (exitstate >= 0) {
      s->states.xZ[exitstate] = -1;
   }
   s->npairs = 0;
   s->regterm = 1.0E-8;
   s->ccnt = 0;
   ae_matrix_set_length(&s->p, n, n);
   ae_matrix_set_length(&s->ec, n, n);
   ae_matrix_set_length(&s->bndl, n, n);
   ae_matrix_set_length(&s->bndu, n, n);
   ae_vector_set_length(&s->pw, n);
   ae_matrix_set_length(&s->priorp, n, n);
   ae_vector_set_length(&s->tmpp, n * n);
   ae_vector_set_length(&s->effectivew, n);
   ae_vector_set_length(&s->effectivebndl, n * n);
   ae_vector_set_length(&s->effectivebndu, n * n);
   ae_vector_set_length(&s->h, n * n);
   for (i = 0; i < n; i++) {
      for (j = 0; j < n; j++) {
         s->p.xyR[i][j] = 0.0;
         s->priorp.xyR[i][j] = 0.0;
         s->bndl.xyR[i][j] = -INFINITY;
         s->bndu.xyR[i][j] = +INFINITY;
         s->ec.xyR[i][j] = NAN;
      }
      s->pw.xR[i] = 0.0;
      s->priorp.xyR[i][i] = 1.0;
   }
   ae_matrix_set_length(&s->data, 1, 2 * n);
   for (i = 0; i < 2 * n; i++) {
      s->data.xyR[0][i] = 0.0;
   }
   for (i = 0; i < n * n; i++) {
      s->tmpp.xR[i] = 0.0;
   }
   minbleiccreate(n * n, &s->tmpp, &s->bs);
}

// This function creates MCPD (Markov Chains for Population Data) solver.
//
// This  solver  can  be  used  to find transition matrix P for N-dimensional
// prediction  problem  where transition from X[i] to X[i+1] is  modelled  as
//     X[i+1] = P*X[i]
// where X[i] and X[i+1] are N-dimensional population vectors (components  of
// each X are non-negative), and P is a N*N transition matrix (elements of  P
// are non-negative, each column sums to 1.0).
//
// Such models arise when when:
// * there is some population of individuals
// * individuals can have different states
// * individuals can transit from one state to another
// * population size is constant, i.e. there is no new individuals and no one
//   leaves population
// * you want to model transitions of individuals from one state into another
//
// USAGE:
//
// Here we give very brief outline of the MCPD. We strongly recommend you  to
// read examples in the ALGLIB Reference Manual and to read ALGLIB User Guide
// on data analysis which is available at http://www.alglib.net/dataanalysis/
//
// 1. User initializes algorithm state with MCPDCreate() call
//
// 2. User  adds  one  or  more  tracks -  sequences of states which describe
//    evolution of a system being modelled from different starting conditions
//
// 3. User may add optional boundary, equality  and/or  linear constraints on
//    the coefficients of P by calling one of the following functions:
//    * MCPDSetEC() to set equality constraints
//    * MCPDSetBC() to set bound constraints
//    * MCPDSetLC() to set linear constraints
//
// 4. Optionally,  user  may  set  custom  weights  for prediction errors (by
//    default, algorithm assigns non-equal, automatically chosen weights  for
//    errors in the prediction of different components of X). It can be  done
//    with a call of MCPDSetPredictionWeights() function.
//
// 5. User calls MCPDSolve() function which takes algorithm  state and
//    pointer (delegate, etc.) to callback function which calculates F/G.
//
// 6. User calls MCPDResults() to get solution
//
// Inputs:
//     N       -   problem dimension, N >= 1
//
// Outputs:
//     State   -   structure stores algorithm state
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdcreate(const ae_int_t n, mcpdstate &s);
void mcpdcreate(ae_int_t n, mcpdstate *s) {
   SetObj(mcpdstate, s);
   ae_assert(n >= 1, "MCPDCreate: N<1");
   mcpd_mcpdinit(n, -1, -1, s);
}

// This function is a specialized version of MCPDCreate()  function,  and  we
// recommend  you  to read comments for this function for general information
// about MCPD solver.
//
// This  function  creates  MCPD (Markov Chains for Population  Data)  solver
// for "Entry-state" model,  i.e. model  where transition from X[i] to X[i+1]
// is modelled as
//     X[i+1] = P*X[i]
// where
//     X[i] and X[i+1] are N-dimensional state vectors
//     P is a N*N transition matrix
// and  one  selected component of X[] is called "entry" state and is treated
// in a special way:
//     system state always transits from "entry" state to some another state
//     system state can not transit from any state into "entry" state
// Such conditions basically mean that row of P which corresponds to  "entry"
// state is zero.
//
// Such models arise when:
// * there is some population of individuals
// * individuals can have different states
// * individuals can transit from one state to another
// * population size is NOT constant -  at every moment of time there is some
//   (unpredictable) amount of "new" individuals, which can transit into  one
//   of the states at the next turn, but still no one leaves population
// * you want to model transitions of individuals from one state into another
// * but you do NOT want to predict amount of "new"  individuals  because  it
//   does not depends on individuals already present (hence  system  can  not
//   transit INTO entry state - it can only transit FROM it).
//
// This model is discussed  in  more  details  in  the ALGLIB User Guide (see
// http://www.alglib.net/dataanalysis/ for more data).
//
// Inputs:
//     N       -   problem dimension, N >= 2
//     EntryState- index of entry state, in 0..N-1
//
// Outputs:
//     State   -   structure stores algorithm state
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdcreateentry(const ae_int_t n, const ae_int_t entrystate, mcpdstate &s);
void mcpdcreateentry(ae_int_t n, ae_int_t entrystate, mcpdstate *s) {
   SetObj(mcpdstate, s);
   ae_assert(n >= 2, "MCPDCreateEntry: N<2");
   ae_assert(entrystate >= 0, "MCPDCreateEntry: EntryState<0");
   ae_assert(entrystate < n, "MCPDCreateEntry: EntryState >= N");
   mcpd_mcpdinit(n, entrystate, -1, s);
}

// This function is a specialized version of MCPDCreate()  function,  and  we
// recommend  you  to read comments for this function for general information
// about MCPD solver.
//
// This  function  creates  MCPD (Markov Chains for Population  Data)  solver
// for "Exit-state" model,  i.e. model  where  transition from X[i] to X[i+1]
// is modelled as
//     X[i+1] = P*X[i]
// where
//     X[i] and X[i+1] are N-dimensional state vectors
//     P is a N*N transition matrix
// and  one  selected component of X[] is called "exit"  state and is treated
// in a special way:
//     system state can transit from any state into "exit" state
//     system state can not transit from "exit" state into any other state
//     transition operator discards "exit" state (makes it zero at each turn)
// Such  conditions  basically  mean  that  column  of P which corresponds to
// "exit" state is zero. Multiplication by such P may decrease sum of  vector
// components.
//
// Such models arise when:
// * there is some population of individuals
// * individuals can have different states
// * individuals can transit from one state to another
// * population size is NOT constant - individuals can move into "exit" state
//   and leave population at the next turn, but there are no new individuals
// * amount of individuals which leave population can be predicted
// * you want to model transitions of individuals from one state into another
//   (including transitions into the "exit" state)
//
// This model is discussed  in  more  details  in  the ALGLIB User Guide (see
// http://www.alglib.net/dataanalysis/ for more data).
//
// Inputs:
//     N       -   problem dimension, N >= 2
//     ExitState-  index of exit state, in 0..N-1
//
// Outputs:
//     State   -   structure stores algorithm state
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdcreateexit(const ae_int_t n, const ae_int_t exitstate, mcpdstate &s);
void mcpdcreateexit(ae_int_t n, ae_int_t exitstate, mcpdstate *s) {
   SetObj(mcpdstate, s);
   ae_assert(n >= 2, "MCPDCreateExit: N<2");
   ae_assert(exitstate >= 0, "MCPDCreateExit: ExitState<0");
   ae_assert(exitstate < n, "MCPDCreateExit: ExitState >= N");
   mcpd_mcpdinit(n, -1, exitstate, s);
}

// This function is a specialized version of MCPDCreate()  function,  and  we
// recommend  you  to read comments for this function for general information
// about MCPD solver.
//
// This  function  creates  MCPD (Markov Chains for Population  Data)  solver
// for "Entry-Exit-states" model, i.e. model where  transition  from  X[i] to
// X[i+1] is modelled as
//     X[i+1] = P*X[i]
// where
//     X[i] and X[i+1] are N-dimensional state vectors
//     P is a N*N transition matrix
// one selected component of X[] is called "entry" state and is treated in  a
// special way:
//     system state always transits from "entry" state to some another state
//     system state can not transit from any state into "entry" state
// and another one component of X[] is called "exit" state and is treated  in
// a special way too:
//     system state can transit from any state into "exit" state
//     system state can not transit from "exit" state into any other state
//     transition operator discards "exit" state (makes it zero at each turn)
// Such conditions basically mean that:
//     row of P which corresponds to "entry" state is zero
//     column of P which corresponds to "exit" state is zero
// Multiplication by such P may decrease sum of vector components.
//
// Such models arise when:
// * there is some population of individuals
// * individuals can have different states
// * individuals can transit from one state to another
// * population size is NOT constant
// * at every moment of time there is some (unpredictable)  amount  of  "new"
//   individuals, which can transit into one of the states at the next turn
// * some  individuals  can  move  (predictably)  into "exit" state and leave
//   population at the next turn
// * you want to model transitions of individuals from one state into another,
//   including transitions from the "entry" state and into the "exit" state.
// * but you do NOT want to predict amount of "new"  individuals  because  it
//   does not depends on individuals already present (hence  system  can  not
//   transit INTO entry state - it can only transit FROM it).
//
// This model is discussed  in  more  details  in  the ALGLIB User Guide (see
// http://www.alglib.net/dataanalysis/ for more data).
//
// Inputs:
//     N       -   problem dimension, N >= 2
//     EntryState- index of entry state, in 0..N-1
//     ExitState-  index of exit state, in 0..N-1
//
// Outputs:
//     State   -   structure stores algorithm state
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdcreateentryexit(const ae_int_t n, const ae_int_t entrystate, const ae_int_t exitstate, mcpdstate &s);
void mcpdcreateentryexit(ae_int_t n, ae_int_t entrystate, ae_int_t exitstate, mcpdstate *s) {
   SetObj(mcpdstate, s);
   ae_assert(n >= 2, "MCPDCreateEntryExit: N<2");
   ae_assert(entrystate >= 0, "MCPDCreateEntryExit: EntryState<0");
   ae_assert(entrystate < n, "MCPDCreateEntryExit: EntryState >= N");
   ae_assert(exitstate >= 0, "MCPDCreateEntryExit: ExitState<0");
   ae_assert(exitstate < n, "MCPDCreateEntryExit: ExitState >= N");
   ae_assert(entrystate != exitstate, "MCPDCreateEntryExit: EntryState=ExitState");
   mcpd_mcpdinit(n, entrystate, exitstate, s);
}

// This  function  is  used to add a track - sequence of system states at the
// different moments of its evolution.
//
// You  may  add  one  or several tracks to the MCPD solver. In case you have
// several tracks, they won't overwrite each other. For example,  if you pass
// two tracks, A1-A2-A3 (system at t=A+1, t=A+2 and t=A+3) and B1-B2-B3, then
// solver will try to model transitions from t=A+1 to t=A+2, t=A+2 to  t=A+3,
// t=B+1 to t=B+2, t=B+2 to t=B+3. But it WONT mix these two tracks - i.e. it
// wont try to model transition from t=A+3 to t=B+1.
//
// Inputs:
//     S       -   solver
//     XY      -   track, array[K,N]:
//                 * I-th row is a state at t=I
//                 * elements of XY must be non-negative (exception will be
//                   thrown on negative elements)
//     K       -   number of points in a track
//                 * if given, only leading K rows of XY are used
//                 * if not given, automatically determined from size of XY
//
// NOTES:
//
// 1. Track may contain either proportional or population data:
//    * with proportional data all rows of XY must sum to 1.0, i.e. we have
//      proportions instead of absolute population values
//    * with population data rows of XY contain population counts and generally
//      do not sum to 1.0 (although they still must be non-negative)
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdaddtrack(const mcpdstate &s, const real_2d_array &xy, const ae_int_t k);
// API: void mcpdaddtrack(const mcpdstate &s, const real_2d_array &xy);
void mcpdaddtrack(mcpdstate *s, RMatrix *xy, ae_int_t k) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t n;
   double s0;
   double s1;
   n = s->n;
   ae_assert(k >= 0, "MCPDAddTrack: K<0");
   ae_assert(xy->cols >= n, "MCPDAddTrack: Cols(XY)<N");
   ae_assert(xy->rows >= k, "MCPDAddTrack: Rows(XY)<K");
   ae_assert(apservisfinitematrix(xy, k, n), "MCPDAddTrack: XY contains infinite or NaN elements");
   for (i = 0; i < k; i++) {
      for (j = 0; j < n; j++) {
         ae_assert(xy->xyR[i][j] >= 0.0, "MCPDAddTrack: XY contains negative elements");
      }
   }
   if (k < 2) {
      return;
   }
   if (s->data.rows < s->npairs + k - 1) {
      rmatrixresize(&s->data, imax2(2 * s->data.rows, s->npairs + k - 1), 2 * n);
   }
   for (i = 0; i < k - 1; i++) {
      s0 = 0.0;
      s1 = 0.0;
      for (j = 0; j < n; j++) {
         if (s->states.xZ[j] >= 0) {
            s0 += xy->xyR[i][j];
         }
         if (s->states.xZ[j] <= 0) {
            s1 += xy->xyR[i + 1][j];
         }
      }
      if (s0 > 0.0 && s1 > 0.0) {
         for (j = 0; j < n; j++) {
            if (s->states.xZ[j] >= 0) {
               s->data.xyR[s->npairs][j] = xy->xyR[i][j] / s0;
            } else {
               s->data.xyR[s->npairs][j] = 0.0;
            }
            if (s->states.xZ[j] <= 0) {
               s->data.xyR[s->npairs][n + j] = xy->xyR[i + 1][j] / s1;
            } else {
               s->data.xyR[s->npairs][n + j] = 0.0;
            }
         }
         s->npairs++;
      }
   }
}

// This function is used to add equality constraints on the elements  of  the
// transition matrix P.
//
// MCPD solver has four types of constraints which can be placed on P:
// * user-specified equality constraints (optional)
// * user-specified bound constraints (optional)
// * user-specified general linear constraints (optional)
// * basic constraints (always present):
//   * non-negativity: P[i,j] >= 0
//   * consistency: every column of P sums to 1.0
//
// Final  constraints  which  are  passed  to  the  underlying  optimizer are
// calculated  as  intersection  of all present constraints. For example, you
// may specify boundary constraint on P[0,0] and equality one:
//     0.1 <= P[0,0] <= 0.9
//     P[0,0]=0.5
// Such  combination  of  constraints  will  be  silently  reduced  to  their
// intersection, which is P[0,0]=0.5.
//
// This  function  can  be  used  to  place equality constraints on arbitrary
// subset of elements of P. Set of constraints is specified by EC, which  may
// contain either NAN's or finite numbers from [0,1]. NAN denotes absence  of
// constraint, finite number denotes equality constraint on specific  element
// of P.
//
// You can also  use  MCPDAddEC()  function  which  allows  to  ADD  equality
// constraint  for  one  element  of P without changing constraints for other
// elements.
//
// These functions (MCPDSetEC and MCPDAddEC) interact as follows:
// * there is internal matrix of equality constraints which is stored in  the
//   MCPD solver
// * MCPDSetEC() replaces this matrix by another one (SET)
// * MCPDAddEC() modifies one element of this matrix and  leaves  other  ones
//   unchanged (ADD)
// * thus  MCPDAddEC()  call  preserves  all  modifications  done by previous
//   calls,  while  MCPDSetEC()  completely discards all changes  done to the
//   equality constraints.
//
// Inputs:
//     S       -   solver
//     EC      -   equality constraints, array[N,N]. Elements of  EC  can  be
//                 either NAN's or finite  numbers from  [0,1].  NAN  denotes
//                 absence  of  constraints,  while  finite  value    denotes
//                 equality constraint on the corresponding element of P.
//
// NOTES:
//
// 1. infinite values of EC will lead to exception being thrown. Values  less
// than 0.0 or greater than 1.0 will lead to error code being returned  after
// call to MCPDSolve().
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdsetec(const mcpdstate &s, const real_2d_array &ec);
void mcpdsetec(mcpdstate *s, RMatrix *ec) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t n;
   n = s->n;
   ae_assert(ec->cols >= n, "MCPDSetEC: Cols(EC)<N");
   ae_assert(ec->rows >= n, "MCPDSetEC: Rows(EC)<N");
   for (i = 0; i < n; i++) {
      for (j = 0; j < n; j++) {
         ae_assert(isfinite(ec->xyR[i][j]) || isnan(ec->xyR[i][j]), "MCPDSetEC: EC containts infinite elements");
         s->ec.xyR[i][j] = ec->xyR[i][j];
      }
   }
}

// This function is used to add equality constraints on the elements  of  the
// transition matrix P.
//
// MCPD solver has four types of constraints which can be placed on P:
// * user-specified equality constraints (optional)
// * user-specified bound constraints (optional)
// * user-specified general linear constraints (optional)
// * basic constraints (always present):
//   * non-negativity: P[i,j] >= 0
//   * consistency: every column of P sums to 1.0
//
// Final  constraints  which  are  passed  to  the  underlying  optimizer are
// calculated  as  intersection  of all present constraints. For example, you
// may specify boundary constraint on P[0,0] and equality one:
//     0.1 <= P[0,0] <= 0.9
//     P[0,0]=0.5
// Such  combination  of  constraints  will  be  silently  reduced  to  their
// intersection, which is P[0,0]=0.5.
//
// This function can be used to ADD equality constraint for one element of  P
// without changing constraints for other elements.
//
// You  can  also  use  MCPDSetEC()  function  which  allows  you  to specify
// arbitrary set of equality constraints in one call.
//
// These functions (MCPDSetEC and MCPDAddEC) interact as follows:
// * there is internal matrix of equality constraints which is stored in the
//   MCPD solver
// * MCPDSetEC() replaces this matrix by another one (SET)
// * MCPDAddEC() modifies one element of this matrix and leaves  other  ones
//   unchanged (ADD)
// * thus  MCPDAddEC()  call  preserves  all  modifications done by previous
//   calls,  while  MCPDSetEC()  completely discards all changes done to the
//   equality constraints.
//
// Inputs:
//     S       -   solver
//     I       -   row index of element being constrained
//     J       -   column index of element being constrained
//     C       -   value (constraint for P[I,J]).  Can  be  either  NAN  (no
//                 constraint) or finite value from [0,1].
//
// NOTES:
//
// 1. infinite values of C  will lead to exception being thrown. Values  less
// than 0.0 or greater than 1.0 will lead to error code being returned  after
// call to MCPDSolve().
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdaddec(const mcpdstate &s, const ae_int_t i, const ae_int_t j, const double c);
void mcpdaddec(mcpdstate *s, ae_int_t i, ae_int_t j, double c) {
   ae_assert(i >= 0, "MCPDAddEC: I<0");
   ae_assert(i < s->n, "MCPDAddEC: I >= N");
   ae_assert(j >= 0, "MCPDAddEC: J<0");
   ae_assert(j < s->n, "MCPDAddEC: J >= N");
   ae_assert(isnan(c) || isfinite(c), "MCPDAddEC: C is not finite number or NAN");
   s->ec.xyR[i][j] = c;
}

// This function is used to add bound constraints  on  the  elements  of  the
// transition matrix P.
//
// MCPD solver has four types of constraints which can be placed on P:
// * user-specified equality constraints (optional)
// * user-specified bound constraints (optional)
// * user-specified general linear constraints (optional)
// * basic constraints (always present):
//   * non-negativity: P[i,j] >= 0
//   * consistency: every column of P sums to 1.0
//
// Final  constraints  which  are  passed  to  the  underlying  optimizer are
// calculated  as  intersection  of all present constraints. For example, you
// may specify boundary constraint on P[0,0] and equality one:
//     0.1 <= P[0,0] <= 0.9
//     P[0,0]=0.5
// Such  combination  of  constraints  will  be  silently  reduced  to  their
// intersection, which is P[0,0]=0.5.
//
// This  function  can  be  used  to  place bound   constraints  on arbitrary
// subset  of  elements  of  P.  Set of constraints is specified by BndL/BndU
// matrices, which may contain arbitrary combination  of  finite  numbers  or
// infinities (like -INF < x <= 0.5 or 0.1 <= x < +INF).
//
// You can also use MCPDAddBC() function which allows to ADD bound constraint
// for one element of P without changing constraints for other elements.
//
// These functions (MCPDSetBC and MCPDAddBC) interact as follows:
// * there is internal matrix of bound constraints which is stored in the
//   MCPD solver
// * MCPDSetBC() replaces this matrix by another one (SET)
// * MCPDAddBC() modifies one element of this matrix and  leaves  other  ones
//   unchanged (ADD)
// * thus  MCPDAddBC()  call  preserves  all  modifications  done by previous
//   calls,  while  MCPDSetBC()  completely discards all changes  done to the
//   equality constraints.
//
// Inputs:
//     S       -   solver
//     BndL    -   lower bounds constraints, array[N,N]. Elements of BndL can
//                 be finite numbers or -INF.
//     BndU    -   upper bounds constraints, array[N,N]. Elements of BndU can
//                 be finite numbers or +INF.
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdsetbc(const mcpdstate &s, const real_2d_array &bndl, const real_2d_array &bndu);
void mcpdsetbc(mcpdstate *s, RMatrix *bndl, RMatrix *bndu) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t n;
   n = s->n;
   ae_assert(bndl->cols >= n, "MCPDSetBC: Cols(BndL)<N");
   ae_assert(bndl->rows >= n, "MCPDSetBC: Rows(BndL)<N");
   ae_assert(bndu->cols >= n, "MCPDSetBC: Cols(BndU)<N");
   ae_assert(bndu->rows >= n, "MCPDSetBC: Rows(BndU)<N");
   for (i = 0; i < n; i++) {
      for (j = 0; j < n; j++) {
         ae_assert(isfinite(bndl->xyR[i][j]) || isneginf(bndl->xyR[i][j]), "MCPDSetBC: BndL containts NAN or +INF");
         ae_assert(isfinite(bndu->xyR[i][j]) || isposinf(bndu->xyR[i][j]), "MCPDSetBC: BndU containts NAN or -INF");
         s->bndl.xyR[i][j] = bndl->xyR[i][j];
         s->bndu.xyR[i][j] = bndu->xyR[i][j];
      }
   }
}

// This function is used to add bound constraints  on  the  elements  of  the
// transition matrix P.
//
// MCPD solver has four types of constraints which can be placed on P:
// * user-specified equality constraints (optional)
// * user-specified bound constraints (optional)
// * user-specified general linear constraints (optional)
// * basic constraints (always present):
//   * non-negativity: P[i,j] >= 0
//   * consistency: every column of P sums to 1.0
//
// Final  constraints  which  are  passed  to  the  underlying  optimizer are
// calculated  as  intersection  of all present constraints. For example, you
// may specify boundary constraint on P[0,0] and equality one:
//     0.1 <= P[0,0] <= 0.9
//     P[0,0]=0.5
// Such  combination  of  constraints  will  be  silently  reduced  to  their
// intersection, which is P[0,0]=0.5.
//
// This  function  can  be  used to ADD bound constraint for one element of P
// without changing constraints for other elements.
//
// You  can  also  use  MCPDSetBC()  function  which  allows to  place  bound
// constraints  on arbitrary subset of elements of P.   Set of constraints is
// specified  by  BndL/BndU matrices, which may contain arbitrary combination
// of finite numbers or infinities (like -INF < x <= 0.5 or 0.1 <= x < +INF).
//
// These functions (MCPDSetBC and MCPDAddBC) interact as follows:
// * there is internal matrix of bound constraints which is stored in the
//   MCPD solver
// * MCPDSetBC() replaces this matrix by another one (SET)
// * MCPDAddBC() modifies one element of this matrix and  leaves  other  ones
//   unchanged (ADD)
// * thus  MCPDAddBC()  call  preserves  all  modifications  done by previous
//   calls,  while  MCPDSetBC()  completely discards all changes  done to the
//   equality constraints.
//
// Inputs:
//     S       -   solver
//     I       -   row index of element being constrained
//     J       -   column index of element being constrained
//     BndL    -   lower bound
//     BndU    -   upper bound
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdaddbc(const mcpdstate &s, const ae_int_t i, const ae_int_t j, const double bndl, const double bndu);
void mcpdaddbc(mcpdstate *s, ae_int_t i, ae_int_t j, double bndl, double bndu) {
   ae_assert(i >= 0, "MCPDAddBC: I<0");
   ae_assert(i < s->n, "MCPDAddBC: I >= N");
   ae_assert(j >= 0, "MCPDAddBC: J<0");
   ae_assert(j < s->n, "MCPDAddBC: J >= N");
   ae_assert(isfinite(bndl) || isneginf(bndl), "MCPDAddBC: BndL is NAN or +INF");
   ae_assert(isfinite(bndu) || isposinf(bndu), "MCPDAddBC: BndU is NAN or -INF");
   s->bndl.xyR[i][j] = bndl;
   s->bndu.xyR[i][j] = bndu;
}

// This function is used to set linear equality/inequality constraints on the
// elements of the transition matrix P.
//
// This function can be used to set one or several general linear constraints
// on the elements of P. Two types of constraints are supported:
// * equality constraints
// * inequality constraints (both less-or-equal and greater-or-equal)
//
// Coefficients  of  constraints  are  specified  by  matrix  C (one  of  the
// parameters).  One  row  of  C  corresponds  to  one  constraint.   Because
// transition  matrix P has N*N elements,  we  need  N*N columns to store all
// coefficients  (they  are  stored row by row), and one more column to store
// right part - hence C has N*N+1 columns.  Constraint  kind is stored in the
// CT array.
//
// Thus, I-th linear constraint is
//     P[0,0]*C[I,0] + P[0,1]*C[I,1] + .. + P[0,N-1]*C[I,N-1] +
//         + P[1,0]*C[I,N] + P[1,1]*C[I,N+1] + ... +
//         + P[N-1,N-1]*C[I,N*N-1]  ?=?  C[I,N*N]
// where ?=? can be either "=" (CT[i]=0), "<=" (CT[i] < 0) or ">=" (CT[i] > 0).
//
// Your constraint may involve only some subset of P (less than N*N elements).
// For example it can be something like
//     P[0,0] + P[0,1] = 0.5
// In this case you still should pass matrix  with N*N+1 columns, but all its
// elements (except for C[0,0], C[0,1] and C[0,N*N-1]) will be zero.
//
// Inputs:
//     S       -   solver
//     C       -   array[K,N*N+1] - coefficients of constraints
//                 (see above for complete description)
//     CT      -   array[K] - constraint types
//                 (see above for complete description)
//     K       -   number of equality/inequality constraints, K >= 0:
//                 * if given, only leading K elements of C/CT are used
//                 * if not given, automatically determined from sizes of C/CT
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdsetlc(const mcpdstate &s, const real_2d_array &c, const integer_1d_array &ct, const ae_int_t k);
// API: void mcpdsetlc(const mcpdstate &s, const real_2d_array &c, const integer_1d_array &ct);
void mcpdsetlc(mcpdstate *s, RMatrix *c, ZVector *ct, ae_int_t k) {
   ae_int_t i;
   ae_int_t j;
   ae_int_t n;
   n = s->n;
   ae_assert(c->cols >= n * n + 1, "MCPDSetLC: Cols(C)<N*N+1");
   ae_assert(c->rows >= k, "MCPDSetLC: Rows(C)<K");
   ae_assert(ct->cnt >= k, "MCPDSetLC: Len(CT)<K");
   ae_assert(apservisfinitematrix(c, k, n * n + 1), "MCPDSetLC: C contains infinite or NaN values!");
   matrixsetlengthatleast(&s->c, k, n * n + 1);
   vectorsetlengthatleast(&s->ct, k);
   for (i = 0; i < k; i++) {
      for (j = 0; j <= n * n; j++) {
         s->c.xyR[i][j] = c->xyR[i][j];
      }
      s->ct.xZ[i] = ct->xZ[i];
   }
   s->ccnt = k;
}

// This function allows to  tune  amount  of  Tikhonov  regularization  being
// applied to your problem.
//
// By default, regularizing term is equal to r*||P-prior_P||^2, where r is  a
// small non-zero value,  P is transition matrix, prior_P is identity matrix,
// ||X||^2 is a sum of squared elements of X.
//
// This  function  allows  you to change coefficient r. You can  also  change
// prior values with MCPDSetPrior() function.
//
// Inputs:
//     S       -   solver
//     V       -   regularization  coefficient, finite non-negative value. It
//                 is  not  recommended  to specify zero value unless you are
//                 pretty sure that you want it.
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdsettikhonovregularizer(const mcpdstate &s, const double v);
void mcpdsettikhonovregularizer(mcpdstate *s, double v) {
   ae_assert(isfinite(v), "MCPDSetTikhonovRegularizer: V is infinite or NAN");
   ae_assert(v >= 0.0, "MCPDSetTikhonovRegularizer: V is less than zero");
   s->regterm = v;
}

// This  function  allows to set prior values used for regularization of your
// problem.
//
// By default, regularizing term is equal to r*||P-prior_P||^2, where r is  a
// small non-zero value,  P is transition matrix, prior_P is identity matrix,
// ||X||^2 is a sum of squared elements of X.
//
// This  function  allows  you to change prior values prior_P. You  can  also
// change r with MCPDSetTikhonovRegularizer() function.
//
// Inputs:
//     S       -   solver
//     PP      -   array[N,N], matrix of prior values:
//                 1. elements must be real numbers from [0,1]
//                 2. columns must sum to 1.0.
//                 First property is checked (exception is thrown otherwise),
//                 while second one is not checked/enforced.
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdsetprior(const mcpdstate &s, const real_2d_array &pp);
void mcpdsetprior(mcpdstate *s, RMatrix *pp) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t n;
   ae_frame_make(&_frame_block);
   DupMatrix(pp);
   n = s->n;
   ae_assert(pp->cols >= n, "MCPDSetPrior: Cols(PP)<N");
   ae_assert(pp->rows >= n, "MCPDSetPrior: Rows(PP)<K");
   for (i = 0; i < n; i++) {
      for (j = 0; j < n; j++) {
         ae_assert(isfinite(pp->xyR[i][j]), "MCPDSetPrior: PP containts infinite elements");
         ae_assert(pp->xyR[i][j] >= 0.0 && pp->xyR[i][j] <= 1.0, "MCPDSetPrior: PP[i,j] is less than 0.0 or greater than 1.0");
         s->priorp.xyR[i][j] = pp->xyR[i][j];
      }
   }
   ae_frame_leave();
}

// This function is used to change prediction weights
//
// MCPD solver scales prediction errors as follows
//     Error(P) = ||W*(y-P*x)||^2
// where
//     x is a system state at time t
//     y is a system state at time t+1
//     P is a transition matrix
//     W is a diagonal scaling matrix
//
// By default, weights are chosen in order  to  minimize  relative prediction
// error instead of absolute one. For example, if one component of  state  is
// about 0.5 in magnitude and another one is about 0.05, then algorithm  will
// make corresponding weights equal to 2.0 and 20.0.
//
// Inputs:
//     S       -   solver
//     PW      -   array[N], weights:
//                 * must be non-negative values (exception will be thrown otherwise)
//                 * zero values will be replaced by automatically chosen values
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdsetpredictionweights(const mcpdstate &s, const real_1d_array &pw);
void mcpdsetpredictionweights(mcpdstate *s, RVector *pw) {
   ae_int_t i;
   ae_int_t n;
   n = s->n;
   ae_assert(pw->cnt >= n, "MCPDSetPredictionWeights: Length(PW)<N");
   for (i = 0; i < n; i++) {
      ae_assert(isfinite(pw->xR[i]), "MCPDSetPredictionWeights: PW containts infinite or NAN elements");
      ae_assert(pw->xR[i] >= 0.0, "MCPDSetPredictionWeights: PW containts negative elements");
      s->pw.xR[i] = pw->xR[i];
   }
}

// This function is used to start solution of the MCPD problem.
//
// After return from this function, you can use MCPDResults() to get solution
// and completion code.
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdsolve(const mcpdstate &s);
void mcpdsolve(mcpdstate *s) {
   ae_int_t n;
   ae_int_t npairs;
   ae_int_t ccnt;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t k2;
   double v;
   double vv;
   n = s->n;
   npairs = s->npairs;
// init fields of S
   s->repterminationtype = 0;
   s->repinneriterationscount = 0;
   s->repouteriterationscount = 0;
   s->repnfev = 0;
   for (k = 0; k < n; k++) {
      for (k2 = 0; k2 < n; k2++) {
         s->p.xyR[k][k2] = NAN;
      }
   }
// Generate "effective" weights for prediction and calculate preconditioner
   for (i = 0; i < n; i++) {
      if (s->pw.xR[i] == 0.0) {
         v = 0.0;
         k = 0;
         for (j = 0; j < npairs; j++) {
            if (s->data.xyR[j][n + i] != 0.0) {
               v += s->data.xyR[j][n + i];
               k++;
            }
         }
         if (k != 0) {
            s->effectivew.xR[i] = k / v;
         } else {
            s->effectivew.xR[i] = 1.0;
         }
      } else {
         s->effectivew.xR[i] = s->pw.xR[i];
      }
   }
   for (i = 0; i < n; i++) {
      for (j = 0; j < n; j++) {
         s->h.xR[i * n + j] = 2 * s->regterm;
      }
   }
   for (k = 0; k < npairs; k++) {
      for (i = 0; i < n; i++) {
         for (j = 0; j < n; j++) {
            s->h.xR[i * n + j] += 2 * ae_sqr(s->effectivew.xR[i]) * ae_sqr(s->data.xyR[k][j]);
         }
      }
   }
   for (i = 0; i < n; i++) {
      for (j = 0; j < n; j++) {
         if (s->h.xR[i * n + j] == 0.0) {
            s->h.xR[i * n + j] = 1.0;
         }
      }
   }
// Generate "effective" BndL/BndU
   for (i = 0; i < n; i++) {
      for (j = 0; j < n; j++) {
      // Set default boundary constraints.
      // Lower bound is always zero, upper bound is calculated
      // with respect to entry/exit states.
         s->effectivebndl.xR[i * n + j] = 0.0;
         if (s->states.xZ[i] > 0 || s->states.xZ[j] < 0) {
            s->effectivebndu.xR[i * n + j] = 0.0;
         } else {
            s->effectivebndu.xR[i * n + j] = 1.0;
         }
      // Calculate intersection of the default and user-specified bound constraints.
      // This code checks consistency of such combination.
         if (isfinite(s->bndl.xyR[i][j]) && s->bndl.xyR[i][j] > s->effectivebndl.xR[i * n + j]) {
            s->effectivebndl.xR[i * n + j] = s->bndl.xyR[i][j];
         }
         if (isfinite(s->bndu.xyR[i][j]) && s->bndu.xyR[i][j] < s->effectivebndu.xR[i * n + j]) {
            s->effectivebndu.xR[i * n + j] = s->bndu.xyR[i][j];
         }
         if (s->effectivebndl.xR[i * n + j] > s->effectivebndu.xR[i * n + j]) {
            s->repterminationtype = -3;
            return;
         }
      // Calculate intersection of the effective bound constraints
      // and user-specified equality constraints.
      // This code checks consistency of such combination.
         if (isfinite(s->ec.xyR[i][j])) {
            if (s->ec.xyR[i][j] < s->effectivebndl.xR[i * n + j] || s->ec.xyR[i][j] > s->effectivebndu.xR[i * n + j]) {
               s->repterminationtype = -3;
               return;
            }
            s->effectivebndl.xR[i * n + j] = s->ec.xyR[i][j];
            s->effectivebndu.xR[i * n + j] = s->ec.xyR[i][j];
         }
      }
   }
// Generate linear constraints:
// * "default" sums-to-one constraints (not generated for "exit" states)
   matrixsetlengthatleast(&s->effectivec, s->ccnt + n, n * n + 1);
   vectorsetlengthatleast(&s->effectivect, s->ccnt + n);
   ccnt = s->ccnt;
   for (i = 0; i < s->ccnt; i++) {
      for (j = 0; j <= n * n; j++) {
         s->effectivec.xyR[i][j] = s->c.xyR[i][j];
      }
      s->effectivect.xZ[i] = s->ct.xZ[i];
   }
   for (i = 0; i < n; i++) {
      if (s->states.xZ[i] >= 0) {
         for (k = 0; k < n * n; k++) {
            s->effectivec.xyR[ccnt][k] = 0.0;
         }
         for (k = 0; k < n; k++) {
            s->effectivec.xyR[ccnt][k * n + i] = 1.0;
         }
         s->effectivec.xyR[ccnt][n * n] = 1.0;
         s->effectivect.xZ[ccnt] = 0;
         ccnt++;
      }
   }
// create optimizer
   for (i = 0; i < n; i++) {
      for (j = 0; j < n; j++) {
         s->tmpp.xR[i * n + j] = 1.0 / (double)n;
      }
   }
   minbleicsetbc(&s->bs, &s->effectivebndl, &s->effectivebndu);
   minbleicsetlc(&s->bs, &s->effectivec, &s->effectivect, ccnt);
   minbleicsetcond(&s->bs, 0.0, 0.0, mcpd_xtol, 0);
   minbleicsetprecdiag(&s->bs, &s->h);
// solve problem
   for (minbleicrestartfrom(&s->bs, &s->tmpp); minbleiciteration(&s->bs); ) {
      if (s->bs.needfg) {
      // Calculate regularization term
         s->bs.f = 0.0;
         vv = s->regterm;
         for (i = 0; i < n; i++) {
            for (j = 0; j < n; j++) {
               s->bs.f += vv * ae_sqr(s->bs.x.xR[i * n + j] - s->priorp.xyR[i][j]);
               s->bs.g.xR[i * n + j] = 2 * vv * (s->bs.x.xR[i * n + j] - s->priorp.xyR[i][j]);
            }
         }
      // calculate prediction error/gradient for K-th pair
         for (k = 0; k < npairs; k++) {
            for (i = 0; i < n; i++) {
               v = ae_v_dotproduct(&s->bs.x.xR[i * n], 1, s->data.xyR[k], 1, n);
               vv = s->effectivew.xR[i];
               s->bs.f += ae_sqr(vv * (v - s->data.xyR[k][n + i]));
               for (j = 0; j < n; j++) {
                  s->bs.g.xR[i * n + j] += 2 * vv * vv * (v - s->data.xyR[k][n + i]) * s->data.xyR[k][j];
               }
            }
         }
      } else ae_assert(false, "MCPDSolve: internal error");
   }
   minbleicresultsbuf(&s->bs, &s->tmpp, &s->br);
   for (i = 0; i < n; i++) {
      for (j = 0; j < n; j++) {
         s->p.xyR[i][j] = s->tmpp.xR[i * n + j];
      }
   }
   s->repterminationtype = s->br.terminationtype;
   s->repinneriterationscount = s->br.inneriterationscount;
   s->repouteriterationscount = s->br.outeriterationscount;
   s->repnfev = s->br.nfev;
}

// MCPD results
//
// Inputs:
//     State   -   algorithm state
//
// Outputs:
//     P       -   array[N,N], transition matrix
//     Rep     -   optimization report. You should check Rep.TerminationType
//                 in  order  to  distinguish  successful  termination  from
//                 unsuccessful one. Speaking short, positive values  denote
//                 success, negative ones are failures.
//                 More information about fields of this  structure  can  be
//                 found in the comments on MCPDReport datatype.
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
// API: void mcpdresults(const mcpdstate &s, real_2d_array &p, mcpdreport &rep);
void mcpdresults(mcpdstate *s, RMatrix *p, mcpdreport *rep) {
   ae_int_t i;
   ae_int_t j;
   SetMatrix(p);
   SetObj(mcpdreport, rep);
   ae_matrix_set_length(p, s->n, s->n);
   for (i = 0; i < s->n; i++) {
      for (j = 0; j < s->n; j++) {
         p->xyR[i][j] = s->p.xyR[i][j];
      }
   }
   rep->terminationtype = s->repterminationtype;
   rep->inneriterationscount = s->repinneriterationscount;
   rep->outeriterationscount = s->repouteriterationscount;
   rep->nfev = s->repnfev;
}

void mcpdstate_init(void *_p, bool make_automatic) {
   mcpdstate *p = (mcpdstate *)_p;
   ae_vector_init(&p->states, 0, DT_INT, make_automatic);
   ae_matrix_init(&p->data, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->ec, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->bndl, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->bndu, 0, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->c, 0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->ct, 0, DT_INT, make_automatic);
   ae_vector_init(&p->pw, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->priorp, 0, 0, DT_REAL, make_automatic);
   minbleicstate_init(&p->bs, make_automatic);
   minbleicreport_init(&p->br, make_automatic);
   ae_vector_init(&p->tmpp, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->effectivew, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->effectivebndl, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->effectivebndu, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->effectivec, 0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->effectivect, 0, DT_INT, make_automatic);
   ae_vector_init(&p->h, 0, DT_REAL, make_automatic);
   ae_matrix_init(&p->p, 0, 0, DT_REAL, make_automatic);
}

void mcpdstate_copy(void *_dst, void *_src, bool make_automatic) {
   mcpdstate *dst = (mcpdstate *)_dst;
   mcpdstate *src = (mcpdstate *)_src;
   dst->n = src->n;
   ae_vector_copy(&dst->states, &src->states, make_automatic);
   dst->npairs = src->npairs;
   ae_matrix_copy(&dst->data, &src->data, make_automatic);
   ae_matrix_copy(&dst->ec, &src->ec, make_automatic);
   ae_matrix_copy(&dst->bndl, &src->bndl, make_automatic);
   ae_matrix_copy(&dst->bndu, &src->bndu, make_automatic);
   ae_matrix_copy(&dst->c, &src->c, make_automatic);
   ae_vector_copy(&dst->ct, &src->ct, make_automatic);
   dst->ccnt = src->ccnt;
   ae_vector_copy(&dst->pw, &src->pw, make_automatic);
   ae_matrix_copy(&dst->priorp, &src->priorp, make_automatic);
   dst->regterm = src->regterm;
   minbleicstate_copy(&dst->bs, &src->bs, make_automatic);
   dst->repinneriterationscount = src->repinneriterationscount;
   dst->repouteriterationscount = src->repouteriterationscount;
   dst->repnfev = src->repnfev;
   dst->repterminationtype = src->repterminationtype;
   minbleicreport_copy(&dst->br, &src->br, make_automatic);
   ae_vector_copy(&dst->tmpp, &src->tmpp, make_automatic);
   ae_vector_copy(&dst->effectivew, &src->effectivew, make_automatic);
   ae_vector_copy(&dst->effectivebndl, &src->effectivebndl, make_automatic);
   ae_vector_copy(&dst->effectivebndu, &src->effectivebndu, make_automatic);
   ae_matrix_copy(&dst->effectivec, &src->effectivec, make_automatic);
   ae_vector_copy(&dst->effectivect, &src->effectivect, make_automatic);
   ae_vector_copy(&dst->h, &src->h, make_automatic);
   ae_matrix_copy(&dst->p, &src->p, make_automatic);
}

void mcpdstate_free(void *_p, bool make_automatic) {
   mcpdstate *p = (mcpdstate *)_p;
   ae_vector_free(&p->states, make_automatic);
   ae_matrix_free(&p->data, make_automatic);
   ae_matrix_free(&p->ec, make_automatic);
   ae_matrix_free(&p->bndl, make_automatic);
   ae_matrix_free(&p->bndu, make_automatic);
   ae_matrix_free(&p->c, make_automatic);
   ae_vector_free(&p->ct, make_automatic);
   ae_vector_free(&p->pw, make_automatic);
   ae_matrix_free(&p->priorp, make_automatic);
   minbleicstate_free(&p->bs, make_automatic);
   minbleicreport_free(&p->br, make_automatic);
   ae_vector_free(&p->tmpp, make_automatic);
   ae_vector_free(&p->effectivew, make_automatic);
   ae_vector_free(&p->effectivebndl, make_automatic);
   ae_vector_free(&p->effectivebndu, make_automatic);
   ae_matrix_free(&p->effectivec, make_automatic);
   ae_vector_free(&p->effectivect, make_automatic);
   ae_vector_free(&p->h, make_automatic);
   ae_matrix_free(&p->p, make_automatic);
}

void mcpdreport_init(void *_p, bool make_automatic) {
}

void mcpdreport_copy(void *_dst, void *_src, bool make_automatic) {
   mcpdreport *dst = (mcpdreport *)_dst;
   mcpdreport *src = (mcpdreport *)_src;
   dst->inneriterationscount = src->inneriterationscount;
   dst->outeriterationscount = src->outeriterationscount;
   dst->nfev = src->nfev;
   dst->terminationtype = src->terminationtype;
}

void mcpdreport_free(void *_p, bool make_automatic) {
}
} // end of namespace alglib_impl

namespace alglib {
// This structure is a MCPD (Markov Chains for Population Data) solver.
// You should use ALGLIB functions in order to work with this object.
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
DefClass(mcpdstate, EndD)

// This structure is a MCPD training report:
//     InnerIterationsCount    -   number of inner iterations of the
//                                 underlying optimization algorithm
//     OuterIterationsCount    -   number of outer iterations of the
//                                 underlying optimization algorithm
//     NFEV                    -   number of merit function evaluations
//     TerminationType         -   termination type
//                                 (same as for MinBLEIC optimizer, positive
//                                 values denote success, negative ones -
//                                 failure)
// ALGLIB: Copyright 23.05.2010 by Sergey Bochkanov
DefClass(mcpdreport, AndD DecVal(inneriterationscount) AndD DecVal(outeriterationscount) AndD DecVal(nfev) AndD DecVal(terminationtype))

void mcpdcreate(const ae_int_t n, mcpdstate &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdcreate(n, ConstT(mcpdstate, s));
   alglib_impl::ae_state_clear();
}

void mcpdcreateentry(const ae_int_t n, const ae_int_t entrystate, mcpdstate &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdcreateentry(n, entrystate, ConstT(mcpdstate, s));
   alglib_impl::ae_state_clear();
}

void mcpdcreateexit(const ae_int_t n, const ae_int_t exitstate, mcpdstate &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdcreateexit(n, exitstate, ConstT(mcpdstate, s));
   alglib_impl::ae_state_clear();
}

void mcpdcreateentryexit(const ae_int_t n, const ae_int_t entrystate, const ae_int_t exitstate, mcpdstate &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdcreateentryexit(n, entrystate, exitstate, ConstT(mcpdstate, s));
   alglib_impl::ae_state_clear();
}

void mcpdaddtrack(const mcpdstate &s, const real_2d_array &xy, const ae_int_t k) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdaddtrack(ConstT(mcpdstate, s), ConstT(ae_matrix, xy), k);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void mcpdaddtrack(const mcpdstate &s, const real_2d_array &xy) {
   ae_int_t k = xy.rows();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdaddtrack(ConstT(mcpdstate, s), ConstT(ae_matrix, xy), k);
   alglib_impl::ae_state_clear();
}
#endif

void mcpdsetec(const mcpdstate &s, const real_2d_array &ec) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdsetec(ConstT(mcpdstate, s), ConstT(ae_matrix, ec));
   alglib_impl::ae_state_clear();
}

void mcpdaddec(const mcpdstate &s, const ae_int_t i, const ae_int_t j, const double c) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdaddec(ConstT(mcpdstate, s), i, j, c);
   alglib_impl::ae_state_clear();
}

void mcpdsetbc(const mcpdstate &s, const real_2d_array &bndl, const real_2d_array &bndu) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdsetbc(ConstT(mcpdstate, s), ConstT(ae_matrix, bndl), ConstT(ae_matrix, bndu));
   alglib_impl::ae_state_clear();
}

void mcpdaddbc(const mcpdstate &s, const ae_int_t i, const ae_int_t j, const double bndl, const double bndu) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdaddbc(ConstT(mcpdstate, s), i, j, bndl, bndu);
   alglib_impl::ae_state_clear();
}

void mcpdsetlc(const mcpdstate &s, const real_2d_array &c, const integer_1d_array &ct, const ae_int_t k) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdsetlc(ConstT(mcpdstate, s), ConstT(ae_matrix, c), ConstT(ae_vector, ct), k);
   alglib_impl::ae_state_clear();
}
#if !defined AE_NO_EXCEPTIONS
void mcpdsetlc(const mcpdstate &s, const real_2d_array &c, const integer_1d_array &ct) {
   if (c.rows() != ct.length()) ThrowError("Error while calling 'mcpdsetlc': looks like one of arguments has wrong size");
   ae_int_t k = c.rows();
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdsetlc(ConstT(mcpdstate, s), ConstT(ae_matrix, c), ConstT(ae_vector, ct), k);
   alglib_impl::ae_state_clear();
}
#endif

void mcpdsettikhonovregularizer(const mcpdstate &s, const double v) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdsettikhonovregularizer(ConstT(mcpdstate, s), v);
   alglib_impl::ae_state_clear();
}

void mcpdsetprior(const mcpdstate &s, const real_2d_array &pp) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdsetprior(ConstT(mcpdstate, s), ConstT(ae_matrix, pp));
   alglib_impl::ae_state_clear();
}

void mcpdsetpredictionweights(const mcpdstate &s, const real_1d_array &pw) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdsetpredictionweights(ConstT(mcpdstate, s), ConstT(ae_vector, pw));
   alglib_impl::ae_state_clear();
}

void mcpdsolve(const mcpdstate &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdsolve(ConstT(mcpdstate, s));
   alglib_impl::ae_state_clear();
}

void mcpdresults(const mcpdstate &s, real_2d_array &p, mcpdreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mcpdresults(ConstT(mcpdstate, s), ConstT(ae_matrix, p), ConstT(mcpdreport, rep));
   alglib_impl::ae_state_clear();
}
} // end of namespace alglib

// === LOGIT Package ===
// Depends on: (Solvers) DIRECTDENSESOLVERS
// Depends on: MLPBASE
namespace alglib_impl {
static const ae_int_t logit_logitvnum = 6;

// The purpose of logit_mnlmcsrch() is to find a step which satisfies a sufficient decrease condition and a curvature condition.
// At each stage the subroutine updates an uncertainty interval with endpoints state->stx and state->sty.
// The uncertainty interval is initially chosen so that it contains a minimizer of the modified function
//	F(x + *stp s) - F(x) - ftol *stp (F'(x)^T s).
// If a step is obtained for which the modified function has a non-positive function value and non-negative derivative,
// then the uncertainty interval is chosen so that it contains a minimizer of F(x + *stp s).
//
// The algorithm is designed to find a step which satisfies the sufficient decrease condition
//	F(x + *stp s) <= F(x) + ftol *stp (F'(x)^T s),
// and the curvature condition
//	|F'(x + *stp s)^T s| <= gtol |F'(x)' s|.
// If ftol < gtol and if, for example, the function is bounded below, then there is always a step which satisfies both conditions.
// If no step can be found which satisfies both conditions,
// then the algorithm usually stops when rounding errors prevent further progress.
// In this case *stp only satisfies the sufficient decrease condition.
//
// Parameters and Inputs:
// *	n:	The number of variables; n > 0.
// *	x:	An n-vector for the base point for the line search, updated to x + *stp s.
// *	f:	The value, set to F(x) and updated to F(x + *stp s).
// *	g:	An n-vector, set to F'(x) and updated to F'(x + *stp s).
// *	s:	An n-vector indicating the search direction.
// *	*stp:	The step estimate; *stp >= 0; updated on output; accessed via the pointer stp.
// *	stpmin:	The minimum step size; stpmin >= 0.
// *	stpmax:	The maximum step size; stpmax >= 0.
// *	xtol:	The tolerance for the relative width of the uncertainty interval; xtol >= 0.
// *	ftol:	The tolerance for sufficient decrease; ftol >= 0.
// *	gtol:	The tolerance for the directional derivative curvature condition; gtol >= 0.
// *	*info:	The return code; accessed via the pointer info:
//		0:	Improper inputs or parameters.
//		1:	The sufficient decrease condition and the directional derivative condition hold.
//		2:	The relative width of the uncertainty interval is at most xtol.
//		3:	The number of function calls has reached maxfev.
//		4:	The step is at the lower bound stpmin.
//		5:	The step is at the upper bound stpmax.
//		6:	Rounding errors prevent further progress.
//			There may not be a step which satisfies the sufficient decrease and curvature conditions.
//			The tolerances may be too small.
// *	*nfev:	The number of function calls; accessed via the pointer nfev.
// *	maxfev:	The number of function calls allowed for the algorithm; maxfev > 0.
// *	wa:	A n-vector for work space.
// *	state:	The algorithm state.
// *	*stage:	The algorithm stage; accessed via the pointer stage.
// Argonne National Laboratory. MINPACK Project. 1983 June.
// Jorge J. More', David J. Thuente.
static bool logit_mnlmcsrch(ae_int_t n, RVector *x, double f, RVector *g, RVector *s, double *stp, ae_int_t *info, ae_int_t *nfev, RVector *wa, logitmcstate *state, ae_int_t *stage) {
   const double xtol = 100.0 * ae_machineepsilon, ftol = 0.0001, gtol = 0.3;
   const ae_int_t maxfev = 20;
   const double stpmin = 0.01, stpmax = 100000.0;
   double v;
// init
   const double p5 = 0.5;
   const double p66 = 0.66;
   state->xtrapf = 4.0;
   const double zero = 0.0;
// Manually threaded two-way signalling.
// A Spawn occurs when the routine is (re-)started.
// A Pause sends an event signal and waits for a response with data before carrying out the matching Resume.
// An Exit sends an exit signal indicating the end of the process.
   if (*stage > 0) switch (*stage) {
   // case 1: goto Resume1; case 2: goto Resume2; case 3: goto Resume3;
      case 4: goto Resume4;
      default: goto Exit;
   }
Spawn:
// Main cycle
#if 0
// Next.
   *stage = 2;
   Resume2:
#endif
   state->infoc = 1;
   *info = 0;
// Check the inputs and parameters for errors.
   if (n <= 0 || *stp <= 0.0 || ftol < 0.0 || gtol < zero || xtol < zero || stpmin < zero || stpmax < stpmin || maxfev <= 0) {
      goto Exit;
   }
// Compute the initial gradient in the search direction and check that s is a descent direction.
   v = ae_v_dotproduct(g->xR, 1, s->xR, 1, n);
   state->dginit = v;
   if (state->dginit >= 0.0) {
      goto Exit;
   }
// Initialize the local variables.
   state->brackt = false;
   state->stage1 = true;
   *nfev = 0;
   state->finit = f;
   state->dgtest = ftol * state->dginit;
   state->width = stpmax - stpmin;
   state->width1 = state->width / p5;
   ae_v_move(wa->xR, 1, x->xR, 1, n);
// The members stx, fx, dgx contain the values of the step, function, and directional derivative at the best step.
// The members sty, fy, dgy contain the values of the step, function, and derivative at the other endpoint of the uncertainty interval.
// The variables *stp, f and member dg contain the values of the step, function, and derivative at the current step.
   state->stx = 0.0;
   state->fx = state->finit;
   state->dgx = state->dginit;
   state->sty = 0.0;
   state->fy = state->finit;
   state->dgy = state->dginit;
   while (true) {
#if 0
   // Next.
      *stage = 3;
      Resume3:
#endif
   // Start the iteration.
   // Set the minimum and maximum steps to correspond to the present uncertainty interval.
      if (state->brackt) {
         if (state->stx < state->sty) {
            state->stmin = state->stx;
            state->stmax = state->sty;
         } else {
            state->stmin = state->sty;
            state->stmax = state->stx;
         }
      } else {
         state->stmin = state->stx;
         state->stmax = *stp + state->xtrapf * (*stp - state->stx);
      }
   // Force the step to be within the bounds stpmax and stpmin.
      if (*stp > stpmax) {
         *stp = stpmax;
      }
      if (*stp < stpmin) {
         *stp = stpmin;
      }
   // If an unusual termination is to occur then let *stp be the lowest point obtained so far.
      if (state->brackt && (*stp <= state->stmin || *stp >= state->stmax) || *nfev >= maxfev - 1 || state->infoc == 0 || state->brackt && state->stmax - state->stmin <= xtol * state->stmax) {
         *stp = state->stx;
      }
   // Evaluate the function and gradient at *stp and compute the directional derivative.
      ae_v_move(x->xR, 1, wa->xR, 1, n);
      ae_v_addd(x->xR, 1, s->xR, 1, n, *stp);
   // Next.
      *stage = 4; goto Pause; Resume4: ++*nfev;
      *info = 0;
      v = ae_v_dotproduct(g->xR, 1, s->xR, 1, n);
      state->dg = v;
      state->ftest1 = state->finit + *stp * state->dgtest;
   // Test for convergence.
      if (state->brackt && (*stp <= state->stmin || *stp >= state->stmax) || state->infoc == 0) {
         *info = 6;
      }
      if (*stp == stpmax && f <= state->ftest1 && state->dg <= state->dgtest) {
         *info = 5;
      }
      if (*stp == stpmin && (f > state->ftest1 || state->dg >= state->dgtest)) {
         *info = 4;
      }
      if (*nfev >= maxfev) {
         *info = 3;
      }
      if (state->brackt && state->stmax - state->stmin <= xtol * state->stmax) {
         *info = 2;
      }
      if (f <= state->ftest1 && SmallAtR(state->dg, -gtol * state->dginit)) {
         *info = 1;
      }
   // Check for termination.
      if (*info != 0) {
         goto Exit;
      }
   // In the first stage we seek a step for which the modified function has a non-positive value and non-negative derivative.
      if (state->stage1 && f <= state->ftest1 && state->dg >= rmin2(ftol, gtol) * state->dginit) {
         state->stage1 = false;
      }
   // A modified function is used to predict the step only if we have not obtained a step
   // for which the modified function has a non-positive function value and non-negative derivative,
   // and if a lower function value has been obtained but the decrease is not sufficient.
      if (state->stage1 && f <= state->fx && f > state->ftest1) {
      // Define the modified function and derivative values.
         state->fm = f - *stp * state->dgtest;
         state->fxm = state->fx - state->stx * state->dgtest;
         state->fym = state->fy - state->sty * state->dgtest;
         state->dgm = state->dg - state->dgtest;
         state->dgxm = state->dgx - state->dgtest;
         state->dgym = state->dgy - state->dgtest;
      // Update the uncertainty interval and compute the new step.
         mcstep(&state->stx, &state->fxm, &state->dgxm, &state->sty, &state->fym, &state->dgym, stp, state->fm, state->dgm, &state->brackt, state->stmin, state->stmax, &state->infoc);
      // Reset the function and gradient values for f.
         state->fx = state->fxm + state->stx * state->dgtest;
         state->fy = state->fym + state->sty * state->dgtest;
         state->dgx = state->dgxm + state->dgtest;
         state->dgy = state->dgym + state->dgtest;
      } else {
      // Update the uncertainty interval and compute the new step.
         mcstep(&state->stx, &state->fx, &state->dgx, &state->sty, &state->fy, &state->dgy, stp, f, state->dg, &state->brackt, state->stmin, state->stmax, &state->infoc);
      }
   // Force a sufficient decrease in the size of the uncertainty interval.
      if (state->brackt) {
         if (!NearR(state->sty, state->stx, p66 * state->width1)) {
            *stp = state->stx + p5 * (state->sty - state->stx);
         }
         state->width1 = state->width;
         state->width = fabs(state->sty - state->stx);
      }
   }
Exit:
   *stage = 0;
   return false;
Pause:
   return true;
}

// This subroutine trains logit model.
//
// Inputs:
//     XY          -   training set, array[0..NPoints-1,0..NVars]
//                     First NVars columns store values of independent
//                     variables, next column stores number of class (from 0
//                     to NClasses-1) which dataset element belongs to. Fractional
//                     values are rounded to nearest integer.
//     NPoints     -   training set size, NPoints >= 1
//     NVars       -   number of independent variables, NVars >= 1
//     NClasses    -   number of classes, NClasses >= 2
//
// Outputs:
//     Info        -   return code:
//                     * -2, if there is a point with class number
//                           outside of [0..NClasses-1].
//                     * -1, if incorrect parameters was passed
//                           (NPoints < NVars+2, NVars < 1, NClasses < 2).
//                     *  1, if task has been solved
//     LM          -   model built
//     Rep         -   training report
// ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
// API: void mnltrainh(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, ae_int_t &info, logitmodel &lm, mnlreport &rep);
void mnltrainh(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses, ae_int_t *info, logitmodel *lm, mnlreport *rep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t ssize;
   bool allsame;
   ae_int_t offs;
   double decay;
   double v;
   double s;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   double e;
   bool spd;
   double wstep;
   ae_int_t mcstage;
   ae_int_t mcinfo;
   ae_int_t mcnfev;
   ae_int_t solverinfo;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(logitmodel, lm);
   SetObj(mnlreport, rep);
   NewObj(multilayerperceptron, network);
   NewVector(g, 0, DT_REAL);
   NewMatrix(h, 0, 0, DT_REAL);
   NewVector(x, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   NewVector(wbase, 0, DT_REAL);
   NewVector(wdir, 0, DT_REAL);
   NewVector(work, 0, DT_REAL);
   NewObj(logitmcstate, mcstate);
   NewObj(densesolverreport, solverrep);
   decay = 0.001;
// Test for inputs
   if (npoints < nvars + 2 || nvars < 1 || nclasses < 2) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   for (i = 0; i < npoints; i++) {
      if (RoundZ(xy->xyR[i][nvars]) < 0 || RoundZ(xy->xyR[i][nvars]) >= nclasses) {
         *info = -2;
         ae_frame_leave();
         return;
      }
   }
   *info = 1;
// Initialize data
   rep->ngrad = 0;
   rep->nhess = 0;
// Allocate array
   offs = 5;
   ssize = 5 + (nvars + 1) * (nclasses - 1) + nclasses;
   ae_vector_set_length(&lm->w, ssize);
   lm->w.xR[0] = (double)ssize;
   lm->w.xR[1] = (double)logit_logitvnum;
   lm->w.xR[2] = (double)nvars;
   lm->w.xR[3] = (double)nclasses;
   lm->w.xR[4] = (double)offs;
// Degenerate case: all outputs are equal
   allsame = true;
   for (i = 1; i < npoints; i++) {
      if (RoundZ(xy->xyR[i][nvars]) != RoundZ(xy->xyR[i - 1][nvars])) {
         allsame = false;
      }
   }
   if (allsame) {
      for (i = 0; i < (nvars + 1) * (nclasses - 1); i++) {
         lm->w.xR[offs + i] = 0.0;
      }
      v = -2 * log(ae_minrealnumber);
      k = RoundZ(xy->xyR[0][nvars]);
      if (k == nclasses - 1) {
         for (i = 0; i < nclasses - 1; i++) {
            lm->w.xR[offs + i * (nvars + 1) + nvars] = -v;
         }
      } else {
         for (i = 0; i < nclasses - 1; i++) {
            if (i == k) {
               lm->w.xR[offs + i * (nvars + 1) + nvars] = v;
            } else {
               lm->w.xR[offs + i * (nvars + 1) + nvars] = 0.0;
            }
         }
      }
      ae_frame_leave();
      return;
   }
// General case.
// Prepare task and network. Allocate space.
   mlpcreatec0(nvars, nclasses, &network);
   mlpinitpreprocessor(&network, xy, npoints);
   mlpproperties(&network, &nin, &nout, &wcount);
   for (i = 0; i < wcount; i++) {
      network.weights.xR[i] = ae_randommid() / nvars;
   }
   ae_vector_set_length(&g, wcount);
   ae_matrix_set_length(&h, wcount, wcount);
   ae_vector_set_length(&wbase, wcount);
   ae_vector_set_length(&wdir, wcount);
   ae_vector_set_length(&work, wcount);
// First stage: optimize in gradient direction.
   for (k = 0; k <= wcount / 3 + 10; k++) {
   // Calculate gradient in starting point
      mlpgradnbatch(&network, xy, npoints, &e, &g);
      v = ae_v_dotproduct(network.weights.xR, 1, network.weights.xR, 1, wcount);
      e += 0.5 * decay * v;
      ae_v_addd(g.xR, 1, network.weights.xR, 1, wcount, decay);
      rep->ngrad++;
   // Setup optimization scheme
      ae_v_moveneg(wdir.xR, 1, g.xR, 1, wcount);
      v = ae_v_dotproduct(wdir.xR, 1, wdir.xR, 1, wcount);
      wstep = sqrt(v);
      v = 1 / sqrt(v);
      ae_v_muld(wdir.xR, 1, wcount, v);
      mcstage = 0;
      while (logit_mnlmcsrch(wcount, &network.weights, e, &g, &wdir, &wstep, &mcinfo, &mcnfev, &work, &mcstate, &mcstage)) {
         mlpgradnbatch(&network, xy, npoints, &e, &g);
         v = ae_v_dotproduct(network.weights.xR, 1, network.weights.xR, 1, wcount);
         e += 0.5 * decay * v;
         ae_v_addd(g.xR, 1, network.weights.xR, 1, wcount, decay);
         rep->ngrad++;
      }
   }
// Second stage: use Hessian when we are close to the minimum
   while (true) {
   // Calculate and update E/G/H
      mlphessiannbatch(&network, xy, npoints, &e, &g, &h);
      v = ae_v_dotproduct(network.weights.xR, 1, network.weights.xR, 1, wcount);
      e += 0.5 * decay * v;
      ae_v_addd(g.xR, 1, network.weights.xR, 1, wcount, decay);
      for (k = 0; k < wcount; k++) {
         h.xyR[k][k] += decay;
      }
      rep->nhess++;
   // Select step direction
   // NOTE: it is important to use lower-triangle Cholesky
   // factorization since it is much faster than higher-triangle version.
      spd = spdmatrixcholesky(&h, wcount, false);
      spdmatrixcholeskysolve(&h, wcount, false, &g, &solverinfo, &solverrep, &wdir);
      spd = solverinfo > 0;
      if (spd) {
      // H is positive definite.
      // Step in Newton direction.
         ae_v_muld(wdir.xR, 1, wcount, -1);
         spd = true;
      } else {
      // H is indefinite.
      // Step in gradient direction.
         ae_v_moveneg(wdir.xR, 1, g.xR, 1, wcount);
         spd = false;
      }
   // Optimize in WDir direction
      v = ae_v_dotproduct(wdir.xR, 1, wdir.xR, 1, wcount);
      wstep = sqrt(v);
      v = 1 / sqrt(v);
      ae_v_muld(wdir.xR, 1, wcount, v);
      mcstage = 0;
      while (logit_mnlmcsrch(wcount, &network.weights, e, &g, &wdir, &wstep, &mcinfo, &mcnfev, &work, &mcstate, &mcstage)) {
         mlpgradnbatch(&network, xy, npoints, &e, &g);
         v = ae_v_dotproduct(network.weights.xR, 1, network.weights.xR, 1, wcount);
         e += 0.5 * decay * v;
         ae_v_addd(g.xR, 1, network.weights.xR, 1, wcount, decay);
         rep->ngrad++;
      }
      if (spd && (mcinfo == 2 || mcinfo == 4 || mcinfo == 6)) {
         break;
      }
   }
// Convert from NN format to MNL format
   ae_v_move(&lm->w.xR[offs], 1, network.weights.xR, 1, wcount);
   for (k = 0; k < nvars; k++) {
      for (i = 0; i < nclasses - 1; i++) {
         s = network.columnsigmas.xR[k];
         if (s == 0.0) {
            s = 1.0;
         }
         j = offs + (nvars + 1) * i;
         v = lm->w.xR[j + k];
         lm->w.xR[j + k] = v / s;
         lm->w.xR[j + nvars] += v * network.columnmeans.xR[k] / s;
      }
   }
   for (k = 0; k < nclasses - 1; k++) {
      lm->w.xR[offs + (nvars + 1) * k + nvars] = -lm->w.xR[offs + (nvars + 1) * k + nvars];
   }
   ae_frame_leave();
}

// Internal subroutine. Places exponents of the anti-overflow shifted
// internal linear outputs into the service part of the W array.
static void logit_mnliexp(RVector *w, RVector *x) {
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t offs;
   ae_int_t i;
   ae_int_t i1;
   double v;
   double mx;
   ae_assert(w->xR[1] == (double)logit_logitvnum, "LOGIT: unexpected model version");
   nvars = RoundZ(w->xR[2]);
   nclasses = RoundZ(w->xR[3]);
   offs = RoundZ(w->xR[4]);
   i1 = offs + (nvars + 1) * (nclasses - 1);
   for (i = 0; i < nclasses - 1; i++) {
      v = ae_v_dotproduct(&w->xR[offs + i * (nvars + 1)], 1, x->xR, 1, nvars);
      w->xR[i1 + i] = v + w->xR[offs + i * (nvars + 1) + nvars];
   }
   w->xR[i1 + nclasses - 1] = 0.0;
   mx = 0.0;
   for (i = i1; i < i1 + nclasses; i++) {
      mx = rmax2(mx, w->xR[i]);
   }
   for (i = i1; i < i1 + nclasses; i++) {
      w->xR[i] = exp(w->xR[i] - mx);
   }
}

// Procesing
//
// Inputs:
//     LM      -   logit model, passed by non-constant reference
//                 (some fields of structure are used as temporaries
//                 when calculating model output).
//     X       -   input vector,  array[0..NVars-1].
//     Y       -   (possibly) preallocated buffer; if size of Y is less than
//                 NClasses, it will be reallocated.If it is large enough, it
//                 is NOT reallocated, so we can save some time on reallocation.
//
// Outputs:
//     Y       -   result, array[0..NClasses-1]
//                 Vector of posterior probabilities for classification task.
// ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
// API: void mnlprocess(const logitmodel &lm, const real_1d_array &x, real_1d_array &y);
void mnlprocess(logitmodel *lm, RVector *x, RVector *y) {
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t offs;
   ae_int_t i;
   ae_int_t i1;
   double s;
   ae_assert(lm->w.xR[1] == (double)logit_logitvnum, "MNLProcess: unexpected model version");
   nvars = RoundZ(lm->w.xR[2]);
   nclasses = RoundZ(lm->w.xR[3]);
   offs = RoundZ(lm->w.xR[4]);
   logit_mnliexp(&lm->w, x);
   s = 0.0;
   i1 = offs + (nvars + 1) * (nclasses - 1);
   for (i = i1; i < i1 + nclasses; i++) {
      s += lm->w.xR[i];
   }
   if (y->cnt < nclasses) {
      ae_vector_set_length(y, nclasses);
   }
   for (i = 0; i < nclasses; i++) {
      y->xR[i] = lm->w.xR[i1 + i] / s;
   }
}

// 'interactive'  variant  of  MNLProcess  for  languages  like  Python which
// support constructs like "Y = MNLProcess(LM,X)" and interactive mode of the
// interpreter
//
// This function allocates new array on each call,  so  it  is  significantly
// slower than its 'non-interactive' counterpart, but it is  more  convenient
// when you call it from command line.
// ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
// API: void mnlprocessi(const logitmodel &lm, const real_1d_array &x, real_1d_array &y);
void mnlprocessi(logitmodel *lm, RVector *x, RVector *y) {
   SetVector(y);
   mnlprocess(lm, x, y);
}

// Unpacks coefficients of logit model. Logit model have form:
//
//     P(class=i) = S(i) / (S(0) + S(1) + ... +S(M-1))
//           S(i) = exp(A[i,0]*X[0] + ... + A[i,N-1]*X[N-1] + A[i,N]), when i < M-1
//         S(M-1) = 1
//
// Inputs:
//     LM          -   logit model in ALGLIB format
//
// Outputs:
//     V           -   coefficients, array[0..NClasses-2,0..NVars]
//     NVars       -   number of independent variables
//     NClasses    -   number of classes
// ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
// API: void mnlunpack(const logitmodel &lm, real_2d_array &a, ae_int_t &nvars, ae_int_t &nclasses);
void mnlunpack(logitmodel *lm, RMatrix *a, ae_int_t *nvars, ae_int_t *nclasses) {
   ae_int_t offs;
   ae_int_t i;
   SetMatrix(a);
   *nvars = 0;
   *nclasses = 0;
   ae_assert(lm->w.xR[1] == (double)logit_logitvnum, "MNLUnpack: unexpected model version");
   *nvars = RoundZ(lm->w.xR[2]);
   *nclasses = RoundZ(lm->w.xR[3]);
   offs = RoundZ(lm->w.xR[4]);
   ae_matrix_set_length(a, *nclasses - 1, *nvars + 1);
   for (i = 0; i < *nclasses - 1; i++) {
      ae_v_move(a->xyR[i], 1, &lm->w.xR[offs + i * (*nvars + 1)], 1, *nvars + 1);
   }
}

// "Packs" coefficients and creates logit model in ALGLIB format (MNLUnpack
// reversed).
//
// Inputs:
//     A           -   model (see MNLUnpack)
//     NVars       -   number of independent variables
//     NClasses    -   number of classes
//
// Outputs:
//     LM          -   logit model.
// ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
// API: void mnlpack(const real_2d_array &a, const ae_int_t nvars, const ae_int_t nclasses, logitmodel &lm);
void mnlpack(RMatrix *a, ae_int_t nvars, ae_int_t nclasses, logitmodel *lm) {
   ae_int_t offs;
   ae_int_t i;
   ae_int_t ssize;
   SetObj(logitmodel, lm);
   offs = 5;
   ssize = 5 + (nvars + 1) * (nclasses - 1) + nclasses;
   ae_vector_set_length(&lm->w, ssize);
   lm->w.xR[0] = (double)ssize;
   lm->w.xR[1] = (double)logit_logitvnum;
   lm->w.xR[2] = (double)nvars;
   lm->w.xR[3] = (double)nclasses;
   lm->w.xR[4] = (double)offs;
   for (i = 0; i < nclasses - 1; i++) {
      ae_v_move(&lm->w.xR[offs + i * (nvars + 1)], 1, a->xyR[i], 1, nvars + 1);
   }
}

// Copying of LogitModel structure
//
// Inputs:
//     LM1 -   original
//
// Outputs:
//     LM2 -   copy
// ALGLIB: Copyright 15.03.2009 by Sergey Bochkanov
void mnlcopy(logitmodel *lm1, logitmodel *lm2) {
   ae_int_t k;
   SetObj(logitmodel, lm2);
   k = RoundZ(lm1->w.xR[0]);
   ae_vector_set_length(&lm2->w, k);
   ae_v_move(lm2->w.xR, 1, lm1->w.xR, 1, k);
}

// Average cross-entropy (in bits per element) on the test set
//
// Inputs:
//     LM      -   logit model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     CrossEntropy/(NPoints*ln(2)).
// ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
// API: double mnlavgce(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints);
double mnlavgce(logitmodel *lm, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t i;
   double result;
   ae_frame_make(&_frame_block);
   NewVector(workx, 0, DT_REAL);
   NewVector(worky, 0, DT_REAL);
   ae_assert(lm->w.xR[1] == (double)logit_logitvnum, "MNLClsError: unexpected model version");
   nvars = RoundZ(lm->w.xR[2]);
   nclasses = RoundZ(lm->w.xR[3]);
   ae_vector_set_length(&workx, nvars);
   ae_vector_set_length(&worky, nclasses);
   result = 0.0;
   for (i = 0; i < npoints; i++) {
      ae_assert(RoundZ(xy->xyR[i][nvars]) >= 0 && RoundZ(xy->xyR[i][nvars]) < nclasses, "MNLAvgCE: incorrect class number!");
   // Process
      ae_v_move(workx.xR, 1, xy->xyR[i], 1, nvars);
      mnlprocess(lm, &workx, &worky);
      if (worky.xR[RoundZ(xy->xyR[i][nvars])] > 0.0) {
         result -= log(worky.xR[RoundZ(xy->xyR[i][nvars])]);
      } else {
         result -= log(ae_minrealnumber);
      }
   }
   result /= npoints * log(2.0);
   ae_frame_leave();
   return result;
}

// Relative classification error on the test set
//
// Inputs:
//     LM      -   logit model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     percent of incorrectly classified cases.
// ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
// API: double mnlrelclserror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints);
double mnlrelclserror(logitmodel *lm, RMatrix *xy, ae_int_t npoints) {
   double result;
   result = (double)mnlclserror(lm, xy, npoints) / (double)npoints;
   return result;
}

// Calculation of all types of errors
// ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
static void logit_mnlallerrors(logitmodel *lm, RMatrix *xy, ae_int_t npoints, double *relcls, double *avgce, double *rms, double *avg, double *avgrel) {
   ae_frame _frame_block;
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   *relcls = 0;
   *avgce = 0;
   *rms = 0;
   *avg = 0;
   *avgrel = 0;
   NewVector(buf, 0, DT_REAL);
   NewVector(workx, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   NewVector(dy, 0, DT_REAL);
   ae_assert(RoundZ(lm->w.xR[1]) == logit_logitvnum, "MNL unit: Incorrect MNL version!");
   nvars = RoundZ(lm->w.xR[2]);
   nclasses = RoundZ(lm->w.xR[3]);
   ae_vector_set_length(&workx, nvars);
   ae_vector_set_length(&y, nclasses);
   ae_vector_set_length(&dy, 0 + 1);
   dserrallocate(nclasses, &buf);
   for (i = 0; i < npoints; i++) {
      ae_v_move(workx.xR, 1, xy->xyR[i], 1, nvars);
      mnlprocess(lm, &workx, &y);
      dy.xR[0] = xy->xyR[i][nvars];
      dserraccumulate(&buf, &y, &dy);
   }
   dserrfinish(&buf);
   *relcls = buf.xR[0];
   *avgce = buf.xR[1];
   *rms = buf.xR[2];
   *avg = buf.xR[3];
   *avgrel = buf.xR[4];
   ae_frame_leave();
}

// RMS error on the test set
//
// Inputs:
//     LM      -   logit model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     root mean square error (error when estimating posterior probabilities).
// ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
// API: double mnlrmserror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints);
double mnlrmserror(logitmodel *lm, RMatrix *xy, ae_int_t npoints) {
   double relcls;
   double avgce;
   double rms;
   double avg;
   double avgrel;
   double result;
   ae_assert(RoundZ(lm->w.xR[1]) == logit_logitvnum, "MNLRMSError: Incorrect MNL version!");
   logit_mnlallerrors(lm, xy, npoints, &relcls, &avgce, &rms, &avg, &avgrel);
   result = rms;
   return result;
}

// Average error on the test set
//
// Inputs:
//     LM      -   logit model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     average error (error when estimating posterior probabilities).
// ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
// API: double mnlavgerror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints);
double mnlavgerror(logitmodel *lm, RMatrix *xy, ae_int_t npoints) {
   double relcls;
   double avgce;
   double rms;
   double avg;
   double avgrel;
   double result;
   ae_assert(RoundZ(lm->w.xR[1]) == logit_logitvnum, "MNLRMSError: Incorrect MNL version!");
   logit_mnlallerrors(lm, xy, npoints, &relcls, &avgce, &rms, &avg, &avgrel);
   result = avg;
   return result;
}

// Average relative error on the test set
//
// Inputs:
//     LM      -   logit model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     average relative error (error when estimating posterior probabilities).
// ALGLIB: Copyright 30.08.2008 by Sergey Bochkanov
// API: double mnlavgrelerror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t ssize);
double mnlavgrelerror(logitmodel *lm, RMatrix *xy, ae_int_t ssize) {
   double relcls;
   double avgce;
   double rms;
   double avg;
   double avgrel;
   double result;
   ae_assert(RoundZ(lm->w.xR[1]) == logit_logitvnum, "MNLRMSError: Incorrect MNL version!");
   logit_mnlallerrors(lm, xy, ssize, &relcls, &avgce, &rms, &avg, &avgrel);
   result = avgrel;
   return result;
}

// Classification error on test set = MNLRelClsError*NPoints
// ALGLIB: Copyright 10.09.2008 by Sergey Bochkanov
// API: ae_int_t mnlclserror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints);
ae_int_t mnlclserror(logitmodel *lm, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   ae_int_t nvars;
   ae_int_t nclasses;
   ae_int_t i;
   ae_int_t j;
   ae_int_t nmax;
   ae_int_t result;
   ae_frame_make(&_frame_block);
   NewVector(workx, 0, DT_REAL);
   NewVector(worky, 0, DT_REAL);
   ae_assert(lm->w.xR[1] == (double)logit_logitvnum, "MNLClsError: unexpected model version");
   nvars = RoundZ(lm->w.xR[2]);
   nclasses = RoundZ(lm->w.xR[3]);
   ae_vector_set_length(&workx, nvars);
   ae_vector_set_length(&worky, nclasses);
   result = 0;
   for (i = 0; i < npoints; i++) {
   // Process
      ae_v_move(workx.xR, 1, xy->xyR[i], 1, nvars);
      mnlprocess(lm, &workx, &worky);
   // Logit version of the answer
      nmax = 0;
      for (j = 0; j < nclasses; j++) {
         if (worky.xR[j] > worky.xR[nmax]) {
            nmax = j;
         }
      }
   // compare
      if (nmax != RoundZ(xy->xyR[i][nvars])) {
         result++;
      }
   }
   ae_frame_leave();
   return result;
}

void logitmodel_init(void *_p, bool make_automatic) {
   logitmodel *p = (logitmodel *)_p;
   ae_vector_init(&p->w, 0, DT_REAL, make_automatic);
}

void logitmodel_copy(void *_dst, void *_src, bool make_automatic) {
   logitmodel *dst = (logitmodel *)_dst;
   logitmodel *src = (logitmodel *)_src;
   ae_vector_copy(&dst->w, &src->w, make_automatic);
}

void logitmodel_free(void *_p, bool make_automatic) {
   logitmodel *p = (logitmodel *)_p;
   ae_vector_free(&p->w, make_automatic);
}

void logitmcstate_init(void *_p, bool make_automatic) {
}

void logitmcstate_copy(void *_dst, void *_src, bool make_automatic) {
   logitmcstate *dst = (logitmcstate *)_dst;
   logitmcstate *src = (logitmcstate *)_src;
   dst->brackt = src->brackt;
   dst->stage1 = src->stage1;
   dst->infoc = src->infoc;
   dst->dg = src->dg;
   dst->dgm = src->dgm;
   dst->dginit = src->dginit;
   dst->dgtest = src->dgtest;
   dst->dgx = src->dgx;
   dst->dgxm = src->dgxm;
   dst->dgy = src->dgy;
   dst->dgym = src->dgym;
   dst->finit = src->finit;
   dst->ftest1 = src->ftest1;
   dst->fm = src->fm;
   dst->fx = src->fx;
   dst->fxm = src->fxm;
   dst->fy = src->fy;
   dst->fym = src->fym;
   dst->stx = src->stx;
   dst->sty = src->sty;
   dst->stmin = src->stmin;
   dst->stmax = src->stmax;
   dst->width = src->width;
   dst->width1 = src->width1;
   dst->xtrapf = src->xtrapf;
}

void logitmcstate_free(void *_p, bool make_automatic) {
}

void mnlreport_init(void *_p, bool make_automatic) {
}

void mnlreport_copy(void *_dst, void *_src, bool make_automatic) {
   mnlreport *dst = (mnlreport *)_dst;
   mnlreport *src = (mnlreport *)_src;
   dst->ngrad = src->ngrad;
   dst->nhess = src->nhess;
}

void mnlreport_free(void *_p, bool make_automatic) {
}
} // end of namespace alglib_impl

namespace alglib {
DefClass(logitmodel, EndD)

// MNLReport structure contains information about training process:
// * NGrad     -   number of gradient calculations
// * NHess     -   number of Hessian calculations
DefClass(mnlreport, AndD DecVal(ngrad) AndD DecVal(nhess))

void mnltrainh(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses, ae_int_t &info, logitmodel &lm, mnlreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mnltrainh(ConstT(ae_matrix, xy), npoints, nvars, nclasses, &info, ConstT(logitmodel, lm), ConstT(mnlreport, rep));
   alglib_impl::ae_state_clear();
}

void mnlprocess(const logitmodel &lm, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mnlprocess(ConstT(logitmodel, lm), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

void mnlprocessi(const logitmodel &lm, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mnlprocessi(ConstT(logitmodel, lm), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

void mnlunpack(const logitmodel &lm, real_2d_array &a, ae_int_t &nvars, ae_int_t &nclasses) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mnlunpack(ConstT(logitmodel, lm), ConstT(ae_matrix, a), &nvars, &nclasses);
   alglib_impl::ae_state_clear();
}

void mnlpack(const real_2d_array &a, const ae_int_t nvars, const ae_int_t nclasses, logitmodel &lm) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mnlpack(ConstT(ae_matrix, a), nvars, nclasses, ConstT(logitmodel, lm));
   alglib_impl::ae_state_clear();
}

double mnlavgce(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mnlavgce(ConstT(logitmodel, lm), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mnlrelclserror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mnlrelclserror(ConstT(logitmodel, lm), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mnlrmserror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mnlrmserror(ConstT(logitmodel, lm), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mnlavgerror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mnlavgerror(ConstT(logitmodel, lm), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double mnlavgrelerror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t ssize) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::mnlavgrelerror(ConstT(logitmodel, lm), ConstT(ae_matrix, xy), ssize);
   alglib_impl::ae_state_clear();
   return D;
}

ae_int_t mnlclserror(const logitmodel &lm, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0)
   ae_int_t Z = alglib_impl::mnlclserror(ConstT(logitmodel, lm), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return Z;
}
} // end of namespace alglib

// === KNN Package ===
// Depends on: (AlgLibMisc) HQRND, NEARESTNEIGHBOR
// Depends on: BDSS
namespace alglib_impl {
static const ae_int_t knn_knnfirstversion = 0;

// This function creates buffer  structure  which  can  be  used  to  perform
// parallel KNN requests.
//
// KNN subpackage provides two sets of computing functions - ones  which  use
// internal buffer of KNN model (these  functions are single-threaded because
// they use same buffer, which can not  shared  between  threads),  and  ones
// which use external buffer.
//
// This function is used to initialize external buffer.
//
// Inputs:
//     Model       -   KNN model which is associated with newly created buffer
//
// Outputs:
//     Buf         -   external buffer.
//
// IMPORTANT: buffer object should be used only with model which was used  to
//            initialize buffer. Any attempt to  use  buffer  with  different
//            object is dangerous - you  may   get  integrity  check  failure
//            (exception) because sizes of internal  arrays  do  not  fit  to
//            dimensions of the model structure.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knncreatebuffer(const knnmodel &model, knnbuffer &buf);
void knncreatebuffer(knnmodel *model, knnbuffer *buf) {
   SetObj(knnbuffer, buf);
   if (!model->isdummy) {
      kdtreecreaterequestbuffer(&model->tree, &buf->treebuf);
   }
   ae_vector_set_length(&buf->x, model->nvars);
   ae_vector_set_length(&buf->y, model->nout);
}

// This subroutine creates KNNBuilder object which is used to train KNN models.
//
// By default, new builder stores empty dataset and some  reasonable  default
// settings. At the very least, you should specify dataset prior to  building
// KNN model. You can also tweak settings of the model construction algorithm
// (recommended, although default settings should work well).
//
// Following actions are mandatory:
// * calling knnbuildersetdataset() to specify dataset
// * calling knnbuilderbuildknnmodel() to build KNN model using current
//   dataset and default settings
//
// Additionally, you may call:
// * knnbuildersetnorm() to change norm being used
//
// Inputs:
//     none
//
// Outputs:
//     S           -   KNN builder
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnbuildercreate(knnbuilder &s);
void knnbuildercreate(knnbuilder *s) {
   SetObj(knnbuilder, s);
// Empty dataset
   s->dstype = -1;
   s->npoints = 0;
   s->nvars = 0;
   s->iscls = false;
   s->nout = 1;
// Default training settings
   s->knnnrm = 2;
}

// Specifies regression problem (one or more continuous  output variables are
// predicted). There also exists "classification" version of this function.
//
// This subroutine adds dense dataset to the internal storage of the  builder
// object. Specifying your dataset in the dense format means that  the  dense
// version of the KNN construction algorithm will be invoked.
//
// Inputs:
//     S           -   KNN builder object
//     XY          -   array[NPoints,NVars+NOut] (note: actual  size  can  be
//                     larger, only leading part is used anyway), dataset:
//                     * first NVars elements of each row store values of the
//                       independent variables
//                     * next NOut elements store  values  of  the  dependent
//                       variables
//     NPoints     -   number of rows in the dataset, NPoints >= 1
//     NVars       -   number of independent variables, NVars >= 1
//     NOut        -   number of dependent variables, NOut >= 1
//
// Outputs:
//     S           -   KNN builder
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnbuildersetdatasetreg(const knnbuilder &s, const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nout);
void knnbuildersetdatasetreg(knnbuilder *s, RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nout) {
   ae_int_t i;
   ae_int_t j;
// Check parameters
   ae_assert(npoints >= 1, "knnbuildersetdatasetreg: npoints<1");
   ae_assert(nvars >= 1, "knnbuildersetdatasetreg: nvars<1");
   ae_assert(nout >= 1, "knnbuildersetdatasetreg: nout<1");
   ae_assert(xy->rows >= npoints, "knnbuildersetdatasetreg: rows(xy)<npoints");
   ae_assert(xy->cols >= nvars + nout, "knnbuildersetdatasetreg: cols(xy)<nvars+nout");
   ae_assert(apservisfinitematrix(xy, npoints, nvars + nout), "knnbuildersetdatasetreg: xy parameter contains INFs or NANs");
// Set dataset
   s->dstype = 0;
   s->iscls = false;
   s->npoints = npoints;
   s->nvars = nvars;
   s->nout = nout;
   matrixsetlengthatleast(&s->dsdata, npoints, nvars);
   for (i = 0; i < npoints; i++) {
      for (j = 0; j < nvars; j++) {
         s->dsdata.xyR[i][j] = xy->xyR[i][j];
      }
   }
   vectorsetlengthatleast(&s->dsrval, npoints * nout);
   for (i = 0; i < npoints; i++) {
      for (j = 0; j < nout; j++) {
         s->dsrval.xR[i * nout + j] = xy->xyR[i][nvars + j];
      }
   }
}

// Specifies classification problem (two  or  more  classes  are  predicted).
// There also exists "regression" version of this function.
//
// This subroutine adds dense dataset to the internal storage of the  builder
// object. Specifying your dataset in the dense format means that  the  dense
// version of the KNN construction algorithm will be invoked.
//
// Inputs:
//     S           -   KNN builder object
//     XY          -   array[NPoints,NVars+1] (note:   actual   size  can  be
//                     larger, only leading part is used anyway), dataset:
//                     * first NVars elements of each row store values of the
//                       independent variables
//                     * next element stores class index, in [0,NClasses)
//     NPoints     -   number of rows in the dataset, NPoints >= 1
//     NVars       -   number of independent variables, NVars >= 1
//     NClasses    -   number of classes, NClasses >= 2
//
// Outputs:
//     S           -   KNN builder
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnbuildersetdatasetcls(const knnbuilder &s, const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses);
void knnbuildersetdatasetcls(knnbuilder *s, RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t nclasses) {
   ae_int_t i;
   ae_int_t j;
// Check parameters
   ae_assert(npoints >= 1, "knnbuildersetdatasetcls: npoints<1");
   ae_assert(nvars >= 1, "knnbuildersetdatasetcls: nvars<1");
   ae_assert(nclasses >= 2, "knnbuildersetdatasetcls: nclasses<2");
   ae_assert(xy->rows >= npoints, "knnbuildersetdatasetcls: rows(xy)<npoints");
   ae_assert(xy->cols >= nvars + 1, "knnbuildersetdatasetcls: cols(xy)<nvars+1");
   ae_assert(apservisfinitematrix(xy, npoints, nvars + 1), "knnbuildersetdatasetcls: xy parameter contains INFs or NANs");
   for (i = 0; i < npoints; i++) {
      j = RoundZ(xy->xyR[i][nvars]);
      ae_assert(j >= 0 && j < nclasses, "knnbuildersetdatasetcls: last column of xy contains invalid class number");
   }
// Set dataset
   s->iscls = true;
   s->dstype = 0;
   s->npoints = npoints;
   s->nvars = nvars;
   s->nout = nclasses;
   matrixsetlengthatleast(&s->dsdata, npoints, nvars);
   for (i = 0; i < npoints; i++) {
      for (j = 0; j < nvars; j++) {
         s->dsdata.xyR[i][j] = xy->xyR[i][j];
      }
   }
   vectorsetlengthatleast(&s->dsival, npoints);
   for (i = 0; i < npoints; i++) {
      s->dsival.xZ[i] = RoundZ(xy->xyR[i][nvars]);
   }
}

// This function sets norm type used for neighbor search.
//
// Inputs:
//     S           -   decision forest builder object
//     NormType    -   norm type:
//                     * 0      inf-norm
//                     * 1      1-norm
//                     * 2      Euclidean norm (default)
//
// Outputs:
//     S           -   decision forest builder
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnbuildersetnorm(const knnbuilder &s, const ae_int_t nrmtype);
void knnbuildersetnorm(knnbuilder *s, ae_int_t nrmtype) {
   ae_assert(nrmtype == 0 || nrmtype == 1 || nrmtype == 2, "knnbuildersetnorm: unexpected norm type");
   s->knnnrm = nrmtype;
}

// Sets report fields to their default values
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
static void knn_clearreport(knnreport *rep) {
   rep->relclserror = 0.0;
   rep->avgce = 0.0;
   rep->rmserror = 0.0;
   rep->avgerror = 0.0;
   rep->avgrelerror = 0.0;
}

// This subroutine builds KNN model  according  to  current  settings,  using
// dataset internally stored in the builder object.
//
// The model being built performs inference using Eps-approximate  K  nearest
// neighbors search algorithm, with:
// * K=1,  Eps=0 corresponding to the "nearest neighbor algorithm"
// * K > 1,  Eps=0 corresponding to the "K nearest neighbors algorithm"
// * K >= 1, Eps > 0 corresponding to "approximate nearest neighbors algorithm"
//
// An approximate KNN is a good option for high-dimensional  datasets  (exact
// KNN works slowly when dimensions count grows).
//
// An ALGLIB implementation of kd-trees is used to perform k-nn searches.
//
// Inputs:
//     S       -   KNN builder object
//     K       -   number of neighbors to search for, K >= 1
//     Eps     -   approximation factor:
//                 * Eps=0 means that exact kNN search is performed
//                 * Eps > 0 means that (1+Eps)-approximate search is performed
//
// Outputs:
//     Model       -   KNN model
//     Rep         -   report
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnbuilderbuildknnmodel(const knnbuilder &s, const ae_int_t k, const double eps, knnmodel &model, knnreport &rep);
void knnbuilderbuildknnmodel(knnbuilder *s, ae_int_t k, double eps, knnmodel *model, knnreport *rep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t nvars;
   ae_int_t nout;
   ae_int_t npoints;
   bool iscls;
   ae_frame_make(&_frame_block);
   SetObj(knnmodel, model);
   SetObj(knnreport, rep);
   NewMatrix(xy, 0, 0, DT_REAL);
   NewVector(tags, 0, DT_INT);
   npoints = s->npoints;
   nvars = s->nvars;
   nout = s->nout;
   iscls = s->iscls;
// Check settings
   ae_assert(k >= 1, "knnbuilderbuildknnmodel: k<1");
   ae_assert(isfinite(eps) && eps >= 0.0, "knnbuilderbuildknnmodel: eps<0");
// Prepare output
   knn_clearreport(rep);
   model->nvars = nvars;
   model->nout = nout;
   model->iscls = iscls;
   model->k = k;
   model->eps = eps;
   model->isdummy = false;
// Quick exit for empty dataset
   if (s->dstype == -1) {
      model->isdummy = true;
      ae_frame_leave();
      return;
   }
// Build kd-tree
   if (iscls) {
      ae_matrix_set_length(&xy, npoints, nvars + 1);
      ae_vector_set_length(&tags, npoints);
      for (i = 0; i < npoints; i++) {
         for (j = 0; j < nvars; j++) {
            xy.xyR[i][j] = s->dsdata.xyR[i][j];
         }
         xy.xyR[i][nvars] = (double)(s->dsival.xZ[i]);
         tags.xZ[i] = s->dsival.xZ[i];
      }
      kdtreebuildtagged(&xy, &tags, npoints, nvars, 0, s->knnnrm, &model->tree);
   } else {
      ae_matrix_set_length(&xy, npoints, nvars + nout);
      for (i = 0; i < npoints; i++) {
         for (j = 0; j < nvars; j++) {
            xy.xyR[i][j] = s->dsdata.xyR[i][j];
         }
         for (j = 0; j < nout; j++) {
            xy.xyR[i][nvars + j] = s->dsrval.xR[i * nout + j];
         }
      }
      kdtreebuild(&xy, npoints, nvars, nout, s->knnnrm, &model->tree);
   }
// Build buffer
   knncreatebuffer(model, &model->buffer);
// Report
   knnallerrors(model, &xy, npoints, rep);
   ae_frame_leave();
}

// Changing search settings of KNN model.
//
// K and EPS parameters of KNN  (AKNN)  search  are  specified  during  model
// construction. However, plain KNN algorithm with Euclidean distance  allows
// you to change them at any moment.
//
// NOTE: future versions of KNN model may support advanced versions  of  KNN,
//       such as NCA or LMNN. It is possible that such algorithms won't allow
//       you to change search settings on the fly. If you call this  function
//       for an algorithm which does not support on-the-fly changes, it  will
//       throw an exception.
//
// Inputs:
//     Model   -   KNN model
//     K       -   K >= 1, neighbors count
//     EPS     -   accuracy of the EPS-approximate NN search. Set to 0.0,  if
//                 you want to perform "classic" KNN search.  Specify  larger
//                 values  if  you  need  to  speed-up  high-dimensional  KNN
//                 queries.
//
// Outputs:
//     nothing on success, exception on failure
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnrewritekeps(const knnmodel &model, const ae_int_t k, const double eps);
void knnrewritekeps(knnmodel *model, ae_int_t k, double eps) {
   ae_assert(k >= 1, "knnrewritekeps: k<1");
   ae_assert(isfinite(eps) && eps >= 0.0, "knnrewritekeps: eps<0");
   model->k = k;
   model->eps = eps;
}

// Inference using KNN model.
//
// See also knnprocess0(), knnprocessi() and knnclassify() for options with a
// bit more convenient interface.
//
// IMPORTANT: this function is thread-unsafe and modifies internal structures
//            of the model! You can not use same model  object  for  parallel
//            evaluation from several threads.
//
//            Use knntsprocess() with independent  thread-local  buffers,  if
//            you need thread-safe evaluation.
//
// Inputs:
//     Model   -   KNN model
//     X       -   input vector,  array[0..NVars-1].
//     Y       -   possible preallocated buffer. Reused if long enough.
//
// Outputs:
//     Y       -   result. Regression estimate when solving regression  task,
//                 vector of posterior probabilities for classification task.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnprocess(const knnmodel &model, const real_1d_array &x, real_1d_array &y);
void knnprocess(knnmodel *model, RVector *x, RVector *y) {
   knntsprocess(model, &model->buffer, x, y);
}

// This function processes buf.X and stores result to buf.Y
//
// Inputs:
//     Model       -   KNN model
//     Buf         -   processing buffer.
//
// IMPORTANT: buffer object should be used only with model which was used  to
//            initialize buffer. Any attempt to  use  buffer  with  different
//            object is dangerous - you  may   get  integrity  check  failure
//            (exception) because sizes of internal  arrays  do  not  fit  to
//            dimensions of the model structure.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
static void knn_processinternal(knnmodel *model, knnbuffer *buf) {
   ae_int_t nvars;
   ae_int_t nout;
   bool iscls;
   ae_int_t nncnt;
   ae_int_t i;
   ae_int_t j;
   double v;
   nvars = model->nvars;
   nout = model->nout;
   iscls = model->iscls;
// Quick exit if needed
   if (model->isdummy) {
      for (i = 0; i < nout; i++) {
         buf->y.xR[i] = 0.0;
      }
      return;
   }
// Perform request, average results
   for (i = 0; i < nout; i++) {
      buf->y.xR[i] = 0.0;
   }
   nncnt = kdtreetsqueryaknn(&model->tree, &buf->treebuf, &buf->x, model->k, true, model->eps);
   v = 1 / coalesce((double)nncnt, 1.0);
   if (iscls) {
      kdtreetsqueryresultstags(&model->tree, &buf->treebuf, &buf->tags);
      for (i = 0; i < nncnt; i++) {
         j = buf->tags.xZ[i];
         buf->y.xR[j] += v;
      }
   } else {
      kdtreetsqueryresultsxy(&model->tree, &buf->treebuf, &buf->xy);
      for (i = 0; i < nncnt; i++) {
         for (j = 0; j < nout; j++) {
            buf->y.xR[j] += v * buf->xy.xyR[i][nvars + j];
         }
      }
   }
}

// This function returns first component of the  inferred  vector  (i.e.  one
// with index #0).
//
// It is a convenience wrapper for knnprocess() intended for either:
// * 1-dimensional regression problems
// * 2-class classification problems
//
// In the former case this function returns inference result as scalar, which
// is definitely more convenient that wrapping it as vector.  In  the  latter
// case it returns probability of object belonging to class #0.
//
// If you call it for anything different from two cases above, it  will  work
// as defined, i.e. return y[0], although it is of less use in such cases.
//
// IMPORTANT: this function is thread-unsafe and modifies internal structures
//            of the model! You can not use same model  object  for  parallel
//            evaluation from several threads.
//
//            Use knntsprocess() with independent  thread-local  buffers,  if
//            you need thread-safe evaluation.
//
// Inputs:
//     Model   -   KNN model
//     X       -   input vector,  array[0..NVars-1].
//
// Result:
//     Y[0]
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: double knnprocess0(const knnmodel &model, const real_1d_array &x);
double knnprocess0(knnmodel *model, RVector *x) {
   ae_int_t i;
   ae_int_t nvars;
   double result;
   nvars = model->nvars;
   for (i = 0; i < nvars; i++) {
      model->buffer.x.xR[i] = x->xR[i];
   }
   knn_processinternal(model, &model->buffer);
   result = model->buffer.y.xR[0];
   return result;
}

// This function returns most probable class number for an  input  X.  It  is
// same as calling knnprocess(model,x,y), then determining i=argmax(y[i]) and
// returning i.
//
// A class number in [0,NOut) range in returned for classification  problems,
// -1 is returned when this function is called for regression problems.
//
// IMPORTANT: this function is thread-unsafe and modifies internal structures
//            of the model! You can not use same model  object  for  parallel
//            evaluation from several threads.
//
//            Use knntsprocess() with independent  thread-local  buffers,  if
//            you need thread-safe evaluation.
//
// Inputs:
//     Model   -   KNN model
//     X       -   input vector,  array[0..NVars-1].
//
// Result:
//     class number, -1 for regression tasks
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: ae_int_t knnclassify(const knnmodel &model, const real_1d_array &x);
ae_int_t knnclassify(knnmodel *model, RVector *x) {
   ae_int_t i;
   ae_int_t nvars;
   ae_int_t nout;
   ae_int_t result;
   if (!model->iscls) {
      result = -1;
      return result;
   }
   nvars = model->nvars;
   nout = model->nout;
   for (i = 0; i < nvars; i++) {
      model->buffer.x.xR[i] = x->xR[i];
   }
   knn_processinternal(model, &model->buffer);
   result = 0;
   for (i = 1; i < nout; i++) {
      if (model->buffer.y.xR[i] > model->buffer.y.xR[result]) {
         result = i;
      }
   }
   return result;
}

// 'interactive' variant of knnprocess()  for  languages  like  Python  which
// support constructs like "y = knnprocessi(model,x)" and interactive mode of
// the interpreter.
//
// This function allocates new array on each call,  so  it  is  significantly
// slower than its 'non-interactive' counterpart, but it is  more  convenient
// when you call it from command line.
//
// IMPORTANT: this  function  is  thread-unsafe  and  may   modify   internal
//            structures of the model! You can not use same model  object for
//            parallel evaluation from several threads.
//
//            Use knntsprocess()  with  independent  thread-local  buffers if
//            you need thread-safe evaluation.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnprocessi(const knnmodel &model, const real_1d_array &x, real_1d_array &y);
void knnprocessi(knnmodel *model, RVector *x, RVector *y) {
   SetVector(y);
   knnprocess(model, x, y);
}

// Thread-safe procesing using external buffer for temporaries.
//
// This function is thread-safe (i.e.  you  can  use  same  KNN  model  from
// multiple threads) as long as you use different buffer objects for different
// threads.
//
// Inputs:
//     Model   -   KNN model
//     Buf     -   buffer object, must be  allocated  specifically  for  this
//                 model with knncreatebuffer().
//     X       -   input vector,  array[NVars]
//
// Outputs:
//     Y       -   result, array[NOut].   Regression  estimate  when  solving
//                 regression task,  vector  of  posterior  probabilities for
//                 a classification task.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knntsprocess(const knnmodel &model, const knnbuffer &buf, const real_1d_array &x, real_1d_array &y);
void knntsprocess(knnmodel *model, knnbuffer *buf, RVector *x, RVector *y) {
   ae_int_t i;
   ae_int_t nvars;
   ae_int_t nout;
   nvars = model->nvars;
   nout = model->nout;
   for (i = 0; i < nvars; i++) {
      buf->x.xR[i] = x->xR[i];
   }
   knn_processinternal(model, buf);
   if (y->cnt < nout) {
      ae_vector_set_length(y, nout);
   }
   for (i = 0; i < nout; i++) {
      y->xR[i] = buf->y.xR[i];
   }
}

// Relative classification error on the test set
//
// Inputs:
//     Model   -   KNN model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     percent of incorrectly classified cases.
//     Zero if model solves regression task.
//
// NOTE: if  you  need several different kinds of error metrics, it is better
//       to use knnallerrors() which computes all error metric  with just one
//       pass over dataset.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: double knnrelclserror(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints);
double knnrelclserror(knnmodel *model, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(knnreport, rep);
   knnallerrors(model, xy, npoints, &rep);
   result = rep.relclserror;
   ae_frame_leave();
   return result;
}

// Average cross-entropy (in bits per element) on the test set
//
// Inputs:
//     Model   -   KNN model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     CrossEntropy/NPoints.
//     Zero if model solves regression task.
//
// NOTE: the cross-entropy metric is too unstable when used to  evaluate  KNN
//       models (such models can report exactly  zero probabilities),  so  we
//       do not recommend using it.
//
// NOTE: if  you  need several different kinds of error metrics, it is better
//       to use knnallerrors() which computes all error metric  with just one
//       pass over dataset.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: double knnavgce(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints);
double knnavgce(knnmodel *model, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(knnreport, rep);
   knnallerrors(model, xy, npoints, &rep);
   result = rep.avgce;
   ae_frame_leave();
   return result;
}

// RMS error on the test set.
//
// Its meaning for regression task is obvious. As for classification problems,
// RMS error means error when estimating posterior probabilities.
//
// Inputs:
//     Model   -   KNN model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     root mean square error.
//
// NOTE: if  you  need several different kinds of error metrics, it is better
//       to use knnallerrors() which computes all error metric  with just one
//       pass over dataset.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: double knnrmserror(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints);
double knnrmserror(knnmodel *model, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(knnreport, rep);
   knnallerrors(model, xy, npoints, &rep);
   result = rep.rmserror;
   ae_frame_leave();
   return result;
}

// Average error on the test set
//
// Its meaning for regression task is obvious. As for classification problems,
// average error means error when estimating posterior probabilities.
//
// Inputs:
//     Model   -   KNN model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     average error
//
// NOTE: if  you  need several different kinds of error metrics, it is better
//       to use knnallerrors() which computes all error metric  with just one
//       pass over dataset.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: double knnavgerror(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints);
double knnavgerror(knnmodel *model, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(knnreport, rep);
   knnallerrors(model, xy, npoints, &rep);
   result = rep.avgerror;
   ae_frame_leave();
   return result;
}

// Average relative error on the test set
//
// Its meaning for regression task is obvious. As for classification problems,
// average relative error means error when estimating posterior probabilities.
//
// Inputs:
//     Model   -   KNN model
//     XY      -   test set
//     NPoints -   test set size
//
// Result:
//     average relative error
//
// NOTE: if  you  need several different kinds of error metrics, it is better
//       to use knnallerrors() which computes all error metric  with just one
//       pass over dataset.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: double knnavgrelerror(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints);
double knnavgrelerror(knnmodel *model, RMatrix *xy, ae_int_t npoints) {
   ae_frame _frame_block;
   double result;
   ae_frame_make(&_frame_block);
   NewObj(knnreport, rep);
   knnallerrors(model, xy, npoints, &rep);
   result = rep.avgrelerror;
   ae_frame_leave();
   return result;
}

// Calculates all kinds of errors for the model in one call.
//
// Inputs:
//     Model   -   KNN model
//     XY      -   test set:
//                 * one row per point
//                 * first NVars columns store independent variables
//                 * depending on problem type:
//                   * next column stores class number in [0,NClasses) -  for
//                     classification problems
//                   * next NOut columns  store  dependent  variables  -  for
//                     regression problems
//     NPoints -   test set size, NPoints >= 0
//
// Outputs:
//     Rep     -   following fields are loaded with errors for both regression
//                 and classification models:
//                 * rep.rmserror - RMS error for the output
//                 * rep.avgerror - average error
//                 * rep.avgrelerror - average relative error
//                 following fields are set only  for classification  models,
//                 zero for regression ones:
//                 * relclserror   - relative classification error, in [0,1]
//                 * avgce - average cross-entropy in bits per dataset entry
//
// NOTE: the cross-entropy metric is too unstable when used to  evaluate  KNN
//       models (such models can report exactly  zero probabilities),  so  we
//       do not recommend using it.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnallerrors(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints, knnreport &rep);
void knnallerrors(knnmodel *model, RMatrix *xy, ae_int_t npoints, knnreport *rep) {
   ae_frame _frame_block;
   ae_int_t nvars;
   ae_int_t nout;
   ae_int_t ny;
   bool iscls;
   ae_int_t i;
   ae_int_t j;
   ae_frame_make(&_frame_block);
   SetObj(knnreport, rep);
   NewObj(knnbuffer, buf);
   NewVector(desiredy, 0, DT_REAL);
   NewVector(errbuf, 0, DT_REAL);
   nvars = model->nvars;
   nout = model->nout;
   iscls = model->iscls;
   if (iscls) {
      ny = 1;
   } else {
      ny = nout;
   }
// Check input
   ae_assert(npoints >= 0, "knnallerrors: npoints<0");
   ae_assert(xy->rows >= npoints, "knnallerrors: rows(xy)<npoints");
   ae_assert(xy->cols >= nvars + ny, "knnallerrors: cols(xy)<nvars+nout");
   ae_assert(apservisfinitematrix(xy, npoints, nvars + ny), "knnallerrors: xy parameter contains INFs or NANs");
// Clean up report
   knn_clearreport(rep);
// Quick exit if needed
   if (model->isdummy || npoints == 0) {
      ae_frame_leave();
      return;
   }
// Process using local buffer
   knncreatebuffer(model, &buf);
   if (iscls) {
      dserrallocate(nout, &errbuf);
   } else {
      dserrallocate(-nout, &errbuf);
   }
   ae_vector_set_length(&desiredy, ny);
   for (i = 0; i < npoints; i++) {
      for (j = 0; j < nvars; j++) {
         buf.x.xR[j] = xy->xyR[i][j];
      }
      if (iscls) {
         j = RoundZ(xy->xyR[i][nvars]);
         ae_assert(j >= 0 && j < nout, "knnallerrors: one of the class labels is not in [0,NClasses)");
         desiredy.xR[0] = (double)j;
      } else {
         for (j = 0; j < nout; j++) {
            desiredy.xR[j] = xy->xyR[i][nvars + j];
         }
      }
      knn_processinternal(model, &buf);
      dserraccumulate(&errbuf, &buf.y, &desiredy);
   }
   dserrfinish(&errbuf);
// Extract results
   if (iscls) {
      rep->relclserror = errbuf.xR[0];
      rep->avgce = errbuf.xR[1];
   }
   rep->rmserror = errbuf.xR[2];
   rep->avgerror = errbuf.xR[3];
   rep->avgrelerror = errbuf.xR[4];
   ae_frame_leave();
}

// Serializer: allocation
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
void knnalloc(ae_serializer *s, knnmodel *model) {
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   ae_serializer_alloc_entry(s);
   if (!model->isdummy) {
      kdtreealloc(s, &model->tree);
   }
}

// Serializer: serialization
// These functions serialize a data structure to a C++ string or stream.
// * serialization can be freely moved across 32-bit and 64-bit systems,
//   and different byte orders. For example, you can serialize a string
//   on a SPARC and unserialize it on an x86.
// * ALGLIB++ serialization is compatible with serialization in ALGLIB,
//   in both directions.
// Important properties of s_out:
// * it contains alphanumeric characters, dots, underscores, minus signs
// * these symbols are grouped into words, which are separated by spaces
//   and Windows-style (CR+LF) newlines
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnserialize(knnmodel &obj, std::string &s_out);
// API: void knnserialize(knnmodel &obj, std::ostream &s_out);
void knnserialize(ae_serializer *s, knnmodel *model) {
   ae_serializer_serialize_int(s, getknnserializationcode());
   ae_serializer_serialize_int(s, knn_knnfirstversion);
   ae_serializer_serialize_int(s, model->nvars);
   ae_serializer_serialize_int(s, model->nout);
   ae_serializer_serialize_int(s, model->k);
   ae_serializer_serialize_double(s, model->eps);
   ae_serializer_serialize_bool(s, model->iscls);
   ae_serializer_serialize_bool(s, model->isdummy);
   if (!model->isdummy) {
      kdtreeserialize(s, &model->tree);
   }
}

// Serializer: unserialization
// These functions unserialize a data structure from a C++ string or stream.
// Important properties of s_in:
// * any combination of spaces, tabs, Windows or Unix stype newlines can
//   be used as separators, so as to allow flexible reformatting of the
//   stream or string from text or XML files.
// * But you should not insert separators into the middle of the "words"
//   nor you should change case of letters.
// ALGLIB: Copyright 15.02.2019 by Sergey Bochkanov
// API: void knnunserialize(const std::string &s_in, knnmodel &obj);
// API: void knnunserialize(const std::istream &s_in, knnmodel &obj);
void knnunserialize(ae_serializer *s, knnmodel *model) {
   ae_int_t i0;
   ae_int_t i1;
   SetObj(knnmodel, model);
// check correctness of header
   i0 = ae_serializer_unserialize_int(s);
   ae_assert(i0 == getknnserializationcode(), "knnunserialize: stream header corrupted");
   i1 = ae_serializer_unserialize_int(s);
   ae_assert(i1 == knn_knnfirstversion, "knnunserialize: stream header corrupted");
// Unserialize data
   model->nvars = ae_serializer_unserialize_int(s);
   model->nout = ae_serializer_unserialize_int(s);
   model->k = ae_serializer_unserialize_int(s);
   model->eps = ae_serializer_unserialize_double(s);
   model->iscls = ae_serializer_unserialize_bool(s);
   model->isdummy = ae_serializer_unserialize_bool(s);
   if (!model->isdummy) {
      kdtreeunserialize(s, &model->tree);
   }
// Prepare local buffer
   knncreatebuffer(model, &model->buffer);
}

void knnbuffer_init(void *_p, bool make_automatic) {
   knnbuffer *p = (knnbuffer *)_p;
   kdtreerequestbuffer_init(&p->treebuf, make_automatic);
   ae_vector_init(&p->x, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->y, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->tags, 0, DT_INT, make_automatic);
   ae_matrix_init(&p->xy, 0, 0, DT_REAL, make_automatic);
}

void knnbuffer_copy(void *_dst, void *_src, bool make_automatic) {
   knnbuffer *dst = (knnbuffer *)_dst;
   knnbuffer *src = (knnbuffer *)_src;
   kdtreerequestbuffer_copy(&dst->treebuf, &src->treebuf, make_automatic);
   ae_vector_copy(&dst->x, &src->x, make_automatic);
   ae_vector_copy(&dst->y, &src->y, make_automatic);
   ae_vector_copy(&dst->tags, &src->tags, make_automatic);
   ae_matrix_copy(&dst->xy, &src->xy, make_automatic);
}

void knnbuffer_free(void *_p, bool make_automatic) {
   knnbuffer *p = (knnbuffer *)_p;
   kdtreerequestbuffer_free(&p->treebuf, make_automatic);
   ae_vector_free(&p->x, make_automatic);
   ae_vector_free(&p->y, make_automatic);
   ae_vector_free(&p->tags, make_automatic);
   ae_matrix_free(&p->xy, make_automatic);
}

void knnbuilder_init(void *_p, bool make_automatic) {
   knnbuilder *p = (knnbuilder *)_p;
   ae_matrix_init(&p->dsdata, 0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->dsrval, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->dsival, 0, DT_INT, make_automatic);
}

void knnbuilder_copy(void *_dst, void *_src, bool make_automatic) {
   knnbuilder *dst = (knnbuilder *)_dst;
   knnbuilder *src = (knnbuilder *)_src;
   dst->dstype = src->dstype;
   dst->npoints = src->npoints;
   dst->nvars = src->nvars;
   dst->iscls = src->iscls;
   dst->nout = src->nout;
   ae_matrix_copy(&dst->dsdata, &src->dsdata, make_automatic);
   ae_vector_copy(&dst->dsrval, &src->dsrval, make_automatic);
   ae_vector_copy(&dst->dsival, &src->dsival, make_automatic);
   dst->knnnrm = src->knnnrm;
}

void knnbuilder_free(void *_p, bool make_automatic) {
   knnbuilder *p = (knnbuilder *)_p;
   ae_matrix_free(&p->dsdata, make_automatic);
   ae_vector_free(&p->dsrval, make_automatic);
   ae_vector_free(&p->dsival, make_automatic);
}

void knnmodel_init(void *_p, bool make_automatic) {
   knnmodel *p = (knnmodel *)_p;
   kdtree_init(&p->tree, make_automatic);
   knnbuffer_init(&p->buffer, make_automatic);
}

void knnmodel_copy(void *_dst, void *_src, bool make_automatic) {
   knnmodel *dst = (knnmodel *)_dst;
   knnmodel *src = (knnmodel *)_src;
   dst->nvars = src->nvars;
   dst->nout = src->nout;
   dst->k = src->k;
   dst->eps = src->eps;
   dst->iscls = src->iscls;
   dst->isdummy = src->isdummy;
   kdtree_copy(&dst->tree, &src->tree, make_automatic);
   knnbuffer_copy(&dst->buffer, &src->buffer, make_automatic);
}

void knnmodel_free(void *_p, bool make_automatic) {
   knnmodel *p = (knnmodel *)_p;
   kdtree_free(&p->tree, make_automatic);
   knnbuffer_free(&p->buffer, make_automatic);
}

void knnreport_init(void *_p, bool make_automatic) {
}

void knnreport_copy(void *_dst, void *_src, bool make_automatic) {
   knnreport *dst = (knnreport *)_dst;
   knnreport *src = (knnreport *)_src;
   dst->relclserror = src->relclserror;
   dst->avgce = src->avgce;
   dst->rmserror = src->rmserror;
   dst->avgerror = src->avgerror;
   dst->avgrelerror = src->avgrelerror;
}

void knnreport_free(void *_p, bool make_automatic) {
}
} // end of namespace alglib_impl

namespace alglib {
// Buffer object which is used to perform  various  requests  (usually  model
// inference) in the multithreaded mode (multiple threads working  with  same
// KNN object).
//
// This object should be created with KNNCreateBuffer().
DefClass(knnbuffer, EndD)

// A KNN builder object; this object encapsulates  dataset  and  all  related
// settings, it is used to create an actual instance of KNN model.
DefClass(knnbuilder, EndD)

// KNN model, can be used for classification or regression
DefClass(knnmodel, EndD)

// KNN training report.
//
// Following fields store training set errors:
// * relclserror       -   fraction of misclassified cases, [0,1]
// * avgce             -   average cross-entropy in bits per symbol
// * rmserror          -   root-mean-square error
// * avgerror          -   average error
// * avgrelerror       -   average relative error
//
// For classification problems:
// * RMS, AVG and AVGREL errors are calculated for posterior probabilities
//
// For regression problems:
// * RELCLS and AVGCE errors are zero
DefClass(knnreport, AndD DecVal(relclserror) AndD DecVal(avgce) AndD DecVal(rmserror) AndD DecVal(avgerror) AndD DecVal(avgrelerror))

void knnserialize(knnmodel &obj, std::string &s_out) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_alloc_start(&serializer);
   alglib_impl::knnalloc(&serializer, obj.c_ptr());
   ae_int_t ssize = alglib_impl::ae_serializer_get_alloc_size(&serializer);
   s_out.clear();
   s_out.reserve((size_t)(ssize + 1));
   alglib_impl::ae_serializer_sstart_str(&serializer, &s_out);
   alglib_impl::knnserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_assert(s_out.length() <= (size_t)ssize, "knnserialize: serialization integrity error");
   alglib_impl::ae_state_clear();
}
void knnserialize(knnmodel &obj, std::ostream &s_out) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_alloc_start(&serializer);
   alglib_impl::knnalloc(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_get_alloc_size(&serializer); // not actually needed, but we have to ask
   alglib_impl::ae_serializer_sstart_stream(&serializer, &s_out);
   alglib_impl::knnserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}

void knnunserialize(const std::string &s_in, knnmodel &obj) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_ustart_str(&serializer, &s_in);
   alglib_impl::knnunserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}
void knnunserialize(const std::istream &s_in, knnmodel &obj) {
   alglib_impl::ae_state_init();
   TryCatch()
   NewSerializer(serializer);
   alglib_impl::ae_serializer_ustart_stream(&serializer, &s_in);
   alglib_impl::knnunserialize(&serializer, obj.c_ptr());
   alglib_impl::ae_serializer_stop(&serializer);
   alglib_impl::ae_state_clear();
}

void knncreatebuffer(const knnmodel &model, knnbuffer &buf) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knncreatebuffer(ConstT(knnmodel, model), ConstT(knnbuffer, buf));
   alglib_impl::ae_state_clear();
}

void knnbuildercreate(knnbuilder &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knnbuildercreate(ConstT(knnbuilder, s));
   alglib_impl::ae_state_clear();
}

void knnbuildersetdatasetreg(const knnbuilder &s, const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nout) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knnbuildersetdatasetreg(ConstT(knnbuilder, s), ConstT(ae_matrix, xy), npoints, nvars, nout);
   alglib_impl::ae_state_clear();
}

void knnbuildersetdatasetcls(const knnbuilder &s, const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t nclasses) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knnbuildersetdatasetcls(ConstT(knnbuilder, s), ConstT(ae_matrix, xy), npoints, nvars, nclasses);
   alglib_impl::ae_state_clear();
}

void knnbuildersetnorm(const knnbuilder &s, const ae_int_t nrmtype) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knnbuildersetnorm(ConstT(knnbuilder, s), nrmtype);
   alglib_impl::ae_state_clear();
}

void knnbuilderbuildknnmodel(const knnbuilder &s, const ae_int_t k, const double eps, knnmodel &model, knnreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knnbuilderbuildknnmodel(ConstT(knnbuilder, s), k, eps, ConstT(knnmodel, model), ConstT(knnreport, rep));
   alglib_impl::ae_state_clear();
}

void knnrewritekeps(const knnmodel &model, const ae_int_t k, const double eps) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knnrewritekeps(ConstT(knnmodel, model), k, eps);
   alglib_impl::ae_state_clear();
}

void knnprocess(const knnmodel &model, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knnprocess(ConstT(knnmodel, model), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

double knnprocess0(const knnmodel &model, const real_1d_array &x) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::knnprocess0(ConstT(knnmodel, model), ConstT(ae_vector, x));
   alglib_impl::ae_state_clear();
   return D;
}

ae_int_t knnclassify(const knnmodel &model, const real_1d_array &x) {
   alglib_impl::ae_state_init();
   TryCatch(0)
   ae_int_t Z = alglib_impl::knnclassify(ConstT(knnmodel, model), ConstT(ae_vector, x));
   alglib_impl::ae_state_clear();
   return Z;
}

void knnprocessi(const knnmodel &model, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knnprocessi(ConstT(knnmodel, model), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

void knntsprocess(const knnmodel &model, const knnbuffer &buf, const real_1d_array &x, real_1d_array &y) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knntsprocess(ConstT(knnmodel, model), ConstT(knnbuffer, buf), ConstT(ae_vector, x), ConstT(ae_vector, y));
   alglib_impl::ae_state_clear();
}

double knnrelclserror(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::knnrelclserror(ConstT(knnmodel, model), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double knnavgce(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::knnavgce(ConstT(knnmodel, model), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double knnrmserror(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::knnrmserror(ConstT(knnmodel, model), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double knnavgerror(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::knnavgerror(ConstT(knnmodel, model), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

double knnavgrelerror(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch(0.0)
   double D = alglib_impl::knnavgrelerror(ConstT(knnmodel, model), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
   return D;
}

void knnallerrors(const knnmodel &model, const real_2d_array &xy, const ae_int_t npoints, knnreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::knnallerrors(ConstT(knnmodel, model), ConstT(ae_matrix, xy), npoints, ConstT(knnreport, rep));
   alglib_impl::ae_state_clear();
}
} // end of namespace alglib

// === MLPTRAIN Package ===
// Depends on: (Solvers) DIRECTDENSESOLVERS
// Depends on: (Optimization) MINLBFGS
// Depends on: MLPE
namespace alglib_impl {
static const double mlptrain_mindecay = 0.001;
static const ae_int_t mlptrain_defaultlbfgsfactor = 6;

// Neural network training  using  modified  Levenberg-Marquardt  with  exact
// Hessian calculation and regularization. Subroutine trains  neural  network
// with restarts from random positions. Algorithm is well  suited  for  small
// and medium scale problems (hundreds of weights).
//
// Inputs:
//     Network     -   neural network with initialized geometry
//     XY          -   training set
//     NPoints     -   training set size
//     Decay       -   weight decay constant, >= 0.001
//                     Decay term 'Decay*||Weights||^2' is added to error
//                     function.
//                     If you don't know what Decay to choose, use 0.001.
//     Restarts    -   number of restarts from random position, > 0.
//                     If you don't know what Restarts to choose, use 2.
//
// Outputs:
//     Network     -   trained neural network.
//     Info        -   return code:
//                     * -9, if internal matrix inverse subroutine failed
//                     * -2, if there is a point with class number
//                           outside of [0..NOut-1].
//                     * -1, if wrong parameters specified
//                           (NPoints < 0, Restarts < 1).
//                     *  2, if task has been solved.
//     Rep         -   training report
// ALGLIB: Copyright 10.03.2009 by Sergey Bochkanov
// API: void mlptrainlm(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, ae_int_t &info, mlpreport &rep);
void mlptrainlm(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints, double decay, ae_int_t restarts, ae_int_t *info, mlpreport *rep) {
   ae_frame _frame_block;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   double lmsteptol;
   ae_int_t i;
   ae_int_t k;
   double v;
   double e;
   double enew;
   double xnorm2;
   double stepnorm;
   bool spd;
   double nu;
   double lambdav;
   double lambdaup;
   double lambdadown;
   ae_int_t pass;
   double ebest;
   ae_int_t invinfo;
   ae_int_t solverinfo;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(mlpreport, rep);
   NewVector(g, 0, DT_REAL);
   NewVector(d, 0, DT_REAL);
   NewMatrix(h, 0, 0, DT_REAL);
   NewMatrix(hmod, 0, 0, DT_REAL);
   NewMatrix(z, 0, 0, DT_REAL);
   NewObj(minlbfgsreport, internalrep);
   NewObj(minlbfgsstate, state);
   NewVector(x, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   NewVector(wbase, 0, DT_REAL);
   NewVector(wdir, 0, DT_REAL);
   NewVector(wt, 0, DT_REAL);
   NewVector(wx, 0, DT_REAL);
   NewVector(wbest, 0, DT_REAL);
   NewObj(matinvreport, invrep);
   NewObj(densesolverreport, solverrep);
   mlpproperties(network, &nin, &nout, &wcount);
   lambdaup = 10.0;
   lambdadown = 0.3;
   lmsteptol = 0.001;
// Test for inputs
   if (npoints <= 0 || restarts < 1) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   if (mlpissoftmax(network)) {
      for (i = 0; i < npoints; i++) {
         if (RoundZ(xy->xyR[i][nin]) < 0 || RoundZ(xy->xyR[i][nin]) >= nout) {
            *info = -2;
            ae_frame_leave();
            return;
         }
      }
   }
   decay = rmax2(decay, mlptrain_mindecay);
   *info = 2;
// Initialize data
   rep->ngrad = 0;
   rep->nhess = 0;
   rep->ncholesky = 0;
// General case.
// Prepare task and network. Allocate space.
   mlpinitpreprocessor(network, xy, npoints);
   ae_vector_set_length(&g, wcount);
   ae_matrix_set_length(&h, wcount, wcount);
   ae_matrix_set_length(&hmod, wcount, wcount);
   ae_vector_set_length(&wbase, wcount);
   ae_vector_set_length(&wdir, wcount);
   ae_vector_set_length(&wbest, wcount);
   ae_vector_set_length(&wt, wcount);
   ae_vector_set_length(&wx, wcount);
   ebest = ae_maxrealnumber;
// Multiple passes
   for (pass = 1; pass <= restarts; pass++) {
   // Initialize weights
      mlprandomize(network);
   // First stage of the hybrid algorithm: LBFGS
      ae_v_move(wbase.xR, 1, network->weights.xR, 1, wcount);
      minlbfgscreate(wcount, imin2(wcount, 5), &wbase, &state);
      minlbfgssetcond(&state, 0.0, 0.0, 0.0, imax2(25, wcount));
      while (minlbfgsiteration(&state)) {
      // gradient
         ae_v_move(network->weights.xR, 1, state.x.xR, 1, wcount);
         mlpgradbatch(network, xy, npoints, &state.f, &state.g);
      // weight decay
         v = ae_v_dotproduct(network->weights.xR, 1, network->weights.xR, 1, wcount);
         state.f += 0.5 * decay * v;
         ae_v_addd(state.g.xR, 1, network->weights.xR, 1, wcount, decay);
      // next iteration
         rep->ngrad++;
      }
      minlbfgsresults(&state, &wbase, &internalrep);
      ae_v_move(network->weights.xR, 1, wbase.xR, 1, wcount);
   // Second stage of the hybrid algorithm: LM
   //
   // Initialize H with identity matrix,
   // G with gradient,
   // E with regularized error.
      mlphessianbatch(network, xy, npoints, &e, &g, &h);
      v = ae_v_dotproduct(network->weights.xR, 1, network->weights.xR, 1, wcount);
      e += 0.5 * decay * v;
      ae_v_addd(g.xR, 1, network->weights.xR, 1, wcount, decay);
      for (k = 0; k < wcount; k++) {
         h.xyR[k][k] += decay;
      }
      rep->nhess++;
      lambdav = 0.001;
      nu = 2.0;
      while (true) {
      // 1. HMod = H+lambda*I
      // 2. Try to solve (H+Lambda*I)*dx = -g.
      //    Increase lambda if left part is not positive definite.
         for (i = 0; i < wcount; i++) {
            ae_v_move(hmod.xyR[i], 1, h.xyR[i], 1, wcount);
            hmod.xyR[i][i] += lambdav;
         }
         spd = spdmatrixcholesky(&hmod, wcount, true);
         rep->ncholesky++;
         if (!spd) {
            lambdav *= lambdaup * nu;
            nu *= 2;
            continue;
         }
         spdmatrixcholeskysolve(&hmod, wcount, true, &g, &solverinfo, &solverrep, &wdir);
         if (solverinfo < 0) {
            lambdav *= lambdaup * nu;
            nu *= 2;
            continue;
         }
         ae_v_muld(wdir.xR, 1, wcount, -1);
      // Lambda found.
      // 1. Save old w in WBase
      // 1. Test some stopping criterions
      // 2. If error(w+wdir)>error(w), increase lambda
         ae_v_add(network->weights.xR, 1, wdir.xR, 1, wcount);
         xnorm2 = ae_v_dotproduct(network->weights.xR, 1, network->weights.xR, 1, wcount);
         stepnorm = ae_v_dotproduct(wdir.xR, 1, wdir.xR, 1, wcount);
         stepnorm = sqrt(stepnorm);
         enew = mlperror(network, xy, npoints) + 0.5 * decay * xnorm2;
         if (stepnorm < lmsteptol * (1 + sqrt(xnorm2))) {
            break;
         }
         if (enew > e) {
            lambdav *= lambdaup * nu;
            nu *= 2;
            continue;
         }
      // Optimize using inv(cholesky(H)) as preconditioner
         rmatrixtrinverse(&hmod, wcount, true, false, &invinfo, &invrep);
         if (invinfo <= 0) {
         // if matrix can't be inverted then exit with errors
         // TODO: make WCount steps in direction suggested by HMod
            *info = -9;
            ae_frame_leave();
            return;
         }
         ae_v_move(wbase.xR, 1, network->weights.xR, 1, wcount);
         for (i = 0; i < wcount; i++) {
            wt.xR[i] = 0.0;
         }
         minlbfgscreatex(wcount, wcount, &wt, 1, 0.0, &state);
         minlbfgssetcond(&state, 0.0, 0.0, 0.0, 5);
         while (minlbfgsiteration(&state)) {
         // gradient
            for (i = 0; i < wcount; i++) {
               v = ae_v_dotproduct(&state.x.xR[i], 1, &hmod.xyR[i][i], 1, wcount - i);
               network->weights.xR[i] = wbase.xR[i] + v;
            }
            mlpgradbatch(network, xy, npoints, &state.f, &g);
            for (i = 0; i < wcount; i++) {
               state.g.xR[i] = 0.0;
            }
            for (i = 0; i < wcount; i++) {
               v = g.xR[i];
               ae_v_addd(&state.g.xR[i], 1, &hmod.xyR[i][i], 1, wcount - i, v);
            }
         // weight decay
         // grad(x'*x) = A'*(x0+A*t)
            v = ae_v_dotproduct(network->weights.xR, 1, network->weights.xR, 1, wcount);
            state.f += 0.5 * decay * v;
            for (i = 0; i < wcount; i++) {
               v = decay * network->weights.xR[i];
               ae_v_addd(&state.g.xR[i], 1, &hmod.xyR[i][i], 1, wcount - i, v);
            }
         // next iteration
            rep->ngrad++;
         }
         minlbfgsresults(&state, &wt, &internalrep);
      // Accept new position.
      // Calculate Hessian
         for (i = 0; i < wcount; i++) {
            v = ae_v_dotproduct(&wt.xR[i], 1, &hmod.xyR[i][i], 1, wcount - i);
            network->weights.xR[i] = wbase.xR[i] + v;
         }
         mlphessianbatch(network, xy, npoints, &e, &g, &h);
         v = ae_v_dotproduct(network->weights.xR, 1, network->weights.xR, 1, wcount);
         e += 0.5 * decay * v;
         ae_v_addd(g.xR, 1, network->weights.xR, 1, wcount, decay);
         for (k = 0; k < wcount; k++) {
            h.xyR[k][k] += decay;
         }
         rep->nhess++;
      // Update lambda
         lambdav *= lambdadown;
         nu = 2.0;
      }
   // update WBest
      v = ae_v_dotproduct(network->weights.xR, 1, network->weights.xR, 1, wcount);
      e = 0.5 * decay * v + mlperror(network, xy, npoints);
      if (e < ebest) {
         ebest = e;
         ae_v_move(wbest.xR, 1, network->weights.xR, 1, wcount);
      }
   }
// copy WBest to output
   ae_v_move(network->weights.xR, 1, wbest.xR, 1, wcount);
   ae_frame_leave();
}

// Neural  network  training  using  L-BFGS  algorithm  with  regularization.
// Subroutine  trains  neural  network  with  restarts from random positions.
// Algorithm  is  well  suited  for  problems  of  any dimensionality (memory
// requirements and step complexity are linear by weights number).
//
// Inputs:
//     Network     -   neural network with initialized geometry
//     XY          -   training set
//     NPoints     -   training set size
//     Decay       -   weight decay constant, >= 0.001
//                     Decay term 'Decay*||Weights||^2' is added to error
//                     function.
//                     If you don't know what Decay to choose, use 0.001.
//     Restarts    -   number of restarts from random position, > 0.
//                     If you don't know what Restarts to choose, use 2.
//     WStep       -   stopping criterion. Algorithm stops if  step  size  is
//                     less than WStep. Recommended value - 0.01.  Zero  step
//                     size means stopping after MaxIts iterations.
//     MaxIts      -   stopping   criterion.  Algorithm  stops  after  MaxIts
//                     iterations (NOT gradient  calculations).  Zero  MaxIts
//                     means stopping when step is sufficiently small.
//
// Outputs:
//     Network     -   trained neural network.
//     Info        -   return code:
//                     * -8, if both WStep=0 and MaxIts=0
//                     * -2, if there is a point with class number
//                           outside of [0..NOut-1].
//                     * -1, if wrong parameters specified
//                           (NPoints < 0, Restarts < 1).
//                     *  2, if task has been solved.
//     Rep         -   training report
// ALGLIB: Copyright 09.12.2007 by Sergey Bochkanov
// API: void mlptrainlbfgs(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, const double wstep, const ae_int_t maxits, ae_int_t &info, mlpreport &rep);
void mlptrainlbfgs(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints, double decay, ae_int_t restarts, double wstep, ae_int_t maxits, ae_int_t *info, mlpreport *rep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t pass;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   double e;
   double v;
   double ebest;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(mlpreport, rep);
   NewVector(w, 0, DT_REAL);
   NewVector(wbest, 0, DT_REAL);
   NewObj(minlbfgsreport, internalrep);
   NewObj(minlbfgsstate, state);
// Test inputs, parse flags, read network geometry
   if (wstep == 0.0 && maxits == 0) {
      *info = -8;
      ae_frame_leave();
      return;
   }
   if (npoints <= 0 || restarts < 1 || wstep < 0.0 || maxits < 0) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   mlpproperties(network, &nin, &nout, &wcount);
   if (mlpissoftmax(network)) {
      for (i = 0; i < npoints; i++) {
         if (RoundZ(xy->xyR[i][nin]) < 0 || RoundZ(xy->xyR[i][nin]) >= nout) {
            *info = -2;
            ae_frame_leave();
            return;
         }
      }
   }
   decay = rmax2(decay, mlptrain_mindecay);
   *info = 2;
// Prepare
   mlpinitpreprocessor(network, xy, npoints);
   ae_vector_set_length(&w, wcount);
   ae_vector_set_length(&wbest, wcount);
   ebest = ae_maxrealnumber;
// Multiple starts
   rep->ncholesky = 0;
   rep->nhess = 0;
   rep->ngrad = 0;
   for (pass = 1; pass <= restarts; pass++) {
   // Process
      mlprandomize(network);
      ae_v_move(w.xR, 1, network->weights.xR, 1, wcount);
      minlbfgscreate(wcount, imin2(wcount, 10), &w, &state);
      minlbfgssetcond(&state, 0.0, 0.0, wstep, maxits);
      while (minlbfgsiteration(&state)) {
         ae_v_move(network->weights.xR, 1, state.x.xR, 1, wcount);
         mlpgradnbatch(network, xy, npoints, &state.f, &state.g);
         v = ae_v_dotproduct(network->weights.xR, 1, network->weights.xR, 1, wcount);
         state.f += 0.5 * decay * v;
         ae_v_addd(state.g.xR, 1, network->weights.xR, 1, wcount, decay);
         rep->ngrad++;
      }
      minlbfgsresults(&state, &w, &internalrep);
      ae_v_move(network->weights.xR, 1, w.xR, 1, wcount);
   // Compare with best
      v = ae_v_dotproduct(network->weights.xR, 1, network->weights.xR, 1, wcount);
      e = mlperrorn(network, xy, npoints) + 0.5 * decay * v;
      if (e < ebest) {
         ae_v_move(wbest.xR, 1, network->weights.xR, 1, wcount);
         ebest = e;
      }
   }
// The best network
   ae_v_move(network->weights.xR, 1, wbest.xR, 1, wcount);
   ae_frame_leave();
}

// Neural network training using early stopping (base algorithm - L-BFGS with
// regularization).
//
// Inputs:
//     Network     -   neural network with initialized geometry
//     TrnXY       -   training set
//     TrnSize     -   training set size, TrnSize > 0
//     ValXY       -   validation set
//     ValSize     -   validation set size, ValSize > 0
//     Decay       -   weight decay constant, >= 0.001
//                     Decay term 'Decay*||Weights||^2' is added to error
//                     function.
//                     If you don't know what Decay to choose, use 0.001.
//     Restarts    -   number of restarts, either:
//                     * strictly positive number - algorithm make specified
//                       number of restarts from random position.
//                     * -1, in which case algorithm makes exactly one run
//                       from the initial state of the network (no randomization).
//                     If you don't know what Restarts to choose, choose one
//                     one the following:
//                     * -1 (deterministic start)
//                     * +1 (one random restart)
//                     * +5 (moderate amount of random restarts)
//
// Outputs:
//     Network     -   trained neural network.
//     Info        -   return code:
//                     * -2, if there is a point with class number
//                           outside of [0..NOut-1].
//                     * -1, if wrong parameters specified
//                           (NPoints < 0, Restarts < 1, ...).
//                     *  2, task has been solved, stopping  criterion  met -
//                           sufficiently small step size.  Not expected  (we
//                           use  EARLY  stopping)  but  possible  and not an
//                           error.
//                     *  6, task has been solved, stopping  criterion  met -
//                           increasing of validation set error.
//     Rep         -   training report
//
// NOTE:
//
// Algorithm stops if validation set error increases for  a  long  enough  or
// step size is small enought  (there  are  task  where  validation  set  may
// decrease for eternity). In any case solution returned corresponds  to  the
// minimum of validation set error.
// ALGLIB: Copyright 10.03.2009 by Sergey Bochkanov
// API: void mlptraines(const multilayerperceptron &network, const real_2d_array &trnxy, const ae_int_t trnsize, const real_2d_array &valxy, const ae_int_t valsize, const double decay, const ae_int_t restarts, ae_int_t &info, mlpreport &rep);
void mlptraines(multilayerperceptron *network, RMatrix *trnxy, ae_int_t trnsize, RMatrix *valxy, ae_int_t valsize, double decay, ae_int_t restarts, ae_int_t *info, mlpreport *rep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t pass;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   double e;
   double v;
   double ebest;
   double efinal;
   ae_int_t itcnt;
   ae_int_t itbest;
   double wstep;
   bool needrandomization;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(mlpreport, rep);
   NewVector(w, 0, DT_REAL);
   NewVector(wbest, 0, DT_REAL);
   NewVector(wfinal, 0, DT_REAL);
   NewObj(minlbfgsreport, internalrep);
   NewObj(minlbfgsstate, state);
   wstep = 0.001;
// Test inputs, parse flags, read network geometry
   if (trnsize <= 0 || valsize <= 0 || restarts < 1 && restarts != -1 || decay < 0.0) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   if (restarts == -1) {
      needrandomization = false;
      restarts = 1;
   } else {
      needrandomization = true;
   }
   mlpproperties(network, &nin, &nout, &wcount);
   if (mlpissoftmax(network)) {
      for (i = 0; i < trnsize; i++) {
         if (RoundZ(trnxy->xyR[i][nin]) < 0 || RoundZ(trnxy->xyR[i][nin]) >= nout) {
            *info = -2;
            ae_frame_leave();
            return;
         }
      }
      for (i = 0; i < valsize; i++) {
         if (RoundZ(valxy->xyR[i][nin]) < 0 || RoundZ(valxy->xyR[i][nin]) >= nout) {
            *info = -2;
            ae_frame_leave();
            return;
         }
      }
   }
   *info = 2;
// Prepare
   mlpinitpreprocessor(network, trnxy, trnsize);
   ae_vector_set_length(&w, wcount);
   ae_vector_set_length(&wbest, wcount);
   ae_vector_set_length(&wfinal, wcount);
   efinal = ae_maxrealnumber;
   for (i = 0; i < wcount; i++) {
      wfinal.xR[i] = 0.0;
   }
// Multiple starts
   rep->ncholesky = 0;
   rep->nhess = 0;
   rep->ngrad = 0;
   for (pass = 1; pass <= restarts; pass++) {
   // Process
      if (needrandomization) {
         mlprandomize(network);
      }
      ebest = mlperror(network, valxy, valsize);
      ae_v_move(wbest.xR, 1, network->weights.xR, 1, wcount);
      itbest = 0;
      itcnt = 0;
      ae_v_move(w.xR, 1, network->weights.xR, 1, wcount);
      minlbfgscreate(wcount, imin2(wcount, 10), &w, &state);
      minlbfgssetcond(&state, 0.0, 0.0, wstep, 0);
      minlbfgssetxrep(&state, true);
      while (minlbfgsiteration(&state)) {
         if (state.needfg) { // Calculate gradient
            ae_v_move(network->weights.xR, 1, state.x.xR, 1, wcount);
            mlpgradnbatch(network, trnxy, trnsize, &state.f, &state.g);
            v = ae_v_dotproduct(network->weights.xR, 1, network->weights.xR, 1, wcount);
            state.f += 0.5 * decay * v;
            ae_v_addd(state.g.xR, 1, network->weights.xR, 1, wcount, decay);
            rep->ngrad++;
         } else if (state.xupdated) { // Validation set
            ae_v_move(network->weights.xR, 1, state.x.xR, 1, wcount);
            e = mlperror(network, valxy, valsize);
            if (e < ebest) {
               ebest = e;
               ae_v_move(wbest.xR, 1, network->weights.xR, 1, wcount);
               itbest = itcnt;
            }
            if (itcnt > 30 && (double)itcnt > 1.5 * itbest) {
               *info = 6;
               break;
            }
            itcnt++;
         }
      }
      minlbfgsresults(&state, &w, &internalrep);
   // Compare with final answer
      if (ebest < efinal) {
         ae_v_move(wfinal.xR, 1, wbest.xR, 1, wcount);
         efinal = ebest;
      }
   }
// The best network
   ae_v_move(network->weights.xR, 1, wfinal.xR, 1, wcount);
   ae_frame_leave();
}

// Subroutine prepares K-fold split of the training set.
//
// NOTES:
//     "NClasses>0" means that we have classification task.
//     "NClasses<0" means regression task with -NClasses real outputs.
static void mlptrain_mlpkfoldsplit(RMatrix *xy, ae_int_t npoints, ae_int_t nclasses, ae_int_t foldscount, bool stratifiedsplits, ZVector *folds) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_frame_make(&_frame_block);
   SetVector(folds);
   NewObj(hqrndstate, rs);
// test parameters
   ae_assert(npoints > 0, "MLPKFoldSplit: wrong NPoints!");
   ae_assert(nclasses > 1 || nclasses < 0, "MLPKFoldSplit: wrong NClasses!");
   ae_assert(foldscount >= 2 && foldscount <= npoints, "MLPKFoldSplit: wrong FoldsCount!");
   ae_assert(!stratifiedsplits, "MLPKFoldSplit: stratified splits are not supported!");
// Folds
   hqrndrandomize(&rs);
   ae_vector_set_length(folds, npoints);
   for (i = 0; i < npoints; i++) {
      folds->xZ[i] = i * foldscount / npoints;
   }
   for (i = 0; i < npoints - 1; i++) {
      j = i + hqrnduniformi(&rs, npoints - i);
      if (j != i) {
         k = folds->xZ[i];
         folds->xZ[i] = folds->xZ[j];
         folds->xZ[j] = k;
      }
   }
   ae_frame_leave();
}

// Internal cross-validation subroutine
static void mlptrain_mlpkfoldcvgeneral(multilayerperceptron *n, RMatrix *xy, ae_int_t npoints, double decay, ae_int_t restarts, ae_int_t foldscount, bool lmalgorithm, double wstep, ae_int_t maxits, ae_int_t *info, mlpreport *rep, mlpcvreport *cvrep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t fold;
   ae_int_t j;
   ae_int_t k;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t rowlen;
   ae_int_t wcount;
   ae_int_t nclasses;
   ae_int_t tssize;
   ae_int_t cvssize;
   ae_int_t relcnt;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(mlpreport, rep);
   SetObj(mlpcvreport, cvrep);
   NewObj(multilayerperceptron, network);
   NewMatrix(cvset, 0, 0, DT_REAL);
   NewMatrix(testset, 0, 0, DT_REAL);
   NewVector(folds, 0, DT_INT);
   NewObj(mlpreport, internalrep);
   NewVector(x, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
// Read network geometry, test parameters
   mlpproperties(n, &nin, &nout, &wcount);
   if (mlpissoftmax(n)) {
      nclasses = nout;
      rowlen = nin + 1;
   } else {
      nclasses = -nout;
      rowlen = nin + nout;
   }
   if (npoints <= 0 || foldscount < 2 || foldscount > npoints) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   mlpcopy(n, &network);
// K-fold out cross-validation.
// First, estimate generalization error
   ae_matrix_set_length(&testset, npoints, rowlen);
   ae_matrix_set_length(&cvset, npoints, rowlen);
   ae_vector_set_length(&x, nin);
   ae_vector_set_length(&y, nout);
   mlptrain_mlpkfoldsplit(xy, npoints, nclasses, foldscount, false, &folds);
   cvrep->relclserror = 0.0;
   cvrep->avgce = 0.0;
   cvrep->rmserror = 0.0;
   cvrep->avgerror = 0.0;
   cvrep->avgrelerror = 0.0;
   rep->ngrad = 0;
   rep->nhess = 0;
   rep->ncholesky = 0;
   relcnt = 0;
   for (fold = 0; fold < foldscount; fold++) {
   // Separate set
      tssize = 0;
      cvssize = 0;
      for (i = 0; i < npoints; i++) {
         if (folds.xZ[i] == fold) {
            ae_v_move(testset.xyR[tssize], 1, xy->xyR[i], 1, rowlen);
            tssize++;
         } else {
            ae_v_move(cvset.xyR[cvssize], 1, xy->xyR[i], 1, rowlen);
            cvssize++;
         }
      }
   // Train on CV training set
      if (lmalgorithm) {
         mlptrainlm(&network, &cvset, cvssize, decay, restarts, info, &internalrep);
      } else {
         mlptrainlbfgs(&network, &cvset, cvssize, decay, restarts, wstep, maxits, info, &internalrep);
      }
      if (*info < 0) {
         cvrep->relclserror = 0.0;
         cvrep->avgce = 0.0;
         cvrep->rmserror = 0.0;
         cvrep->avgerror = 0.0;
         cvrep->avgrelerror = 0.0;
         ae_frame_leave();
         return;
      }
      rep->ngrad += internalrep.ngrad;
      rep->nhess += internalrep.nhess;
      rep->ncholesky += internalrep.ncholesky;
   // Estimate error using CV test set
      if (mlpissoftmax(&network)) {
      // classification-only code
         cvrep->relclserror += mlpclserror(&network, &testset, tssize);
         cvrep->avgce += mlperrorn(&network, &testset, tssize);
      }
      for (i = 0; i < tssize; i++) {
         ae_v_move(x.xR, 1, testset.xyR[i], 1, nin);
         mlpprocess(&network, &x, &y);
         if (mlpissoftmax(&network)) {
         // Classification-specific code
            k = RoundZ(testset.xyR[i][nin]);
            for (j = 0; j < nout; j++) {
               if (j == k) {
                  cvrep->rmserror += ae_sqr(y.xR[j] - 1);
                  cvrep->avgerror += fabs(y.xR[j] - 1);
                  cvrep->avgrelerror += fabs(y.xR[j] - 1);
                  relcnt++;
               } else {
                  cvrep->rmserror += ae_sqr(y.xR[j]);
                  cvrep->avgerror += fabs(y.xR[j]);
               }
            }
         } else {
         // Regression-specific code
            for (j = 0; j < nout; j++) {
               cvrep->rmserror += ae_sqr(y.xR[j] - testset.xyR[i][nin + j]);
               cvrep->avgerror += fabs(y.xR[j] - testset.xyR[i][nin + j]);
               if (testset.xyR[i][nin + j] != 0.0) {
                  cvrep->avgrelerror += fabs((y.xR[j] - testset.xyR[i][nin + j]) / testset.xyR[i][nin + j]);
                  relcnt++;
               }
            }
         }
      }
   }
   if (mlpissoftmax(&network)) {
      cvrep->relclserror /= npoints;
      cvrep->avgce /= log(2.0) * npoints;
   }
   cvrep->rmserror = sqrt(cvrep->rmserror / (npoints * nout));
   cvrep->avgerror /= npoints * nout;
   if (relcnt > 0) {
      cvrep->avgrelerror /= relcnt;
   }
   *info = 1;
   ae_frame_leave();
}

// Cross-validation estimate of generalization error.
//
// Base algorithm - L-BFGS.
//
// Inputs:
//     Network     -   neural network with initialized geometry.   Network is
//                     not changed during cross-validation -  it is used only
//                     as a representative of its architecture.
//     XY          -   training set.
//     SSize       -   training set size
//     Decay       -   weight  decay, same as in MLPTrainLBFGS
//     Restarts    -   number of restarts, > 0.
//                     restarts are counted for each partition separately, so
//                     total number of restarts will be Restarts*FoldsCount.
//     WStep       -   stopping criterion, same as in MLPTrainLBFGS
//     MaxIts      -   stopping criterion, same as in MLPTrainLBFGS
//     FoldsCount  -   number of folds in k-fold cross-validation,
//                     2 <= FoldsCount <= SSize.
//                     recommended value: 10.
//
// Outputs:
//     Info        -   return code, same as in MLPTrainLBFGS
//     Rep         -   report, same as in MLPTrainLM/MLPTrainLBFGS
//     CVRep       -   generalization error estimates
// ALGLIB: Copyright 09.12.2007 by Sergey Bochkanov
// API: void mlpkfoldcvlbfgs(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, const double wstep, const ae_int_t maxits, const ae_int_t foldscount, ae_int_t &info, mlpreport &rep, mlpcvreport &cvrep);
void mlpkfoldcvlbfgs(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints, double decay, ae_int_t restarts, double wstep, ae_int_t maxits, ae_int_t foldscount, ae_int_t *info, mlpreport *rep, mlpcvreport *cvrep) {
   *info = 0;
   SetObj(mlpreport, rep);
   SetObj(mlpcvreport, cvrep);
   mlptrain_mlpkfoldcvgeneral(network, xy, npoints, decay, restarts, foldscount, false, wstep, maxits, info, rep, cvrep);
}

// Cross-validation estimate of generalization error.
//
// Base algorithm - Levenberg-Marquardt.
//
// Inputs:
//     Network     -   neural network with initialized geometry.   Network is
//                     not changed during cross-validation -  it is used only
//                     as a representative of its architecture.
//     XY          -   training set.
//     SSize       -   training set size
//     Decay       -   weight  decay, same as in MLPTrainLBFGS
//     Restarts    -   number of restarts, > 0.
//                     restarts are counted for each partition separately, so
//                     total number of restarts will be Restarts*FoldsCount.
//     FoldsCount  -   number of folds in k-fold cross-validation,
//                     2 <= FoldsCount <= SSize.
//                     recommended value: 10.
//
// Outputs:
//     Info        -   return code, same as in MLPTrainLBFGS
//     Rep         -   report, same as in MLPTrainLM/MLPTrainLBFGS
//     CVRep       -   generalization error estimates
// ALGLIB: Copyright 09.12.2007 by Sergey Bochkanov
// API: void mlpkfoldcvlm(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, const ae_int_t foldscount, ae_int_t &info, mlpreport &rep, mlpcvreport &cvrep);
void mlpkfoldcvlm(multilayerperceptron *network, RMatrix *xy, ae_int_t npoints, double decay, ae_int_t restarts, ae_int_t foldscount, ae_int_t *info, mlpreport *rep, mlpcvreport *cvrep) {
   *info = 0;
   SetObj(mlpreport, rep);
   SetObj(mlpcvreport, cvrep);
   mlptrain_mlpkfoldcvgeneral(network, xy, npoints, decay, restarts, foldscount, true, 0.0, 0, info, rep, cvrep);
}

// This function initializes temporaries needed for training session.
// ALGLIB: Copyright 01.07.2013 by Sergey Bochkanov
static void mlptrain_initmlptrnsession(multilayerperceptron *networktrained, bool randomizenetwork, mlptrainer *trainer, smlptrnsession *session) {
   ae_frame _frame_block;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t pcount;
   ae_frame_make(&_frame_block);
   NewVector(dummysubset, 0, DT_INT);
// Prepare network:
// * copy input network to Session.Network
// * re-initialize preprocessor and weights if RandomizeNetwork=True
   mlpcopy(networktrained, &session->network);
   if (randomizenetwork) {
      ae_assert(trainer->datatype == 0 || trainer->datatype == 1, "InitTemporaries: unexpected Trainer.DataType");
      if (trainer->datatype == 0) {
         mlpinitpreprocessorsubset(&session->network, &trainer->densexy, trainer->npoints, &dummysubset, -1);
      }
      if (trainer->datatype == 1) {
         mlpinitpreprocessorsparsesubset(&session->network, &trainer->sparsexy, trainer->npoints, &dummysubset, -1);
      }
      mlprandomize(&session->network);
      session->randomizenetwork = true;
   } else {
      session->randomizenetwork = false;
   }
// Determine network geometry and initialize optimizer
   mlpproperties(&session->network, &nin, &nout, &wcount);
   minlbfgscreate(wcount, imin2(wcount, trainer->lbfgsfactor), &session->network.weights, &session->optimizer);
   minlbfgssetxrep(&session->optimizer, true);
// Create buffers
   ae_vector_set_length(&session->wbuf0, wcount);
   ae_vector_set_length(&session->wbuf1, wcount);
// Initialize session result
   mlpexporttunableparameters(&session->network, &session->bestparameters, &pcount);
   session->bestrmserror = ae_maxrealnumber;
   ae_frame_leave();
}

// This function initializes temporaries needed for training session.
//
static void mlptrain_initmlptrnsessions(multilayerperceptron *networktrained, bool randomizenetwork, mlptrainer *trainer, ae_shared_pool *sessions) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   NewVector(dummysubset, 0, DT_INT);
   NewObj(smlptrnsession, t);
   RefObj(smlptrnsession, p);
   if (ae_shared_pool_is_initialized(sessions)) {
   // Pool was already initialized.
   // Clear sessions stored in the pool.
      for (ae_shared_pool_first_recycled(sessions, &_p); p != NULL; ae_shared_pool_next_recycled(sessions, &_p)) {
         ae_assert(mlpsamearchitecture(&p->network, networktrained), "InitMLPTrnSessions: internal consistency error");
         p->bestrmserror = ae_maxrealnumber;
      }
   } else {
   // Prepare session and seed pool
      mlptrain_initmlptrnsession(networktrained, randomizenetwork, trainer, &t);
      ae_shared_pool_set_seed(sessions, &t, sizeof(t), smlptrnsession_init, smlptrnsession_copy, smlptrnsession_free);
   }
   ae_frame_leave();
}

// This function performs step-by-step training of the neural  network.  Here
// "step-by-step" means that training  starts  with  MLPStartTrainingX  call,
// and then user subsequently calls MLPContinueTrainingX  to perform one more
// iteration of the training.
//
// After call to this function trainer object remembers network and  is ready
// to  train  it.  However,  no  training  is  performed  until first call to
// MLPContinueTraining() function. Subsequent calls  to MLPContinueTraining()
// will advance traing progress one iteration further.
// ALGLIB: Copyright 13.08.2012 by Sergey Bochkanov
static void mlptrain_mlpstarttrainingx(mlptrainer *s, bool randomstart, ae_int_t algokind, ZVector *subset, ae_int_t subsetsize, smlptrnsession *session) {
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntype;
   ae_int_t ttype;
   ae_int_t i;
// Check parameters
   ae_assert(s->npoints >= 0, "MLPStartTrainingX: internal error - parameter S is not initialized or is spoiled(S.NPoints < 0)");
   ae_assert(algokind == 0 || algokind == -1, "MLPStartTrainingX: unexpected AlgoKind");
   if (s->rcpar) {
      ttype = 0;
   } else {
      ttype = 1;
   }
   if (!mlpissoftmax(&session->network)) {
      ntype = 0;
   } else {
      ntype = 1;
   }
   ae_assert(ntype == ttype, "MLPStartTrainingX: internal error - type of the resulting network is not similar to network type in trainer object");
   mlpproperties(&session->network, &nin, &nout, &wcount);
   ae_assert(s->nin == nin, "MLPStartTrainingX: number of inputs in trainer is not equal to number of inputs in the network.");
   ae_assert(s->nout == nout, "MLPStartTrainingX: number of outputs in trainer is not equal to number of outputs in the network.");
   ae_assert(subset->cnt >= subsetsize, "MLPStartTrainingX: internal error - parameter SubsetSize more than input subset size(Length(Subset)<SubsetSize)");
   for (i = 0; i < subsetsize; i++) {
      ae_assert(subset->xZ[i] >= 0 && subset->xZ[i] < s->npoints, "MLPStartTrainingX: internal error - parameter Subset contains incorrect index(Subset[I] < 0 or Subset[I]>S.NPoints-1)");
   }
// Prepare session
   minlbfgssetcond(&session->optimizer, 0.0, 0.0, s->wstep, s->maxits);
   if (s->npoints > 0 && subsetsize != 0) {
      if (randomstart) {
         mlprandomize(&session->network);
      }
      minlbfgsrestartfrom(&session->optimizer, &session->network.weights);
   } else {
      for (i = 0; i < wcount; i++) {
         session->network.weights.xR[i] = 0.0;
      }
   }
   if (algokind == -1) {
      session->algoused = s->algokind;
      if (s->algokind == 1) {
         session->minibatchsize = s->minibatchsize;
      }
   } else {
      session->algoused = 0;
   }
   hqrndrandomize(&session->generator);
   session->PQ = -1;
}

// This function performs step-by-step training of the neural  network.  Here
// "step-by-step" means  that training starts  with  MLPStartTrainingX  call,
// and then user subsequently calls MLPContinueTrainingX  to perform one more
// iteration of the training.
//
// This  function  performs  one  more  iteration of the training and returns
// either True (training continues) or False (training stopped). In case True
// was returned, Network weights are updated according to the  current  state
// of the optimization progress. In case False was  returned,  no  additional
// updates is performed (previous update of  the  network weights moved us to
// the final point, and no additional updates is needed).
//
// EXAMPLE:
//     >
//     > [initialize network and trainer object]
//     >
//     > MLPStartTraining(Trainer, Network, True)
//     > while MLPContinueTraining(Trainer, Network) do
//     >     [visualize training progress]
//     >
// ALGLIB: Copyright 13.08.2012 by Sergey Bochkanov
static bool mlptrain_mlpcontinuetrainingx(mlptrainer *s, ZVector *subset, ae_int_t subsetsize, ae_int_t *ngradbatch, smlptrnsession *session) {
   AutoS ae_int_t nin;
   AutoS ae_int_t nout;
   AutoS ae_int_t wcount;
   AutoS ae_int_t twcount;
   AutoS ae_int_t ntype;
   AutoS ae_int_t ttype;
   AutoS double decay;
   AutoS double v;
   AutoS ae_int_t i;
   AutoS ae_int_t j;
   AutoS ae_int_t k;
   AutoS ae_int_t trnsetsize;
   AutoS ae_int_t epoch;
   AutoS ae_int_t minibatchcount;
   AutoS ae_int_t minibatchidx;
   AutoS ae_int_t cursize;
   AutoS ae_int_t idx0;
   AutoS ae_int_t idx1;
// Manually threaded two-way signalling.
// Locals are set arbitrarily the first time around and are retained between pauses and subsequent resumes.
// A Spawn occurs when the routine is (re-)started.
// A Pause sends an event signal and waits for a response with data before carrying out the matching Resume.
// An Exit sends an exit signal indicating the end of the process.
   if (session->PQ >= 0) switch (session->PQ) {
      case 0: goto Resume0;
      default: goto Exit;
   }
Spawn:
   nin = 359;
   nout = -58;
   wcount = -919;
   twcount = -909;
   j = -788;
   k = 809;
   trnsetsize = 205;
   epoch = -838;
   minibatchcount = 939;
   minibatchidx = -526;
   cursize = 763;
   idx0 = -541;
   idx1 = -698;
   v = -318;
// Check correctness of inputs
   ae_assert(s->npoints >= 0, "MLPContinueTrainingX: internal error - parameter S is not initialized or is spoiled(S.NPoints < 0).");
   if (s->rcpar) {
      ttype = 0;
   } else {
      ttype = 1;
   }
   if (!mlpissoftmax(&session->network)) {
      ntype = 0;
   } else {
      ntype = 1;
   }
   ae_assert(ntype == ttype, "MLPContinueTrainingX: internal error - type of the resulting network is not similar to network type in trainer object.");
   mlpproperties(&session->network, &nin, &nout, &wcount);
   ae_assert(s->nin == nin, "MLPContinueTrainingX: internal error - number of inputs in trainer is not equal to number of inputs in the network.");
   ae_assert(s->nout == nout, "MLPContinueTrainingX: internal error - number of outputs in trainer is not equal to number of outputs in the network.");
   ae_assert(subset->cnt >= subsetsize, "MLPContinueTrainingX: internal error - parameter SubsetSize more than input subset size(Length(Subset)<SubsetSize).");
   for (i = 0; i < subsetsize; i++) {
      ae_assert(subset->xZ[i] >= 0 && subset->xZ[i] < s->npoints, "MLPContinueTrainingX: internal error - parameter Subset contains incorrect index(Subset[I] < 0 or Subset[I]>S.NPoints-1).");
   }
// Quick exit on empty training set
   if (s->npoints == 0 || subsetsize == 0) {
      goto Exit;
   }
// Minibatch training
   if (session->algoused == 1) {
      ae_assert(false, "MINIBATCH TRAINING IS NOT IMPLEMENTED YET");
   }
// Last option: full batch training
   decay = s->decay;
   while (minlbfgsiteration(&session->optimizer)) {
      if (session->optimizer.xupdated) {
         ae_v_move(session->network.weights.xR, 1, session->optimizer.x.xR, 1, wcount);
         session->PQ = 0; goto Pause; Resume0: ;
      }
      ae_v_move(session->network.weights.xR, 1, session->optimizer.x.xR, 1, wcount);
      if (s->datatype == 0) {
         mlpgradbatchsubset(&session->network, &s->densexy, s->npoints, subset, subsetsize, &session->optimizer.f, &session->optimizer.g);
      }
      if (s->datatype == 1) {
         mlpgradbatchsparsesubset(&session->network, &s->sparsexy, s->npoints, subset, subsetsize, &session->optimizer.f, &session->optimizer.g);
      }
   // Increment number of operations performed on batch gradient
      ++*ngradbatch;
      v = ae_v_dotproduct(session->network.weights.xR, 1, session->network.weights.xR, 1, wcount);
      session->optimizer.f += 0.5 * decay * v;
      ae_v_addd(session->optimizer.g.xR, 1, session->network.weights.xR, 1, wcount, decay);
   }
   minlbfgsresultsbuf(&session->optimizer, &session->network.weights, &session->optimizerrep);
Exit:
   session->PQ = -1;
   return false;
Pause:
   return true;
}

// This function trains neural network passed to this function, using current
// dataset (one which was passed to MLPSetDataset() or MLPSetSparseDataset())
// and current training settings. Training  from  NRestarts  random  starting
// positions is performed, best network is chosen.
//
// This function is inteded to be used internally. It may be used in  several
// settings:
// * training with ValSubsetSize=0, corresponds  to  "normal"  training  with
//   termination  criteria  based on S.MaxIts (steps count) and S.WStep (step
//   size). Training sample is given by TrnSubset/TrnSubsetSize.
// * training with ValSubsetSize>0, corresponds to  early  stopping  training
//   with additional MaxIts/WStep stopping criteria. Training sample is given
//   by TrnSubset/TrnSubsetSize, validation sample  is  given  by  ValSubset/
//   ValSubsetSize.
// ALGLIB: Copyright 13.08.2012 by Sergey Bochkanov
static void mlptrain_mlptrainnetworkx(mlptrainer *s, ae_int_t nrestarts, ae_int_t algokind, ZVector *trnsubset, ae_int_t trnsubsetsize, ZVector *valsubset, ae_int_t valsubsetsize, multilayerperceptron *network, mlpreport *rep, bool isrootcall, ae_shared_pool *sessions) {
   ae_frame _frame_block;
   double eval;
   double ebest;
   ae_int_t ngradbatch;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t pcount;
   ae_int_t itbest;
   ae_int_t itcnt;
   ae_int_t ntype;
   ae_int_t ttype;
   bool rndstart;
   ae_int_t i;
   ae_int_t nr0;
   ae_int_t nr1;
   bool randomizenetwork;
   double bestrmserror;
   ae_frame_make(&_frame_block);
   NewObj(modelerrors, modrep);
   NewObj(mlpreport, rep0);
   NewObj(mlpreport, rep1);
   RefObj(smlptrnsession, psession);
   mlpproperties(network, &nin, &nout, &wcount);
// Process root call
   if (isrootcall) {
   // Try parallelization
   // We expect that minimum number of iterations before convergence is 100.
   // Hence is our approach to evaluation of task complexity.
   // Parallelism was activated if: imax2(nrestarts, 1) * 100.0 * (2 * wcount) * s->npoints >= smpactivationlevel()
   // Check correctness of parameters
      ae_assert(algokind == 0 || algokind == -1, "MLPTrainNetworkX: unexpected AlgoKind");
      ae_assert(s->npoints >= 0, "MLPTrainNetworkX: internal error - parameter S is not initialized or is spoiled(S.NPoints < 0)");
      if (s->rcpar) {
         ttype = 0;
      } else {
         ttype = 1;
      }
      if (!mlpissoftmax(network)) {
         ntype = 0;
      } else {
         ntype = 1;
      }
      ae_assert(ntype == ttype, "MLPTrainNetworkX: internal error - type of the training network is not similar to network type in trainer object");
      ae_assert(s->nin == nin, "MLPTrainNetworkX: internal error - number of inputs in trainer is not equal to number of inputs in the training network.");
      ae_assert(s->nout == nout, "MLPTrainNetworkX: internal error - number of outputs in trainer is not equal to number of outputs in the training network.");
      ae_assert(nrestarts >= 0, "MLPTrainNetworkX: internal error - NRestarts<0.");
      ae_assert(trnsubset->cnt >= trnsubsetsize, "MLPTrainNetworkX: internal error - parameter TrnSubsetSize more than input subset size(Length(TrnSubset)<TrnSubsetSize)");
      for (i = 0; i < trnsubsetsize; i++) {
         ae_assert(trnsubset->xZ[i] >= 0 && trnsubset->xZ[i] < s->npoints, "MLPTrainNetworkX: internal error - parameter TrnSubset contains incorrect index(TrnSubset[I] < 0 or TrnSubset[I]>S.NPoints-1)");
      }
      ae_assert(valsubset->cnt >= valsubsetsize, "MLPTrainNetworkX: internal error - parameter ValSubsetSize more than input subset size(Length(ValSubset)<ValSubsetSize)");
      for (i = 0; i < valsubsetsize; i++) {
         ae_assert(valsubset->xZ[i] >= 0 && valsubset->xZ[i] < s->npoints, "MLPTrainNetworkX: internal error - parameter ValSubset contains incorrect index(ValSubset[I] < 0 or ValSubset[I]>S.NPoints-1)");
      }
   // Train
      randomizenetwork = nrestarts > 0;
      mlptrain_initmlptrnsessions(network, randomizenetwork, s, sessions);
      mlptrain_mlptrainnetworkx(s, nrestarts, algokind, trnsubset, trnsubsetsize, valsubset, valsubsetsize, network, rep, false, sessions);
   // Choose best network
      bestrmserror = ae_maxrealnumber;
      for (ae_shared_pool_first_recycled(sessions, &_psession); psession != NULL; ae_shared_pool_next_recycled(sessions, &_psession)) {
         if (psession->bestrmserror < bestrmserror) {
            mlpimporttunableparameters(network, &psession->bestparameters);
            bestrmserror = psession->bestrmserror;
         }
      }
   // Calculate errors
      if (s->datatype == 0) {
         mlpallerrorssubset(network, &s->densexy, s->npoints, trnsubset, trnsubsetsize, &modrep);
      }
      if (s->datatype == 1) {
         mlpallerrorssparsesubset(network, &s->sparsexy, s->npoints, trnsubset, trnsubsetsize, &modrep);
      }
      rep->relclserror = modrep.relclserror;
      rep->avgce = modrep.avgce;
      rep->rmserror = modrep.rmserror;
      rep->avgerror = modrep.avgerror;
      rep->avgrelerror = modrep.avgrelerror;
   // Done
      ae_frame_leave();
      return;
   }
// Split problem, if we have more than 1 restart
   if (nrestarts >= 2) {
   // Divide problem with NRestarts into two: NR0 and NR1.
      nr0 = nrestarts / 2;
      nr1 = nrestarts - nr0;
      mlptrain_mlptrainnetworkx(s, nr0, algokind, trnsubset, trnsubsetsize, valsubset, valsubsetsize, network, &rep0, false, sessions);
      mlptrain_mlptrainnetworkx(s, nr1, algokind, trnsubset, trnsubsetsize, valsubset, valsubsetsize, network, &rep1, false, sessions);
   // Aggregate results
      rep->ngrad = rep0.ngrad + rep1.ngrad;
      rep->nhess = rep0.nhess + rep1.nhess;
      rep->ncholesky = rep0.ncholesky + rep1.ncholesky;
   // Done :)
      ae_frame_leave();
      return;
   }
// Execution with NRestarts=1 or NRestarts=0:
// * NRestarts=1 means that network is restarted from random position
// * NRestarts=0 means that network is not randomized
   ae_assert(nrestarts == 0 || nrestarts == 1, "MLPTrainNetworkX: internal error");
   rep->ngrad = 0;
   rep->nhess = 0;
   rep->ncholesky = 0;
   ae_shared_pool_retrieve(sessions, &_psession);
   if ((s->datatype == 0 || s->datatype == 1) && s->npoints > 0 && trnsubsetsize != 0) {
   // Train network using combination of early stopping and step-size
   // and step-count based criteria. Network state with best value of
   // validation set error is stored in WBuf0. When validation set is
   // zero, most recent state of network is stored.
      rndstart = nrestarts != 0;
      ngradbatch = 0;
      eval = 0.0;
      ebest = 0.0;
      itbest = 0;
      itcnt = 0;
      mlptrain_mlpstarttrainingx(s, rndstart, algokind, trnsubset, trnsubsetsize, psession);
      if (s->datatype == 0) {
         ebest = mlperrorsubset(&psession->network, &s->densexy, s->npoints, valsubset, valsubsetsize);
      }
      if (s->datatype == 1) {
         ebest = mlperrorsparsesubset(&psession->network, &s->sparsexy, s->npoints, valsubset, valsubsetsize);
      }
      ae_v_move(psession->wbuf0.xR, 1, psession->network.weights.xR, 1, wcount);
      while (mlptrain_mlpcontinuetrainingx(s, trnsubset, trnsubsetsize, &ngradbatch, psession)) {
         if (s->datatype == 0) {
            eval = mlperrorsubset(&psession->network, &s->densexy, s->npoints, valsubset, valsubsetsize);
         }
         if (s->datatype == 1) {
            eval = mlperrorsparsesubset(&psession->network, &s->sparsexy, s->npoints, valsubset, valsubsetsize);
         }
         if (eval <= ebest || valsubsetsize == 0) {
            ae_v_move(psession->wbuf0.xR, 1, psession->network.weights.xR, 1, wcount);
            ebest = eval;
            itbest = itcnt;
         }
         if (itcnt > 30 && (double)itcnt > 1.5 * itbest) {
            break;
         }
         itcnt++;
      }
      ae_v_move(psession->network.weights.xR, 1, psession->wbuf0.xR, 1, wcount);
      rep->ngrad = ngradbatch;
   } else {
      for (i = 0; i < wcount; i++) {
         psession->network.weights.xR[i] = 0.0;
      }
   }
// Evaluate network performance and update PSession.BestParameters/BestRMSError
// (if needed).
   if (s->datatype == 0) {
      mlpallerrorssubset(&psession->network, &s->densexy, s->npoints, trnsubset, trnsubsetsize, &modrep);
   }
   if (s->datatype == 1) {
      mlpallerrorssparsesubset(&psession->network, &s->sparsexy, s->npoints, trnsubset, trnsubsetsize, &modrep);
   }
   if (modrep.rmserror < psession->bestrmserror) {
      mlpexporttunableparameters(&psession->network, &psession->bestparameters, &pcount);
      psession->bestrmserror = modrep.rmserror;
   }
// Move session back to pool
   ae_shared_pool_recycle(sessions, &_psession);
   ae_frame_leave();
}

// Internal subroutine for parallelization function MLPFoldCV.
//
// Inputs:
//     S         -   trainer object;
//     RowSize   -   row size(eitherNIn+NOut or NIn+1);
//     NRestarts -   number of restarts( >= 0);
//     Folds     -   cross-validation set;
//     Fold      -   the number of first cross-validation( >= 0);
//     DFold     -   the number of second cross-validation( >= Fold+1);
//     CVY       -   parameter which stores  the result is returned by network,
//                   training on I-th cross-validation set.
//                   It has to be preallocated.
//     PoolDataCV-   parameter for parallelization.
//     WCount    -   number of weights in network, used to make decisions on
//                   parallelization.
//
// NOTE: There are no checks on the parameters correctness.
// ALGLIB: Copyright 25.09.2012 by Sergey Bochkanov
static void mlptrain_mthreadcv(mlptrainer *s, ae_int_t rowsize, ae_int_t nrestarts, ZVector *folds, ae_int_t fold, ae_int_t dfold, RMatrix *cvy, ae_shared_pool *pooldatacv, ae_int_t wcount) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_frame_make(&_frame_block);
   RefObj(mlpparallelizationcv, datacv);
   if (fold == dfold - 1) {
   // Separate set
      ae_shared_pool_retrieve(pooldatacv, &_datacv);
      datacv->subsetsize = 0;
      for (i = 0; i < s->npoints; i++) {
         if (folds->xZ[i] != fold) {
            datacv->subset.xZ[datacv->subsetsize] = i;
            datacv->subsetsize++;
         }
      }
   // Train on CV training set
      mlptrain_mlptrainnetworkx(s, nrestarts, -1, &datacv->subset, datacv->subsetsize, &datacv->subset, 0, &datacv->network, &datacv->rep, true, &datacv->trnpool);
      datacv->ngrad += datacv->rep.ngrad;
   // Estimate error using CV test set
      for (i = 0; i < s->npoints; i++) {
         if (folds->xZ[i] == fold) {
            if (s->datatype == 0) {
               ae_v_move(datacv->xyrow.xR, 1, s->densexy.xyR[i], 1, rowsize);
            }
            if (s->datatype == 1) {
               sparsegetrow(&s->sparsexy, i, &datacv->xyrow);
            }
            mlpprocess(&datacv->network, &datacv->xyrow, &datacv->y);
            ae_v_move(cvy->xyR[i], 1, datacv->y.xR, 1, s->nout);
         }
      }
      ae_shared_pool_recycle(pooldatacv, &_datacv);
   } else {
      ae_assert(fold < dfold - 1, "MThreadCV: internal error(Fold>DFold-1).");
   // We expect that minimum number of iterations before convergence is 100.
   // Hence is our approach to evaluation of task complexity.
   // Parallelism was activated if: imax2(nrestarts, 1) * 100.0 * (2 * wcount) * s->npoints >= smpactivationlevel()
   // Split task
      mlptrain_mthreadcv(s, rowsize, nrestarts, folds, fold, (fold + dfold) / 2, cvy, pooldatacv, wcount);
      mlptrain_mthreadcv(s, rowsize, nrestarts, folds, (fold + dfold) / 2, dfold, cvy, pooldatacv, wcount);
   }
   ae_frame_leave();
}

// This function estimates generalization error using cross-validation on the
// current dataset with current training settings.
//
// Inputs:
//     S           -   trainer object
//     Network     -   neural network. It must have same number of inputs and
//                     output/classes as was specified during creation of the
//                     trainer object. Network is not changed  during  cross-
//                     validation and is not trained - it  is  used  only  as
//                     representative of its architecture. I.e., we  estimate
//                     generalization properties of  ARCHITECTURE,  not  some
//                     specific network.
//     NRestarts   -   number of restarts, >= 0:
//                     * NRestarts > 0  means  that  for  each cross-validation
//                       round   specified  number   of  random  restarts  is
//                       performed,  with  best  network  being  chosen after
//                       training.
//                     * NRestarts=0 is same as NRestarts=1
//     FoldsCount  -   number of folds in k-fold cross-validation:
//                     * 2 <= FoldsCount <= size of dataset
//                     * recommended value: 10.
//                     * values larger than dataset size will be silently
//                       truncated down to dataset size
//
// Outputs:
//     Rep         -   structure which contains cross-validation estimates:
//                     * Rep.RelCLSError - fraction of misclassified cases.
//                     * Rep.AvgCE - acerage cross-entropy
//                     * Rep.RMSError - root-mean-square error
//                     * Rep.AvgError - average error
//                     * Rep.AvgRelError - average relative error
//
// NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
//       or subset with only one point  was  given,  zeros  are  returned  as
//       estimates.
//
// NOTE: this method performs FoldsCount cross-validation  rounds,  each  one
//       with NRestarts random starts.  Thus,  FoldsCount*NRestarts  networks
//       are trained in total.
//
// NOTE: Rep.RelCLSError/Rep.AvgCE are zero on regression problems.
//
// NOTE: on classification problems Rep.RMSError/Rep.AvgError/Rep.AvgRelError
//       contain errors in prediction of posterior probabilities.
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlpkfoldcv(const mlptrainer &s, const multilayerperceptron &network, const ae_int_t nrestarts, const ae_int_t foldscount, mlpreport &rep);
void mlpkfoldcv(mlptrainer *s, multilayerperceptron *network, ae_int_t nrestarts, ae_int_t foldscount, mlpreport *rep) {
   ae_frame _frame_block;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t rowsize;
   ae_int_t ntype;
   ae_int_t ttype;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_frame_make(&_frame_block);
   SetObj(mlpreport, rep);
   NewObj(ae_shared_pool, pooldatacv);
   NewObj(mlpparallelizationcv, datacv);
   RefObj(mlpparallelizationcv, sdatacv);
   NewMatrix(cvy, 0, 0, DT_REAL);
   NewVector(folds, 0, DT_INT);
   NewVector(buf, 0, DT_REAL);
   NewVector(dy, 0, DT_REAL);
   NewObj(hqrndstate, rs);
   if (!mlpissoftmax(network)) {
      ntype = 0;
   } else {
      ntype = 1;
   }
   if (s->rcpar) {
      ttype = 0;
   } else {
      ttype = 1;
   }
   ae_assert(ntype == ttype, "MLPKFoldCV: type of input network is not similar to network type in trainer object");
   ae_assert(s->npoints >= 0, "MLPKFoldCV: possible trainer S is not initialized(S.NPoints < 0)");
   mlpproperties(network, &nin, &nout, &wcount);
   ae_assert(s->nin == nin, "MLPKFoldCV:  number of inputs in trainer is not equal to number of inputs in network");
   ae_assert(s->nout == nout, "MLPKFoldCV:  number of outputs in trainer is not equal to number of outputs in network");
   ae_assert(nrestarts >= 0, "MLPKFoldCV: NRestarts<0");
   ae_assert(foldscount >= 2, "MLPKFoldCV: FoldsCount<2");
   if (foldscount > s->npoints) {
      foldscount = s->npoints;
   }
   rep->relclserror = 0.0;
   rep->avgce = 0.0;
   rep->rmserror = 0.0;
   rep->avgerror = 0.0;
   rep->avgrelerror = 0.0;
   hqrndrandomize(&rs);
   rep->ngrad = 0;
   rep->nhess = 0;
   rep->ncholesky = 0;
   if (s->npoints == 0 || s->npoints == 1) {
      ae_frame_leave();
      return;
   }
// Read network geometry, test parameters
   if (s->rcpar) {
      rowsize = nin + nout;
      ae_vector_set_length(&dy, nout);
      dserrallocate(-nout, &buf);
   } else {
      rowsize = nin + 1;
      ae_vector_set_length(&dy, 1);
      dserrallocate(nout, &buf);
   }
// Folds
   ae_vector_set_length(&folds, s->npoints);
   for (i = 0; i < s->npoints; i++) {
      folds.xZ[i] = i * foldscount / s->npoints;
   }
   for (i = 0; i < s->npoints - 1; i++) {
      j = i + hqrnduniformi(&rs, s->npoints - i);
      if (j != i) {
         k = folds.xZ[i];
         folds.xZ[i] = folds.xZ[j];
         folds.xZ[j] = k;
      }
   }
   ae_matrix_set_length(&cvy, s->npoints, nout);
// Initialize SEED-value for shared pool
   datacv.ngrad = 0;
   mlpcopy(network, &datacv.network);
   ae_vector_set_length(&datacv.subset, s->npoints);
   ae_vector_set_length(&datacv.xyrow, rowsize);
   ae_vector_set_length(&datacv.y, nout);
// Create shared pool
   ae_shared_pool_set_seed(&pooldatacv, &datacv, sizeof(datacv), mlpparallelizationcv_init, mlpparallelizationcv_copy, mlpparallelizationcv_free);
// Parallelization
   mlptrain_mthreadcv(s, rowsize, nrestarts, &folds, 0, foldscount, &cvy, &pooldatacv, wcount);
// Calculate value for NGrad
   for (ae_shared_pool_first_recycled(&pooldatacv, &_sdatacv); sdatacv != NULL; ae_shared_pool_next_recycled(&pooldatacv, &_sdatacv)) {
      rep->ngrad += sdatacv->ngrad;
   }
// Connect of results and calculate cross-validation error
   for (i = 0; i < s->npoints; i++) {
      if (s->datatype == 0) {
         ae_v_move(datacv.xyrow.xR, 1, s->densexy.xyR[i], 1, rowsize);
      }
      if (s->datatype == 1) {
         sparsegetrow(&s->sparsexy, i, &datacv.xyrow);
      }
      ae_v_move(datacv.y.xR, 1, cvy.xyR[i], 1, nout);
      if (s->rcpar) {
         ae_v_move(dy.xR, 1, &datacv.xyrow.xR[nin], 1, nout);
      } else {
         dy.xR[0] = datacv.xyrow.xR[nin];
      }
      dserraccumulate(&buf, &datacv.y, &dy);
   }
   dserrfinish(&buf);
   rep->relclserror = buf.xR[0];
   rep->avgce = buf.xR[1];
   rep->rmserror = buf.xR[2];
   rep->avgerror = buf.xR[3];
   rep->avgrelerror = buf.xR[4];
   ae_frame_leave();
}

// Creation of the network trainer object for regression networks
//
// Inputs:
//     NIn         -   number of inputs, NIn >= 1
//     NOut        -   number of outputs, NOut >= 1
//
// Outputs:
//     S           -   neural network trainer object.
//                     This structure can be used to train any regression
//                     network with NIn inputs and NOut outputs.
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlpcreatetrainer(const ae_int_t nin, const ae_int_t nout, mlptrainer &s);
void mlpcreatetrainer(ae_int_t nin, ae_int_t nout, mlptrainer *s) {
   SetObj(mlptrainer, s);
   ae_assert(nin >= 1, "MLPCreateTrainer: NIn<1.");
   ae_assert(nout >= 1, "MLPCreateTrainer: NOut<1.");
   s->nin = nin;
   s->nout = nout;
   s->rcpar = true;
   s->lbfgsfactor = mlptrain_defaultlbfgsfactor;
   s->decay = 1.0E-6;
   mlpsetcond(s, 0.0, 0);
   s->datatype = 0;
   s->npoints = 0;
   mlpsetalgobatch(s);
}

// Creation of the network trainer object for classification networks
//
// Inputs:
//     NIn         -   number of inputs, NIn >= 1
//     NClasses    -   number of classes, NClasses >= 2
//
// Outputs:
//     S           -   neural network trainer object.
//                     This structure can be used to train any classification
//                     network with NIn inputs and NOut outputs.
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlpcreatetrainercls(const ae_int_t nin, const ae_int_t nclasses, mlptrainer &s);
void mlpcreatetrainercls(ae_int_t nin, ae_int_t nclasses, mlptrainer *s) {
   SetObj(mlptrainer, s);
   ae_assert(nin >= 1, "MLPCreateTrainerCls: NIn<1.");
   ae_assert(nclasses >= 2, "MLPCreateTrainerCls: NClasses < 2.");
   s->nin = nin;
   s->nout = nclasses;
   s->rcpar = false;
   s->lbfgsfactor = mlptrain_defaultlbfgsfactor;
   s->decay = 1.0E-6;
   mlpsetcond(s, 0.0, 0);
   s->datatype = 0;
   s->npoints = 0;
   mlpsetalgobatch(s);
}

// This function sets "current dataset" of the trainer object to  one  passed
// by user.
//
// Inputs:
//     S           -   trainer object
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format. This function checks  correctness
//                     of  the  dataset  (no  NANs/INFs,  class  numbers  are
//                     correct) and throws exception when  incorrect  dataset
//                     is passed.
//     NPoints     -   points count, >= 0.
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// datasetformat is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlpsetdataset(const mlptrainer &s, const real_2d_array &xy, const ae_int_t npoints);
void mlpsetdataset(mlptrainer *s, RMatrix *xy, ae_int_t npoints) {
   ae_int_t ndim;
   ae_int_t i;
   ae_int_t j;
   ae_assert(s->nin >= 1, "MLPSetDataset: possible parameter S is not initialized or spoiled(S.NIn <= 0).");
   ae_assert(npoints >= 0, "MLPSetDataset: NPoint<0");
   ae_assert(npoints <= xy->rows, "MLPSetDataset: invalid size of matrix XY(NPoint more then rows of matrix XY)");
   s->datatype = 0;
   s->npoints = npoints;
   if (npoints == 0) {
      return;
   }
   if (s->rcpar) {
      ae_assert(s->nout >= 1, "MLPSetDataset: possible parameter S is not initialized or is spoiled(NOut<1 for regression).");
      ndim = s->nin + s->nout;
      ae_assert(ndim <= xy->cols, "MLPSetDataset: invalid size of matrix XY(too few columns in matrix XY).");
      ae_assert(apservisfinitematrix(xy, npoints, ndim), "MLPSetDataset: parameter XY contains Infinite or NaN.");
   } else {
      ae_assert(s->nout >= 2, "MLPSetDataset: possible parameter S is not initialized or is spoiled(NClasses < 2 for classifier).");
      ndim = s->nin + 1;
      ae_assert(ndim <= xy->cols, "MLPSetDataset: invalid size of matrix XY(too few columns in matrix XY).");
      ae_assert(apservisfinitematrix(xy, npoints, ndim), "MLPSetDataset: parameter XY contains Infinite or NaN.");
      for (i = 0; i < npoints; i++) {
         ae_assert(RoundZ(xy->xyR[i][s->nin]) >= 0 && RoundZ(xy->xyR[i][s->nin]) < s->nout, "MLPSetDataset: invalid parameter XY(in classifier used nonexistent class number: either XY[.,NIn] < 0 or XY[.,NIn] >= NClasses).");
      }
   }
   matrixsetlengthatleast(&s->densexy, npoints, ndim);
   for (i = 0; i < npoints; i++) {
      for (j = 0; j < ndim; j++) {
         s->densexy.xyR[i][j] = xy->xyR[i][j];
      }
   }
}

// This function sets "current dataset" of the trainer object to  one  passed
// by user (sparse matrix is used to store dataset).
//
// Inputs:
//     S           -   trainer object
//     XY          -   training  set,  see  below  for  information  on   the
//                     training set format. This function checks  correctness
//                     of  the  dataset  (no  NANs/INFs,  class  numbers  are
//                     correct) and throws exception when  incorrect  dataset
//                     is passed. Any  sparse  storage  format  can be  used:
//                     Hash-table, CRS...
//     NPoints     -   points count, >= 0
//
// DATASET FORMAT:
//
// This  function  uses  two  different  dataset formats - one for regression
// networks, another one for classification networks.
//
// For regression networks with NIn inputs and NOut outputs following dataset
// format is used:
// * dataset is given by NPoints*(NIn+NOut) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, next NOut columns are outputs
//
// For classification networks with NIn inputs and NClasses clases  following
// datasetformat is used:
// * dataset is given by NPoints*(NIn+1) matrix
// * each row corresponds to one example
// * first NIn columns are inputs, last column stores class number (from 0 to
//   NClasses-1).
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlpsetsparsedataset(const mlptrainer &s, const sparsematrix &xy, const ae_int_t npoints);
void mlpsetsparsedataset(mlptrainer *s, sparsematrix *xy, ae_int_t npoints) {
   double v;
   ae_int_t t0;
   ae_int_t t1;
   ae_int_t i;
   ae_int_t j;
// Check correctness of the data
   ae_assert(s->nin > 0, "MLPSetSparseDataset: possible parameter S is not initialized or spoiled(S.NIn <= 0).");
   ae_assert(npoints >= 0, "MLPSetSparseDataset: NPoint<0");
   ae_assert(npoints <= sparsegetnrows(xy), "MLPSetSparseDataset: invalid size of sparse matrix XY(NPoint more then rows of matrix XY)");
   if (npoints > 0) {
      t0 = 0;
      t1 = 0;
      if (s->rcpar) {
         ae_assert(s->nout >= 1, "MLPSetSparseDataset: possible parameter S is not initialized or is spoiled(NOut<1 for regression).");
         ae_assert(s->nin + s->nout <= sparsegetncols(xy), "MLPSetSparseDataset: invalid size of sparse matrix XY(too few columns in sparse matrix XY).");
         while (sparseenumerate(xy, &t0, &t1, &i, &j, &v)) {
            if (i < npoints && j < s->nin + s->nout) {
               ae_assert(isfinite(v), "MLPSetSparseDataset: sparse matrix XY contains Infinite or NaN.");
            }
         }
      } else {
         ae_assert(s->nout >= 2, "MLPSetSparseDataset: possible parameter S is not initialized or is spoiled(NClasses < 2 for classifier).");
         ae_assert(s->nin + 1 <= sparsegetncols(xy), "MLPSetSparseDataset: invalid size of sparse matrix XY(too few columns in sparse matrix XY).");
         while (sparseenumerate(xy, &t0, &t1, &i, &j, &v)) {
            if (i < npoints && j <= s->nin) {
               if (j != s->nin) {
                  ae_assert(isfinite(v), "MLPSetSparseDataset: sparse matrix XY contains Infinite or NaN.");
               } else {
                  ae_assert(isfinite(v) && RoundZ(v) >= 0 && RoundZ(v) < s->nout, "MLPSetSparseDataset: invalid sparse matrix XY(in classifier used nonexistent class number: either XY[.,NIn] < 0 or XY[.,NIn] >= NClasses).");
               }
            }
         }
      }
   }
// Set dataset
   s->datatype = 1;
   s->npoints = npoints;
   sparsecopytocrs(xy, &s->sparsexy);
}

// This function sets weight decay coefficient which is used for training.
//
// Inputs:
//     S           -   trainer object
//     Decay       -   weight  decay  coefficient, >= 0.  Weight  decay  term
//                     'Decay*||Weights||^2' is added to error  function.  If
//                     you don't know what Decay to choose, use 1.0E-3.
//                     Weight decay can be set to zero,  in this case network
//                     is trained without weight decay.
//
// NOTE: by default network uses some small nonzero value for weight decay.
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlpsetdecay(const mlptrainer &s, const double decay);
void mlpsetdecay(mlptrainer *s, double decay) {
   ae_assert(isfinite(decay), "MLPSetDecay: parameter Decay contains Infinite or NaN.");
   ae_assert(decay >= 0.0, "MLPSetDecay: Decay<0.");
   s->decay = decay;
}

// This function sets stopping criteria for the optimizer.
//
// Inputs:
//     S           -   trainer object
//     WStep       -   stopping criterion. Algorithm stops if  step  size  is
//                     less than WStep. Recommended value - 0.01.  Zero  step
//                     size means stopping after MaxIts iterations.
//                     WStep >= 0.
//     MaxIts      -   stopping   criterion.  Algorithm  stops  after  MaxIts
//                     epochs (full passes over entire dataset).  Zero MaxIts
//                     means stopping when step is sufficiently small.
//                     MaxIts >= 0.
//
// NOTE: by default, WStep=0.005 and MaxIts=0 are used. These values are also
//       used when MLPSetCond() is called with WStep=0 and MaxIts=0.
//
// NOTE: these stopping criteria are used for all kinds of neural training  -
//       from "conventional" networks to early stopping ensembles. When  used
//       for "conventional" networks, they are  used  as  the  only  stopping
//       criteria. When combined with early stopping, they used as ADDITIONAL
//       stopping criteria which can terminate early stopping algorithm.
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlpsetcond(const mlptrainer &s, const double wstep, const ae_int_t maxits);
void mlpsetcond(mlptrainer *s, double wstep, ae_int_t maxits) {
   ae_assert(isfinite(wstep), "MLPSetCond: parameter WStep contains Infinite or NaN.");
   ae_assert(wstep >= 0.0, "MLPSetCond: WStep<0.");
   ae_assert(maxits >= 0, "MLPSetCond: MaxIts<0.");
   if (wstep != 0.0 || maxits != 0) {
      s->wstep = wstep;
      s->maxits = maxits;
   } else {
      s->wstep = 0.005;
      s->maxits = 0;
   }
}

// This function sets training algorithm: batch training using L-BFGS will be
// used.
//
// This algorithm:
// * the most robust for small-scale problems, but may be too slow for  large
//   scale ones.
// * perfoms full pass through the dataset before performing step
// * uses conditions specified by MLPSetCond() for stopping
// * is default one used by trainer object
//
// Inputs:
//     S           -   trainer object
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlpsetalgobatch(const mlptrainer &s);
void mlpsetalgobatch(mlptrainer *s) {
   s->algokind = 0;
}

// This function trains neural network passed to this function, using current
// dataset (one which was passed to MLPSetDataset() or MLPSetSparseDataset())
// and current training settings. Training  from  NRestarts  random  starting
// positions is performed, best network is chosen.
//
// Training is performed using current training algorithm.
//
// Inputs:
//     S           -   trainer object
//     Network     -   neural network. It must have same number of inputs and
//                     output/classes as was specified during creation of the
//                     trainer object.
//     NRestarts   -   number of restarts, >= 0:
//                     * NRestarts > 0 means that specified  number  of  random
//                       restarts are performed, best network is chosen after
//                       training
//                     * NRestarts=0 means that current state of the  network
//                       is used for training.
//
// Outputs:
//     Network     -   trained network
//
// NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
//       network  is  filled  by zero  values.  Same  behavior  for functions
//       MLPStartTraining and MLPContinueTraining.
//
// NOTE: this method uses sum-of-squares error function for training.
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlptrainnetwork(const mlptrainer &s, const multilayerperceptron &network, const ae_int_t nrestarts, mlpreport &rep);
void mlptrainnetwork(mlptrainer *s, multilayerperceptron *network, ae_int_t nrestarts, mlpreport *rep) {
   ae_frame _frame_block;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntype;
   ae_int_t ttype;
   ae_frame_make(&_frame_block);
   SetObj(mlpreport, rep);
   NewObj(ae_shared_pool, trnpool);
   ae_assert(s->npoints >= 0, "MLPTrainNetwork: parameter S is not initialized or is spoiled(S.NPoints < 0)");
   if (!mlpissoftmax(network)) {
      ntype = 0;
   } else {
      ntype = 1;
   }
   if (s->rcpar) {
      ttype = 0;
   } else {
      ttype = 1;
   }
   ae_assert(ntype == ttype, "MLPTrainNetwork: type of input network is not similar to network type in trainer object");
   mlpproperties(network, &nin, &nout, &wcount);
   ae_assert(s->nin == nin, "MLPTrainNetwork: number of inputs in trainer is not equal to number of inputs in network");
   ae_assert(s->nout == nout, "MLPTrainNetwork: number of outputs in trainer is not equal to number of outputs in network");
   ae_assert(nrestarts >= 0, "MLPTrainNetwork: NRestarts<0.");
// Train
   mlptrain_mlptrainnetworkx(s, nrestarts, -1, &s->subset, -1, &s->subset, 0, network, rep, true, &trnpool);
   ae_frame_leave();
}

// IMPORTANT: this is an "expert" version of the MLPTrain() function.  We  do
//            not recommend you to use it unless you are pretty sure that you
//            need ability to monitor training progress.
//
// This function performs step-by-step training of the neural  network.  Here
// "step-by-step" means that training  starts  with  MLPStartTraining() call,
// and then user subsequently calls MLPContinueTraining() to perform one more
// iteration of the training.
//
// After call to this function trainer object remembers network and  is ready
// to  train  it.  However,  no  training  is  performed  until first call to
// MLPContinueTraining() function. Subsequent calls  to MLPContinueTraining()
// will advance training progress one iteration further.
//
// EXAMPLE:
//     >
//     > ...initialize network and trainer object....
//     >
//     > MLPStartTraining(Trainer, Network, True)
//     > while MLPContinueTraining(Trainer, Network) do
//     >     ...visualize training progress...
//     >
//
// Inputs:
//     S           -   trainer object
//     Network     -   neural network. It must have same number of inputs and
//                     output/classes as was specified during creation of the
//                     trainer object.
//     RandomStart -   randomize network before training or not:
//                     * True  means  that  network  is  randomized  and  its
//                       initial state (one which was passed to  the  trainer
//                       object) is lost.
//                     * False  means  that  training  is  started  from  the
//                       current state of the network
//
// Outputs:
//     Network     -   neural network which is ready to training (weights are
//                     initialized, preprocessor is initialized using current
//                     training set)
//
// NOTE: this method uses sum-of-squares error function for training.
//
// NOTE: it is expected that trainer object settings are NOT  changed  during
//       step-by-step training, i.e. no  one  changes  stopping  criteria  or
//       training set during training. It is possible and there is no defense
//       against  such  actions,  but  algorithm  behavior  in  such cases is
//       undefined and can be unpredictable.
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: void mlpstarttraining(const mlptrainer &s, const multilayerperceptron &network, const bool randomstart);
void mlpstarttraining(mlptrainer *s, multilayerperceptron *network, bool randomstart) {
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntype;
   ae_int_t ttype;
   ae_assert(s->npoints >= 0, "MLPStartTraining: parameter S is not initialized or is spoiled(S.NPoints < 0)");
   if (!mlpissoftmax(network)) {
      ntype = 0;
   } else {
      ntype = 1;
   }
   if (s->rcpar) {
      ttype = 0;
   } else {
      ttype = 1;
   }
   ae_assert(ntype == ttype, "MLPStartTraining: type of input network is not similar to network type in trainer object");
   mlpproperties(network, &nin, &nout, &wcount);
   ae_assert(s->nin == nin, "MLPStartTraining: number of inputs in trainer is not equal to number of inputs in the network.");
   ae_assert(s->nout == nout, "MLPStartTraining: number of outputs in trainer is not equal to number of outputs in the network.");
// Initialize temporaries
   mlptrain_initmlptrnsession(network, randomstart, s, &s->session);
// Train network
   mlptrain_mlpstarttrainingx(s, randomstart, -1, &s->subset, -1, &s->session);
// Update network
   mlpcopytunableparameters(&s->session.network, network);
}

// IMPORTANT: this is an "expert" version of the MLPTrain() function.  We  do
//            not recommend you to use it unless you are pretty sure that you
//            need ability to monitor training progress.
//
// This function performs step-by-step training of the neural  network.  Here
// "step-by-step" means that training starts  with  MLPStartTraining()  call,
// and then user subsequently calls MLPContinueTraining() to perform one more
// iteration of the training.
//
// This  function  performs  one  more  iteration of the training and returns
// either True (training continues) or False (training stopped). In case True
// was returned, Network weights are updated according to the  current  state
// of the optimization progress. In case False was  returned,  no  additional
// updates is performed (previous update of  the  network weights moved us to
// the final point, and no additional updates is needed).
//
// EXAMPLE:
//     >
//     > [initialize network and trainer object]
//     >
//     > MLPStartTraining(Trainer, Network, True)
//     > while MLPContinueTraining(Trainer, Network) do
//     >     [visualize training progress]
//     >
//
// Inputs:
//     S           -   trainer object
//     Network     -   neural  network  structure,  which  is  used to  store
//                     current state of the training process.
//
// Outputs:
//     Network     -   weights of the neural network  are  rewritten  by  the
//                     current approximation.
//
// NOTE: this method uses sum-of-squares error function for training.
//
// NOTE: it is expected that trainer object settings are NOT  changed  during
//       step-by-step training, i.e. no  one  changes  stopping  criteria  or
//       training set during training. It is possible and there is no defense
//       against  such  actions,  but  algorithm  behavior  in  such cases is
//       undefined and can be unpredictable.
//
// NOTE: It  is  expected that Network is the same one which  was  passed  to
//       MLPStartTraining() function.  However,  THIS  function  checks  only
//       following:
//       * that number of network inputs is consistent with trainer object
//         settings
//       * that number of network outputs/classes is consistent with  trainer
//         object settings
//       * that number of network weights is the same as number of weights in
//         the network passed to MLPStartTraining() function
//       Exception is thrown when these conditions are violated.
//
//       It is also expected that you do not change state of the  network  on
//       your own - the only party who has right to change network during its
//       training is a trainer object. Any attempt to interfere with  trainer
//       may lead to unpredictable results.
// ALGLIB: Copyright 23.07.2012 by Sergey Bochkanov
// API: bool mlpcontinuetraining(const mlptrainer &s, const multilayerperceptron &network);
bool mlpcontinuetraining(mlptrainer *s, multilayerperceptron *network) {
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t ntype;
   ae_int_t ttype;
   bool result;
   ae_assert(s->npoints >= 0, "MLPContinueTraining: parameter S is not initialized or is spoiled(S.NPoints < 0)");
   if (s->rcpar) {
      ttype = 0;
   } else {
      ttype = 1;
   }
   if (!mlpissoftmax(network)) {
      ntype = 0;
   } else {
      ntype = 1;
   }
   ae_assert(ntype == ttype, "MLPContinueTraining: type of input network is not similar to network type in trainer object.");
   mlpproperties(network, &nin, &nout, &wcount);
   ae_assert(s->nin == nin, "MLPContinueTraining: number of inputs in trainer is not equal to number of inputs in the network.");
   ae_assert(s->nout == nout, "MLPContinueTraining: number of outputs in trainer is not equal to number of outputs in the network.");
   result = mlptrain_mlpcontinuetrainingx(s, &s->subset, -1, &s->ngradbatch, &s->session);
   if (result) {
      ae_v_move(network->weights.xR, 1, s->session.network.weights.xR, 1, wcount);
   }
   return result;
}

// Internal bagging subroutine.
// ALGLIB: Copyright 19.02.2009 by Sergey Bochkanov
static void mlptrain_mlpebagginginternal(mlpensemble *ensemble, RMatrix *xy, ae_int_t npoints, double decay, ae_int_t restarts, double wstep, ae_int_t maxits, bool lmalgorithm, ae_int_t *info, mlpreport *rep, mlpcvreport *ooberrors) {
   ae_frame _frame_block;
   ae_int_t ccnt;
   ae_int_t pcnt;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   double v;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(mlpreport, rep);
   SetObj(mlpcvreport, ooberrors);
   NewMatrix(xys, 0, 0, DT_REAL);
   NewVector(s, 0, DT_BOOL);
   NewMatrix(oobbuf, 0, 0, DT_REAL);
   NewVector(oobcntbuf, 0, DT_INT);
   NewVector(x, 0, DT_REAL);
   NewVector(y, 0, DT_REAL);
   NewVector(dy, 0, DT_REAL);
   NewVector(dsbuf, 0, DT_REAL);
   NewObj(mlpreport, tmprep);
   NewObj(hqrndstate, rs);
   nin = mlpgetinputscount(&ensemble->network);
   nout = mlpgetoutputscount(&ensemble->network);
   wcount = mlpgetweightscount(&ensemble->network);
// Test for inputs
   if (!lmalgorithm && wstep == 0.0 && maxits == 0) {
      *info = -8;
      ae_frame_leave();
      return;
   }
   if (npoints <= 0 || restarts < 1 || wstep < 0.0 || maxits < 0) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   if (mlpissoftmax(&ensemble->network)) {
      for (i = 0; i < npoints; i++) {
         if (RoundZ(xy->xyR[i][nin]) < 0 || RoundZ(xy->xyR[i][nin]) >= nout) {
            *info = -2;
            ae_frame_leave();
            return;
         }
      }
   }
// allocate temporaries
   *info = 2;
   rep->ngrad = 0;
   rep->nhess = 0;
   rep->ncholesky = 0;
   ooberrors->relclserror = 0.0;
   ooberrors->avgce = 0.0;
   ooberrors->rmserror = 0.0;
   ooberrors->avgerror = 0.0;
   ooberrors->avgrelerror = 0.0;
   if (mlpissoftmax(&ensemble->network)) {
      ccnt = nin + 1;
      pcnt = nin;
   } else {
      ccnt = nin + nout;
      pcnt = nin + nout;
   }
   ae_matrix_set_length(&xys, npoints, ccnt);
   ae_vector_set_length(&s, npoints);
   ae_matrix_set_length(&oobbuf, npoints, nout);
   ae_vector_set_length(&oobcntbuf, npoints);
   ae_vector_set_length(&x, nin);
   ae_vector_set_length(&y, nout);
   if (mlpissoftmax(&ensemble->network)) {
      ae_vector_set_length(&dy, 1);
   } else {
      ae_vector_set_length(&dy, nout);
   }
   for (i = 0; i < npoints; i++) {
      for (j = 0; j < nout; j++) {
         oobbuf.xyR[i][j] = 0.0;
      }
   }
   for (i = 0; i < npoints; i++) {
      oobcntbuf.xZ[i] = 0;
   }
// main bagging cycle
   hqrndrandomize(&rs);
   for (k = 0; k < ensemble->ensemblesize; k++) {
   // prepare dataset
      for (i = 0; i < npoints; i++) {
         s.xB[i] = false;
      }
      for (i = 0; i < npoints; i++) {
         j = hqrnduniformi(&rs, npoints);
         s.xB[j] = true;
         ae_v_move(xys.xyR[i], 1, xy->xyR[j], 1, ccnt);
      }
   // train
      if (lmalgorithm) {
         mlptrainlm(&ensemble->network, &xys, npoints, decay, restarts, info, &tmprep);
      } else {
         mlptrainlbfgs(&ensemble->network, &xys, npoints, decay, restarts, wstep, maxits, info, &tmprep);
      }
      if (*info < 0) {
         ae_frame_leave();
         return;
      }
   // save results
      rep->ngrad += tmprep.ngrad;
      rep->nhess += tmprep.nhess;
      rep->ncholesky += tmprep.ncholesky;
      ae_v_move(&ensemble->weights.xR[k * wcount], 1, ensemble->network.weights.xR, 1, wcount);
      ae_v_move(&ensemble->columnmeans.xR[k * pcnt], 1, ensemble->network.columnmeans.xR, 1, pcnt);
      ae_v_move(&ensemble->columnsigmas.xR[k * pcnt], 1, ensemble->network.columnsigmas.xR, 1, pcnt);
   // OOB estimates
      for (i = 0; i < npoints; i++) {
         if (!s.xB[i]) {
            ae_v_move(x.xR, 1, xy->xyR[i], 1, nin);
            mlpprocess(&ensemble->network, &x, &y);
            ae_v_add(oobbuf.xyR[i], 1, y.xR, 1, nout);
            oobcntbuf.xZ[i]++;
         }
      }
   }
// OOB estimates
   if (mlpissoftmax(&ensemble->network)) {
      dserrallocate(nout, &dsbuf);
   } else {
      dserrallocate(-nout, &dsbuf);
   }
   for (i = 0; i < npoints; i++) {
      if (oobcntbuf.xZ[i] != 0) {
         v = 1.0 / (double)oobcntbuf.xZ[i];
         ae_v_moved(y.xR, 1, oobbuf.xyR[i], 1, nout, v);
         if (mlpissoftmax(&ensemble->network)) {
            dy.xR[0] = xy->xyR[i][nin];
         } else {
            ae_v_moved(dy.xR, 1, &xy->xyR[i][nin], 1, nout, v);
         }
         dserraccumulate(&dsbuf, &y, &dy);
      }
   }
   dserrfinish(&dsbuf);
   ooberrors->relclserror = dsbuf.xR[0];
   ooberrors->avgce = dsbuf.xR[1];
   ooberrors->rmserror = dsbuf.xR[2];
   ooberrors->avgerror = dsbuf.xR[3];
   ooberrors->avgrelerror = dsbuf.xR[4];
   ae_frame_leave();
}

// Training neural networks ensemble using  bootstrap  aggregating (bagging).
// Modified Levenberg-Marquardt algorithm is used as base training method.
//
// Inputs:
//     Ensemble    -   model with initialized geometry
//     XY          -   training set
//     NPoints     -   training set size
//     Decay       -   weight decay coefficient, >= 0.001
//     Restarts    -   restarts, > 0.
//
// Outputs:
//     Ensemble    -   trained model
//     Info        -   return code:
//                     * -2, if there is a point with class number
//                           outside of [0..NClasses-1].
//                     * -1, if incorrect parameters was passed
//                           (NPoints < 0, Restarts < 1).
//                     *  2, if task has been solved.
//     Rep         -   training report.
//     OOBErrors   -   out-of-bag generalization error estimate
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: void mlpebagginglm(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, ae_int_t &info, mlpreport &rep, mlpcvreport &ooberrors);
void mlpebagginglm(mlpensemble *ensemble, RMatrix *xy, ae_int_t npoints, double decay, ae_int_t restarts, ae_int_t *info, mlpreport *rep, mlpcvreport *ooberrors) {
   *info = 0;
   SetObj(mlpreport, rep);
   SetObj(mlpcvreport, ooberrors);
   mlptrain_mlpebagginginternal(ensemble, xy, npoints, decay, restarts, 0.0, 0, true, info, rep, ooberrors);
}

// Training neural networks ensemble using  bootstrap  aggregating (bagging).
// L-BFGS algorithm is used as base training method.
//
// Inputs:
//     Ensemble    -   model with initialized geometry
//     XY          -   training set
//     NPoints     -   training set size
//     Decay       -   weight decay coefficient, >= 0.001
//     Restarts    -   restarts, > 0.
//     WStep       -   stopping criterion, same as in MLPTrainLBFGS
//     MaxIts      -   stopping criterion, same as in MLPTrainLBFGS
//
// Outputs:
//     Ensemble    -   trained model
//     Info        -   return code:
//                     * -8, if both WStep=0 and MaxIts=0
//                     * -2, if there is a point with class number
//                           outside of [0..NClasses-1].
//                     * -1, if incorrect parameters was passed
//                           (NPoints < 0, Restarts < 1).
//                     *  2, if task has been solved.
//     Rep         -   training report.
//     OOBErrors   -   out-of-bag generalization error estimate
// ALGLIB: Copyright 17.02.2009 by Sergey Bochkanov
// API: void mlpebagginglbfgs(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, const double wstep, const ae_int_t maxits, ae_int_t &info, mlpreport &rep, mlpcvreport &ooberrors);
void mlpebagginglbfgs(mlpensemble *ensemble, RMatrix *xy, ae_int_t npoints, double decay, ae_int_t restarts, double wstep, ae_int_t maxits, ae_int_t *info, mlpreport *rep, mlpcvreport *ooberrors) {
   *info = 0;
   SetObj(mlpreport, rep);
   SetObj(mlpcvreport, ooberrors);
   mlptrain_mlpebagginginternal(ensemble, xy, npoints, decay, restarts, wstep, maxits, false, info, rep, ooberrors);
}

// Training neural networks ensemble using early stopping.
//
// Inputs:
//     Ensemble    -   model with initialized geometry
//     XY          -   training set
//     NPoints     -   training set size
//     Decay       -   weight decay coefficient, >= 0.001
//     Restarts    -   restarts, > 0.
//
// Outputs:
//     Ensemble    -   trained model
//     Info        -   return code:
//                     * -2, if there is a point with class number
//                           outside of [0..NClasses-1].
//                     * -1, if incorrect parameters was passed
//                           (NPoints < 0, Restarts < 1).
//                     *  6, if task has been solved.
//     Rep         -   training report.
//     OOBErrors   -   out-of-bag generalization error estimate
// ALGLIB: Copyright 10.03.2009 by Sergey Bochkanov
// API: void mlpetraines(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, ae_int_t &info, mlpreport &rep);
void mlpetraines(mlpensemble *ensemble, RMatrix *xy, ae_int_t npoints, double decay, ae_int_t restarts, ae_int_t *info, mlpreport *rep) {
   ae_frame _frame_block;
   ae_int_t i;
   ae_int_t k;
   ae_int_t ccount;
   ae_int_t pcount;
   ae_int_t trnsize;
   ae_int_t valsize;
   ae_int_t tmpinfo;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetObj(mlpreport, rep);
   NewMatrix(trnxy, 0, 0, DT_REAL);
   NewMatrix(valxy, 0, 0, DT_REAL);
   NewObj(mlpreport, tmprep);
   NewObj(modelerrors, moderr);
   nin = mlpgetinputscount(&ensemble->network);
   nout = mlpgetoutputscount(&ensemble->network);
   wcount = mlpgetweightscount(&ensemble->network);
   if (npoints < 2 || restarts < 1 || decay < 0.0) {
      *info = -1;
      ae_frame_leave();
      return;
   }
   if (mlpissoftmax(&ensemble->network)) {
      for (i = 0; i < npoints; i++) {
         if (RoundZ(xy->xyR[i][nin]) < 0 || RoundZ(xy->xyR[i][nin]) >= nout) {
            *info = -2;
            ae_frame_leave();
            return;
         }
      }
   }
   *info = 6;
// allocate
   if (mlpissoftmax(&ensemble->network)) {
      ccount = nin + 1;
      pcount = nin;
   } else {
      ccount = nin + nout;
      pcount = nin + nout;
   }
   ae_matrix_set_length(&trnxy, npoints, ccount);
   ae_matrix_set_length(&valxy, npoints, ccount);
   rep->ngrad = 0;
   rep->nhess = 0;
   rep->ncholesky = 0;
// train networks
   for (k = 0; k < ensemble->ensemblesize; k++) {
      const double pv = 2.0/3.0; // Randomly assign samples to training or validation sets with 2:1 odds.
   // Split set
      do {
         trnsize = 0;
         valsize = 0;
         for (i = 0; i < npoints; i++) {
            if (ae_randombool(pv)) {
            // Assign sample to training set
               ae_v_move(trnxy.xyR[trnsize], 1, xy->xyR[i], 1, ccount);
               trnsize++;
            } else {
            // Assign sample to validation set
               ae_v_move(valxy.xyR[valsize], 1, xy->xyR[i], 1, ccount);
               valsize++;
            }
         }
      } while (!(trnsize != 0 && valsize != 0));
   // Train
      mlptraines(&ensemble->network, &trnxy, trnsize, &valxy, valsize, decay, restarts, &tmpinfo, &tmprep);
      if (tmpinfo < 0) {
         *info = tmpinfo;
         ae_frame_leave();
         return;
      }
   // save results
      ae_v_move(&ensemble->weights.xR[k * wcount], 1, ensemble->network.weights.xR, 1, wcount);
      ae_v_move(&ensemble->columnmeans.xR[k * pcount], 1, ensemble->network.columnmeans.xR, 1, pcount);
      ae_v_move(&ensemble->columnsigmas.xR[k * pcount], 1, ensemble->network.columnsigmas.xR, 1, pcount);
      rep->ngrad += tmprep.ngrad;
      rep->nhess += tmprep.nhess;
      rep->ncholesky += tmprep.ncholesky;
   }
   mlpeallerrorsx(ensemble, xy, &ensemble->network.dummysxy, npoints, 0, &ensemble->network.dummyidx, 0, npoints, 0, &ensemble->network.buf, &moderr);
   rep->relclserror = moderr.relclserror;
   rep->avgce = moderr.avgce;
   rep->rmserror = moderr.rmserror;
   rep->avgerror = moderr.avgerror;
   rep->avgrelerror = moderr.avgrelerror;
   ae_frame_leave();
}

// This function initializes temporaries needed for ensemble training.
//
static void mlptrain_initmlpetrnsession(multilayerperceptron *individualnetwork, mlptrainer *trainer, mlpetrnsession *session) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   NewVector(dummysubset, 0, DT_INT);
// Prepare network:
// * copy input network to Session.Network
// * re-initialize preprocessor and weights if RandomizeNetwork=True
   mlpcopy(individualnetwork, &session->network);
   mlptrain_initmlptrnsessions(individualnetwork, true, trainer, &session->mlpsessions);
   vectorsetlengthatleast(&session->trnsubset, trainer->npoints);
   vectorsetlengthatleast(&session->valsubset, trainer->npoints);
   ae_frame_leave();
}

// This function initializes temporaries needed for training session.
//
static void mlptrain_initmlpetrnsessions(multilayerperceptron *individualnetwork, mlptrainer *trainer, ae_shared_pool *sessions) {
   ae_frame _frame_block;
   ae_frame_make(&_frame_block);
   NewObj(mlpetrnsession, t);
   if (!ae_shared_pool_is_initialized(sessions)) {
      mlptrain_initmlpetrnsession(individualnetwork, trainer, &t);
      ae_shared_pool_set_seed(sessions, &t, sizeof(t), mlpetrnsession_init, mlpetrnsession_copy, mlpetrnsession_free);
   }
   ae_frame_leave();
}

// This function trains neural network ensemble passed to this function using
// current dataset and early stopping training algorithm. Each early stopping
// round performs NRestarts  random  restarts  (thus,  EnsembleSize*NRestarts
// training rounds is performed in total).
// ALGLIB: Copyright 22.08.2012 by Sergey Bochkanov
static void mlptrain_mlptrainensemblex(mlptrainer *s, mlpensemble *ensemble, ae_int_t idx0, ae_int_t idx1, ae_int_t nrestarts, ae_int_t trainingmethod, ae_int_t *ngrad, bool isrootcall, ae_shared_pool *esessions) {
   ae_frame _frame_block;
   ae_int_t pcount;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t wcount;
   ae_int_t i;
   ae_int_t j;
   ae_int_t k;
   ae_int_t trnsubsetsize;
   ae_int_t valsubsetsize;
   ae_int_t k0;
   ae_int_t ngrad0;
   ae_int_t ngrad1;
   ae_frame_make(&_frame_block);
   RefObj(mlpetrnsession, psession);
   NewObj(hqrndstate, rs);
   nin = mlpgetinputscount(&ensemble->network);
   nout = mlpgetoutputscount(&ensemble->network);
   wcount = mlpgetweightscount(&ensemble->network);
   if (mlpissoftmax(&ensemble->network)) {
      pcount = nin;
   } else {
      pcount = nin + nout;
   }
   if (nrestarts <= 0) {
      nrestarts = 1;
   }
// Handle degenerate case
   if (s->npoints < 2) {
      for (i = idx0; i < idx1; i++) {
         for (j = 0; j < wcount; j++) {
            ensemble->weights.xR[i * wcount + j] = 0.0;
         }
         for (j = 0; j < pcount; j++) {
            ensemble->columnmeans.xR[i * pcount + j] = 0.0;
            ensemble->columnsigmas.xR[i * pcount + j] = 1.0;
         }
      }
      ae_frame_leave();
      return;
   }
// Process root call
   if (isrootcall) {
   // Try parallelization
   // We expect that minimum number of iterations before convergence is 100.
   // Hence is our approach to evaluation of task complexity.
   // Was activated if: imax2(nrestarts, 1) * (idx1 - idx0) * 100.0 * (2 * wcount) * s->npoints >= smpactivationlevel()
   // Prepare:
   // * prepare MLPETrnSessions
   // * fill ensemble by zeros (helps to detect errors)
      mlptrain_initmlpetrnsessions(&ensemble->network, s, esessions);
      for (i = idx0; i < idx1; i++) {
         for (j = 0; j < wcount; j++) {
            ensemble->weights.xR[i * wcount + j] = 0.0;
         }
         for (j = 0; j < pcount; j++) {
            ensemble->columnmeans.xR[i * pcount + j] = 0.0;
            ensemble->columnsigmas.xR[i * pcount + j] = 0.0;
         }
      }
   // Train in non-root mode and exit
      mlptrain_mlptrainensemblex(s, ensemble, idx0, idx1, nrestarts, trainingmethod, ngrad, false, esessions);
      ae_frame_leave();
      return;
   }
// Split problem
   if (idx1 - idx0 >= 2) {
      k0 = (idx1 - idx0) / 2;
      ngrad0 = 0;
      ngrad1 = 0;
      mlptrain_mlptrainensemblex(s, ensemble, idx0, idx0 + k0, nrestarts, trainingmethod, &ngrad0, false, esessions);
      mlptrain_mlptrainensemblex(s, ensemble, idx0 + k0, idx1, nrestarts, trainingmethod, &ngrad1, false, esessions);
      *ngrad = ngrad0 + ngrad1;
      ae_frame_leave();
      return;
   }
// Retrieve and prepare session
   ae_shared_pool_retrieve(esessions, &_psession);
// Train
   hqrndrandomize(&rs);
   for (k = idx0; k < idx1; k++) {
   // Split set
      trnsubsetsize = 0;
      valsubsetsize = 0;
      if (trainingmethod == 0) {
         do {
            trnsubsetsize = 0;
            valsubsetsize = 0;
            const double pv = 2.0/3.0; // Randomly assign samples to training or validation sets with 2:1 odds.
            for (i = 0; i < s->npoints; i++) {
               if (ae_randombool(pv)) {
               // Assign sample to training set
                  psession->trnsubset.xZ[trnsubsetsize] = i;
                  trnsubsetsize++;
               } else {
               // Assign sample to validation set
                  psession->valsubset.xZ[valsubsetsize] = i;
                  valsubsetsize++;
               }
            }
         } while (!(trnsubsetsize != 0 && valsubsetsize != 0));
      }
      if (trainingmethod == 1) {
         valsubsetsize = 0;
         trnsubsetsize = s->npoints;
         for (i = 0; i < s->npoints; i++) {
            psession->trnsubset.xZ[i] = hqrnduniformi(&rs, s->npoints);
         }
      }
   // Train
      mlptrain_mlptrainnetworkx(s, nrestarts, -1, &psession->trnsubset, trnsubsetsize, &psession->valsubset, valsubsetsize, &psession->network, &psession->mlprep, true, &psession->mlpsessions);
      *ngrad += psession->mlprep.ngrad;
   // Save results
      ae_v_move(&ensemble->weights.xR[k * wcount], 1, psession->network.weights.xR, 1, wcount);
      ae_v_move(&ensemble->columnmeans.xR[k * pcount], 1, psession->network.columnmeans.xR, 1, pcount);
      ae_v_move(&ensemble->columnsigmas.xR[k * pcount], 1, psession->network.columnsigmas.xR, 1, pcount);
   }
// Recycle session
   ae_shared_pool_recycle(esessions, &_psession);
   ae_frame_leave();
}

// This function trains neural network ensemble passed to this function using
// current dataset and early stopping training algorithm. Each early stopping
// round performs NRestarts  random  restarts  (thus,  EnsembleSize*NRestarts
// training rounds is performed in total).
//
// Inputs:
//     S           -   trainer object;
//     Ensemble    -   neural network ensemble. It must have same  number  of
//                     inputs and outputs/classes  as  was  specified  during
//                     creation of the trainer object.
//     NRestarts   -   number of restarts, >= 0:
//                     * NRestarts > 0 means that specified  number  of  random
//                       restarts are performed during each ES round;
//                     * NRestarts=0 is silently replaced by 1.
//
// Outputs:
//     Ensemble    -   trained ensemble;
//     Rep         -   it contains all type of errors.
//
// NOTE: this training method uses BOTH early stopping and weight decay!  So,
//       you should select weight decay before starting training just as  you
//       select it before training "conventional" networks.
//
// NOTE: when no dataset was specified with MLPSetDataset/SetSparseDataset(),
//       or  single-point  dataset  was  passed,  ensemble  is filled by zero
//       values.
//
// NOTE: this method uses sum-of-squares error function for training.
// ALGLIB: Copyright 22.08.2012 by Sergey Bochkanov
// API: void mlptrainensemblees(const mlptrainer &s, const mlpensemble &ensemble, const ae_int_t nrestarts, mlpreport &rep);
void mlptrainensemblees(mlptrainer *s, mlpensemble *ensemble, ae_int_t nrestarts, mlpreport *rep) {
   ae_frame _frame_block;
   ae_int_t nin;
   ae_int_t nout;
   ae_int_t ntype;
   ae_int_t ttype;
   ae_int_t sgrad;
   ae_frame_make(&_frame_block);
   SetObj(mlpreport, rep);
   NewObj(ae_shared_pool, esessions);
   NewObj(modelerrors, tmprep);
   ae_assert(s->npoints >= 0, "MLPTrainEnsembleES: parameter S is not initialized or is spoiled(S.NPoints < 0)");
   if (!mlpeissoftmax(ensemble)) {
      ntype = 0;
   } else {
      ntype = 1;
   }
   if (s->rcpar) {
      ttype = 0;
   } else {
      ttype = 1;
   }
   ae_assert(ntype == ttype, "MLPTrainEnsembleES: internal error - type of input network is not similar to network type in trainer object");
   nin = mlpgetinputscount(&ensemble->network);
   ae_assert(s->nin == nin, "MLPTrainEnsembleES: number of inputs in trainer is not equal to number of inputs in ensemble network");
   nout = mlpgetoutputscount(&ensemble->network);
   ae_assert(s->nout == nout, "MLPTrainEnsembleES: number of outputs in trainer is not equal to number of outputs in ensemble network");
   ae_assert(nrestarts >= 0, "MLPTrainEnsembleES: NRestarts<0.");
// Initialize parameter Rep
   rep->relclserror = 0.0;
   rep->avgce = 0.0;
   rep->rmserror = 0.0;
   rep->avgerror = 0.0;
   rep->avgrelerror = 0.0;
   rep->ngrad = 0;
   rep->nhess = 0;
   rep->ncholesky = 0;
// Allocate
   vectorsetlengthatleast(&s->subset, s->npoints);
   vectorsetlengthatleast(&s->valsubset, s->npoints);
// Start training
//
// NOTE: ESessions is not initialized because MLPTrainEnsembleX
//       needs uninitialized pool.
   sgrad = 0;
   mlptrain_mlptrainensemblex(s, ensemble, 0, ensemble->ensemblesize, nrestarts, 0, &sgrad, true, &esessions);
   rep->ngrad = sgrad;
// Calculate errors.
   if (s->datatype == 0) {
      mlpeallerrorsx(ensemble, &s->densexy, &s->sparsexy, s->npoints, 0, &ensemble->network.dummyidx, 0, s->npoints, 0, &ensemble->network.buf, &tmprep);
   }
   if (s->datatype == 1) {
      mlpeallerrorsx(ensemble, &s->densexy, &s->sparsexy, s->npoints, 1, &ensemble->network.dummyidx, 0, s->npoints, 0, &ensemble->network.buf, &tmprep);
   }
   rep->relclserror = tmprep.relclserror;
   rep->avgce = tmprep.avgce;
   rep->rmserror = tmprep.rmserror;
   rep->avgerror = tmprep.avgerror;
   rep->avgrelerror = tmprep.avgrelerror;
   ae_frame_leave();
}

void mlpreport_init(void *_p, bool make_automatic) {
}

void mlpreport_copy(void *_dst, void *_src, bool make_automatic) {
   mlpreport *dst = (mlpreport *)_dst;
   mlpreport *src = (mlpreport *)_src;
   dst->relclserror = src->relclserror;
   dst->avgce = src->avgce;
   dst->rmserror = src->rmserror;
   dst->avgerror = src->avgerror;
   dst->avgrelerror = src->avgrelerror;
   dst->ngrad = src->ngrad;
   dst->nhess = src->nhess;
   dst->ncholesky = src->ncholesky;
}

void mlpreport_free(void *_p, bool make_automatic) {
}

void mlpcvreport_init(void *_p, bool make_automatic) {
}

void mlpcvreport_copy(void *_dst, void *_src, bool make_automatic) {
   mlpcvreport *dst = (mlpcvreport *)_dst;
   mlpcvreport *src = (mlpcvreport *)_src;
   dst->relclserror = src->relclserror;
   dst->avgce = src->avgce;
   dst->rmserror = src->rmserror;
   dst->avgerror = src->avgerror;
   dst->avgrelerror = src->avgrelerror;
}

void mlpcvreport_free(void *_p, bool make_automatic) {
}

void smlptrnsession_init(void *_p, bool make_automatic) {
   smlptrnsession *p = (smlptrnsession *)_p;
   ae_vector_init(&p->bestparameters, 0, DT_REAL, make_automatic);
   multilayerperceptron_init(&p->network, make_automatic);
   minlbfgsstate_init(&p->optimizer, make_automatic);
   minlbfgsreport_init(&p->optimizerrep, make_automatic);
   ae_vector_init(&p->wbuf0, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->wbuf1, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->allminibatches, 0, DT_INT, make_automatic);
   ae_vector_init(&p->currentminibatch, 0, DT_INT, make_automatic);
   hqrndstate_init(&p->generator, make_automatic);
}

void smlptrnsession_copy(void *_dst, void *_src, bool make_automatic) {
   smlptrnsession *dst = (smlptrnsession *)_dst;
   smlptrnsession *src = (smlptrnsession *)_src;
   ae_vector_copy(&dst->bestparameters, &src->bestparameters, make_automatic);
   dst->bestrmserror = src->bestrmserror;
   dst->randomizenetwork = src->randomizenetwork;
   multilayerperceptron_copy(&dst->network, &src->network, make_automatic);
   minlbfgsstate_copy(&dst->optimizer, &src->optimizer, make_automatic);
   minlbfgsreport_copy(&dst->optimizerrep, &src->optimizerrep, make_automatic);
   ae_vector_copy(&dst->wbuf0, &src->wbuf0, make_automatic);
   ae_vector_copy(&dst->wbuf1, &src->wbuf1, make_automatic);
   ae_vector_copy(&dst->allminibatches, &src->allminibatches, make_automatic);
   ae_vector_copy(&dst->currentminibatch, &src->currentminibatch, make_automatic);
   dst->PQ = src->PQ;
   dst->algoused = src->algoused;
   dst->minibatchsize = src->minibatchsize;
   hqrndstate_copy(&dst->generator, &src->generator, make_automatic);
}

void smlptrnsession_free(void *_p, bool make_automatic) {
   smlptrnsession *p = (smlptrnsession *)_p;
   ae_vector_free(&p->bestparameters, make_automatic);
   multilayerperceptron_free(&p->network, make_automatic);
   minlbfgsstate_free(&p->optimizer, make_automatic);
   minlbfgsreport_free(&p->optimizerrep, make_automatic);
   ae_vector_free(&p->wbuf0, make_automatic);
   ae_vector_free(&p->wbuf1, make_automatic);
   ae_vector_free(&p->allminibatches, make_automatic);
   ae_vector_free(&p->currentminibatch, make_automatic);
   hqrndstate_free(&p->generator, make_automatic);
}

void mlpetrnsession_init(void *_p, bool make_automatic) {
   mlpetrnsession *p = (mlpetrnsession *)_p;
   ae_vector_init(&p->trnsubset, 0, DT_INT, make_automatic);
   ae_vector_init(&p->valsubset, 0, DT_INT, make_automatic);
   ae_shared_pool_init(&p->mlpsessions, make_automatic);
   mlpreport_init(&p->mlprep, make_automatic);
   multilayerperceptron_init(&p->network, make_automatic);
}

void mlpetrnsession_copy(void *_dst, void *_src, bool make_automatic) {
   mlpetrnsession *dst = (mlpetrnsession *)_dst;
   mlpetrnsession *src = (mlpetrnsession *)_src;
   ae_vector_copy(&dst->trnsubset, &src->trnsubset, make_automatic);
   ae_vector_copy(&dst->valsubset, &src->valsubset, make_automatic);
   ae_shared_pool_copy(&dst->mlpsessions, &src->mlpsessions, make_automatic);
   mlpreport_copy(&dst->mlprep, &src->mlprep, make_automatic);
   multilayerperceptron_copy(&dst->network, &src->network, make_automatic);
}

void mlpetrnsession_free(void *_p, bool make_automatic) {
   mlpetrnsession *p = (mlpetrnsession *)_p;
   ae_vector_free(&p->trnsubset, make_automatic);
   ae_vector_free(&p->valsubset, make_automatic);
   ae_shared_pool_free(&p->mlpsessions, make_automatic);
   mlpreport_free(&p->mlprep, make_automatic);
   multilayerperceptron_free(&p->network, make_automatic);
}

void mlptrainer_init(void *_p, bool make_automatic) {
   mlptrainer *p = (mlptrainer *)_p;
   ae_matrix_init(&p->densexy, 0, 0, DT_REAL, make_automatic);
   sparsematrix_init(&p->sparsexy, make_automatic);
   smlptrnsession_init(&p->session, make_automatic);
   ae_vector_init(&p->subset, 0, DT_INT, make_automatic);
   ae_vector_init(&p->valsubset, 0, DT_INT, make_automatic);
}

void mlptrainer_copy(void *_dst, void *_src, bool make_automatic) {
   mlptrainer *dst = (mlptrainer *)_dst;
   mlptrainer *src = (mlptrainer *)_src;
   dst->nin = src->nin;
   dst->nout = src->nout;
   dst->rcpar = src->rcpar;
   dst->lbfgsfactor = src->lbfgsfactor;
   dst->decay = src->decay;
   dst->wstep = src->wstep;
   dst->maxits = src->maxits;
   dst->datatype = src->datatype;
   dst->npoints = src->npoints;
   ae_matrix_copy(&dst->densexy, &src->densexy, make_automatic);
   sparsematrix_copy(&dst->sparsexy, &src->sparsexy, make_automatic);
   smlptrnsession_copy(&dst->session, &src->session, make_automatic);
   dst->ngradbatch = src->ngradbatch;
   ae_vector_copy(&dst->subset, &src->subset, make_automatic);
   dst->subsetsize = src->subsetsize;
   ae_vector_copy(&dst->valsubset, &src->valsubset, make_automatic);
   dst->valsubsetsize = src->valsubsetsize;
   dst->algokind = src->algokind;
   dst->minibatchsize = src->minibatchsize;
}

void mlptrainer_free(void *_p, bool make_automatic) {
   mlptrainer *p = (mlptrainer *)_p;
   ae_matrix_free(&p->densexy, make_automatic);
   sparsematrix_free(&p->sparsexy, make_automatic);
   smlptrnsession_free(&p->session, make_automatic);
   ae_vector_free(&p->subset, make_automatic);
   ae_vector_free(&p->valsubset, make_automatic);
}

void mlpparallelizationcv_init(void *_p, bool make_automatic) {
   mlpparallelizationcv *p = (mlpparallelizationcv *)_p;
   multilayerperceptron_init(&p->network, make_automatic);
   mlpreport_init(&p->rep, make_automatic);
   ae_vector_init(&p->subset, 0, DT_INT, make_automatic);
   ae_vector_init(&p->xyrow, 0, DT_REAL, make_automatic);
   ae_vector_init(&p->y, 0, DT_REAL, make_automatic);
   ae_shared_pool_init(&p->trnpool, make_automatic);
}

void mlpparallelizationcv_copy(void *_dst, void *_src, bool make_automatic) {
   mlpparallelizationcv *dst = (mlpparallelizationcv *)_dst;
   mlpparallelizationcv *src = (mlpparallelizationcv *)_src;
   multilayerperceptron_copy(&dst->network, &src->network, make_automatic);
   mlpreport_copy(&dst->rep, &src->rep, make_automatic);
   ae_vector_copy(&dst->subset, &src->subset, make_automatic);
   dst->subsetsize = src->subsetsize;
   ae_vector_copy(&dst->xyrow, &src->xyrow, make_automatic);
   ae_vector_copy(&dst->y, &src->y, make_automatic);
   dst->ngrad = src->ngrad;
   ae_shared_pool_copy(&dst->trnpool, &src->trnpool, make_automatic);
}

void mlpparallelizationcv_free(void *_p, bool make_automatic) {
   mlpparallelizationcv *p = (mlpparallelizationcv *)_p;
   multilayerperceptron_free(&p->network, make_automatic);
   mlpreport_free(&p->rep, make_automatic);
   ae_vector_free(&p->subset, make_automatic);
   ae_vector_free(&p->xyrow, make_automatic);
   ae_vector_free(&p->y, make_automatic);
   ae_shared_pool_free(&p->trnpool, make_automatic);
}
} // end of namespace alglib_impl

namespace alglib {
// Training report:
//     * RelCLSError   -   fraction of misclassified cases.
//     * AvgCE         -   acerage cross-entropy
//     * RMSError      -   root-mean-square error
//     * AvgError      -   average error
//     * AvgRelError   -   average relative error
//     * NGrad         -   number of gradient calculations
//     * NHess         -   number of Hessian calculations
//     * NCholesky     -   number of Cholesky decompositions
//
// NOTE 1: RelCLSError/AvgCE are zero on regression problems.
//
// NOTE 2: on classification problems  RMSError/AvgError/AvgRelError  contain
//         errors in prediction of posterior probabilities
DefClass(mlpreport, AndD DecVal(relclserror) AndD DecVal(avgce) AndD DecVal(rmserror) AndD DecVal(avgerror) AndD DecVal(avgrelerror) AndD DecVal(ngrad) AndD DecVal(nhess) AndD DecVal(ncholesky))

// Cross-validation estimates of generalization error
DefClass(mlpcvreport, AndD DecVal(relclserror) AndD DecVal(avgce) AndD DecVal(rmserror) AndD DecVal(avgerror) AndD DecVal(avgrelerror))

// Trainer object for neural network.
// You should not try to access fields of this object directly -  use  ALGLIB
// functions to work with this object.
DefClass(mlptrainer, EndD)

void mlptrainlm(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, ae_int_t &info, mlpreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlptrainlm(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints, decay, restarts, &info, ConstT(mlpreport, rep));
   alglib_impl::ae_state_clear();
}

void mlptrainlbfgs(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, const double wstep, const ae_int_t maxits, ae_int_t &info, mlpreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlptrainlbfgs(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints, decay, restarts, wstep, maxits, &info, ConstT(mlpreport, rep));
   alglib_impl::ae_state_clear();
}

void mlptraines(const multilayerperceptron &network, const real_2d_array &trnxy, const ae_int_t trnsize, const real_2d_array &valxy, const ae_int_t valsize, const double decay, const ae_int_t restarts, ae_int_t &info, mlpreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlptraines(ConstT(multilayerperceptron, network), ConstT(ae_matrix, trnxy), trnsize, ConstT(ae_matrix, valxy), valsize, decay, restarts, &info, ConstT(mlpreport, rep));
   alglib_impl::ae_state_clear();
}

void mlpkfoldcvlbfgs(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, const double wstep, const ae_int_t maxits, const ae_int_t foldscount, ae_int_t &info, mlpreport &rep, mlpcvreport &cvrep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpkfoldcvlbfgs(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints, decay, restarts, wstep, maxits, foldscount, &info, ConstT(mlpreport, rep), ConstT(mlpcvreport, cvrep));
   alglib_impl::ae_state_clear();
}

void mlpkfoldcvlm(const multilayerperceptron &network, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, const ae_int_t foldscount, ae_int_t &info, mlpreport &rep, mlpcvreport &cvrep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpkfoldcvlm(ConstT(multilayerperceptron, network), ConstT(ae_matrix, xy), npoints, decay, restarts, foldscount, &info, ConstT(mlpreport, rep), ConstT(mlpcvreport, cvrep));
   alglib_impl::ae_state_clear();
}

void mlpkfoldcv(const mlptrainer &s, const multilayerperceptron &network, const ae_int_t nrestarts, const ae_int_t foldscount, mlpreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpkfoldcv(ConstT(mlptrainer, s), ConstT(multilayerperceptron, network), nrestarts, foldscount, ConstT(mlpreport, rep));
   alglib_impl::ae_state_clear();
}

void mlpcreatetrainer(const ae_int_t nin, const ae_int_t nout, mlptrainer &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreatetrainer(nin, nout, ConstT(mlptrainer, s));
   alglib_impl::ae_state_clear();
}

void mlpcreatetrainercls(const ae_int_t nin, const ae_int_t nclasses, mlptrainer &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpcreatetrainercls(nin, nclasses, ConstT(mlptrainer, s));
   alglib_impl::ae_state_clear();
}

void mlpsetdataset(const mlptrainer &s, const real_2d_array &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpsetdataset(ConstT(mlptrainer, s), ConstT(ae_matrix, xy), npoints);
   alglib_impl::ae_state_clear();
}

void mlpsetsparsedataset(const mlptrainer &s, const sparsematrix &xy, const ae_int_t npoints) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpsetsparsedataset(ConstT(mlptrainer, s), ConstT(sparsematrix, xy), npoints);
   alglib_impl::ae_state_clear();
}

void mlpsetdecay(const mlptrainer &s, const double decay) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpsetdecay(ConstT(mlptrainer, s), decay);
   alglib_impl::ae_state_clear();
}

void mlpsetcond(const mlptrainer &s, const double wstep, const ae_int_t maxits) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpsetcond(ConstT(mlptrainer, s), wstep, maxits);
   alglib_impl::ae_state_clear();
}

void mlpsetalgobatch(const mlptrainer &s) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpsetalgobatch(ConstT(mlptrainer, s));
   alglib_impl::ae_state_clear();
}

void mlptrainnetwork(const mlptrainer &s, const multilayerperceptron &network, const ae_int_t nrestarts, mlpreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlptrainnetwork(ConstT(mlptrainer, s), ConstT(multilayerperceptron, network), nrestarts, ConstT(mlpreport, rep));
   alglib_impl::ae_state_clear();
}

void mlpstarttraining(const mlptrainer &s, const multilayerperceptron &network, const bool randomstart) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpstarttraining(ConstT(mlptrainer, s), ConstT(multilayerperceptron, network), randomstart);
   alglib_impl::ae_state_clear();
}

bool mlpcontinuetraining(const mlptrainer &s, const multilayerperceptron &network) {
   alglib_impl::ae_state_init();
   TryCatch(false)
   bool Ok = alglib_impl::mlpcontinuetraining(ConstT(mlptrainer, s), ConstT(multilayerperceptron, network));
   alglib_impl::ae_state_clear();
   return Ok;
}

void mlpebagginglm(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, ae_int_t &info, mlpreport &rep, mlpcvreport &ooberrors) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpebagginglm(ConstT(mlpensemble, ensemble), ConstT(ae_matrix, xy), npoints, decay, restarts, &info, ConstT(mlpreport, rep), ConstT(mlpcvreport, ooberrors));
   alglib_impl::ae_state_clear();
}

void mlpebagginglbfgs(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, const double wstep, const ae_int_t maxits, ae_int_t &info, mlpreport &rep, mlpcvreport &ooberrors) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpebagginglbfgs(ConstT(mlpensemble, ensemble), ConstT(ae_matrix, xy), npoints, decay, restarts, wstep, maxits, &info, ConstT(mlpreport, rep), ConstT(mlpcvreport, ooberrors));
   alglib_impl::ae_state_clear();
}

void mlpetraines(const mlpensemble &ensemble, const real_2d_array &xy, const ae_int_t npoints, const double decay, const ae_int_t restarts, ae_int_t &info, mlpreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlpetraines(ConstT(mlpensemble, ensemble), ConstT(ae_matrix, xy), npoints, decay, restarts, &info, ConstT(mlpreport, rep));
   alglib_impl::ae_state_clear();
}

void mlptrainensemblees(const mlptrainer &s, const mlpensemble &ensemble, const ae_int_t nrestarts, mlpreport &rep) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::mlptrainensemblees(ConstT(mlptrainer, s), ConstT(mlpensemble, ensemble), nrestarts, ConstT(mlpreport, rep));
   alglib_impl::ae_state_clear();
}
} // end of namespace alglib

// === DATACOMP Package ===
// Depends on: CLUSTERING
namespace alglib_impl {
// k-means++ clusterization.
// Backward compatibility function, we recommend to use CLUSTERING subpackage
// as better replacement.
// ALGLIB: Copyright 21.03.2009 by Sergey Bochkanov
// API: void kmeansgenerate(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t k, const ae_int_t restarts, ae_int_t &info, real_2d_array &c, integer_1d_array &xyc);
void kmeansgenerate(RMatrix *xy, ae_int_t npoints, ae_int_t nvars, ae_int_t k, ae_int_t restarts, ae_int_t *info, RMatrix *c, ZVector *xyc) {
   ae_frame _frame_block;
   ae_int_t itscnt;
   double e;
   ae_frame_make(&_frame_block);
   *info = 0;
   SetMatrix(c);
   SetVector(xyc);
   NewMatrix(dummy, 0, 0, DT_REAL);
   NewObj(kmeansbuffers, buf);
   kmeansinitbuf(&buf);
   kmeansgenerateinternal(xy, npoints, nvars, k, 0, 1, 0, restarts, false, info, &itscnt, c, true, &dummy, false, xyc, &e, &buf);
   ae_frame_leave();
}
} // end of namespace alglib_impl

namespace alglib {
void kmeansgenerate(const real_2d_array &xy, const ae_int_t npoints, const ae_int_t nvars, const ae_int_t k, const ae_int_t restarts, ae_int_t &info, real_2d_array &c, integer_1d_array &xyc) {
   alglib_impl::ae_state_init();
   TryCatch()
   alglib_impl::kmeansgenerate(ConstT(ae_matrix, xy), npoints, nvars, k, restarts, &info, ConstT(ae_matrix, c), ConstT(ae_vector, xyc));
   alglib_impl::ae_state_clear();
}
} // end of namespace alglib
